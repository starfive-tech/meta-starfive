From eb7ca5cf34ace8751fedd8796492d7073490f0a7 Mon Sep 17 00:00:00 2001
From: Kito Cheng <kito.cheng@sifive.com>
Date: Wed, 18 Sep 2019 20:03:16 -0700
Subject: [PATCH 40/60] RISC-V: Support vector intrinsic functions

This patch is merge lots of vector patch and only used for SiFive
release only, we'll prepare another patch set for upstream.

Last commit in this patch.
commit 8154c576b4e46971ed9ada225a7976b4f6fd0789
Refs: [riscv-gcc-10.1-rvv-dev-kito-dev], [riscv-gcc-10.1-rvv-dev], {origin/riscv-gcc-10.1-rvv-dev-kito-dev}
Author:     Kito Cheng <kito.cheng@sifive.com>
AuthorDate: Tue Dec 15 15:35:29 2020 +0800
Commit:     Kito Cheng <kito.cheng@sifive.com>
CommitDate: Tue Dec 15 15:37:25 2020 +0800

    Fix riscv_promote_function_mode

Co-authored-by: Monk Chiang <monk.chiang@sifive.com>
Co-authored-by: Jim Wilson  <jimw@sifive.com>
---
 gcc/common/config/riscv/riscv-common.c        |    10 +
 gcc/config.gcc                                |     1 +
 gcc/config/riscv/arch-canonicalize            |     1 +
 gcc/config/riscv/constraints.md               |    81 +
 gcc/config/riscv/gen-vector-iterator          |  1638 +++
 gcc/config/riscv/predicates.md                |   100 +-
 gcc/config/riscv/riscv-builtins.c             |  2852 +++-
 gcc/config/riscv/riscv-c.c                    |     3 +
 gcc/config/riscv/riscv-ftypes.def             |    42 +-
 gcc/config/riscv/riscv-modes.def              |   103 +
 gcc/config/riscv/riscv-opts.h                 |    11 +
 gcc/config/riscv/riscv-protos.h               |    10 +-
 gcc/config/riscv/riscv-sr.c                   |     2 +-
 gcc/config/riscv/riscv-vector-ftypes.def      |  7444 +++++++++++
 gcc/config/riscv/riscv-vector-iterator.h      |  1571 +++
 gcc/config/riscv/riscv.c                      |  1101 +-
 gcc/config/riscv/riscv.h                      |   104 +-
 gcc/config/riscv/riscv.md                     |   145 +-
 gcc/config/riscv/riscv.opt                    |    28 +
 gcc/config/riscv/riscv_vector.h               |  4572 +++++++
 gcc/config/riscv/riscv_vector_itr.h           |  1537 +++
 gcc/config/riscv/t-riscv                      |     5 +-
 gcc/config/riscv/vector-iterator.md           |   700 +
 gcc/config/riscv/vector.md                    | 10769 ++++++++++++++++
 gcc/expr.c                                    |     5 +-
 gcc/testsuite/g++.target/riscv/rvv_merge-1.C  |    45 +
 gcc/testsuite/gcc.dg/torture/pr39074-2.c      |     2 +-
 gcc/testsuite/gcc.dg/torture/pr39074.c        |     2 +-
 .../gcc.dg/torture/pta-structcopy-1.c         |     2 +-
 gcc/testsuite/gcc.target/riscv/check-stack.c  |    12 +
 .../gcc.target/riscv/rvv/github-pr779.c       |    11 +
 .../gcc.target/riscv/rvv/riscv-rvv.exp        |    43 +
 .../gcc.target/riscv/rvv/rvv-common.h         |  1064 ++
 .../gcc.target/riscv/rvv/rvv_aaddsub.c        |    19 +
 .../gcc.target/riscv/rvv/rvv_aaddsub2.c       |    19 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_adc.c  |    22 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_add.c  |    17 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_add2.c |    15 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_add3.c |    15 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_amo.c  |    98 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_amo2.c |   104 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_and.c  |    14 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_and2.c |    12 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_and3.c |    13 +
 .../riscv/rvv/rvv_biquad_df2T_stage_f32.c     |   164 +
 .../gcc.target/riscv/rvv/rvv_bitwise.c        |    22 +
 .../gcc.target/riscv/rvv/rvv_bitwise2.c       |    17 +
 .../riscv/rvv/rvv_compact_non_zero.c          |    78 +
 .../gcc.target/riscv/rvv/rvv_compare-1.c      |    83 +
 .../gcc.target/riscv/rvv/rvv_compare-2.c      |    90 +
 .../gcc.target/riscv/rvv/rvv_compress.c       |    23 +
 .../gcc.target/riscv/rvv/rvv_cond_example.c   |    52 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_div.c  |    19 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_div2.c |    31 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_div3.c |    33 +
 .../gcc.target/riscv/rvv/rvv_divrem.c         |    19 +
 .../gcc.target/riscv/rvv/rvv_divrem2.c        |    19 +
 .../riscv/rvv/rvv_dup_const_double.c          |    51 +
 .../gcc.target/riscv/rvv/rvv_example1.c       |    36 +
 .../gcc.target/riscv/rvv/rvv_example2.c       |    52 +
 .../gcc.target/riscv/rvv/rvv_example3.c       |    39 +
 .../gcc.target/riscv/rvv/rvv_example4.c       |    45 +
 .../gcc.target/riscv/rvv/rvv_example5.c       |    38 +
 .../riscv/rvv/rvv_explicit_load_store.c       |    26 +
 .../gcc.target/riscv/rvv/rvv_extend.c         |    36 +
 .../gcc.target/riscv/rvv/rvv_extend2.c        |    42 +
 .../gcc.target/riscv/rvv/rvv_fcompare-1.c     |    50 +
 .../gcc.target/riscv/rvv/rvv_fcompare-2.c     |    54 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt.c |    72 +
 .../gcc.target/riscv/rvv/rvv_fcvt2.c          |    90 +
 .../gcc.target/riscv/rvv/rvv_fir_filter.c     |   121 +
 .../riscv/rvv/rvv_fsign-injection.c           |    16 +
 .../riscv/rvv/rvv_fsign-injection2.c          |    16 +
 .../gcc.target/riscv/rvv/rvv_funary.c         |    23 +
 .../gcc.target/riscv/rvv/rvv_funary2.c        |    26 +
 .../gcc.target/riscv/rvv/rvv_func_call.c      |    22 +
 .../gcc.target/riscv/rvv/rvv_fwmac.c          |    49 +
 .../gcc.target/riscv/rvv/rvv_fwmac2.c         |    59 +
 .../riscv/rvv/rvv_implicit_load_store.c       |    55 +
 .../gcc.target/riscv/rvv/rvv_indexed-load-2.c |    30 +
 .../gcc.target/riscv/rvv/rvv_indexed-load.c   |    27 +
 .../riscv/rvv/rvv_indexed-store-2.c           |    47 +
 .../gcc.target/riscv/rvv/rvv_indexed-store.c  |    45 +
 .../gcc.target/riscv/rvv/rvv_load-global.c    |    12 +
 .../gcc.target/riscv/rvv/rvv_load-stack.c     |    12 +
 .../riscv/rvv/rvv_loop_switch_elem.c          |    49 +
 .../riscv/rvv/rvv_loop_widest_elem.c          |    43 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_mac.c  |    37 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_macc.c |    73 +
 .../gcc.target/riscv/rvv/rvv_madc-vv.c        |    48 +
 .../gcc.target/riscv/rvv/rvv_madc-vvm.c       |    50 +
 .../gcc.target/riscv/rvv/rvv_mask-clr.c       |    23 +
 .../gcc.target/riscv/rvv/rvv_mask-op-1.c      |    86 +
 .../gcc.target/riscv/rvv/rvv_mask-op-2.c      |   117 +
 .../gcc.target/riscv/rvv/rvv_mask-op-3.c      |    42 +
 .../gcc.target/riscv/rvv/rvv_mask-set.c       |    23 +
 .../gcc.target/riscv/rvv/rvv_maxmin.c         |    27 +
 .../gcc.target/riscv/rvv/rvv_maxmin2.c        |    28 +
 .../gcc.target/riscv/rvv/rvv_memcpy.c         |    39 +
 .../gcc.target/riscv/rvv/rvv_merge.c          |    74 +
 .../gcc.target/riscv/rvv/rvv_minmax.c         |    19 +
 .../gcc.target/riscv/rvv/rvv_minmax2.c        |    19 +
 .../gcc.target/riscv/rvv/rvv_mixed_width.c    |    57 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_mul.c  |    16 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_mul2.c |    15 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_mul3.c |    14 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh.c |    42 +
 .../gcc.target/riscv/rvv/rvv_mulh2.c          |    46 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_mv.c   |    42 +
 .../gcc.target/riscv/rvv/rvv_nclip.c          |    15 +
 .../gcc.target/riscv/rvv/rvv_nclip2.c         |    15 +
 .../gcc.target/riscv/rvv/rvv_nfcvt.c          |    91 +
 .../gcc.target/riscv/rvv/rvv_nfcvt2.c         |   115 +
 .../gcc.target/riscv/rvv/rvv_nshift.c         |    15 +
 .../gcc.target/riscv/rvv/rvv_nshift2.c        |    15 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_or.c   |    14 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_or2.c  |    12 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_or3.c  |    12 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac.c |    52 +
 .../gcc.target/riscv/rvv/rvv_qmac2.c          |    60 +
 .../gcc.target/riscv/rvv/rvv_readvl.c         |    13 +
 .../gcc.target/riscv/rvv/rvv_reduc.c          |    66 +
 .../gcc.target/riscv/rvv/rvv_reduc2.c         |    74 +
 .../gcc.target/riscv/rvv/rvv_reint.c          |    68 +
 .../gcc.target/riscv/rvv/rvv_rgather.c        |    76 +
 .../gcc.target/riscv/rvv/rvv_rgather2.c       |   112 +
 .../gcc.target/riscv/rvv/rvv_saddsub.c        |    21 +
 .../gcc.target/riscv/rvv/rvv_saddsub2.c       |    21 +
 .../gcc.target/riscv/rvv/rvv_saxpy.c          |    26 +
 .../gcc.target/riscv/rvv/rvv_saxpy2.c         |    26 +
 .../gcc.target/riscv/rvv/rvv_setvlmax.c       |    16 +
 .../gcc.target/riscv/rvv/rvv_sgemm.c          |    60 +
 .../gcc.target/riscv/rvv/rvv_shift.c          |    23 +
 .../gcc.target/riscv/rvv/rvv_shift2.c         |    23 +
 .../gcc.target/riscv/rvv/rvv_slide.c          |    62 +
 .../gcc.target/riscv/rvv/rvv_slide2.c         |    80 +
 .../gcc.target/riscv/rvv/rvv_stack-1.c        |    10 +
 .../gcc.target/riscv/rvv/rvv_strcpy.c         |    51 +
 .../gcc.target/riscv/rvv/rvv_strided-load-2.c |    28 +
 .../gcc.target/riscv/rvv/rvv_strided-load.c   |    27 +
 .../riscv/rvv/rvv_strided-store-2.c           |    28 +
 .../gcc.target/riscv/rvv/rvv_strided-store.c  |    27 +
 .../gcc.target/riscv/rvv/rvv_strlen.c         |    53 +
 .../gcc.target/riscv/rvv/rvv_strncpy.c        |    73 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_sub.c  |    17 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_sub2.c |    42 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_sub3.c |    47 +
 .../gcc.target/riscv/rvv/rvv_tuple_type.c     |    16 +
 .../gcc.target/riscv/rvv/rvv_tuple_utils.c    |    16 +
 .../gcc.target/riscv/rvv/rvv_unary.c          |    10 +
 .../gcc.target/riscv/rvv/rvv_unary2.c         |    10 +
 .../gcc.target/riscv/rvv/rvv_uninit_val.c     |    18 +
 .../riscv/rvv/rvv_unit-stride-ff-load-2.c     |    28 +
 .../riscv/rvv/rvv_unit-stride-ff-load.c       |    27 +
 .../riscv/rvv/rvv_unit-stride-load-2.c        |    28 +
 .../riscv/rvv/rvv_unit-stride-load.c          |    27 +
 .../riscv/rvv/rvv_unit-stride-store-2.c       |    28 +
 .../riscv/rvv/rvv_unit-stride-store.c         |    27 +
 .../gcc.target/riscv/rvv/rvv_wadd-wv.c        |    34 +
 .../gcc.target/riscv/rvv/rvv_wbinop-vv.c      |    93 +
 .../gcc.target/riscv/rvv/rvv_wbinop-vv2.c     |   111 +
 .../gcc.target/riscv/rvv/rvv_wbinop-wv.c      |    63 +
 .../gcc.target/riscv/rvv/rvv_wbinop-wv2.c     |    74 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt.c |    30 +
 .../gcc.target/riscv/rvv/rvv_wcvt2.c          |    36 +
 .../gcc.target/riscv/rvv/rvv_wfcvt.c          |    82 +
 .../gcc.target/riscv/rvv/rvv_wfcvt2.c         |   103 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac.c |    52 +
 .../gcc.target/riscv/rvv/rvv_wmac2.c          |    60 +
 .../gcc.target/riscv/rvv/rvv_wmadd_scalar.c   |    28 +
 .../gcc.target/riscv/rvv/rvv_wreduc.c         |    47 +
 .../gcc.target/riscv/rvv/rvv_wreduc2.c        |    54 +
 .../gcc.target/riscv/rvv/rvv_wsub-vv2.c       |    40 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_xor.c  |    14 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_xor2.c |    12 +
 gcc/testsuite/gcc.target/riscv/rvv/rvv_xor3.c |    13 +
 gcc/tree-vect-generic.c                       |    19 +-
 177 files changed, 39841 insertions(+), 179 deletions(-)
 create mode 100755 gcc/config/riscv/gen-vector-iterator
 create mode 100644 gcc/config/riscv/riscv-vector-ftypes.def
 create mode 100644 gcc/config/riscv/riscv-vector-iterator.h
 create mode 100644 gcc/config/riscv/riscv_vector.h
 create mode 100644 gcc/config/riscv/riscv_vector_itr.h
 create mode 100644 gcc/config/riscv/vector-iterator.md
 create mode 100644 gcc/config/riscv/vector.md
 create mode 100644 gcc/testsuite/g++.target/riscv/rvv_merge-1.C
 create mode 100644 gcc/testsuite/gcc.target/riscv/check-stack.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/github-pr779.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/riscv-rvv.exp
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv-common.h
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_adc.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_add.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_add2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_add3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_amo.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_amo2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_and.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_and2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_and3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_biquad_df2T_stage_f32.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_compact_non_zero.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-1.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_compress.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_cond_example.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_div.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_div2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_div3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_dup_const_double.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_example1.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_example2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_example3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_example4.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_example5.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_explicit_load_store.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_extend.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_extend2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-1.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fir_filter.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_funary.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_funary2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_func_call.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_implicit_load_store.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_load-global.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_load-stack.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_switch_elem.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_widest_elem.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mac.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_macc.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vv.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vvm.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-clr.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-1.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-set.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_memcpy.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_merge.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mixed_width.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mul.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mul2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mul3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_mv.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_or.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_or2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_or3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_readvl.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_reint.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_setvlmax.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_sgemm.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_shift.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_shift2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_slide.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_slide2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_stack-1.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_strcpy.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_strlen.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_strncpy.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_sub.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_sub2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_sub3.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_type.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_utils.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unary.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unary2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_uninit_val.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store-2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wadd-wv.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wmadd_scalar.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_wsub-vv2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_xor.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_xor2.c
 create mode 100644 gcc/testsuite/gcc.target/riscv/rvv/rvv_xor3.c

diff --git a/gcc/common/config/riscv/riscv-common.c b/gcc/common/config/riscv/riscv-common.c
index b2bab838ed7..1fd61bd12d7 100644
--- a/gcc/common/config/riscv/riscv-common.c
+++ b/gcc/common/config/riscv/riscv-common.c
@@ -61,6 +61,9 @@ static const riscv_implied_info_t riscv_implied_info[] =
   {"d", "f"},
   {"f", "zicsr"},
   {"d", "zicsr"},
+
+  {"v", "zvamo"},
+  {"v", "zvlsseg"},
   {NULL, NULL}
 };
 
@@ -114,6 +117,11 @@ static const struct riscv_ext_version riscv_ext_version_table[] =
 
   {"zfh", ISA_SPEC_CLASS_NONE, 0, 1},
 
+  {"v",       ISA_SPEC_CLASS_NONE, 1, 0},
+  {"zvamo",   ISA_SPEC_CLASS_NONE, 1, 0},
+  {"zvlsseg", ISA_SPEC_CLASS_NONE, 1, 0},
+  {"zvqmac",  ISA_SPEC_CLASS_NONE, 1, 0},
+
   /* Terminate the list.  */
   {NULL, ISA_SPEC_CLASS_NONE, 0, 0}
 };
@@ -1021,6 +1029,8 @@ static const riscv_ext_flag_table_t riscv_ext_flag_table[] =
   {"zicsr",    &gcc_options::x_riscv_zi_subext, MASK_ZICSR},
   {"zifencei", &gcc_options::x_riscv_zi_subext, MASK_ZIFENCEI},
 
+  {"v", &gcc_options::x_target_flags, MASK_VECTOR},
+
   {NULL, NULL, 0}
 };
 
diff --git a/gcc/config.gcc b/gcc/config.gcc
index bf621cf76c7..c005da78631 100644
--- a/gcc/config.gcc
+++ b/gcc/config.gcc
@@ -526,6 +526,7 @@ pru-*-*)
 riscv*)
 	cpu_type=riscv
 	extra_objs="riscv-builtins.o riscv-c.o riscv-sr.o riscv-shorten-memrefs.o"
+	extra_headers="riscv_vector.h riscv_vector_itr.h"
 	d_target_objs="riscv-d.o"
 	;;
 rs6000*-*-*)
diff --git a/gcc/config/riscv/arch-canonicalize b/gcc/config/riscv/arch-canonicalize
index 2b4289e320d..ed700f34218 100755
--- a/gcc/config/riscv/arch-canonicalize
+++ b/gcc/config/riscv/arch-canonicalize
@@ -36,6 +36,7 @@ LONG_EXT_PREFIXES = ['z', 's', 'h', 'x']
 #
 IMPLIED_EXT = {
   "d" : ["f"],
+  "v" : ['zvlsseg', 'zvamo'],
 }
 
 def arch_canonicalize(arch):
diff --git a/gcc/config/riscv/constraints.md b/gcc/config/riscv/constraints.md
index ef9c81e424e..bf98f566b36 100644
--- a/gcc/config/riscv/constraints.md
+++ b/gcc/config/riscv/constraints.md
@@ -54,6 +54,36 @@
   (and (match_code "const_int")
        (match_test "LUI_OPERAND (ival)")))
 
+(define_constraint "Wsa"
+  "Integer one."
+  (and (match_code "const_int")
+       (match_test "ival == 1")))
+
+(define_constraint "Wsb"
+  "A constraint that matches an immediate shift constant in QImode."
+  (and (match_code "const_int")
+       (match_test "ival == 8")))
+
+(define_constraint "Wsh"
+  "A constraint that matches an immediate shift constant in HImode."
+  (and (match_code "const_int")
+       (match_test "ival == 16")))
+
+(define_constraint "Wsw"
+  "A constraint that matches an immediate shift constant in SImode."
+  (and (match_code "const_int")
+       (match_test "ival == 32")))
+
+(define_constraint "Wsd"
+  "A constraint that matches an immediate shift constant in DImode."
+  (and (match_code "const_int")
+       (match_test "ival == 64")))
+
+(define_constraint "Ws5"
+  "Signed immediate 5-bit value"
+  (and (match_code "const_int")
+       (match_test "IN_RANGE (INTVAL (op), -16, 15)")))
+
 ;; Floating-point constant +0.0, used for FCVT-based moves when FMV is
 ;; not available in RV32.
 (define_constraint "G"
@@ -81,3 +111,54 @@
    A constant @code{move_operand}."
   (and (match_operand 0 "move_operand")
        (match_test "CONSTANT_P (op)")))
+
+;; Vector constraints.
+
+(define_register_constraint "vr" "TARGET_VECTOR ? VECTOR_REGS : NO_REGS"
+  "A vector register (if available).")
+
+;; TODO: This could be wrong if vector mask can use other than v0.
+(define_register_constraint "vd" "TARGET_VECTOR ? VECTOR_NO_MASK_REGS : NO_REGS"
+  "A vector register except mask register (if available).")
+
+;; ??? Not used yet.
+(define_register_constraint "vm" "TARGET_VECTOR ? VECTOR_MASK_REGS : NO_REGS"
+  "A vector mask register (if available).")
+
+(define_register_constraint "vt" "TARGET_VECTOR ? VTYPE_REGS : NO_REGS"
+  "VTYPE register (if available).")
+
+(define_constraint "vc"
+  "Any vector duplicate constant."
+  (and (match_code "const_vector")
+       (match_test "const_vec_duplicate_p (op)")))
+
+(define_constraint "vi"
+  "A vector 5-bit signed immediate."
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, -16, 15)")))
+
+(define_constraint "vj"
+  "A vector negated 5-bit signed immediate."
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, -15, 16)")))
+
+(define_constraint "vk"
+  "A vector 5-bit unsigned immediate."
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, 0, 31)")))
+
+(define_constraint "v0"
+  "A vector with constant zero."
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, 0, 0)")))
+
+(define_constraint "v1"
+  "A vector with constant all bit set."
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, 1, 1)")))
+
+(define_constraint "vp"
+  "POLY_INT"
+  (and (match_code "const_poly_int")
+       (match_test "CONST_POLY_INT_COEFFS (op)[0] == UNITS_PER_V_REG.coeffs[0]")))
diff --git a/gcc/config/riscv/gen-vector-iterator b/gcc/config/riscv/gen-vector-iterator
new file mode 100755
index 00000000000..7eee445cc9c
--- /dev/null
+++ b/gcc/config/riscv/gen-vector-iterator
@@ -0,0 +1,1638 @@
+#!/usr/bin/env python3
+
+from __future__ import print_function
+import sys
+import os
+import subprocess
+# -------------------------------------------------
+#   Prepare datas
+# -------------------------------------------------
+base_scale = 128
+# mode name -> width
+base_modes = [
+  ("QI", 8), ("HI", 16), ("SI", 32), ("DI", 64),
+  ("HF", 16), ("SF", 32), ("DF", 64)
+]
+base_int_modes = {
+  8: "QI",
+  16: "HI",
+  32: "SI",
+  64: "DI",
+  128: "TI",
+}
+
+base_fp_modes = {
+  16: "HF",
+  32: "SF",
+  64: "DF",
+}
+
+SEWS = [8, 16, 32, 64]
+
+SEW_LETTERS = {
+   8 : 'b',
+  16 : 'h',
+  32 : 'w',
+  64 : 'd',
+ 128 : 't',
+}
+
+class VectorTypeInfo:
+  def __repr__(self):
+    return "%s (%s)" %(self.ft_vtype, self.mode)
+  def __str__(self):
+    return "%s (%s)" %(self.ft_vtype, self.mode)
+
+  def __init__(self, sew, lmul, int_p, nf = 1,
+               eew=None, elmul=None, check=False, eew_reinterpret=False):
+    assert (elmul is None) or eew_reinterpret
+    self.sew = sew
+    self.sew_letter = SEW_LETTERS[sew]
+    self.lmul = lmul
+    self.nf = nf
+    self.int_p = int_p
+
+    if int_p:
+      self.scalar_mode = base_int_modes[sew]
+      self.scalar_ctype = "int%d_t" % sew
+    else:
+      self.scalar_mode = base_fp_modes[sew]
+      self.scalar_ctype = "__float%d_t" % sew
+
+    self.scalar_int_mode = base_int_modes[sew]
+    self.scalar_int_ctype = "int%d_t" % sew
+
+    self.scalar_uint_mode = "U" + base_int_modes[sew]
+    if sew in base_fp_modes:
+      self.scalar_float_mode = base_fp_modes[sew]
+    else:
+      self.scalar_float_mode = None
+
+    self.scale = (base_scale // sew) * lmul
+    self.mask_mode = "VNx%sBI" % (self.scale)
+    self._base_vector_mode = "VNx%s%s" % (self.scale, mode)
+
+    if self.nf == 1:
+      mode_name = "VNx%s%s" % (self.scale, self.scalar_mode)
+    else:
+      mode_name = "VNx%sx%s%s" % (self.nf, self.scale, self.scalar_mode)
+    self.mode = mode_name
+
+    self.int_vector_mode = mode_name[:-1] + 'I'
+    self.float_vector_mode = mode_name[:-1] + 'F'
+    self.mlen = self.sew // self.lmul
+
+    if self.int_p:
+      self.ft_vtype = "VI%sM%s" % (self.sew, self.lmul)
+      self.ft_m1_vtype = "VI%sM1" % (self.sew)
+      self.ft_vt_type = "VI%sM%sX%s" % (self.sew, self.lmul, self.nf)
+    else:
+      self.ft_vtype = "VF%sM%s" % (self.sew, self.lmul)
+      self.ft_m1_vtype = "VF%sM1" % (self.sew)
+      self.ft_vt_type = "VF%sM%sX%s" % (self.sew, self.lmul, self.nf)
+
+    self.ft_vftype = "VF%sM%s" % (self.sew, self.lmul)
+    self.ft_vitype = "VI%sM%s" % (self.sew, self.lmul)
+    self.ft_vutype = "VUI%sM%s" % (self.sew, self.lmul)
+
+    self.ft_vt_itype = "VI%sM%sX%s" % (self.sew, self.lmul, self.nf)
+    self.ft_vt_utype = "VUI%sM%sX%s" % (self.sew, self.lmul, self.nf)
+    self.ft_vt_ftype = "VF%sM%sX%s" % (self.sew, self.lmul, self.nf)
+
+    self.ft_m1_vftype = "VF%sM1" % (self.sew)
+    self.ft_m1_vitype = "VI%sM1" % (self.sew)
+    self.ft_m1_vutype = "VUI%sM1" % (self.sew)
+
+    self.ft_vmasktype = "VB%s" % (self.mlen)
+
+    self.arg_map = {
+      'SEW'    : self.sew,
+      'SEW_LETTER':self.sew_letter,
+      'LMUL'   : self.lmul,
+      'MODE'   : self.mode.lower(),
+      'IMODE': self.int_vector_mode.lower(),
+      'NF'     : self.nf,
+      'SUBMODE'  : self.scalar_mode,
+      'ISUBMODE' : self.scalar_int_mode,
+      'CTYPE'  : self.scalar_ctype,
+      'ICTYPE' : self.scalar_int_ctype,
+      'MLEN'   : self.mlen,
+      'SUBMODE_PREFIX_UPPER': self.scalar_mode[0].upper(),
+      'SUBMODE_PREFIX_LOWER': self.scalar_mode[0].lower(),
+      'MODE_PREFIX_UPPER' : self.mode[:-1],
+      'MODE_PREFIX_LOWER' : self.mode[:-1].lower(),
+
+      'FT_MASK'  : self.ft_vmasktype,
+      'FT_VEC'  : self.ft_vtype,
+      'FT_F_VEC' : self.ft_vftype,
+      'FT_I_VEC' : self.ft_vitype,
+      'FT_U_VEC' : self.ft_vutype,
+
+      'FT_VEC_TUPLE'  : self.ft_vt_type,
+      'FT_F_VEC_TUPLE' : self.ft_vt_ftype,
+      'FT_I_VEC_TUPLE' : self.ft_vt_itype,
+      'FT_U_VEC_TUPLE' : self.ft_vt_utype,
+
+      'FT_VEC_M1'  : self.ft_m1_vtype,
+      'FT_VEC_M1_F' : self.ft_m1_vftype,
+      'FT_VEC_M1_I' : self.ft_m1_vitype,
+      'FT_VEC_M1_U' : self.ft_m1_vutype,
+
+      'FT_SCALAR' : self.scalar_mode,
+      'FT_F_SCALAR' : self.scalar_float_mode,
+      'FT_I_SCALAR' : self.scalar_int_mode,
+      'FT_U_SCALAR' : self.scalar_uint_mode,
+    }
+
+    if eew:
+      if eew_reinterpret:
+        if elmul is None:
+          elmul = self.lmul
+        self.eew_type = VectorTypeInfo(sew=eew, lmul=elmul, int_p=self.int_p)
+        eew_type = self.eew_type
+        self.arg_map["EEW"] = eew_type.sew
+        self.arg_map["EMUL"] = eew_type.lmul
+        self.arg_map["EEW_MODE"] = eew_type.mode.lower()
+        self.arg_map["EEW_IMODE"] = eew_type.int_vector_mode.lower()
+        self.arg_map['FT_I_XVEC'] = eew_type.ft_vitype
+        self.arg_map['FT_U_XVEC'] = eew_type.ft_vutype
+        self.arg_map['FT_XVEC'] = eew_type.ft_vtype
+      else:
+        assert (self.valid_eew_p(eew))
+        self.eew_type = VectorTypeInfo(sew=eew, lmul=self.emul(eew), int_p=True)
+        eew_type = self.eew_type
+
+        self.arg_map["EEW"] = eew_type.sew
+        self.arg_map["EMUL"] = eew_type.lmul
+        self.arg_map["EEW_MODE"] = eew_type.mode.lower()
+        self.arg_map["EEW_IMODE"] = eew_type.int_vector_mode.lower()
+        self.arg_map["EEW_SUBMODE"] = eew_type.scalar_mode
+        self.arg_map["EEW_ISUBMODE"] = eew_type.scalar_int_mode
+
+        self.arg_map["FT_INDEX"] = "VUI%sM%s" %(eew_type.sew, eew_type.lmul)
+
+  def __getitem__(self, item):
+    if item[0] == 'W':
+      if item[1] == '1':
+        return self.widen_lmul_1_type[item[2:]]
+      else:
+        return self.widen_type[item[1:]]
+    elif item[0] == 'Q':
+      return self.quad_widen_type[item[1:]]
+    elif item[0] == 'E' and not (item.startswith("EEW") or item == "EMUL"):
+      return self.eightfold_widen_type[item[1:]]
+    return self.arg_map[item]
+
+  def __str__(self):
+    return self.mode
+
+  def has_eightfold_widen_type(self):
+    if not self.has_quad_widen_type():
+      return False
+
+    return self.quad_widen_type.has_widen_type()
+
+  def has_quad_widen_type(self):
+    if not self.has_widen_type():
+      return False
+
+    return self.widen_type.has_widen_type()
+
+  def has_widen_type(self):
+    if self.sew == 64:
+      return False
+
+    if self.lmul == 8:
+      return False
+
+    return True
+
+  @property
+  def int_type(self):
+    if self.int_p:
+      return self
+    return VectorTypeInfo(sew=self.sew, lmul=self.lmul, nf=self.nf,
+                          int_p=True)
+
+  @property
+  def widen_type(self):
+    assert self.nf == 1 and (self.has_widen_type())
+    return VectorTypeInfo(sew=(self.sew * 2), lmul=(self.lmul * 2),
+                          int_p=self.int_p)
+
+  @property
+  def xwiden_type(self):
+    assert self.nf == 1
+    return VectorTypeInfo(sew=(self.sew * 2), lmul=(self.lmul * 2),
+                          int_p=self.int_p, check=False)
+
+  @property
+  def quad_widen_type(self):
+    assert self.nf == 1 and (self.has_quad_widen_type())
+    return self.widen_type.widen_type
+
+  @property
+  def eightfold_widen_type(self):
+    assert self.nf == 1 and (self.has_eightfold_widen_type())
+    return self.quad_widen_type.widen_type
+
+  @property
+  def lmul_1_type(self):
+    return VectorTypeInfo(sew=self.sew, lmul=1,
+                          int_p=self.int_p)
+
+  @property
+  def base_vector_type(self):
+    assert self.nf >= 1
+    return VectorTypeInfo(sew=self.sew, lmul=self.lmul, nf=1,
+                          int_p=self.int_p)
+
+  @property
+  def base_vector_mode(self):
+    assert self.nf >= 1
+    return self._base_vector_mode
+
+  def has_widen_lmul_1_type(self):
+    return self.sew != 64
+
+  @property
+  def widen_lmul_1_type(self):
+    assert self.nf == 1 and (self.has_widen_lmul_1_type())
+    return VectorTypeInfo(sew=(self.sew * 2), lmul=1,
+                          int_p=self.int_p)
+
+  @property
+  def index_mode(self):
+    if self.nf >= 1:
+      return self.base_vector_type.int_vector_mode
+    else:
+      return self.int_vector_mode
+
+  def emul(self, eew):
+    return int((eew / self.sew) * self.lmul)
+
+  def valid_eew_p(self, eew):
+    if eew == self.sew:
+      return True
+    # TODO: Not consider fractional LMUL yet.
+    emul = self.emul(eew)
+    return emul <= 8 and emul >= 1
+
+LMULs = [1, 2, 4, 8]
+NF = range(2, 9) # 2~8
+# Construct all vector and vector tuple modes.
+vector_modes = []
+for mode, width in base_modes:
+  for lmul in LMULs:
+    vt = VectorTypeInfo(sew=width, lmul=lmul, int_p=mode.endswith("I"))
+    vector_modes.append(vt)
+
+int_vector_modes = list(filter(lambda x : x.int_p, vector_modes))
+float_vector_modes = list(filter(lambda x : not x.int_p, vector_modes))
+
+vector_tuple_modes = []
+for mode, width in base_modes:
+  for lmul in LMULs:
+    for nf in NF:
+      if nf * lmul > 8:
+        continue
+      vt = VectorTypeInfo(sew=width, lmul=lmul, int_p=mode.endswith("I"), nf=nf)
+      vector_tuple_modes.append(vt)
+
+int_vector_tuple_modes = list(filter(lambda x : x.int_p, vector_tuple_modes))
+float_vector_tuple_modes = list(filter(lambda x : not x.int_p, vector_tuple_modes))
+
+vector_masking_modes = []
+for n in range(7):
+  scale = 2 << n
+  mode_name = "VNx%sBI" % scale
+  vector_masking_modes.append(mode_name)
+
+# -------------------------------------------------
+#    Util functions
+# -------------------------------------------------
+def dump_md_iterator(name, comment, vals):
+  print ("")
+  print (";; %s" % comment.replace("\n", "\n;; "))
+  print ("(define_mode_iterator %s [" % name)
+  first_col = True
+  n = 0;
+  for val in vals:
+    if first_col:
+      first_col = False
+      print (" ", end='')
+    print (" %s" % val, end='')
+    n += 1;
+    if n == 4:
+      print ("")
+      first_col = True
+      n = 0
+  print ("])")
+
+def dump_md_attr(name, comment, indexes, val_func,
+                 lower_version_p = False, lower_p=False):
+  print ("")
+  print (";; %s" % comment.replace("\n", "\n;; "))
+  print ("(define_mode_attr %s [" % (name))
+  first_col = True
+  n = 0;
+  for index in indexes:
+    val = val_func(index)
+    if lower_p:
+      val = str(val).lower()
+    if first_col:
+      first_col = False
+      print (" ", end='')
+    print (" (%s \"%s\")" % (index, val), end='')
+    n += 1;
+    if n == 4:
+      print ("")
+      first_col = True
+      n = 0
+  print ("])")
+
+  if lower_version_p:
+    dump_md_attr(name.lower(), comment, indexes, val_func, lower_p=True)
+
+
+def get_vlmode(typeinfo):
+  global vector_modes
+  if isinstance (typeinfo, VectorTypeInfo):
+    if typeinfo.nf == 1:
+      return typeinfo.int_vector_mode
+    else:
+      return typeinfo.base_vector_type.int_vector_mode
+
+  # It must be a mask type.
+  # Try to find first compatible mode for masking mode,
+  # Should find better way in future.
+  all_vmodes = list(map(lambda x: str(x), vector_modes))
+  for try_mode in ["QI", "HI", "SI", "DI"]:
+    eqv_vmode = typeinfo.replace("BI", try_mode)
+    if eqv_vmode in all_vmodes:
+      return eqv_vmode
+
+  assert False
+
+def _dump_c_iterator(name, comment, arg_list_template, vals, extra_arg_p=True):
+  if (extra_arg_p):
+    print ("#define %s_ARG(MACRO, ...) \\" % name)
+  else:
+    print ("#define %s(MACRO) \\" % name)
+  for val in vals:
+    print ("  MACRO (", end='')
+    print (arg_list_template.format_map(val), end='')
+    if extra_arg_p:
+      print (", __VA_ARGS__", end='')
+    print (") \\")
+  print ("")
+
+def expand_eew_types(vals, eew_index=False, eew_reinterpret=False):
+  assert not (eew_index and eew_reinterpret)
+  if eew_index:
+    new_vals = []
+    for val in vals:
+      for eew in SEWS:
+        if val.valid_eew_p(eew):
+          new_vals.append(VectorTypeInfo(val.sew, val.lmul, val.int_p,
+                                         val.nf, eew=eew))
+    vals = new_vals
+
+  if eew_reinterpret:
+    new_vals = []
+    for val in vals:
+      if val.int_p:
+        for eew in SEWS:
+          if eew != val.sew:
+            new_vals.append(VectorTypeInfo(val.sew, val.lmul, val.int_p,
+                                           val.nf, eew=eew, eew_reinterpret=True))
+      for elmul in LMULs:
+        if elmul != val.lmul:
+          new_vals.append(VectorTypeInfo(val.sew, val.lmul, val.int_p,
+                                         val.nf, eew=val.sew, elmul=elmul, eew_reinterpret=True))
+    vals = new_vals
+  return vals
+
+def dump_c_iterator(name, comment, arg_list_template, vals,
+                    gen_arg_version_p=True, eew_index=False, eew_reinterpret=False):
+  vals = expand_eew_types(vals, eew_index, eew_reinterpret)
+  print (comment)
+  _dump_c_iterator(name, comment, arg_list_template, vals, False)
+  if gen_arg_version_p:
+    print ("/* Same as above but with an extra argument.  */")
+    _dump_c_iterator(name, comment, arg_list_template, vals, True)
+
+# -------------------------------------------------
+#  Generation for vector-iterator.md
+# -------------------------------------------------
+def gen_md():
+  global vector_modes, int_vector_modes, float_vector_modes
+  global vector_tuple_modes, vector_masking_modes
+  print (";; DO NOT EDIT, please edit generator instead.")
+  print (";; This file was generated by gen-vector-iterator with the command:")
+  print (";; $ ./gen-vector-iterator -md > vector-iterator.md")
+
+  dump_md_iterator(
+    name = "VMODES",
+    comment = "All vector modes supported.",
+    vals = vector_modes
+  )
+
+  dump_md_iterator(
+    name = "VIMODES",
+    comment = "All vector modes supported for integer load/store/alu.",
+    vals = int_vector_modes
+  )
+
+  dump_md_iterator(
+    name = "VIMODES2",
+    comment = "Same as VIMODES, used for combination.",
+    vals = int_vector_modes
+  )
+
+  dump_md_iterator(
+    name = "VWIMODES",
+    comment = "All vector modes supported for widening integer alu.",
+    vals = filter(lambda x : x.has_widen_type(), int_vector_modes)
+  )
+
+  dump_md_iterator(
+    name = "VWRED_IMODES",
+    comment = "All vector modes supported for widening integer point reduction operation.",
+    vals = filter(lambda x : x.has_widen_lmul_1_type(), int_vector_modes)
+  )
+
+
+  dump_md_iterator(
+    name = "FCVT_VWIMODES",
+    comment = "All vector modes supported for FP type-convert.",
+    vals = list(map(lambda x: x.int_type,
+                    filter(lambda x : x.has_widen_type(), float_vector_modes)))
+  )
+
+  dump_md_iterator(
+    name = "VQWIMODES",
+    comment = "All vector modes supported for quad-widening integer alu.",
+    vals = filter(lambda x : x.has_quad_widen_type(), int_vector_modes)
+  )
+
+  dump_md_iterator(
+    name = "VFMODES",
+    comment = "All vector modes supported for FP load/store/alu.",
+    vals = float_vector_modes
+  )
+
+  dump_md_iterator(
+    name = "VFMODES2",
+    comment = "Same as VFMODES, used for combination.",
+    vals = float_vector_modes
+  )
+
+  dump_md_iterator(
+    name = "VWFMODES",
+    comment = "All vector modes supported for widening floating point alu.",
+    vals = filter(lambda x : x.has_widen_type(), float_vector_modes)
+  )
+
+  dump_md_iterator(
+    name = "VWRED_FMODES",
+    comment = "All vector modes supported for widening floating point reduction operation.",
+    vals = filter(lambda x : x.has_widen_lmul_1_type(), float_vector_modes)
+  )
+
+  dump_md_iterator(
+    name = "VTMODES",
+    comment = "All vector tuple modes supported.",
+    vals = vector_tuple_modes
+  )
+
+  dump_md_iterator(
+    name = "VMASKMODES",
+    comment = "All vector masking modes.",
+    vals = vector_masking_modes
+  )
+
+  for nf in NF:
+    dump_md_iterator(
+      name = "VTNF%sMODES" % nf,
+      comment = "All vector tuple modes with NF=%s." % nf,
+      vals = filter(lambda x : x.nf == nf ,vector_tuple_modes)
+    )
+
+  VINTEQUIV_MODES = \
+    list(filter(lambda x : not x.int_p, vector_modes))
+
+  dump_md_attr(
+    name = "VINTEQUIV",
+    comment = "Map a vector float mode to a vector int mode of the same size.",
+    indexes = VINTEQUIV_MODES,
+    val_func = lambda x: x.int_vector_mode,
+    lower_version_p = True,
+  )
+
+  dump_md_attr(
+    name = "VWMODE",
+    comment = "Map a vector int or float mode to widening vector mode.",
+    indexes = list(filter(lambda x : x.has_widen_type(), vector_modes)),
+    val_func = lambda x: x.widen_type,
+    lower_version_p = True,
+  )
+
+  dump_md_attr(
+    name = "VQWMODE",
+    comment = "Map a vector int or float mode to quad-widening vector mode.",
+    indexes = list(filter(lambda x : x.has_quad_widen_type(), vector_modes)),
+    val_func = lambda x: x.quad_widen_type,
+    lower_version_p = True,
+  )
+
+  dump_md_attr(
+    name = "EXT_VIMODES",
+    comment = "Map a vector int or float mode to widening vector mode.",
+    indexes = int_vector_modes,
+    val_func = lambda x: x.xwiden_type,
+    lower_version_p = False,
+  )
+
+  dump_md_attr(
+    name = "VCMPEQUIV",
+    comment = "Map a vector int or float mode to a vector compare mode.",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.mask_mode,
+    lower_version_p = False,
+  )
+
+  dump_md_attr(
+    name = "vmaskmode",
+    comment = "Map a vector int or float mode to a vector compare mode.",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.mask_mode.lower(),
+    lower_version_p = False,
+  )
+
+  dump_md_attr(
+    name = "VSUBMODE",
+    comment = "Map a vector mode to its element mode.",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.scalar_mode,
+    lower_version_p = False,
+  )
+
+  dump_md_attr(
+    name = "VTSUBMODE",
+    comment = "Map a vector tuple mode to its vector mode.",
+    indexes = vector_tuple_modes,
+    val_func = lambda x: x.base_vector_mode,
+    lower_version_p = True,
+  )
+
+  dump_md_attr(
+    name = "V1MODES",
+    comment = "Map a vector mode to its LMUL==1 equivalent.\n"
+              "This is for reductions which use scalars in vector registers.",
+    indexes = vector_modes,
+    val_func =  lambda x: x.lmul_1_type,
+    lower_version_p = False
+  )
+
+  dump_md_attr(
+    name = "VW1MODES",
+    comment = "Map a vector mode to its LMUL==1 widen vector type.\n"
+              "This is for widening reductions which use scalars in vector registers.",
+    indexes = filter(lambda x : x.has_widen_lmul_1_type(), vector_modes),
+    val_func =  lambda x: x.widen_lmul_1_type,
+    lower_version_p = False
+  )
+
+  dump_md_attr(
+    name = "NF",
+    comment = "Map a vector tuple mode to its NF value.",
+    indexes = vector_tuple_modes,
+    val_func =  lambda x: x.nf,
+    lower_version_p = False
+  )
+
+  dump_md_attr(
+    name = "VLMODE",
+    comment = "Map a vector mode to its VSETVLI mode, which for now is always the integer\n"
+              "vector mode, as the integer vemode/vmmode is a superset of the float ones.",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = get_vlmode,
+    lower_version_p = True
+  )
+
+  dump_md_attr(
+    name = "VWVLMODE",
+    comment = "Map a vector mode to its VSETVLI mode of widening vector mode\n"
+              ", which for now is always the integer\n"
+              "vector mode, as the integer vemode/vmmode is a superset of the float ones.",
+    indexes = list(filter(lambda x : x.has_widen_type(), vector_modes)),
+    val_func = lambda x: get_vlmode(x.widen_type),
+    lower_version_p = True,
+  )
+
+  dump_md_attr(
+    name = "VQWVLMODE",
+    comment = "Map a vector mode to its VSETVLI mode of quad-widening vector mode\n"
+              ", which for now is always the integer\n"
+              "vector mode, as the integer vemode/vmmode is a superset of the float ones.",
+    indexes = list(filter(lambda x : x.has_quad_widen_type(), vector_modes)),
+    val_func = lambda x: get_vlmode(x.quad_widen_type),
+    lower_version_p = True,
+  )
+
+  dump_md_attr(
+    name = "VMEMINXMODE",
+    comment = "Map a vector mode to its index load mode",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.index_mode,
+    lower_version_p = True
+  )
+
+  dump_md_attr(
+    name = "VFWIMODE",
+    comment = "Map a vector float mode to vector widening int mode.",
+    indexes = list(filter(lambda x : x.has_widen_type(), float_vector_modes)),
+    val_func = lambda x: x.widen_type.int_vector_mode,
+    lower_version_p = True
+  )
+
+  dump_md_attr(
+    name = "VIWFMODE",
+    comment = "Map a vector int mode to vector widening float mode.",
+    indexes = list(map(lambda x: x.int_type,
+                       filter(lambda x : x.has_widen_type(),
+                              float_vector_modes))),
+    val_func = lambda x: x.widen_type.float_vector_mode,
+    lower_version_p = True
+  )
+
+  dump_md_attr(
+    name = "sew",
+    comment = "Map a vector mode to SEW",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.sew,
+    lower_version_p = False
+  )
+
+  dump_md_attr(
+    name = "lmul",
+    comment = "Map a vector mode to its LMUL.",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.lmul,
+    lower_version_p = False
+  )
+
+  dump_md_attr(
+    name = "vmsize",
+    comment = "Equivalent of \"size\" for a vector element.",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.sew_letter,
+    lower_version_p = False
+  )
+
+  dump_md_attr(
+    name = "VSINGLE",
+    comment = "Map a vector mode to LMUL=1 vector mode.",
+    indexes = vector_modes + vector_tuple_modes,
+    val_func = lambda x: x.lmul_1_type.mode,
+    lower_version_p = True
+  )
+
+# -------------------------------------------------
+#  Generation for riscv_vector_itr.h
+# -------------------------------------------------
+def gen_usr_c():
+  print ("/* DO NOT EDIT, please edit generator instead.")
+  print ("   This file was generated by gen-vector-iterator with the command:")
+  print ("   $ ./gen-vector-iterator -usr-c > riscv_vector_itr.h  */")
+
+  print ("#ifndef _GCC_RISCV_VECTOR_H\n")
+  print ("#error \"Never included riscv_vector_itr.h, plz include riscv_vector.h\"\n")
+  print ("#endif\n")
+
+  dump_c_iterator(
+    name = "_RVV_INT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector and integer types.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}",
+    vals = int_vector_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer types, and info for\n"
+              "   corresponding widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{WSEW}, {WLMUL}, {WCTYPE}",
+    vals = list(filter(lambda x: x.has_widen_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WRED_INT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer types, and info for\n"
+              "   corresponding widening vector type but with LMUL 1.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{W1SEW}, {W1LMUL}, {W1CTYPE}",
+    vals = list(filter(lambda x: x.has_widen_lmul_1_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_QINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer types, and info for\n"
+              "   corresponding quad widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{QSEW}, {QLMUL}, {QCTYPE}",
+    vals = list(filter(lambda x: x.has_quad_widen_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_EINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer modes, and info for\n"
+              "   corresponding eightfold widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{ESEW}, {ELMUL}, {ECTYPE}",
+    vals = list(filter(lambda x: x.has_eightfold_widen_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector and floating point modes.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}",
+    vals = float_vector_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WFLOAT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating modes, and info for\n"
+              "   corresponding widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{WSEW}, {WLMUL}, {WCTYPE}",
+    vals = list(filter(lambda x: x.has_widen_type(), float_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WRED_FLOAT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating types, and info for\n"
+              "   corresponding widening vector type but with LMUL 1.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{W1SEW}, {W1LMUL}, {W1CTYPE}",
+    vals = list(filter(lambda x: x.has_widen_lmul_1_type(), float_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_INT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, {ICTYPE}",
+    vals = float_vector_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_INT_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer types, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{EEW}, {EMUL}",
+    vals = int_vector_modes,
+    eew_index=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point types, and info for\n"
+              "   corresponding floating point and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, "
+                        "{EEW}, {EMUL}",
+    vals = float_vector_modes,
+    eew_index=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_INT_REINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer types, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, {EEW}, {EMUL}",
+    vals = int_vector_modes,
+    eew_reinterpret=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_REINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point types, and info for\n"
+              "   corresponding floating and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {CTYPE}, {EEW}, {EMUL}",
+    vals = float_vector_modes,
+    eew_reinterpret=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_INT_TUPLE_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding floating point and vector tuple type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {CTYPE}",
+    vals = int_vector_tuple_modes,
+  )
+
+  dump_c_iterator(
+    name = "_RVV_INT_TUPLE_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding floating point and vector tuple type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {CTYPE}, {EEW}, {EMUL}",
+    vals = int_vector_tuple_modes,
+    eew_index=True
+  )
+
+  for nf in range(2, 9):
+    dump_c_iterator(
+      name = "_RVV_INT_TUPLE_NF%d_ITERATOR" % nf,
+      comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+                "   along with its corresponding vector, floating point modes, and info for\n"
+                "   corresponding floating point and vector tuple type.  */",
+      arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {CTYPE}",
+      vals = list(filter(lambda x: x.nf == nf, int_vector_tuple_modes))
+    )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_TUPLE_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding floating point and vector tuple type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {CTYPE}",
+    vals = float_vector_tuple_modes,
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_TUPLE_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding floating point and vector tuple type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {CTYPE}, {EEW}, {EMUL}",
+    vals = float_vector_tuple_modes,
+    eew_index=True
+  )
+
+  for nf in range(2, 9):
+    dump_c_iterator(
+      name = "_RVV_FLOAT_TUPLE_NF%d_ITERATOR" % nf,
+      comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+                "   along with its corresponding vector, floating point modes, and info for\n"
+                "   corresponding floating point and vector tuple type.  */",
+      arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {CTYPE}",
+      vals = list(filter(lambda x: x.nf == nf, float_vector_tuple_modes))
+    )
+
+# -------------------------------------------------
+#  Generation for riscv-vector-iterator.h
+# -------------------------------------------------
+def gen_c():
+  global int_vector_modes, float_vector_modes
+  print ("/* DO NOT EDIT, please edit generator instead.")
+  print ("   This file was generated by gen-vector-iterator with the command:")
+  print ("   $ ./gen-vector-iterator -c > riscv-vector-iterator.h  */")
+
+  dump_c_iterator(
+    name = "_RVV_INT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector and integer modes.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}",
+    vals = int_vector_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer modes, and info for\n"
+              "   corresponding widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{WSEW}, {WLMUL}, {WMODE}, {WSUBMODE}",
+    vals = list(filter(lambda x: x.has_widen_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WRED_INT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer modes, and info for\n"
+              "   corresponding widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{W1SEW}, {W1LMUL}, {W1MODE}, {W1SUBMODE}",
+    vals = list(filter(lambda x: x.has_widen_lmul_1_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_QINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer modes, and info for\n"
+              "   corresponding quad widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{QSEW}, {QLMUL}, {QMODE}, {QSUBMODE}",
+    vals = list(filter(lambda x: x.has_quad_widen_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_EINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer modes, and info for\n"
+              "   corresponding eightfold widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{ESEW}, {ELMUL}, {EMODE}, {ESUBMODE}",
+    vals = list(filter(lambda x: x.has_eightfold_widen_type(), int_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_INT_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer point modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{EEW}, {EMUL}, {EEW_MODE}, {EEW_SUBMODE}",
+    vals = int_vector_modes,
+    eew_index=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_INT_REINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer point modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{EEW}, {EMUL}, {EEW_MODE}",
+    vals = int_vector_modes,
+    eew_reinterpret=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector and floating point modes.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}",
+    vals = float_vector_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WFLOAT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating modes, and info for\n"
+              "   corresponding widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{WSEW}, {WLMUL}, {WMODE}, {WSUBMODE}",
+    vals = list(filter(lambda x: x.has_widen_type(), float_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WRED_FLOAT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating modes, and info for\n"
+              "   corresponding widening vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{W1SEW}, {W1LMUL}, {W1MODE}, {W1SUBMODE}",
+    vals = list(filter(lambda x: x.has_widen_lmul_1_type(), float_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_INT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, {IMODE}, {ISUBMODE}",
+    vals = float_vector_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_WINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{WSEW}, {WLMUL}, {WIMODE}, {WISUBMODE}",
+    vals = list(filter(lambda x: x.has_widen_type(), float_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_WFLOAT_INT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding floating point and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {IMODE}, {ISUBMODE}, "
+                        "{WSEW}, {WLMUL}, {WMODE}, {WSUBMODE}",
+    vals = list(filter(lambda x: x.has_widen_type(), float_vector_modes))
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point point modes, and info for\n"
+              "   corresponding floating point and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{EEW}, {EMUL}, {EEW_IMODE}, {EEW_ISUBMODE}",
+    vals = float_vector_modes,
+    eew_index=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_FLOAT_REINT_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer point modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{EEW}, {EMUL}, {EEW_MODE}",
+    vals = float_vector_modes,
+    eew_reinterpret=True
+  )
+
+  dump_c_iterator(
+    name = "_RVV_SEG",
+    comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding floating point and vector tuple type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, "
+                        "{SUBMODE_PREFIX_UPPER}, {SUBMODE_PREFIX_LOWER},"
+                        "{MODE_PREFIX_UPPER}, {MODE_PREFIX_LOWER}",
+    vals = int_vector_tuple_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_SEG_INT_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, integer tuple modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{EEW}, {EMUL}, {EEW_MODE}, {EEW_SUBMODE}",
+    vals = int_vector_tuple_modes,
+    eew_index=True
+  )
+
+  for nf in range(2, 9):
+    dump_c_iterator(
+      name = "_RVV_SEG_NF%d" % nf,
+      comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+                "   along with its corresponding vector, floating point modes, and info for\n"
+                "   corresponding floating point and vector tuple type.  */",
+      arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, "
+                          "{SUBMODE_PREFIX_UPPER}, {SUBMODE_PREFIX_LOWER}, "
+                          "{MODE_PREFIX_UPPER}, {MODE_PREFIX_LOWER}",
+      vals = list(filter(lambda x: x.nf == nf, int_vector_tuple_modes))
+    )
+
+  dump_c_iterator(
+    name = "_RVV_SEG_NO_SEW8",
+    comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, floating point modes, and info for\n"
+              "   corresponding floating point and vector tuple type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, "
+                        "{SUBMODE_PREFIX_UPPER}, {SUBMODE_PREFIX_LOWER},"
+                        "{MODE_PREFIX_UPPER}, {MODE_PREFIX_LOWER}",
+    vals = float_vector_tuple_modes
+  )
+
+  dump_c_iterator(
+    name = "_RVV_SEG_FLOAT_INDEX_ITERATOR",
+    comment = "/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,\n"
+              "   along with its corresponding vector, float-point tuple modes, and info for\n"
+              "   corresponding integer and vector type.  */",
+    arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, {MODE}, {SUBMODE}, "
+                        "{EEW}, {EMUL}, {EEW_MODE}, {EEW_SUBMODE}",
+    vals = float_vector_tuple_modes,
+    eew_index=True
+  )
+
+  for nf in range(2, 9):
+    dump_c_iterator(
+      name = "_RVV_SEG_NF%d_NO_SEW8" % nf,
+      comment = "/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,\n"
+                "   along with its corresponding vector, floating point modes, and info for\n"
+                "   corresponding floating point and vector tuple type.  */",
+      arg_list_template = "{SEW}, {LMUL}, {NF}, {MLEN}, "
+                          "{SUBMODE_PREFIX_UPPER}, {SUBMODE_PREFIX_LOWER}, "
+                          "{MODE_PREFIX_UPPER}, {MODE_PREFIX_LOWER}",
+      vals = list(filter(lambda x: x.nf == nf, float_vector_tuple_modes))
+    )
+
+
+class FTtypes:
+  def __init__(self):
+    self.ftypes = set()
+
+  def add(self, arg_list_templates, vals, eew_index=False, eew_reinterpret=False):
+    vals = expand_eew_types(vals, eew_index, eew_reinterpret)
+
+    for arg_list_template in arg_list_templates:
+      unsigned_version = False
+      # We should repeat again if there is FT_VEC for unsigned
+      # interger vector types.
+      if '{FT_VEC}' in arg_list_template:
+        unsigned_version = True
+      if '{WFT_VEC}' in arg_list_template:
+        unsigned_version = True
+      if '{W1FT_VEC}' in arg_list_template:
+        unsigned_version = True
+      if '{QFT_VEC}' in arg_list_template:
+        unsigned_version = True
+      if '{EFT_VEC}' in arg_list_template:
+        unsigned_version = True
+      if '{FT_VEC_TUPLE}' in arg_list_template:
+        unsigned_version = True
+
+      for val in vals:
+        arg_list = []
+        for arg in arg_list_template:
+            arg = arg.format_map(val)
+            arg_list.append(arg)
+        self.ftypes.add(tuple(arg_list))
+
+        if unsigned_version and val.int_p:
+          arg_list = []
+          for arg in arg_list_template:
+	      # TODO: Clean up
+              if '{FT_VEC}' in arg:
+                arg = arg.replace("{FT_VEC}", "{FT_U_VEC}")
+              if '{WFT_VEC}' in arg:
+                arg = arg.replace("{WFT_VEC}", "{WFT_U_VEC}")
+              if '{W1FT_VEC}' in arg:
+                arg = arg.replace("{W1FT_VEC}", "{W1FT_U_VEC}")
+              if '{QFT_VEC}' in arg:
+                arg = arg.replace("{QFT_VEC}", "{QFT_U_VEC}")
+              if '{EFT_VEC}' in arg:
+                arg = arg.replace("{EFT_VEC}", "{EFT_U_VEC}")
+
+              if '{FT_VEC_M1}' in arg:
+                arg = arg.replace("{FT_VEC_M1}", "{FT_VEC_M1_U}")
+              if '{WFT_VEC_M1}' in arg:
+                arg = arg.replace("{WFT_VEC_M1}", "{WFT_VEC_M1_U}")
+
+              if '{FT_VEC_TUPLE}' in arg:
+                arg = arg.replace("{FT_VEC_TUPLE}", "{FT_U_VEC_TUPLE}")
+
+              if '{FT_SCALAR}' in arg:
+                arg = arg.replace("{FT_SCALAR}", "{FT_U_SCALAR}")
+
+              arg = arg.format_map(val)
+              arg_list.append(arg)
+          self.ftypes.add(tuple(arg_list))
+
+  def gen(self):
+    print ("/* DO NOT EDIT, please edit generator instead.\n"
+           "   This file was generated by gen-vector-iterator with the command:\n"
+           "   $ ./gen-vector-iterator -ftype > riscv-vector-ftypes.def  */\n")
+    ftypes = sorted(list(self.ftypes))
+    for ftype in ftypes:
+      arg_num = len(ftype) - 1
+      args = ", ".join(ftype)
+      print ("DEF_RISCV_FTYPE (%d, (%s))" % (arg_num, args))
+
+def gen_ftype():
+  ft = FTtypes()
+
+  # Prototype for vector modes, both for integer and floating point
+  ft.add(
+    arg_list_templates=[
+      # 0 args.
+      ('{FT_VEC}',),
+      # 1 args.
+      ('{FT_VEC}', 'C_{FT_SCALAR}_PTR'),
+
+      # 2 args.
+      # Vector Binary Operations.
+      ('{FT_VEC}', '{FT_VEC}', '{FT_VEC}'),
+      # Vector Binary Operations with scalar.
+      ('{FT_VEC}', '{FT_VEC}', '{FT_SCALAR}'),
+
+      ('{FT_VEC}', '{FT_VEC}', 'SIZE'),
+      ('{FT_MASK}', '{FT_VEC}', '{FT_VEC}'),
+      ('{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}'),
+
+      ('VOID', '{FT_VEC}', '{FT_SCALAR}_PTR'),
+
+      ('{FT_VEC}', 'C_{FT_SCALAR}_PTR', 'SI'),
+      ('{FT_VEC}', 'C_{FT_SCALAR}_PTR', 'DI'),
+
+      # 3 args.
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', 'C_{FT_SCALAR}_PTR'),
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', 'C_{FT_SCALAR}_PTR'),
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_VEC}'),
+
+      ('{FT_VEC}', '{FT_VEC}', '{FT_VEC}', '{FT_VEC}'),
+      ('{FT_VEC}', '{FT_VEC}', '{FT_SCALAR}', '{FT_VEC}'),
+      ('{FT_VEC}', '{FT_VEC}', '{FT_VEC}', 'SIZE'),
+
+      ('{FT_VEC_M1}', '{FT_VEC_M1}', '{FT_VEC_M1}', '{FT_VEC}'),
+
+      ('VOID', '{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}_PTR'),
+      ('VOID', '{FT_VEC}', '{FT_SCALAR}_PTR', 'SI'),
+      ('VOID', '{FT_VEC}', '{FT_SCALAR}_PTR', 'DI'),
+
+      # 4 args.
+      ('{FT_VEC_M1}', '{FT_MASK}', '{FT_VEC_M1}', '{FT_VEC_M1}', '{FT_VEC}'),
+
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_VEC}', '{FT_VEC}'),
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_VEC}', '{FT_SCALAR}'),
+      ('{FT_MASK}', '{FT_MASK}', '{FT_MASK}', '{FT_VEC}', '{FT_VEC}'),
+      ('{FT_MASK}', '{FT_MASK}', '{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}'),
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_VEC}', 'SIZE'),
+
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}', '{FT_VEC}'), # For masked mac
+
+      # Vector Strided Load.
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', 'C_{FT_SCALAR}_PTR', 'SI'),
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', 'C_{FT_SCALAR}_PTR', 'DI'),
+      # Vector Strided Store.
+      ('VOID', '{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}_PTR', 'SI'),
+      ('VOID', '{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}_PTR', 'DI'),
+    ],
+    vals=vector_modes)
+
+  # Same as above, but for widening vector.
+  ft.add(
+    arg_list_templates=[
+      # 2 args.
+      ('{WFT_VEC}', '{FT_VEC}', '{FT_VEC}'),
+      ('{WFT_VEC}', '{FT_VEC}', '{FT_SCALAR}'),
+
+      ('{WFT_VEC}', '{WFT_VEC}', '{FT_VEC}'),
+      ('{WFT_VEC}', '{WFT_VEC}', '{FT_SCALAR}'),
+
+      # 3 args.
+      ('{WFT_VEC}', '{WFT_VEC}', '{FT_VEC}', '{FT_VEC}'),
+      ('{WFT_VEC}', '{WFT_VEC}', '{FT_SCALAR}', '{FT_VEC}'),
+
+      # 4 args.
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{FT_VEC}', '{FT_VEC}'),
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{FT_VEC}', '{FT_SCALAR}'),
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{WFT_VEC}', '{FT_VEC}'),
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{WFT_VEC}', '{FT_SCALAR}'),
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{FT_SCALAR}', '{FT_VEC}'), # For maksed wmac.
+    ],
+    vals = list(filter(lambda x: x.has_widen_type(), vector_modes)))
+
+  # Same as above, but for widening vector with LMUL 1 (used for reduction
+  # operation).
+  ft.add(
+    arg_list_templates=[
+      # 2 args.
+      # Widening reduction vector operation.
+      ('{W1FT_VEC}', '{W1FT_VEC}', '{W1FT_VEC}', '{FT_VEC}'),
+
+      # 4 args.
+      # Masked widening reduction vector operation.
+      ('{W1FT_VEC}', '{FT_MASK}', '{W1FT_VEC}', '{W1FT_VEC}', '{FT_VEC}'),
+    ],
+    vals = list(filter(lambda x: x.has_widen_lmul_1_type(), vector_modes)))
+
+  # Prototype for integer vector mode only
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      ('{FT_VEC}', '{FT_VEC}'),
+      ('{FT_VEC}', '{FT_SCALAR}'),
+      ('{FT_SCALAR}', '{FT_VEC}'),
+      ('{FT_VEC}', '{FT_MASK}'),
+      ('{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{FT_U_VEC}', '{FT_I_VEC}'),
+      # 2 args.
+      ('{FT_VEC}', '{FT_VEC}', 'LONG'),
+      ('{FT_I_VEC}', '{FT_I_VEC}', 'UQI'),
+      ('{FT_I_VEC}', '{FT_I_VEC}', '{FT_U_SCALAR}'),
+      ('{FT_U_VEC}', '{FT_U_VEC}', 'UQI'),
+      ('{FT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{FT_MASK}', '{FT_I_VEC}', 'UQI'),
+      ('{FT_MASK}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{FT_MASK}', '{FT_I_VEC}', '{FT_MASK}'),
+
+      # Vector Element Index.
+      ('{FT_U_VEC}', '{FT_MASK}', '{FT_U_VEC}'), # For vid
+      # 3 args.
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_MASK}'),
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}'),
+
+      ('{FT_VEC}', '{FT_VEC}', '{FT_VEC}', '{FT_MASK}'),
+      ('{FT_MASK}', '{FT_VEC}', '{FT_VEC}', '{FT_MASK}'),
+      ('{FT_VEC}', '{FT_VEC}', '{FT_SCALAR}', '{FT_MASK}'),
+      ('{FT_MASK}', '{FT_VEC}', '{FT_SCALAR}', '{FT_MASK}'),
+
+      # 4 args.
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', '{FT_VEC}', 'LONG'),
+
+      ('{FT_MASK}', '{FT_MASK}', '{FT_MASK}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{FT_MASK}', '{FT_MASK}', '{FT_MASK}', '{FT_I_VEC}', 'UQI'),
+
+      ('{FT_I_VEC}', '{FT_MASK}', '{FT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{FT_I_VEC}', '{FT_MASK}', '{FT_I_VEC}', '{FT_I_VEC}', 'UQI'),
+      ('{FT_I_VEC}', '{FT_MASK}', '{FT_I_VEC}', '{FT_I_VEC}', '{FT_U_SCALAR}'), # For masked mulh.
+      # Masked Shift
+      ('{FT_U_VEC}', '{FT_MASK}', '{FT_U_VEC}', '{FT_U_VEC}', 'UQI'),
+    ],
+    vals=int_vector_modes)
+
+  # Same as above, but for widening vector.
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      # Vector Integer Extension
+      ('{WFT_VEC}', '{FT_VEC}'),
+      # 2 args.
+      ('{WFT_I_VEC}', '{FT_I_VEC}', '{FT_U_SCALAR}'),
+      ('{WFT_I_VEC}', '{WFT_I_VEC}', '{FT_U_SCALAR}'),
+      ('{WFT_I_VEC}', '{WFT_I_VEC}', '{FT_U_VEC}'),
+      ('{WFT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'),
+
+      ('{FT_I_VEC}', '{WFT_I_VEC}', '{FT_U_VEC}'),
+      ('{FT_I_VEC}', '{WFT_I_VEC}', 'UQI'),
+
+      ('{FT_U_VEC}', '{WFT_U_VEC}', '{FT_U_VEC}'),
+      ('{FT_U_VEC}', '{WFT_U_VEC}', 'UQI'),
+
+      # 3 args.
+      ('{WFT_I_VEC}', '{WFT_I_VEC}', '{FT_U_SCALAR}', '{FT_I_VEC}'),
+      ('{WFT_I_VEC}', '{WFT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{WFT_I_VEC}', '{WFT_I_VEC}', '{FT_SCALAR}', '{FT_U_VEC}'),
+
+      # Maksed Vector Integer Extension
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{FT_VEC}'),
+
+      # 4 args.
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{WFT_VEC}', '{FT_VEC}'),
+      ('{WFT_VEC}', '{FT_MASK}', '{WFT_VEC}', '{WFT_VEC}', '{FT_SCALAR}'),
+
+      ('{WFT_I_VEC}', '{FT_MASK}', '{WFT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{WFT_I_VEC}', '{FT_MASK}', '{WFT_I_VEC}', '{FT_I_VEC}', '{FT_U_SCALAR}'),
+
+      ('{WFT_I_VEC}', '{FT_MASK}', '{WFT_I_VEC}', '{FT_I_SCALAR}', '{FT_U_VEC}'), # For maksed wmac.
+      ('{WFT_I_VEC}', '{FT_MASK}', '{WFT_I_VEC}', '{FT_U_SCALAR}', '{FT_I_VEC}'), # For maksed wmac.
+
+      # Masked Narrowing Vector Arithmetic.
+      ('{FT_I_VEC}', '{FT_MASK}', '{FT_I_VEC}', '{WFT_I_VEC}', '{FT_U_VEC}'),
+      ('{FT_I_VEC}', '{FT_MASK}', '{FT_I_VEC}', '{WFT_I_VEC}', 'UQI'),
+
+      ('{FT_U_VEC}', '{FT_MASK}', '{FT_U_VEC}', '{WFT_U_VEC}', '{FT_U_VEC}'),
+      ('{FT_U_VEC}', '{FT_MASK}', '{FT_U_VEC}', '{WFT_U_VEC}', 'UQI'),
+    ],
+    vals = list(filter(lambda x: x.has_widen_type(), int_vector_modes)))
+
+  # Same as above, but for quad-widening vector.
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      # Vector Integer Extension
+      ('{QFT_VEC}', '{FT_VEC}'),
+      # 2 args.
+      ('{QFT_I_VEC}', '{FT_I_VEC}', '{FT_U_SCALAR}'),
+      ('{QFT_I_VEC}', '{QFT_I_VEC}', '{FT_U_SCALAR}'),
+      ('{QFT_I_VEC}', '{QFT_I_VEC}', '{FT_U_VEC}'),
+      ('{QFT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      # 3 args.
+      ('{QFT_VEC}', '{QFT_VEC}', '{FT_SCALAR}', '{FT_VEC}'),
+      ('{QFT_VEC}', '{QFT_VEC}', '{FT_VEC}', '{FT_VEC}'),
+      ('{QFT_I_VEC}', '{QFT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'),
+      ('{QFT_I_VEC}', '{QFT_I_VEC}', '{FT_I_SCALAR}', '{FT_U_VEC}'),
+      ('{QFT_I_VEC}', '{QFT_I_VEC}', '{FT_U_SCALAR}', '{FT_I_VEC}'),
+
+      # Maksed Vector Integer Extension
+      ('{QFT_VEC}', '{FT_MASK}', '{QFT_VEC}', '{FT_VEC}'),
+
+      # 4 args.
+      ('{QFT_VEC}', '{FT_MASK}', '{QFT_VEC}', '{FT_VEC}', '{FT_VEC}'), # For maksed qmac.
+      ('{QFT_VEC}', '{FT_MASK}', '{QFT_VEC}', '{FT_SCALAR}', '{FT_VEC}'), # For maksed qmac.
+      ('{QFT_I_VEC}', '{FT_MASK}', '{QFT_I_VEC}', '{FT_I_VEC}', '{FT_U_VEC}'), # For maksed qmac.
+      ('{QFT_I_VEC}', '{FT_MASK}', '{QFT_I_VEC}', '{FT_I_SCALAR}', '{FT_U_VEC}'), # For maksed qmac.
+      ('{QFT_I_VEC}', '{FT_MASK}', '{QFT_I_VEC}', '{FT_U_SCALAR}', '{FT_I_VEC}'), # For maksed qmac.
+    ],
+    vals = list(filter(lambda x: x.has_quad_widen_type(), int_vector_modes)))
+
+  # Same as above, but for eightfold-widening vector.
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      # Vector Integer Extension
+      ('{EFT_VEC}', '{FT_VEC}'),
+      # 3 args.
+      # Maksed Vector Integer Extension
+      ('{EFT_VEC}', '{FT_MASK}', '{EFT_VEC}', '{FT_VEC}'),
+    ],
+    vals = list(filter(lambda x: x.has_eightfold_widen_type(), int_vector_modes)))
+
+  # Reinterpret Prototype for integer vector mode only
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      ('{FT_I_VEC}', '{FT_I_XVEC}'),
+      ('{FT_I_VEC}', '{FT_U_XVEC}'),
+      ('{FT_U_VEC}', '{FT_I_XVEC}'),
+      ('{FT_U_VEC}', '{FT_U_XVEC}'),
+    ],
+    vals=int_vector_modes,
+    eew_reinterpret=True)
+
+  # Prototype for floating point vector mode only
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      ('{FT_F_VEC}', '{FT_F_VEC}'),
+      ('{FT_SCALAR}', '{FT_F_VEC}'),
+      ('{FT_F_VEC}', '{FT_SCALAR}'),
+      ('{FT_F_VEC}', '{FT_I_VEC}'),
+      ('{FT_F_VEC}', '{FT_U_VEC}'),
+      ('{FT_I_VEC}', '{FT_F_VEC}'),
+      ('{FT_U_VEC}', '{FT_F_VEC}'),
+
+      # 2 args.
+      ('{FT_F_VEC}', '{FT_F_VEC}', '{FT_U_VEC}'),
+      ('{FT_F_VEC}', '{FT_F_VEC}', '{FT_U_SCALAR}'),
+
+      # 3 args.
+      ('{FT_I_VEC}', '{FT_MASK}', '{FT_I_VEC}', '{FT_F_VEC}'),
+      ('{FT_U_VEC}', '{FT_MASK}', '{FT_U_VEC}', '{FT_F_VEC}'),
+
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{FT_F_SCALAR}'),
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{FT_I_VEC}'),
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{FT_U_VEC}'),
+
+      # 4 args.
+      # Vector Register Gather.
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{FT_F_VEC}', '{FT_U_VEC}'),
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{FT_F_VEC}', '{FT_U_SCALAR}'),
+    ],
+    vals=float_vector_modes)
+
+  # Same as above, but for widening vector.
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      # Widening Floating-Point/Integer Type-Convert.
+      ('{WFT_F_VEC}', '{FT_F_VEC}'),
+      ('{WFT_F_VEC}', '{FT_I_VEC}'),
+      ('{WFT_F_VEC}', '{FT_U_VEC}'),
+
+      ('{WFT_I_VEC}', '{FT_F_VEC}'),
+      ('{WFT_U_VEC}', '{FT_F_VEC}'),
+
+      # Narrowing Floating-Point/Integer Type-Convert.
+      ('{FT_F_VEC}', '{WFT_F_VEC}'),
+      ('{FT_F_VEC}', '{WFT_I_VEC}'),
+      ('{FT_F_VEC}', '{WFT_U_VEC}'),
+
+      ('{FT_I_VEC}', '{WFT_F_VEC}'),
+      ('{FT_U_VEC}', '{WFT_F_VEC}'),
+
+      # 3 args.
+      # Masked Widening Floating-Point/Integer Type-Convert.
+      ('{WFT_F_VEC}', '{FT_MASK}', '{WFT_F_VEC}', '{FT_F_VEC}'),
+      ('{WFT_F_VEC}', '{FT_MASK}', '{WFT_F_VEC}', '{FT_I_VEC}'),
+      ('{WFT_F_VEC}', '{FT_MASK}', '{WFT_F_VEC}', '{FT_U_VEC}'),
+
+      ('{WFT_I_VEC}', '{FT_MASK}', '{WFT_I_VEC}', '{FT_F_VEC}'),
+      ('{WFT_U_VEC}', '{FT_MASK}', '{WFT_U_VEC}', '{FT_F_VEC}'),
+
+      # Masked Narrowing Floating-Point/Integer Type-Convert.
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{WFT_I_VEC}'),
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{WFT_U_VEC}'),
+      ('{FT_F_VEC}', '{FT_MASK}', '{FT_F_VEC}', '{WFT_F_VEC}'),
+
+      ('{FT_I_VEC}', '{FT_MASK}', '{FT_I_VEC}', '{WFT_F_VEC}'),
+      ('{FT_U_VEC}', '{FT_MASK}', '{FT_U_VEC}', '{WFT_F_VEC}'),
+    ],
+    vals = list(filter(lambda x: x.has_widen_type(), float_vector_modes)))
+
+  # Reinterpret Prototype for floating point vector mode only
+  ft.add(
+    arg_list_templates=[
+      # 1 args.
+      ('{FT_VEC}', '{FT_XVEC}'),
+    ],
+    vals=float_vector_modes,
+    eew_reinterpret=True)
+
+  # Prototype for vector tuple modes, both for integer and floating point
+  ft.add(
+    arg_list_templates=[
+      # 0 args.
+      ('{FT_VEC_TUPLE}',),
+      # 1 args.
+      ('{FT_VEC_TUPLE}', 'C_{FT_SCALAR}_PTR'), # For vec tuple load
+
+      # 2 args.
+      ('VOID', '{FT_VEC_TUPLE}', '{FT_SCALAR}_PTR'), # For vec tuple store
+
+      ('{FT_VEC}', '{FT_VEC_TUPLE}', 'SI'), # For vec tuple extraction.
+
+      ('{FT_VEC_TUPLE}', 'C_{FT_SCALAR}_PTR', 'PTRDIFF'), # For vec tuple indexed load
+
+      # 3 args.
+      # Vector Tuple Type Insertion
+      ('{FT_VEC_TUPLE}', '{FT_VEC_TUPLE}', '{FT_VEC}', 'SI'),
+
+      # Masked Unit-Stride Segment Load
+      ('{FT_VEC_TUPLE}', '{FT_MASK}', '{FT_VEC_TUPLE}', 'C_{FT_SCALAR}_PTR'),
+
+      # Vector Unit-Stride Segment Store.
+      ('VOID', '{FT_MASK}', '{FT_VEC_TUPLE}', '{FT_SCALAR}_PTR'),
+
+      # Vector Strided Segment Store.
+      ('VOID', '{FT_VEC_TUPLE}', '{FT_SCALAR}_PTR', 'PTRDIFF'),
+
+      # 4 args.
+      # Masked Vector Strided Segment Load.
+      ('{FT_VEC_TUPLE}', '{FT_MASK}', '{FT_VEC_TUPLE}', 'C_{FT_SCALAR}_PTR', 'PTRDIFF'),
+      # Masked Vector Strided Segment Store.
+      ('VOID', '{FT_MASK}', '{FT_VEC_TUPLE}', '{FT_SCALAR}_PTR', 'PTRDIFF'),
+    ],
+    vals=vector_tuple_modes)
+
+  # Prototype for vector modes, both for integer and floating point,
+  # and with EEW
+  ft.add(
+    arg_list_templates=[
+      # 2 args.
+      # Vector Indexed Load.
+      ('{FT_VEC}', 'C_{FT_SCALAR}_PTR', '{FT_INDEX}'),
+
+      # 3 args.
+      # Vector Indexed Store.
+      ('VOID', '{FT_SCALAR}_PTR', '{FT_INDEX}', '{FT_VEC}'),
+
+      # Vector AMO.
+      ('{FT_VEC}', 'C_{FT_SCALAR}_PTR', '{FT_INDEX}', '{FT_VEC}'),
+
+      # 4 args.
+      # Masked Vector Indexed Load.
+      ('{FT_VEC}', '{FT_MASK}', '{FT_VEC}', 'C_{FT_SCALAR}_PTR', '{FT_INDEX}'),
+
+      # Masked Vector Indexed Store.
+      ('VOID', '{FT_MASK}', '{FT_SCALAR}_PTR', '{FT_INDEX}', '{FT_VEC}'),
+
+      # Masked Vector AMO.
+      ('{FT_VEC}', '{FT_MASK}', 'C_{FT_SCALAR}_PTR', '{FT_INDEX}', '{FT_VEC}'),
+    ],
+    vals=vector_modes,
+    eew_index=True)
+
+  # Prototype for Vector Tuple Type Creation.
+  for nf in NF:
+    args = ['{FT_VEC_TUPLE}'] + (['{FT_VEC}'] * nf)
+    ft.add(
+      arg_list_templates=[
+        tuple(args)
+      ],
+      vals=vector_tuple_modes)
+
+  # Prototype for Vector Indexed Segment Loads and Stores.
+  ft.add(
+    arg_list_templates=[
+      # 2 args.
+      # Vector Indexed Segment Load.
+      ('{FT_VEC_TUPLE}', 'C_{FT_SCALAR}_PTR', '{FT_INDEX}'),
+
+      # 3 args.
+      # Vector Indexed Segment Store.
+      ('VOID', '{FT_VEC_TUPLE}', '{FT_SCALAR}_PTR', '{FT_INDEX}'),
+
+      # 4 args.
+      # Masked Vector Indexed Segment Load.
+      ('{FT_VEC_TUPLE}', '{FT_MASK}', '{FT_VEC_TUPLE}', 'C_{FT_SCALAR}_PTR', '{FT_INDEX}'),
+      # Masked Vector Indexed Segment Store.
+      ('VOID', '{FT_MASK}', '{FT_VEC_TUPLE}', '{FT_SCALAR}_PTR', '{FT_INDEX}'),
+    ],
+    vals=vector_tuple_modes,
+    eew_index=True)
+
+  ft.gen()
+
+def gen(mode, filename):
+  this_script = os.path.abspath(__file__)
+  cwd = os.path.dirname(this_script)
+  py = sys.executable
+  output_path = os.path.join(cwd, filename)
+  subprocess.call([py, this_script, mode],
+                   stdout=open(output_path, 'w'),
+                  cwd=cwd)
+
+if __name__ == '__main__':
+  if sys.argv[1] == '-md':
+    gen_md()
+  elif sys.argv[1] == '-c':
+    gen_c()
+  elif sys.argv[1] == '-ftype':
+    gen_ftype()
+  elif sys.argv[1] == '-usr-c':
+    gen_usr_c()
+  elif sys.argv[1] == '-all':
+    gen('-md', 'vector-iterator.md')
+    gen('-c', 'riscv-vector-iterator.h')
+    gen('-ftype', 'riscv-vector-ftypes.def')
+    gen('-usr-c', 'riscv_vector_itr.h')
+  else:
+    print ("Usage: %s [-md|-c|-ftype|-usr-c|-all]" % sys.argv[0])
+    sys.exit(1)
diff --git a/gcc/config/riscv/predicates.md b/gcc/config/riscv/predicates.md
index f764fe7ba01..b5d7beaf6c4 100644
--- a/gcc/config/riscv/predicates.md
+++ b/gcc/config/riscv/predicates.md
@@ -55,10 +55,29 @@
   (and (match_code "const_int,const_wide_int,const_double,const_vector")
        (match_test "op == CONST0_RTX (GET_MODE (op))")))
 
+(define_predicate "const_1_operand"
+  (and (match_code "const_int,const_wide_int,const_double,const_vector")
+       (match_test "op == CONST1_RTX (GET_MODE (op))")))
+
 (define_predicate "reg_or_0_operand"
   (ior (match_operand 0 "const_0_operand")
        (match_operand 0 "register_operand")))
 
+(define_predicate "reg_or_mem_operand"
+  (ior (match_operand 0 "memory_operand")
+       (match_operand 0 "register_operand")))
+
+(define_predicate "uimm5_operand"
+  (match_operand 0 "const_csr_operand"))
+
+(define_predicate "reg_or_uimm5_operand"
+  (match_operand 0 "csr_operand"))
+
+(define_predicate "reg_or_simm5_operand"
+  (ior (match_operand 0 "register_operand")
+       (and (match_operand 0 "const_int_operand")
+	    (match_test "IN_RANGE (INTVAL (op), -16, 15)"))))
+
 ;; Only use branch-on-bit sequences when the mask is not an ANDI immediate.
 (define_predicate "branch_on_bit_operand"
   (and (match_code "const_int")
@@ -71,7 +90,7 @@
 {
   /* Don't handle multi-word moves this way; we don't want to introduce
      the individual word-mode moves until after reload.  */
-  if (GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+  if (GET_MODE_SIZE (mode).to_constant () > UNITS_PER_WORD)
     return false;
 
   /* Otherwise check whether the constant can be loaded in a single
@@ -141,6 +160,9 @@
     case CONST_INT:
       return !splittable_const_int_operand (op, mode);
 
+    case CONST_POLY_INT:
+      return satisfies_constraint_vp (op);
+
     case CONST:
     case SYMBOL_REF:
     case LABEL_REF:
@@ -192,6 +214,12 @@
 (define_predicate "equality_operator"
   (match_code "eq,ne"))
 
+(define_predicate "ltge_operator"
+  (match_code "lt,ltu,ge,geu"))
+
+(define_predicate "vector_comparison_operator"
+  (match_code "eq,ne,le,leu,gt,gtu"))
+
 (define_predicate "order_operator"
   (match_code "eq,ne,lt,ltu,le,leu,ge,geu,gt,gtu"))
 
@@ -212,3 +240,73 @@
 {
   return riscv_gpr_save_operation_p (op);
 })
+
+;; Vector predicates.
+
+(define_predicate "const_poly_int_operand"
+  (match_code "const_poly_int"))
+
+(define_predicate "const_vector_int_operand"
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, -16, 15)")))
+
+(define_predicate "const_vector_int_0_operand"
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, 0, 0)")))
+
+(define_predicate "const_vector_int_1_operand"
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, 1, 1)")))
+
+(define_predicate "vector_arith_operand"
+  (ior (match_operand 0 "const_vector_int_operand")
+       (match_operand 0 "register_operand")))
+
+;; A negated const_vector_int_operand, for subtract.
+(define_predicate "neg_const_vector_int_operand"
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, -15, 16)")))
+
+;; A negated vector_arith_operand, for subtract.
+(define_predicate "neg_vector_arith_operand"
+  (ior (match_operand 0 "neg_const_vector_int_operand")
+       (match_operand 0 "register_operand")))
+
+(define_predicate "vector_move_operand"
+  (ior (match_operand 0 "nonimmediate_operand")
+       (match_test "const_vec_duplicate_p (op)")))
+
+(define_predicate "const_vector_shift_operand"
+  (and (match_code "const_vector")
+       (match_test "riscv_const_vec_all_same_in_range_p (op, 0, 31)")))
+
+(define_predicate "vector_shift_operand"
+  (ior (match_operand 0 "const_vector_shift_operand")
+       (match_operand 0 "register_operand")))
+
+(define_predicate "ltge_const_vector_int_operand"
+  (match_code "const_vector")
+{
+  op = unwrap_const_vec_duplicate (op);
+  return CONST_INT_P (op) && IN_RANGE (INTVAL (op), -15, 16);
+})
+
+(define_predicate "ltge_vector_arith_operand"
+  (ior (match_operand 0 "ltge_const_vector_int_operand")
+       (match_operand 0 "register_operand")))
+
+(define_predicate "shift_b_operand"
+  (and (match_code "const_int")
+       (match_test "INTVAL (op) == 8")))
+
+(define_predicate "shift_h_operand"
+  (and (match_code "const_int")
+       (match_test "INTVAL (op) == 16")))
+
+(define_predicate "shift_w_operand"
+  (and (match_code "const_int")
+       (match_test "INTVAL (op) == 32")))
+
+(define_predicate "shift_d_operand"
+  (and (match_code "const_int")
+       (match_test "INTVAL (op) == 64")))
diff --git a/gcc/config/riscv/riscv-builtins.c b/gcc/config/riscv/riscv-builtins.c
index bc330590d82..e640401ca45 100644
--- a/gcc/config/riscv/riscv-builtins.c
+++ b/gcc/config/riscv/riscv-builtins.c
@@ -36,10 +36,70 @@ along with GCC; see the file COPYING3.  If not see
 #include "stor-layout.h"
 #include "expr.h"
 #include "langhooks.h"
+#include "stringpool.h"
+#include "riscv-vector-iterator.h"
+
+/* We don't want the PTR definition from ansi-decl.h.  */
+#undef PTR
+
+/* An iterator to call a macro with every supported scalar integer mode with
+   its size in bits..  */
+#define _SCALAR_INT_ITERATOR(MACRO)	\
+  MACRO (8, QI)				\
+  MACRO (16, HI)			\
+  MACRO (32, SI)			\
+  MACRO (64, DI)
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   for shift operations, along with its corresponding vector
+   and integer modes.  */
+#define _RVV_INT_ITERATOR_SHIFT(MACRO)	\
+  MACRO (16, 1, 16,  vnx8hi, HI)	\
+  MACRO (32, 1, 32,  vnx4si, SI)	\
+  MACRO (32, 2, 16,  vnx8si, SI)	\
+  MACRO (32, 4,  8, vnx16si, SI)	\
+  MACRO (32, 8,  4, vnx32si, SI)	\
+  MACRO (64, 1, 64,  vnx2di, DI)	\
+  MACRO (64, 2, 32,  vnx4di, DI)	\
+  MACRO (64, 4, 16,  vnx8di, DI)	\
+  MACRO (64, 8,  8, vnx16di, DI)
+
+/* An iterator to call a macro with every supported MLEN and internal
+   type numbering on VNx<N>BI for vector masking modes.  */
+#define _RVV_MASK_ITERATOR(MACRO)	\
+  MACRO (1, 128)			\
+  MACRO (2, 64)				\
+  MACRO (4, 32)				\
+  MACRO (8, 16)				\
+  MACRO (16, 8)				\
+  MACRO (32, 4)				\
+  MACRO (64, 2)
+
+/* An iterator to call a macro with every supported MLEN and internal
+   type numbering on VNx<N>BI with extra arguments for vector masking modes.  */
+#define _RVV_MASK_ITERATOR_ARG(MACRO, ...)	\
+  MACRO (1, 128, __VA_ARGS__)			\
+  MACRO (2, 64, __VA_ARGS__)			\
+  MACRO (4, 32, __VA_ARGS__)			\
+  MACRO (8, 16, __VA_ARGS__)			\
+  MACRO (16, 8, __VA_ARGS__)			\
+  MACRO (32, 4, __VA_ARGS__)			\
+  MACRO (64, 2, __VA_ARGS__)
 
 /* Macros to create an enumeration identifier for a function prototype.  */
 #define RISCV_FTYPE_NAME0(A) RISCV_##A##_FTYPE
 #define RISCV_FTYPE_NAME1(A, B) RISCV_##A##_FTYPE_##B
+#define RISCV_FTYPE_NAME2(A, B, C) RISCV_##A##_FTYPE_##B##_##C
+#define RISCV_FTYPE_NAME3(A, B, C, D) RISCV_##A##_FTYPE_##B##_##C##_##D
+#define RISCV_FTYPE_NAME4(A, B, C, D, E) RISCV_##A##_FTYPE_##B##_##C##_##D##_##E
+#define RISCV_FTYPE_NAME5(A, B, C, D, E, F) \
+  RISCV_##A##_FTYPE_##B##_##C##_##D##_##E##_##F
+#define RISCV_FTYPE_NAME6(A, B, C, D, E, F, G) \
+  RISCV_##A##_FTYPE_##B##_##C##_##D##_##E##_##F##_##G
+#define RISCV_FTYPE_NAME7(A, B, C, D, E, F, G, H) \
+  RISCV_##A##_FTYPE_##B##_##C##_##D##_##E##_##F##_##G##_##H
+#define RISCV_FTYPE_NAME8(A, B, C, D, E, F, G, H, I) \
+  RISCV_##A##_FTYPE_##B##_##C##_##D##_##E##_##F##_##G##_##H##_##I
 
 /* Classifies the prototype of a built-in function.  */
 enum riscv_function_type {
@@ -55,7 +115,10 @@ enum riscv_builtin_type {
   RISCV_BUILTIN_DIRECT,
 
   /* Likewise, but with return type VOID.  */
-  RISCV_BUILTIN_DIRECT_NO_TARGET
+  RISCV_BUILTIN_DIRECT_NO_TARGET,
+  RISCV_BUILTIN_SHIFT_SCALAR,
+  RISCV_BUILTIN_SHIFT_MASK_SCALAR,
+  RISCV_BUILTIN_MV_XS
 };
 
 /* Declare an availability predicate for built-in functions.  */
@@ -86,6 +149,7 @@ struct riscv_builtin_description {
 };
 
 AVAIL (hard_float, TARGET_HARD_FLOAT)
+AVAIL (vector, TARGET_VECTOR)
 
 /* Construct a riscv_builtin_description from the given arguments.
 
@@ -99,7 +163,7 @@ AVAIL (hard_float, TARGET_HARD_FLOAT)
 
    AVAIL is the name of the availability predicate, without the leading
    riscv_builtin_avail_.  */
-#define RISCV_BUILTIN(INSN, NAME, BUILTIN_TYPE,	FUNCTION_TYPE, AVAIL)	\
+#define RISCV_BUILTIN(INSN, NAME, BUILTIN_TYPE, FUNCTION_TYPE, AVAIL)	\
   { CODE_FOR_riscv_ ## INSN, "__builtin_riscv_" NAME,			\
     BUILTIN_TYPE, FUNCTION_TYPE, riscv_builtin_avail_ ## AVAIL }
 
@@ -114,11 +178,348 @@ AVAIL (hard_float, TARGET_HARD_FLOAT)
    and AVAIL are as for RISCV_BUILTIN.  */
 #define DIRECT_NO_TARGET_BUILTIN(INSN, FUNCTION_TYPE, AVAIL)		\
   RISCV_BUILTIN (INSN, #INSN, RISCV_BUILTIN_DIRECT_NO_TARGET,		\
-		FUNCTION_TYPE, AVAIL)
+		 FUNCTION_TYPE, AVAIL)
+
+/* Same as above, but for using named patterns in the md file.  It drops the
+   riscv after CODE_FOR_, but it is otherwise the same.  */
+#define RISCV_NAMED(INSN, NAME, BUILTIN_TYPE, FUNCTION_TYPE, AVAIL)	\
+  { CODE_FOR_ ## INSN, "__builtin_riscv_" NAME,			\
+    BUILTIN_TYPE, FUNCTION_TYPE, riscv_builtin_avail_ ## AVAIL }
+
+/* This has an extra NAME argument, as the builtin name and the pattern name
+   are constructed differently.  */
+#define DIRECT_NAMED(INSN, NAME, FUNCTION_TYPE, AVAIL)			\
+  RISCV_NAMED (INSN, #NAME, RISCV_BUILTIN_DIRECT, FUNCTION_TYPE, AVAIL)
+
+/* This has an extra NAME argument, as the builtin name and the pattern name
+   are constructed differently.  */
+#define DIRECT_NAMED_NO_TARGET(INSN, NAME, FUNCTION_TYPE, AVAIL)	\
+  RISCV_NAMED (INSN, #NAME, RISCV_BUILTIN_DIRECT_NO_TARGET,		\
+	       FUNCTION_TYPE, AVAIL)
+
+#define SHIFT_NAMED(INSN, NAME, FUNCTION_TYPE, AVAIL)			\
+  RISCV_NAMED (INSN, #NAME, RISCV_BUILTIN_SHIFT_SCALAR, FUNCTION_TYPE, AVAIL)
+
+#define SHIFT_MASK_NAMED(INSN, NAME, FUNCTION_TYPE, AVAIL)		\
+  RISCV_NAMED (INSN, #NAME, RISCV_BUILTIN_SHIFT_MASK_SCALAR,		\
+	       FUNCTION_TYPE, AVAIL)
+
+#define MV_XS_NAMED(INSN, NAME, FUNCTION_TYPE, AVAIL)			\
+  RISCV_NAMED (INSN, #NAME, RISCV_BUILTIN_MV_XS, FUNCTION_TYPE, AVAIL)
 
 /* Argument types.  */
 #define RISCV_ATYPE_VOID void_type_node
-#define RISCV_ATYPE_USI unsigned_intSI_type_node
+#define RISCV_ATYPE_UQI unsigned_int8_type_node
+#define RISCV_ATYPE_UHI unsigned_int16_type_node
+#define RISCV_ATYPE_USI unsigned_int32_type_node
+#define RISCV_ATYPE_UDI unsigned_int64_type_node
+#define RISCV_ATYPE_QI int8_type_node
+#define RISCV_ATYPE_HI int16_type_node
+#define RISCV_ATYPE_SI int32_type_node
+#define RISCV_ATYPE_DI int64_type_node
+#define RISCV_ATYPE_SIZE size_type_node
+#define RISCV_ATYPE_LONG long_integer_type_node
+
+#define RISCV_ATYPE_PTRDIFF ptrdiff_type_node
+
+#define RISCV_ATYPE_QI_PTR intQI_ptr_type_node
+#define RISCV_ATYPE_HI_PTR intHI_ptr_type_node
+#define RISCV_ATYPE_SI_PTR intSI_ptr_type_node
+#define RISCV_ATYPE_DI_PTR intDI_ptr_type_node
+#define RISCV_ATYPE_UQI_PTR unsigned_intQI_ptr_type_node
+#define RISCV_ATYPE_UHI_PTR unsigned_intHI_ptr_type_node
+#define RISCV_ATYPE_USI_PTR unsigned_intSI_ptr_type_node
+#define RISCV_ATYPE_UDI_PTR unsigned_intDI_ptr_type_node
+
+#define RISCV_ATYPE_C_QI_PTR const_intQI_ptr_type_node
+#define RISCV_ATYPE_C_HI_PTR const_intHI_ptr_type_node
+#define RISCV_ATYPE_C_SI_PTR const_intSI_ptr_type_node
+#define RISCV_ATYPE_C_DI_PTR const_intDI_ptr_type_node
+#define RISCV_ATYPE_C_UQI_PTR const_unsigned_intQI_ptr_type_node
+#define RISCV_ATYPE_C_UHI_PTR const_unsigned_intHI_ptr_type_node
+#define RISCV_ATYPE_C_USI_PTR const_unsigned_intSI_ptr_type_node
+#define RISCV_ATYPE_C_UDI_PTR const_unsigned_intDI_ptr_type_node
+
+#define RISCV_ATYPE_HF fp16_type_node
+#define RISCV_ATYPE_SF float_type_node
+#define RISCV_ATYPE_DF double_type_node
+
+#define RISCV_ATYPE_HF_PTR float16_ptr_type_node
+#define RISCV_ATYPE_SF_PTR float_ptr_type_node
+#define RISCV_ATYPE_DF_PTR double_ptr_type_node
+
+#define RISCV_ATYPE_C_HF_PTR const_float16_ptr_type_node
+#define RISCV_ATYPE_C_SF_PTR const_float_ptr_type_node
+#define RISCV_ATYPE_C_DF_PTR const_double_ptr_type_node
+
+#define RISCV_ATYPE_VF16M1 rvvfloat16m1_t_node
+#define RISCV_ATYPE_VF16M2 rvvfloat16m2_t_node
+#define RISCV_ATYPE_VF16M4 rvvfloat16m4_t_node
+#define RISCV_ATYPE_VF16M8 rvvfloat16m8_t_node
+#define RISCV_ATYPE_VF32M1 rvvfloat32m1_t_node
+#define RISCV_ATYPE_VF32M2 rvvfloat32m2_t_node
+#define RISCV_ATYPE_VF32M4 rvvfloat32m4_t_node
+#define RISCV_ATYPE_VF32M8 rvvfloat32m8_t_node
+#define RISCV_ATYPE_VF64M1 rvvfloat64m1_t_node
+#define RISCV_ATYPE_VF64M2 rvvfloat64m2_t_node
+#define RISCV_ATYPE_VF64M4 rvvfloat64m4_t_node
+#define RISCV_ATYPE_VF64M8 rvvfloat64m8_t_node
+
+#define RISCV_ATYPE_VB1 rvvbool1_t_node
+#define RISCV_ATYPE_VB2 rvvbool2_t_node
+#define RISCV_ATYPE_VB4 rvvbool4_t_node
+#define RISCV_ATYPE_VB8 rvvbool8_t_node
+#define RISCV_ATYPE_VB16 rvvbool16_t_node
+#define RISCV_ATYPE_VB32 rvvbool32_t_node
+#define RISCV_ATYPE_VB64 rvvbool64_t_node
+
+#define RISCV_ATYPE_VI8M1 rvvint8m1_t_node
+#define RISCV_ATYPE_VI8M2 rvvint8m2_t_node
+#define RISCV_ATYPE_VI8M4 rvvint8m4_t_node
+#define RISCV_ATYPE_VI8M8 rvvint8m8_t_node
+#define RISCV_ATYPE_VI16M1 rvvint16m1_t_node
+#define RISCV_ATYPE_VI16M2 rvvint16m2_t_node
+#define RISCV_ATYPE_VI16M4 rvvint16m4_t_node
+#define RISCV_ATYPE_VI16M8 rvvint16m8_t_node
+#define RISCV_ATYPE_VI32M1 rvvint32m1_t_node
+#define RISCV_ATYPE_VI32M2 rvvint32m2_t_node
+#define RISCV_ATYPE_VI32M4 rvvint32m4_t_node
+#define RISCV_ATYPE_VI32M8 rvvint32m8_t_node
+#define RISCV_ATYPE_VI64M1 rvvint64m1_t_node
+#define RISCV_ATYPE_VI64M2 rvvint64m2_t_node
+#define RISCV_ATYPE_VI64M4 rvvint64m4_t_node
+#define RISCV_ATYPE_VI64M8 rvvint64m8_t_node
+
+#define RISCV_ATYPE_VUI8M1 rvvuint8m1_t_node
+#define RISCV_ATYPE_VUI8M2 rvvuint8m2_t_node
+#define RISCV_ATYPE_VUI8M4 rvvuint8m4_t_node
+#define RISCV_ATYPE_VUI8M8 rvvuint8m8_t_node
+#define RISCV_ATYPE_VUI16M1 rvvuint16m1_t_node
+#define RISCV_ATYPE_VUI16M2 rvvuint16m2_t_node
+#define RISCV_ATYPE_VUI16M4 rvvuint16m4_t_node
+#define RISCV_ATYPE_VUI16M8 rvvuint16m8_t_node
+#define RISCV_ATYPE_VUI32M1 rvvuint32m1_t_node
+#define RISCV_ATYPE_VUI32M2 rvvuint32m2_t_node
+#define RISCV_ATYPE_VUI32M4 rvvuint32m4_t_node
+#define RISCV_ATYPE_VUI32M8 rvvuint32m8_t_node
+#define RISCV_ATYPE_VUI64M1 rvvuint64m1_t_node
+#define RISCV_ATYPE_VUI64M2 rvvuint64m2_t_node
+#define RISCV_ATYPE_VUI64M4 rvvuint64m4_t_node
+#define RISCV_ATYPE_VUI64M8 rvvuint64m8_t_node
+
+#define RISCV_ATYPE_VI8M1X2  rvvint8m1x2_t_node
+#define RISCV_ATYPE_VUI8M1X2 rvvuint8m1x2_t_node
+#define RISCV_ATYPE_VI8M1X3  rvvint8m1x3_t_node
+#define RISCV_ATYPE_VUI8M1X3 rvvuint8m1x3_t_node
+#define RISCV_ATYPE_VI8M1X4  rvvint8m1x4_t_node
+#define RISCV_ATYPE_VUI8M1X4 rvvuint8m1x4_t_node
+#define RISCV_ATYPE_VI8M1X5  rvvint8m1x5_t_node
+#define RISCV_ATYPE_VUI8M1X5 rvvuint8m1x5_t_node
+#define RISCV_ATYPE_VI8M1X6  rvvint8m1x6_t_node
+#define RISCV_ATYPE_VUI8M1X6 rvvuint8m1x6_t_node
+#define RISCV_ATYPE_VI8M1X7  rvvint8m1x7_t_node
+#define RISCV_ATYPE_VUI8M1X7 rvvuint8m1x7_t_node
+#define RISCV_ATYPE_VI8M1X8  rvvint8m1x8_t_node
+#define RISCV_ATYPE_VUI8M1X8 rvvuint8m1x8_t_node
+#define RISCV_ATYPE_VI8M2X2  rvvint8m2x2_t_node
+#define RISCV_ATYPE_VUI8M2X2 rvvuint8m2x2_t_node
+#define RISCV_ATYPE_VI8M2X3  rvvint8m2x3_t_node
+#define RISCV_ATYPE_VUI8M2X3 rvvuint8m2x3_t_node
+#define RISCV_ATYPE_VI8M2X4  rvvint8m2x4_t_node
+#define RISCV_ATYPE_VUI8M2X4 rvvuint8m2x4_t_node
+#define RISCV_ATYPE_VI8M4X2  rvvint8m4x2_t_node
+#define RISCV_ATYPE_VUI8M4X2 rvvuint8m4x2_t_node
+#define RISCV_ATYPE_VI16M1X2  rvvint16m1x2_t_node
+#define RISCV_ATYPE_VUI16M1X2 rvvuint16m1x2_t_node
+#define RISCV_ATYPE_VF16M1X2  rvvfloat16m1x2_t_node
+#define RISCV_ATYPE_VI16M1X3  rvvint16m1x3_t_node
+#define RISCV_ATYPE_VUI16M1X3 rvvuint16m1x3_t_node
+#define RISCV_ATYPE_VF16M1X3  rvvfloat16m1x3_t_node
+#define RISCV_ATYPE_VI16M1X4  rvvint16m1x4_t_node
+#define RISCV_ATYPE_VUI16M1X4 rvvuint16m1x4_t_node
+#define RISCV_ATYPE_VF16M1X4  rvvfloat16m1x4_t_node
+#define RISCV_ATYPE_VI16M1X5  rvvint16m1x5_t_node
+#define RISCV_ATYPE_VUI16M1X5 rvvuint16m1x5_t_node
+#define RISCV_ATYPE_VF16M1X5  rvvfloat16m1x5_t_node
+#define RISCV_ATYPE_VI16M1X6  rvvint16m1x6_t_node
+#define RISCV_ATYPE_VUI16M1X6 rvvuint16m1x6_t_node
+#define RISCV_ATYPE_VF16M1X6  rvvfloat16m1x6_t_node
+#define RISCV_ATYPE_VI16M1X7  rvvint16m1x7_t_node
+#define RISCV_ATYPE_VUI16M1X7 rvvuint16m1x7_t_node
+#define RISCV_ATYPE_VF16M1X7  rvvfloat16m1x7_t_node
+#define RISCV_ATYPE_VI16M1X8  rvvint16m1x8_t_node
+#define RISCV_ATYPE_VUI16M1X8 rvvuint16m1x8_t_node
+#define RISCV_ATYPE_VF16M1X8  rvvfloat16m1x8_t_node
+#define RISCV_ATYPE_VI16M2X2  rvvint16m2x2_t_node
+#define RISCV_ATYPE_VUI16M2X2 rvvuint16m2x2_t_node
+#define RISCV_ATYPE_VF16M2X2  rvvfloat16m2x2_t_node
+#define RISCV_ATYPE_VI16M2X3  rvvint16m2x3_t_node
+#define RISCV_ATYPE_VUI16M2X3 rvvuint16m2x3_t_node
+#define RISCV_ATYPE_VF16M2X3  rvvfloat16m2x3_t_node
+#define RISCV_ATYPE_VI16M2X4  rvvint16m2x4_t_node
+#define RISCV_ATYPE_VUI16M2X4 rvvuint16m2x4_t_node
+#define RISCV_ATYPE_VF16M2X4  rvvfloat16m2x4_t_node
+#define RISCV_ATYPE_VI16M4X2  rvvint16m4x2_t_node
+#define RISCV_ATYPE_VUI16M4X2 rvvuint16m4x2_t_node
+#define RISCV_ATYPE_VF16M4X2  rvvfloat16m4x2_t_node
+#define RISCV_ATYPE_VI32M1X2  rvvint32m1x2_t_node
+#define RISCV_ATYPE_VUI32M1X2 rvvuint32m1x2_t_node
+#define RISCV_ATYPE_VF32M1X2  rvvfloat32m1x2_t_node
+#define RISCV_ATYPE_VI32M1X3  rvvint32m1x3_t_node
+#define RISCV_ATYPE_VUI32M1X3 rvvuint32m1x3_t_node
+#define RISCV_ATYPE_VF32M1X3  rvvfloat32m1x3_t_node
+#define RISCV_ATYPE_VI32M1X4  rvvint32m1x4_t_node
+#define RISCV_ATYPE_VUI32M1X4 rvvuint32m1x4_t_node
+#define RISCV_ATYPE_VF32M1X4  rvvfloat32m1x4_t_node
+#define RISCV_ATYPE_VI32M1X5  rvvint32m1x5_t_node
+#define RISCV_ATYPE_VUI32M1X5 rvvuint32m1x5_t_node
+#define RISCV_ATYPE_VF32M1X5  rvvfloat32m1x5_t_node
+#define RISCV_ATYPE_VI32M1X6  rvvint32m1x6_t_node
+#define RISCV_ATYPE_VUI32M1X6 rvvuint32m1x6_t_node
+#define RISCV_ATYPE_VF32M1X6  rvvfloat32m1x6_t_node
+#define RISCV_ATYPE_VI32M1X7  rvvint32m1x7_t_node
+#define RISCV_ATYPE_VUI32M1X7 rvvuint32m1x7_t_node
+#define RISCV_ATYPE_VF32M1X7  rvvfloat32m1x7_t_node
+#define RISCV_ATYPE_VI32M1X8  rvvint32m1x8_t_node
+#define RISCV_ATYPE_VUI32M1X8 rvvuint32m1x8_t_node
+#define RISCV_ATYPE_VF32M1X8  rvvfloat32m1x8_t_node
+#define RISCV_ATYPE_VI32M2X2  rvvint32m2x2_t_node
+#define RISCV_ATYPE_VUI32M2X2 rvvuint32m2x2_t_node
+#define RISCV_ATYPE_VF32M2X2  rvvfloat32m2x2_t_node
+#define RISCV_ATYPE_VI32M2X3  rvvint32m2x3_t_node
+#define RISCV_ATYPE_VUI32M2X3 rvvuint32m2x3_t_node
+#define RISCV_ATYPE_VF32M2X3  rvvfloat32m2x3_t_node
+#define RISCV_ATYPE_VI32M2X4  rvvint32m2x4_t_node
+#define RISCV_ATYPE_VUI32M2X4 rvvuint32m2x4_t_node
+#define RISCV_ATYPE_VF32M2X4  rvvfloat32m2x4_t_node
+#define RISCV_ATYPE_VI32M4X2  rvvint32m4x2_t_node
+#define RISCV_ATYPE_VUI32M4X2 rvvuint32m4x2_t_node
+#define RISCV_ATYPE_VF32M4X2  rvvfloat32m4x2_t_node
+#define RISCV_ATYPE_VI64M1X2  rvvint64m1x2_t_node
+#define RISCV_ATYPE_VUI64M1X2 rvvuint64m1x2_t_node
+#define RISCV_ATYPE_VF64M1X2  rvvfloat64m1x2_t_node
+#define RISCV_ATYPE_VI64M1X3  rvvint64m1x3_t_node
+#define RISCV_ATYPE_VUI64M1X3 rvvuint64m1x3_t_node
+#define RISCV_ATYPE_VF64M1X3  rvvfloat64m1x3_t_node
+#define RISCV_ATYPE_VI64M1X4  rvvint64m1x4_t_node
+#define RISCV_ATYPE_VUI64M1X4 rvvuint64m1x4_t_node
+#define RISCV_ATYPE_VF64M1X4  rvvfloat64m1x4_t_node
+#define RISCV_ATYPE_VI64M1X5  rvvint64m1x5_t_node
+#define RISCV_ATYPE_VUI64M1X5 rvvuint64m1x5_t_node
+#define RISCV_ATYPE_VF64M1X5  rvvfloat64m1x5_t_node
+#define RISCV_ATYPE_VI64M1X6  rvvint64m1x6_t_node
+#define RISCV_ATYPE_VUI64M1X6 rvvuint64m1x6_t_node
+#define RISCV_ATYPE_VF64M1X6  rvvfloat64m1x6_t_node
+#define RISCV_ATYPE_VI64M1X7  rvvint64m1x7_t_node
+#define RISCV_ATYPE_VUI64M1X7 rvvuint64m1x7_t_node
+#define RISCV_ATYPE_VF64M1X7  rvvfloat64m1x7_t_node
+#define RISCV_ATYPE_VI64M1X8  rvvint64m1x8_t_node
+#define RISCV_ATYPE_VUI64M1X8 rvvuint64m1x8_t_node
+#define RISCV_ATYPE_VF64M1X8  rvvfloat64m1x8_t_node
+#define RISCV_ATYPE_VI64M2X2  rvvint64m2x2_t_node
+#define RISCV_ATYPE_VUI64M2X2 rvvuint64m2x2_t_node
+#define RISCV_ATYPE_VF64M2X2  rvvfloat64m2x2_t_node
+#define RISCV_ATYPE_VI64M2X3  rvvint64m2x3_t_node
+#define RISCV_ATYPE_VUI64M2X3 rvvuint64m2x3_t_node
+#define RISCV_ATYPE_VF64M2X3  rvvfloat64m2x3_t_node
+#define RISCV_ATYPE_VI64M2X4  rvvint64m2x4_t_node
+#define RISCV_ATYPE_VUI64M2X4 rvvuint64m2x4_t_node
+#define RISCV_ATYPE_VF64M2X4  rvvfloat64m2x4_t_node
+#define RISCV_ATYPE_VI64M4X2  rvvint64m4x2_t_node
+#define RISCV_ATYPE_VUI64M4X2 rvvuint64m4x2_t_node
+#define RISCV_ATYPE_VF64M4X2  rvvfloat64m4x2_t_node
+
+/* Helper type nodes for vector support.  */
+tree const_float_ptr_type_node;
+tree const_double_ptr_type_node;
+tree float16_ptr_type_node;
+tree const_float16_type_node;
+tree const_float16_ptr_type_node;
+
+#define DECLARE_SCALAR_INT_PTR_TYPE_NODE(WIDTH, MODE)	\
+  tree int##WIDTH##_type_node;				\
+  tree unsigned_int##WIDTH##_type_node;			\
+  tree int##MODE##_ptr_type_node;			\
+  tree unsigned_int##MODE##_ptr_type_node;		\
+  tree const_int##MODE##_ptr_type_node;			\
+  tree const_unsigned_int##MODE##_ptr_type_node;
+
+_SCALAR_INT_ITERATOR(DECLARE_SCALAR_INT_PTR_TYPE_NODE)
+
+/* Vector type nodes.  */
+tree rvvint8m1_t_node;
+tree rvvint8m2_t_node;
+tree rvvint8m4_t_node;
+tree rvvint8m8_t_node;
+tree rvvint16m1_t_node;
+tree rvvint16m2_t_node;
+tree rvvint16m4_t_node;
+tree rvvint16m8_t_node;
+tree rvvint32m1_t_node;
+tree rvvint32m2_t_node;
+tree rvvint32m4_t_node;
+tree rvvint32m8_t_node;
+tree rvvint64m1_t_node;
+tree rvvint64m2_t_node;
+tree rvvint64m4_t_node;
+tree rvvint64m8_t_node;
+
+tree rvvuint8m1_t_node;
+tree rvvuint8m2_t_node;
+tree rvvuint8m4_t_node;
+tree rvvuint8m8_t_node;
+tree rvvuint16m1_t_node;
+tree rvvuint16m2_t_node;
+tree rvvuint16m4_t_node;
+tree rvvuint16m8_t_node;
+tree rvvuint32m1_t_node;
+tree rvvuint32m2_t_node;
+tree rvvuint32m4_t_node;
+tree rvvuint32m8_t_node;
+tree rvvuint64m1_t_node;
+tree rvvuint64m2_t_node;
+tree rvvuint64m4_t_node;
+tree rvvuint64m8_t_node;
+
+tree rvvfloat16m1_t_node;
+tree rvvfloat16m2_t_node;
+tree rvvfloat16m4_t_node;
+tree rvvfloat16m8_t_node;
+tree rvvfloat32m1_t_node;
+tree rvvfloat32m2_t_node;
+tree rvvfloat32m4_t_node;
+tree rvvfloat32m8_t_node;
+tree rvvfloat64m1_t_node;
+tree rvvfloat64m2_t_node;
+tree rvvfloat64m4_t_node;
+tree rvvfloat64m8_t_node;
+
+tree rvvbool1_t_node;
+tree rvvbool2_t_node;
+tree rvvbool4_t_node;
+tree rvvbool8_t_node;
+tree rvvbool16_t_node;
+tree rvvbool32_t_node;
+tree rvvbool64_t_node;
+
+#define F16_TYPE_NODE fp16_type_node
+#define F32_TYPE_NODE float_type_node
+#define F64_TYPE_NODE double_type_node
+
+#define RISCV_DECL_FSEG_TYPES(SEW, LMUL, NR, MLEN,			\
+			      SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			      VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER,X)	\
+  tree rvvfloat##SEW##m##LMUL##x##NR##_t_node;
+
+_RVV_SEG_NO_SEW8_ARG (RISCV_DECL_FSEG_TYPES, X)
+
+#define RISCV_DECL_SEG_TYPES(SEW, LMUL, NR, MLEN,			\
+			     SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			     VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER,X)	\
+  tree rvvint##SEW##m##LMUL##x##NR##_t_node;				\
+  tree rvvuint##SEW##m##LMUL##x##NR##_t_node;
+
+_RVV_SEG_ARG (RISCV_DECL_SEG_TYPES, X)
 
 /* RISCV_FTYPE_ATYPESN takes N RISCV_FTYPES-like type codes and lists
    their associated RISCV_ATYPEs.  */
@@ -126,10 +527,2035 @@ AVAIL (hard_float, TARGET_HARD_FLOAT)
   RISCV_ATYPE_##A
 #define RISCV_FTYPE_ATYPES1(A, B) \
   RISCV_ATYPE_##A, RISCV_ATYPE_##B
+#define RISCV_FTYPE_ATYPES2(A, B, C) \
+  RISCV_ATYPE_##A, RISCV_ATYPE_##B, RISCV_ATYPE_##C
+#define RISCV_FTYPE_ATYPES3(A, B, C, D)	\
+  RISCV_ATYPE_##A, RISCV_ATYPE_##B, RISCV_ATYPE_##C, RISCV_ATYPE_##D
+#define RISCV_FTYPE_ATYPES4(A, B, C, D, E) \
+  RISCV_ATYPE_##A, RISCV_ATYPE_##B, RISCV_ATYPE_##C, RISCV_ATYPE_##D, \
+  RISCV_ATYPE_##E
+#define RISCV_FTYPE_ATYPES5(A, B, C, D, E, F) \
+  RISCV_ATYPE_##A, RISCV_ATYPE_##B, RISCV_ATYPE_##C, RISCV_ATYPE_##D, \
+  RISCV_ATYPE_##E, RISCV_ATYPE_##F
+#define RISCV_FTYPE_ATYPES6(A, B, C, D, E, F, G) \
+  RISCV_ATYPE_##A, RISCV_ATYPE_##B, RISCV_ATYPE_##C, RISCV_ATYPE_##D, \
+  RISCV_ATYPE_##E, RISCV_ATYPE_##F, RISCV_ATYPE_##G
+#define RISCV_FTYPE_ATYPES7(A, B, C, D, E, F, G, H) \
+  RISCV_ATYPE_##A, RISCV_ATYPE_##B, RISCV_ATYPE_##C, RISCV_ATYPE_##D, \
+  RISCV_ATYPE_##E, RISCV_ATYPE_##F, RISCV_ATYPE_##G, RISCV_ATYPE_##H
+#define RISCV_FTYPE_ATYPES8(A, B, C, D, E, F, G, H, I) \
+  RISCV_ATYPE_##A, RISCV_ATYPE_##B, RISCV_ATYPE_##C, RISCV_ATYPE_##D, \
+  RISCV_ATYPE_##E, RISCV_ATYPE_##F, RISCV_ATYPE_##G, RISCV_ATYPE_##H, \
+  RISCV_ATYPE_##I
+
+#define SETVL_BUILTINS(E, L, MLEN, MODE, SUBMODE)			\
+  DIRECT_BUILTIN (vsetvl##E##m##L##_si, RISCV_SI_FTYPE_SI, vector),	\
+  DIRECT_BUILTIN (vsetvl##E##m##L##_di, RISCV_DI_FTYPE_DI, vector),
+
+#define SETVTYPE_BUILTINS(E, L, MLEN, MODE, SUBMODE)			\
+  DIRECT_NAMED_NO_TARGET (vsetvli_x0_##MODE, vsetvtype##E##m##L, RISCV_VOID_FTYPE, vector),
+
+#define _VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+			       PNAME, PMODE, SUBTYPE, VCLASS)		\
+  DIRECT_NAMED (							\
+    vleff##MODE##_##PNAME, vleff##SUBTYPE##E##m##L##_##PNAME,		\
+    RISCV_##VCLASS##E##M##L##_FTYPE_C_##SUBMODE##_PTR,			\
+    vector),								\
+  DIRECT_NAMED (							\
+    vleff##MODE##_##PNAME##_mask,					\
+    vleff##SUBTYPE##E##m##L##_##PNAME##_mask,				\
+    RISCV_##VCLASS##E##M##L##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##_C_##SUBMODE##_PTR, \
+    vector),
+
+#define VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, SUBMODE)		\
+  _VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, SUBMODE, si, SI,		\
+			 int, VI)					\
+  _VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, SUBMODE, di, DI,		\
+			 int, VI)					\
+  _VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, U##SUBMODE, si, SI,		\
+			 uint, VUI)					\
+  _VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, U##SUBMODE, di, DI,		\
+			 uint, VUI)
+
+#define VFLOAT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, SUBMODE)		\
+  _VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, SUBMODE, si, SI,		\
+			 float, VF)					\
+  _VINT_LOAD_FF_BUILTINS(E, L, MLEN, MODE, SUBMODE, di, DI,		\
+			 float, VF)
+
+#define _VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+				  PNAME, PMODE, SUBTYPE, VCLASS)	\
+  DIRECT_NAMED (							\
+    vle##MODE##_##PNAME, vle##SUBTYPE##E##m##L##_##PNAME,		\
+    RISCV_##VCLASS##E##M##L##_FTYPE_C_##SUBMODE##_PTR,			\
+    vector),								\
+  DIRECT_NAMED (							\
+    vle##MODE##_##PNAME##_mask,						\
+    vle##SUBTYPE##E##m##L##_##PNAME##_mask,				\
+    RISCV_##VCLASS##E##M##L##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##_C_##SUBMODE##_PTR, \
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vse##MODE##_##PNAME,						\
+    vse##SUBTYPE##E##m##L##_##PNAME,					\
+    RISCV_VOID_FTYPE_##VCLASS##E##M##L##_##SUBMODE##_PTR,		\
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vse##MODE##_##PNAME##_mask,						\
+    vse##SUBTYPE##E##m##L##_##PNAME##_mask,				\
+    RISCV_VOID_FTYPE_VB##MLEN##_##VCLASS##E##M##L##_##SUBMODE##_PTR,	\
+    vector),
+
+#define VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE)		\
+  _VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, si, SI,		\
+			    int, VI)					\
+  _VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, di, DI,		\
+			    int, VI)					\
+  _VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, U##SUBMODE, si, SI,	\
+			    uint, VUI)					\
+  _VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, U##SUBMODE, di, DI,	\
+			    uint, VUI)
+
+#define VFLOAT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE)		\
+  _VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, si, SI,		\
+			    float, VF)					\
+  _VINT_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, di, DI,		\
+			    float, VF)
+
+#define _VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE,	\
+					  PNAME, PMODE, SUBTYPE, VCLASS)\
+  DIRECT_NAMED (							\
+    vlse##MODE##_##PNAME, vlse##SUBTYPE##E##m##L##_##PNAME,		\
+    RISCV_##VCLASS##E##M##L##_FTYPE_C_##SUBMODE##_PTR_##PMODE,		\
+    vector),								\
+  DIRECT_NAMED (							\
+    vlse##MODE##_##PNAME##_mask,					\
+    vlse##SUBTYPE##E##m##L##_##PNAME##_mask,				\
+    RISCV_##VCLASS##E##M##L##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##_C_##SUBMODE##_PTR_##PMODE, \
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vsse##MODE##_##PNAME,						\
+    vsse##SUBTYPE##E##m##L##_##PNAME,					\
+    RISCV_VOID_FTYPE_##VCLASS##E##M##L##_##SUBMODE##_PTR_##PMODE,	\
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vsse##MODE##_##PNAME##_mask,					\
+    vsse##SUBTYPE##E##m##L##_##PNAME##_mask,				\
+    RISCV_VOID_FTYPE_VB##MLEN##_##VCLASS##E##M##L##_##SUBMODE##_PTR_##PMODE,\
+    vector),								\
+
+#define VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE)	\
+  _VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, si, SI,	\
+				    int, VI)				\
+  _VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, di, DI,	\
+				    int, VI)				\
+  _VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, U##SUBMODE, si, SI,\
+				    uint, VUI)				\
+  _VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, U##SUBMODE, di, DI,\
+				    uint, VUI)
+
+#define VFLOAT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE)	\
+  _VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, si, SI,	\
+				    float, VF)				\
+  _VINT_STRIDED_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE, di, DI,	\
+				    float, VF)
+
+#define _VINT_INDEX_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+					IE, IL, IMODE, ISUBMODE,		\
+					PNAME, TYPE_US, VCLASS)			\
+  DIRECT_NAMED (								\
+    vlxei##MODE##IMODE##_##PNAME,						\
+    vlxei##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME,				\
+    RISCV_##VCLASS##E##M##L##_FTYPE_C_##SUBMODE##_PTR##_VUI##IE##M##IL,		\
+    vector),									\
+  DIRECT_NAMED (								\
+    vlxei##MODE##IMODE##_##PNAME##_mask,					\
+    vlxei##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME##_mask,			\
+    RISCV_##VCLASS##E##M##L##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##_C_##SUBMODE##_PTR##_VUI##IE##M##IL,\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsxei##MODE##IMODE##_##PNAME,						\
+    vsxei##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME,				\
+    RISCV_VOID_FTYPE_##SUBMODE##_PTR##_VUI##IE##M##IL##_##VCLASS##E##M##L,	\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsxei##MODE##IMODE##_##PNAME##_mask,					\
+    vsxei##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME##_mask,			\
+    RISCV_VOID_FTYPE_VB##MLEN##_##SUBMODE##_PTR##_VUI##IE##M##IL##_##VCLASS##E##M##L,\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsuxei##MODE##IMODE##_##PNAME,						\
+    vsuxei##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME,				\
+    RISCV_VOID_FTYPE_##SUBMODE##_PTR##_VUI##IE##M##IL##_##VCLASS##E##M##L,	\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsuxei##MODE##IMODE##_##PNAME##_mask,					\
+    vsuxei##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME##_mask,			\
+    RISCV_VOID_FTYPE_VB##MLEN##_##SUBMODE##_PTR##_VUI##IE##M##IL##_##VCLASS##E##M##L,\
+    vector),
+
+#define VINT_INDEX_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE,	\
+				       IE, IL, IMODE, ISUBMODE)		\
+  _VINT_INDEX_LOAD_STORE_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+				   IE, IL, IMODE, ISUBMODE, si, i, VI)	\
+  _VINT_INDEX_LOAD_STORE_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+				   IE, IL, IMODE, ISUBMODE, di, i, VI)	\
+  _VINT_INDEX_LOAD_STORE_BUILTINS (E, L, MLEN, MODE, U##SUBMODE,	\
+				   IE, IL, IMODE, ISUBMODE, si, u, VUI)	\
+  _VINT_INDEX_LOAD_STORE_BUILTINS (E, L, MLEN, MODE, U##SUBMODE,	\
+				   IE, IL, IMODE, ISUBMODE, di, u, VUI)
+
+#define _VFLOAT_INDEX_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+					  IE, IL, IMODE, ISUBMODE, PNAME)	\
+  DIRECT_NAMED (								\
+    vlxei##MODE##IMODE##_##PNAME,						\
+    vlxeif##E##m##L##_##IE##m##IL##_##PNAME,					\
+    RISCV_VF##E##M##L##_FTYPE_C_##SUBMODE##_PTR##_VUI##IE##M##IL,		\
+    vector),									\
+  DIRECT_NAMED (								\
+    vlxei##MODE##IMODE##_##PNAME##_mask,					\
+    vlxeif##E##m##L##_##IE##m##IL##_##PNAME##_mask,				\
+    RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_C_##SUBMODE##_PTR##_VUI##IE##M##IL,\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsxei##MODE##IMODE##_##PNAME,						\
+    vsxeif##E##m##L##_##IE##m##IL##_##PNAME,					\
+    RISCV_VOID_FTYPE_##SUBMODE##_PTR##_VUI##IE##M##IL##_VF##E##M##L,		\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsxei##MODE##IMODE##_##PNAME##_mask,					\
+    vsxeif##E##m##L##_##IE##m##IL##_##PNAME##_mask,				\
+    RISCV_VOID_FTYPE_VB##MLEN##_##SUBMODE##_PTR##_VUI##IE##M##IL##_VF##E##M##L,\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsuxei##MODE##IMODE##_##PNAME,						\
+    vsuxeif##E##m##L##_##IE##m##IL##_##PNAME,					\
+    RISCV_VOID_FTYPE_##SUBMODE##_PTR##_VUI##IE##M##IL##_VF##E##M##L,		\
+    vector),									\
+  DIRECT_NAMED_NO_TARGET (							\
+    vsuxei##MODE##IMODE##_##PNAME##_mask,					\
+    vsuxeif##E##m##L##_##IE##m##IL##_##PNAME##_mask,				\
+    RISCV_VOID_FTYPE_VB##MLEN##_##SUBMODE##_PTR##_VUI##IE##M##IL##_VF##E##M##L,\
+    vector),
+
+#define VFLOAT_INDEX_LOAD_STORE_BUILTINS(E, L, MLEN, MODE, SUBMODE,	\
+					 IE, IL, IMODE, ISUBMODE)	\
+  _VFLOAT_INDEX_LOAD_STORE_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+				     IE, IL, IMODE, ISUBMODE, si)	\
+  _VFLOAT_INDEX_LOAD_STORE_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+				     IE, IL, IMODE, ISUBMODE, di)
+
+#define _VINT_AMO_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+			   IE, IL, IMODE, ISUBMODE,		\
+			   PNAME, TYPE_US, VCLASS, OP)		\
+  DIRECT_NAMED (						\
+    OP##MODE##IMODE##_##PNAME,					\
+    OP##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME,		\
+    RISCV_##VCLASS##E##M##L##_FTYPE_C_##SUBMODE##_PTR##_VUI##IE##M##IL##_##VCLASS##E##M##L,\
+    vector),							\
+  DIRECT_NAMED (						\
+    OP##MODE##IMODE##_##PNAME##_mask,				\
+    OP##TYPE_US##E##m##L##_##IE##m##IL##_##PNAME##_mask,	\
+    RISCV_##VCLASS##E##M##L##_FTYPE_VB##MLEN##_C_##SUBMODE##_PTR##_VUI##IE##M##IL##_##VCLASS##E##M##L,\
+    vector),
+
+#define VINT_AMO_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+			  IE, IL, IMODE, ISUBMODE, OP)		\
+  _VINT_AMO_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+		      IE, IL, IMODE, ISUBMODE, si, i, VI, OP)	\
+  _VINT_AMO_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+		     IE, IL, IMODE, ISUBMODE, di, i, VI, OP)	\
+  _VINT_AMO_BUILTINS (E, L, MLEN, MODE, U##SUBMODE,		\
+		      IE, IL, IMODE, ISUBMODE, si, u, VUI, OP)	\
+  _VINT_AMO_BUILTINS (E, L, MLEN, MODE, U##SUBMODE,		\
+		      IE, IL, IMODE, ISUBMODE, di, u, VUI, OP)
+
+#define _VFLOAT_AMO_BUILTINS(E, L, MLEN, MODE, SUBMODE,				\
+			     IE, IL, IMODE, ISUBMODE,				\
+			     PNAME, OP)						\
+  DIRECT_NAMED (								\
+    OP##MODE##IMODE##_##PNAME,							\
+    OP##f##E##m##L##_##IE##m##IL##_##PNAME,					\
+    RISCV_VF##E##M##L##_FTYPE_C_##SUBMODE##_PTR##_VUI##IE##M##IL##_VF##E##M##L,	\
+    vector),									\
+  DIRECT_NAMED (								\
+    OP##MODE##IMODE##_##PNAME##_mask,						\
+    OP##f##E##m##L##_##IE##m##IL##_##PNAME##_mask,				\
+    RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_C_##SUBMODE##_PTR##_VUI##IE##M##IL##_VF##E##M##L,\
+    vector),
+
+#define VFLOAT_AMO_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+			    IE, IL, IMODE, ISUBMODE, OP)	\
+  _VFLOAT_AMO_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+			IE, IL, IMODE, ISUBMODE, si, OP)	\
+  _VFLOAT_AMO_BUILTINS (E, L, MLEN, MODE, SUBMODE,		\
+			IE, IL, IMODE, ISUBMODE, di, OP)
+
+#define VINT_MV_X_S_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, NAME)	\
+  MV_XS_NAMED (OP##MODE, v##NAME##i##E##m##L,				\
+	       RISCV_##SUBMODE##_FTYPE_VI##E##M##L,			\
+	       vector),							\
+  MV_XS_NAMED (OP##MODE, v##NAME##u##E##m##L,				\
+	       RISCV_U##SUBMODE##_FTYPE_VUI##E##M##L,			\
+	       vector),
+
+#define VINT_MV_S_X_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##MODE, v##NAME##i##E##m##L,				\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE, v##NAME##u##E##m##L,				\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),
+
+#define VFLOAT_MV_F_S_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##MODE, v##NAME##f##E##m##L,				\
+		RISCV_##SUBMODE##_FTYPE_VF##E##M##L,			\
+		vector),
+
+#define VFLOAT_MV_S_F_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##MODE, v##NAME##f##E##m##L,				\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_##SUBMODE,	\
+		vector),
+
+#define VINT_UNARY_OP_BUILTINS_SCALAR(E, L, MLEN, MODE, SUBMODE, OP)\
+  DIRECT_NAMED (OP##MODE, v##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_##SUBMODE,		\
+		vector),					\
+  DIRECT_NAMED (OP##MODE, v##OP##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_U##SUBMODE,		\
+		vector),
+
+#define VFLOAT_UNARY_OP_BUILTINS_SCALAR(E, L, MLEN, MODE, SUBMODE, OP)\
+  DIRECT_NAMED (OP##MODE, v##OP##f##E##m##L,			\
+		RISCV_VF##E##M##L##_FTYPE_##SUBMODE,		\
+		vector),
+
+#define VINT_UNARY_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##2, v##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##2, v##OP##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L,		\
+		vector),
+
+#define VINT_UNARY_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VINT_UNARY_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##2_mask, v##OP##int##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##2_mask, v##OP##uint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),
+
+#define VINT_SAT_BIN_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##3, v##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar, v##OP##int##E##m##L##_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE,	\
+		vector),
+
+#define VINT_SAT_BIN_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VINT_SAT_BIN_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##3_mask, v##OP##int##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, v##OP##int##E##m##L##_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_##SUBMODE,\
+		vector),
+
+#define VUINT_SAT_BIN_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##3, v##OP##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar, v##OP##uint##E##m##L##_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),
+
+#define VUINT_SAT_BIN_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VUINT_SAT_BIN_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##3_mask, v##OP##uint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, v##OP##uint##E##m##L##_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define VINT_BIN_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##3, v##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3, v##OP##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar, v##OP##int##E##m##L##_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar, v##OP##uint##E##m##L##_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),
+
+#define VINT_BIN_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VINT_BIN_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##3_mask, v##OP##int##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_mask, v##OP##uint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, v##OP##int##E##m##L##_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_##SUBMODE,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, v##OP##uint##E##m##L##_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define VINT_SCALAR_ONLY_BIN_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##3_scalar, v##OP##int##E##m##L##_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar, v##OP##uint##E##m##L##_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, v##OP##int##E##m##L##_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_##SUBMODE,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, v##OP##uint##E##m##L##_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define VFLOAT_UNARY_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##2, vf##OP##float##E##m##L,			\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##2_mask, vf##OP##float##E##m##L##_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_BIN_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VFLOAT_SCALAR_ONLY_BIN_OP_BUILTINS (E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##3, vf##OP##float##E##m##L,			\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_VF##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_mask, vf##OP##float##E##m##L##_mask,		\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_SCALAR_ONLY_BIN_OP_BUILTINS(E, L, MLEN, MODE,		\
+					   SUBMODE, OP)			\
+  DIRECT_NAMED (OP##MODE##3_scalar, vf##OP##float##E##m##L##_scalar,	\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, vf##OP##float##E##m##L##_scalar_mask,\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_##SUBMODE,\
+		vector),
+
+#define VFLOAT_CVT_XF_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+			       IMODE, ISMODE, OP, NAME)			\
+  DIRECT_NAMED (OP##FMODE##IMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VI##E##M##L##_FTYPE_VF##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##IMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_CVT_XUF_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+				IMODE, ISMODE, OP, NAME)		\
+  DIRECT_NAMED (OP##FMODE##IMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VUI##E##M##L##_FTYPE_VF##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##IMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_CVT_FX_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+			       IMODE, ISMODE, OP, NAME)			\
+  DIRECT_NAMED (OP##IMODE##FMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##E##M##L##_FTYPE_VI##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (OP##IMODE##FMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VI##E##M##L,\
+		vector),
+
+#define VFLOAT_CVT_FXU_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+				IMODE, ISMODE, OP, NAME)		\
+  DIRECT_NAMED (OP##IMODE##FMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##E##M##L##_FTYPE_VUI##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (OP##IMODE##FMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VUI##E##M##L,\
+		vector),
+
+#define VFLOAT_WCVT_XF_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+				WE, WL, WIMODE, WISMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##FMODE##WIMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VI##WE##M##WL##_FTYPE_VF##E##M##L,		\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##WIMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_WCVT_XUF_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+				 WE, WL, WIMODE, WISMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##FMODE##WIMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VUI##WE##M##WL##_FTYPE_VF##E##M##L,		\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##WIMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_WCVT_FX_BUILTINS(E, L, MLEN, IMODE, ISMODE,		\
+				WE, WL, WFMODE, WFSMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##IMODE##WFMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##WE##M##WL##_FTYPE_VI##E##M##L,		\
+		vector),						\
+  DIRECT_NAMED (OP##IMODE##WFMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VI##E##M##L,\
+		vector),
+
+#define VFLOAT_WCVT_FXU_BUILTINS(E, L, MLEN, IMODE, ISMODE,		\
+				 WE, WL, WFMODE, WFSMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##IMODE##WFMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##WE##M##WL##_FTYPE_VUI##E##M##L,		\
+		vector),						\
+  DIRECT_NAMED (OP##IMODE##WFMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VUI##E##M##L,\
+		vector),
+
+#define VFLOAT_WCVT_FF_BUILTINS(E, L, MLEN, VMODE, SMODE,		\
+				WE, WL, WVMODE, WSMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##VMODE##WVMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##E##M##L,		\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##WVMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_NCVT_XF_BUILTINS(E, L, MLEN, IMODE, ISMODE,		\
+				WE, WL, WFMODE, WFSMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##WFMODE##IMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VI##E##M##L##_FTYPE_VF##WE##M##WL,		\
+		vector),						\
+  DIRECT_NAMED (OP##WFMODE##IMODE##2_mask, vf##NAME##f##E##m##L##_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VF##WE##M##WL,\
+		vector),
+
+#define VFLOAT_NCVT_XUF_BUILTINS(E, L, MLEN, IMODE, ISMODE,		\
+				 WE, WL, WFMODE, WFSMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##WFMODE##IMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VUI##E##M##L##_FTYPE_VF##WE##M##WL,		\
+		vector),						\
+  DIRECT_NAMED (OP##WFMODE##IMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VF##WE##M##WL,\
+		vector),
+
+#define VFLOAT_NCVT_FX_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+				WE, WL, WIMODE, WISMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##WIMODE##FMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##E##M##L##_FTYPE_VI##WE##M##WL,		\
+		vector),						\
+  DIRECT_NAMED (OP##WIMODE##FMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VI##WE##M##WL,\
+		vector),
+
+#define VFLOAT_NCVT_FXU_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+				 WE, WL, WIMODE, WISMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##WIMODE##FMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##E##M##L##_FTYPE_VUI##WE##M##WL,		\
+		vector),						\
+  DIRECT_NAMED (OP##WIMODE##FMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VUI##WE##M##WL,\
+		vector),
+
+#define VFLOAT_NCVT_FF_BUILTINS(E, L, MLEN, VMODE, SMODE,		\
+				WE, WL, WVMODE, WSMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##WVMODE##VMODE##2, vf##NAME##f##E##m##L,		\
+		RISCV_VF##E##M##L##_FTYPE_VF##WE##M##WL,		\
+		vector),						\
+  DIRECT_NAMED (OP##WVMODE##VMODE##2_mask, vf##NAME##f##E##m##L##_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##WE##M##WL,\
+		vector),
+
+#define VFLOAT_VFCLASS_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##2, vf##OP##float##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VF##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##2_mask, vf##OP##float##E##m##L##_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_MAC_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (vf##OP##MODE, vf##OP##_sv_f##E##m##L,			\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (vf##OP##MODE##_scalar, vf##OP##_sv_f##E##m##L##_scalar,\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_##SUBMODE##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (vf##OP##MODE##_mask, vf##OP##_sv_f##E##m##L##_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (vf##OP##MODE##_scalar_mask, vf##OP##_sv_f##E##m##L##_scalar_mask,\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_##SUBMODE##_VF##E##M##L,\
+		vector),
+
+#define VFLOAT_WIDENING_MAC_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE,	\
+					WE, WL, WMODE, WSMODE, OP)	\
+  DIRECT_NAMED (vf##OP##MODE, vf##OP##_sv_f##E##m##L,			\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##WE##M##WL##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (vf##OP##MODE##_scalar, vf##OP##_sv_f##E##m##L##_scalar,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##WE##M##WL##_##SUBMODE##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (vf##OP##MODE##_mask, vf##OP##_sv_f##E##m##L##_mask,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (vf##OP##MODE##_scalar_mask, vf##OP##_sv_f##E##m##L##_scalar_mask,\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_##SUBMODE##_VF##E##M##L,\
+		vector),
+
+#define VINT_ADC_SBC_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##4, v##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4m, vm##OP##mint##E##m##L,			\
+		RISCV_VB##MLEN##_FTYPE_VI##E##M##L##_VI##E##M##L##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4, vm##OP##int##E##m##L,			\
+		RISCV_VB##MLEN##_FTYPE_VI##E##M##L##_VI##E##M##L, 	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##4_scalar, v##OP##int##E##m##L##_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4m_scalar, vm##OP##mint##E##m##L##_scalar,	\
+		RISCV_VB##MLEN##_FTYPE_VI##E##M##L##_##SUBMODE##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4_scalar, vm##OP##int##E##m##L##_scalar,	\
+		RISCV_VB##MLEN##_FTYPE_VI##E##M##L##_##SUBMODE, 	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##4, v##OP##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4m, vm##OP##muint##E##m##L,		\
+		RISCV_VB##MLEN##_FTYPE_VUI##E##M##L##_VUI##E##M##L##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4, vm##OP##uint##E##m##L,			\
+		RISCV_VB##MLEN##_FTYPE_VUI##E##M##L##_VUI##E##M##L, 	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##4_scalar, v##OP##uint##E##m##L##_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4m_scalar, vm##OP##muint##E##m##L##_scalar,\
+		RISCV_VB##MLEN##_FTYPE_VUI##E##M##L##_U##SUBMODE##_VB##MLEN, \
+		vector),						\
+  DIRECT_NAMED (m##OP##MODE##4_scalar, vm##OP##uint##E##m##L##_scalar,	\
+		RISCV_VB##MLEN##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),
+
+#define ICMP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, OPU)		\
+  DIRECT_NAMED (s##OP##MODE, vs##OP##int##E##m##L,			\
+		RISCV_VB##MLEN##_FTYPE_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (s##OPU##MODE, vs##OPU##uint##E##m##L,			\
+		RISCV_VB##MLEN##_FTYPE_VUI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (s##OP##MODE##_mask, vs##OP##int##E##m##L##_mask,	\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN##_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (s##OPU##MODE##_mask, vs##OPU##uint##E##m##L##_mask,	\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L, \
+		vector),						\
+  DIRECT_NAMED (s##OP##MODE##_scalar, vs##OP##int##E##m##L##_scalar,	\
+		RISCV_VB##MLEN##_FTYPE_VI##E##M##L##_##SUBMODE,		\
+		vector),						\
+  DIRECT_NAMED (s##OPU##MODE##_scalar, vs##OPU##uint##E##m##L##_scalar,	\
+		RISCV_VB##MLEN##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (s##OP##MODE##_scalar_mask, vs##OP##int##E##m##L##_scalar_mask,	\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN##_VI##E##M##L##_##SUBMODE,\
+		vector),						\
+  DIRECT_NAMED (s##OPU##MODE##_scalar_mask, vs##OPU##uint##E##m##L##_scalar_mask,\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN##_VUI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define FCMP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)			\
+  DIRECT_NAMED (OP##MODE, OP##int##E##m##L,				\
+		RISCV_VB##MLEN##_FTYPE_VF##E##M##L##_VF##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_mask, OP##int##E##m##L##_mask,		\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_scalar, OP##int##E##m##L##_scalar,		\
+		RISCV_VB##MLEN##_FTYPE_VF##E##M##L##_##SUBMODE,		\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_scalar_mask, OP##int##E##m##L##_scalar_mask,	\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN##_VF##E##M##L##_##SUBMODE,\
+		vector),
+
+#define VINT_BIN_OP_OPU_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, OPU)	\
+  DIRECT_NAMED (OP##MODE##3, vv##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OPU##MODE##3, vv##OPU##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_mask, vv##OP##int##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OPU##MODE##3_mask, vv##OPU##uint##E##m##L##_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar, vs##OP##int##E##m##L##_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OPU##MODE##3_scalar, vs##OPU##uint##E##m##L##_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##3_scalar_mask, vs##OP##int##E##m##L##_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_##SUBMODE,\
+		vector),						\
+  DIRECT_NAMED (OPU##MODE##3_scalar_mask, vs##OPU##uint##E##m##L##_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define VINT_MULH_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##s##MODE, vv##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##u##MODE, vv##OP##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##su##MODE, vv##OP##su_int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##s##MODE##_scalar, vv##OP##int##E##m##L##_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##u##MODE##_scalar, vv##OP##uint##E##m##L##_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##su##MODE##_scalar, vv##OP##su_int##E##m##L##_scalar,\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_U##SUBMODE,	\
+		vector),
+
+#define VINT_MULH_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)	\
+  VINT_MULH_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##s##MODE##_mask, vv##OP##int##E##m##L##_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VI##E##M##L,\
+		vector),					\
+  DIRECT_NAMED (OP##u##MODE##_mask, vv##OP##uint##E##m##L##_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),					\
+  DIRECT_NAMED (OP##su##MODE##_mask, vv##OP##su_int##E##m##L##_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VUI##E##M##L,\
+		vector),					\
+  DIRECT_NAMED (OP##s##MODE##_scalar_mask, vv##OP##int##E##m##L##_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_##SUBMODE,\
+		vector),					\
+  DIRECT_NAMED (OP##u##MODE##_scalar_mask, vv##OP##uint##E##m##L##_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_U##SUBMODE,\
+		vector),					\
+  DIRECT_NAMED (OP##su##MODE##_scalar_mask, vv##OP##su_int##E##m##L##_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define VINT_WMUL_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE,		\
+				  WE, WL, WMODE, WSUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE, vv##OP##int##E##m##L,				\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##u##MODE, vv##OP##uint##E##m##L,			\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##su##MODE, vv##OP##su_int##E##m##L,			\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_scalar, vv##OP##int##E##m##L##_scalar,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##u##MODE##_scalar, vv##OP##uint##E##m##L##_scalar,	\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##E##M##L##_U##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##su##MODE##_scalar, vv##OP##su_int##E##m##L##_scalar,\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##E##M##L##_U##SUBMODE,	\
+		vector),
+
+#define VINT_WMUL_BUILTINS(E, L, MLEN, MODE, SUBMODE,		\
+			   WE, WL, WMODE, WSUBMODE, OP)		\
+  VINT_WMUL_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE,		\
+			    WE, WL, WMODE, WSUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##_mask, vv##OP##int##E##m##L##_mask,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_VI##E##M##L,\
+		vector),					\
+  DIRECT_NAMED (OP##u##MODE##_mask, vv##OP##uint##E##m##L##_mask,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),					\
+  DIRECT_NAMED (OP##su##MODE##_mask, vv##OP##su_int##E##m##L##_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_VUI##E##M##L,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##_scalar_mask, vv##OP##int##E##m##L##_scalar_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_##SUBMODE,\
+		vector),					\
+  DIRECT_NAMED (OP##u##MODE##_scalar_mask, vv##OP##uint##E##m##L##_scalar_mask,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##E##M##L##_U##SUBMODE,\
+		vector),					\
+  DIRECT_NAMED (OP##su##MODE##_scalar_mask, vv##OP##su_int##E##m##L##_scalar_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define VFLOAT_WMUL_BUILTINS(E, L, MLEN, VMODE, SDEMODE,		\
+			     WE, WL, WVMODE, WSMODE, OP)		\
+  DIRECT_NAMED (OP##VMODE##_vv, v##OP##_vv_f##E##m##L,			\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##E##M##L##_VF##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_vv_scalar, v##OP##_vv_f##E##m##L##_scalar,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##E##M##L##_##SDEMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_vv_mask, v##OP##_vv_f##E##m##L##_mask,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_vv_scalar_mask, v##OP##_vv_f##E##m##L##_scalar_mask,\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##E##M##L##_##SDEMODE,\
+		vector),
+
+#define VINT_MAC_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (v##OP##MODE, v##OP##_sv_i##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (v##OP##MODE##_scalar, v##OP##_sv_i##E##m##L##_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_##SUBMODE##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (v##OP##MODE, v##OP##_sv_u##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (v##OP##MODE##_scalar, v##OP##_sv_u##E##m##L##_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_U##SUBMODE##_VUI##E##M##L,\
+		vector),
+
+#define VINT_MAC_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VINT_MAC_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (v##OP##MODE##_mask, v##OP##_sv_i##E##m##L##_mask,	\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (v##OP##MODE##_mask, v##OP##_sv_u##E##m##L##_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (v##OP##MODE##_scalar_mask, v##OP##_sv_i##E##m##L##_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_##SUBMODE##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (v##OP##MODE##_scalar_mask, v##OP##_sv_u##E##m##L##_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_U##SUBMODE##_VUI##E##M##L,\
+		vector),
+
+#define VINT_WIDENING_MAC_OP_BUILTINS_SCALAR_NOMASK(E, L, MLEN, MODE, SUBMODE,\
+						    WE, WL, WVMODE, WSMODE, OP, NAME)\
+  DIRECT_NAMED (OP##MODE##WVMODE##4_scalar, v##NAME##_sv_i##E##m##L##_scalar,\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##WE##M##WL##_##SUBMODE##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (u##OP##MODE##WVMODE##4_scalar, v##NAME##_sv_u##E##m##L##_scalar,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##WE##M##WL##_U##SUBMODE##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (su##OP##MODE##WVMODE##4_scalar, v##NAME##su_sv_i##E##m##L##_scalar,\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##WE##M##WL##_##SUBMODE##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (us##OP##MODE##WVMODE##4_scalar, v##NAME##us_sv_i##E##m##L##_scalar,\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##WE##M##WL##_U##SUBMODE##_VI##E##M##L,\
+		vector),
+
+#define VINT_WIDENING_MAC_OP_BUILTINS_SCALAR(E, L, MLEN, MODE, SUBMODE,\
+					     WE, WL, WVMODE, WSMODE, OP, NAME)\
+  VINT_WIDENING_MAC_OP_BUILTINS_SCALAR_NOMASK(E, L, MLEN, MODE, SUBMODE,\
+					      WE, WL, WVMODE, WSMODE, OP, NAME)\
+  DIRECT_NAMED (OP##MODE##WVMODE##4_scalar_mask, v##NAME##_sv_i##E##m##L##_scalar_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_##SUBMODE##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (u##OP##MODE##WVMODE##4_scalar_mask, v##NAME##_sv_u##E##m##L##_scalar_mask,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_U##SUBMODE##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (su##OP##MODE##WVMODE##4_scalar_mask, v##NAME##su_sv_i##E##m##L##_scalar_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_##SUBMODE##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (us##OP##MODE##WVMODE##4_scalar_mask, v##NAME##us_sv_i##E##m##L##_scalar_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_U##SUBMODE##_VI##E##M##L,\
+		vector),
+
+#define VINT_WIDENING_MAC_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE,\
+					     WE, WL, WVMODE, WSMODE, OP, NAME)\
+  DIRECT_NAMED (OP##MODE##WVMODE##4, v##NAME##_sv_i##E##m##L,		\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##WE##M##WL##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (u##OP##MODE##WVMODE##4, v##NAME##_sv_u##E##m##L,	\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##WE##M##WL##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (su##OP##MODE##WVMODE##4, v##NAME##su_sv_i##E##m##L,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##WE##M##WL##_VI##E##M##L##_VUI##E##M##L,\
+		vector),
+
+#define VINT_WIDENING_MAC_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE,\
+				      WE, WL, WVMODE, WSMODE, OP, NAME)\
+  VINT_WIDENING_MAC_OP_BUILTINS_NOMASK(E, L, MLEN, MODE, SUBMODE,\
+				       WE, WL, WVMODE, WSMODE, OP, NAME)\
+  DIRECT_NAMED (OP##MODE##WVMODE##4_mask, v##NAME##_sv_i##E##m##L##_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (u##OP##MODE##WVMODE##4_mask, v##NAME##_sv_u##E##m##L##_mask,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (su##OP##MODE##WVMODE##4_mask, v##NAME##su_sv_i##E##m##L##_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_VUI##E##M##L,\
+		vector),
+
+#define MASK_LOGICAL_BUILTINS(MLEN, N, OP)				\
+  DIRECT_NAMED (OP##vnx##N##bi3, v##OP##bool##MLEN,			\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN,		\
+		vector),
+
+#define MASK_SCALAR_UNARY_BUILTINS(MLEN, N, OP)				\
+  DIRECT_NAMED (OP##vnx##N##bi2_si, v##OP##bool##MLEN##_si,		\
+		RISCV_SI_FTYPE_VB##MLEN,				\
+		vector),						\
+  DIRECT_NAMED (OP##vnx##N##bi2_di, v##OP##bool##MLEN##_di,		\
+		RISCV_DI_FTYPE_VB##MLEN,				\
+		vector),						\
+  DIRECT_NAMED (OP##vnx##N##bi2_si_mask, v##OP##bool##MLEN##_si_mask,	\
+		RISCV_SI_FTYPE_VB##MLEN##_VB##MLEN,			\
+		vector),						\
+  DIRECT_NAMED (OP##vnx##N##bi2_di_mask, v##OP##bool##MLEN##_di_mask,	\
+		RISCV_DI_FTYPE_VB##MLEN##_VB##MLEN,			\
+		vector),
+
+#define MASK_NULLARY_BUILTINS(MLEN, N, OP)				\
+  DIRECT_NAMED (OP##vnx##N##bi, v##OP##bool##MLEN,			\
+		RISCV_VB##MLEN##_FTYPE,					\
+		vector),
+
+#define UNMAKED_MASK_UNARY_BUILTINS(MLEN, N, OP)			\
+  DIRECT_NAMED (OP##vnx##N##bi, v##OP##bool##MLEN,			\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN,			\
+		vector),						\
+
+#define MASK_UNARY_BUILTINS(MLEN, N, OP)				\
+  UNMAKED_MASK_UNARY_BUILTINS(MLEN, N, OP)				\
+  DIRECT_NAMED (OP##vnx##N##bi_mask, v##OP##bool##MLEN##_mask,		\
+		RISCV_VB##MLEN##_FTYPE_VB##MLEN##_VB##MLEN##_VB##MLEN,	\
+		vector),						\
+
+#define IOTA_BUILTINS(E, L, MLEN, MODE, SUBMODE)			\
+  DIRECT_NAMED (iota##MODE##2, viotaint##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN,			\
+		vector),						\
+  DIRECT_NAMED (iota##MODE##2, viotauint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN,			\
+		vector),						\
+  DIRECT_NAMED (iota##MODE##2_mask, viotaint##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VB##MLEN,\
+		vector),						\
+  DIRECT_NAMED (iota##MODE##2_mask, viotauint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VB##MLEN,\
+		vector),
+
+#define VID_BUILTINS(E, L, MLEN, MODE, SUBMODE)				\
+  DIRECT_NAMED (vid##MODE, viduint##E##m##L,				\
+		RISCV_VUI##E##M##L##_FTYPE,				\
+		vector),						\
+  DIRECT_NAMED (vid##MODE##_mask, viduint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L,	\
+		vector),
+
+#define VINT_WIDENING_ADD_SUB_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,\
+					      WE, WL, WVMODE, WSMODE, OP)\
+  DIRECT_NAMED (OP##VMODE##_vv, v##OP##_vv_i##E##m##L,			\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##E##M##L##_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_vv, v##OP##_vv_u##E##m##L,		\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_vv_scalar, v##OP##_vv_i##E##m##L##_scalar,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##E##M##L##_##SDEMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_vv_scalar, v##OP##_vv_u##E##m##L##_scalar,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##E##M##L##_U##SDEMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv, v##OP##_wv_i##E##m##L,			\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##WE##M##WL##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_wv, v##OP##_wv_u##E##m##L,		\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##WE##M##WL##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv_scalar, v##OP##_wv_i##E##m##L##_scalar,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##WE##M##WL##_##SDEMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_wv_scalar, v##OP##_wv_u##E##m##L##_scalar,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##WE##M##WL##_##U##SDEMODE,\
+		vector),
+
+#define VINT_WIDENING_ADD_SUB_BUILTINS(E, L, MLEN, VMODE, SDEMODE,	\
+				       WE, WL, WVMODE, WSMODE, OP)	\
+  VINT_WIDENING_ADD_SUB_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,	\
+					WE, WL, WVMODE, WSMODE, OP)	\
+  DIRECT_NAMED (OP##VMODE##_vv_mask, v##OP##_vv_i##E##m##L##_mask,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_vv_mask, v##OP##_vv_u##E##m##L##_mask,	\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_vv_scalar_mask, v##OP##_vv_i##E##m##L##_scalar_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L##_##SDEMODE,\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_vv_scalar_mask, v##OP##_vv_u##E##m##L##_scalar_mask,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##E##M##L##_U##SDEMODE,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv_mask, v##OP##_wv_i##E##m##L##_mask,	\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##WE##M##WL##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_wv_mask, v##OP##_wv_u##E##m##L##_mask,	\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##WE##M##WL##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv_scalar_mask, v##OP##_wv_i##E##m##L##_scalar_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##WE##M##WL##_##SDEMODE,\
+		vector),						\
+  DIRECT_NAMED (OP##u##VMODE##_wv_scalar_mask, v##OP##_wv_u##E##m##L##_scalar_mask,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##WE##M##WL##_U##SDEMODE,\
+		vector),
+
+#define VINT_WIDENING_CVT_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,	\
+					  WE, WL, WVMODE, WSMODE,	\
+					  OP, OPU, NAME)		\
+  DIRECT_NAMED (OP##VMODE##WVMODE##2, v##NAME##_vv_i##E##m##L,		\
+		RISCV_VI##WE##M##WL##_FTYPE_VI##E##M##L,		\
+		vector),						\
+  DIRECT_NAMED (OPU##VMODE##WVMODE##2, v##NAME##_vv_u##E##m##L,		\
+		RISCV_VUI##WE##M##WL##_FTYPE_VUI##E##M##L,		\
+		vector),
+
+#define VINT_WIDENING_CVT_BUILTINS(E, L, MLEN, VMODE, SDEMODE,		\
+				   WE, WL, WVMODE, WSMODE,		\
+				   OP, OPU, NAME)			\
+  VINT_WIDENING_CVT_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,		\
+				    WE, WL, WVMODE, WSMODE,		\
+				    OP, OPU, NAME)			\
+  DIRECT_NAMED (OP##VMODE##WVMODE##2_mask, v##NAME##_vv_i##E##m##L##_mask,\
+		RISCV_VI##WE##M##WL##_FTYPE_VB##MLEN##_VI##WE##M##WL##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OPU##VMODE##WVMODE##2_mask, v##NAME##_vv_u##E##m##L##_mask,\
+		RISCV_VUI##WE##M##WL##_FTYPE_VB##MLEN##_VUI##WE##M##WL##_VUI##E##M##L,\
+		vector),
+
+#define VFLOAT_WIDENING_ADD_SUB_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,\
+						WE, WL, WVMODE, WSMODE, OP)\
+  DIRECT_NAMED (OP##VMODE##_vv, v##OP##_vv_f##E##m##L,			\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##E##M##L##_VF##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_vv_scalar, v##OP##_vv_f##E##m##L##_scalar,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##E##M##L##_##SDEMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv, v##OP##_wv_f##E##m##L,			\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##WE##M##WL##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv_scalar, v##OP##_wv_f##E##m##L##_scalar,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VF##WE##M##WL##_##SDEMODE,	\
+		vector),
+
+#define VFLOAT_WIDENING_ADD_SUB_BUILTINS(E, L, MLEN, VMODE, SDEMODE,	\
+					 WE, WL, WVMODE, WSMODE, OP)	\
+  VFLOAT_WIDENING_ADD_SUB_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,	\
+					  WE, WL, WVMODE, WSMODE, OP)	\
+  DIRECT_NAMED (OP##VMODE##_vv_mask, v##OP##_vv_f##E##m##L##_mask,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_vv_scalar_mask, v##OP##_vv_f##E##m##L##_scalar_mask,\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##E##M##L##_##SDEMODE,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv_mask, v##OP##_wv_f##E##m##L##_mask,	\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##WE##M##WL##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##VMODE##_wv_scalar_mask, v##OP##_wv_f##E##m##L##_scalar_mask,\
+		RISCV_VF##WE##M##WL##_FTYPE_VB##MLEN##_VF##WE##M##WL##_VF##WE##M##WL##_##SDEMODE,\
+		vector),
+
+#define VINT_REDUC_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, OPU)	\
+  DIRECT_NAMED (reduc_##OP##MODE, reduc_##OP##int##E##m##L,		\
+		RISCV_VI##E##M1_FTYPE_VI##E##M1_VI##E##M1_VI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (reduc_##OPU##MODE, reduc_##OPU##uint##E##m##L,		\
+		RISCV_VUI##E##M1_FTYPE_VUI##E##M1_VUI##E##M1_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (reduc_##OP##MODE##_mask, reduc_##OP##int##E##m##L##_mask, \
+		RISCV_VI##E##M1_FTYPE_VB##MLEN##_VI##E##M1_VI##E##M1_VI##E##M##L, \
+		vector),						\
+  DIRECT_NAMED (reduc_##OPU##MODE##_mask, reduc_##OPU##uint##E##m##L##_mask,\
+		RISCV_VUI##E##M1_FTYPE_VB##MLEN##_VUI##E##M1_VUI##E##M1_VUI##E##M##L, \
+		vector),
+
+#define VINT_WREDUC_OP_BUILTINS(SEW, LMUL, MLEN, VMODE, SDEMODE,	\
+				WSEW, WLMUL, WVMODE, WSMODE, OP, OPU)	\
+  DIRECT_NAMED (wreduc_##OP##VMODE, wreduc_##OP##int##SEW##m##LMUL,	\
+		RISCV_VI##WSEW##M1_FTYPE_VI##WSEW##M1_VI##WSEW##M1_VI##SEW##M##LMUL,	\
+		vector),						\
+  DIRECT_NAMED (wreduc_##OPU##VMODE, wreduc_##OPU##uint##SEW##m##LMUL,	\
+		RISCV_VUI##WSEW##M1_FTYPE_VUI##WSEW##M1_VUI##WSEW##M1_VUI##SEW##M##LMUL,\
+		vector),						\
+  DIRECT_NAMED (wreduc_##OP##VMODE##_mask, wreduc_##OP##int##SEW##m##LMUL##_mask, \
+		RISCV_VI##WSEW##M1_FTYPE_VB##MLEN##_VI##WSEW##M1_VI##WSEW##M1_VI##SEW##M##LMUL, \
+		vector),						\
+  DIRECT_NAMED (wreduc_##OPU##VMODE##_mask, wreduc_##OPU##uint##SEW##m##LMUL##_mask,\
+		RISCV_VUI##WSEW##M1_FTYPE_VB##MLEN##_VUI##WSEW##M1_VUI##WSEW##M1_VUI##SEW##M##LMUL, \
+		vector),
+
+#define VFLOAT_REDUC_OP_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (reduc_##OP##MODE, freduc_##OP##float##E##m##L,		\
+		RISCV_VF##E##M1_FTYPE_VF##E##M1_VF##E##M1_VF##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (reduc_##OP##MODE##_mask, freduc_##OP##float##E##m##L##_mask, \
+		RISCV_VF##E##M1_FTYPE_VB##MLEN##_VF##E##M1_VF##E##M1_VF##E##M##L, \
+		vector),
+
+#define VFLOAT_WREDUC_OP_BUILTINS(SEW, LMUL, MLEN, VMODE, SDEMODE,	\
+				  WSEW, WLMUL, WVMODE, WSMODE, OP)	\
+  DIRECT_NAMED (wreduc_##OP##VMODE, fwreduc_##OP##float##SEW##m##LMUL,	\
+		RISCV_VF##WSEW##M1_FTYPE_VF##WSEW##M1_VF##WSEW##M1_VF##SEW##M##LMUL,\
+		vector),						\
+  DIRECT_NAMED (wreduc_##OP##VMODE##_mask, fwreduc_##OP##float##SEW##m##LMUL##_mask, \
+		RISCV_VF##WSEW##M1_FTYPE_VB##MLEN##_VF##WSEW##M1_VF##WSEW##M1_VF##SEW##M##LMUL, \
+		vector),
+
+
+#define VUNDEFINED_INT(E, L, MLEN, MODE, SUBMODE)			\
+  DIRECT_NAMED (vundefined_##MODE,					\
+		vundefined_i##E##m##L,					\
+		RISCV_VI##E##M##L##_FTYPE,				\
+		vector),						\
+  DIRECT_NAMED (vundefined_##MODE,					\
+		vundefined_u##E##m##L,					\
+		RISCV_VUI##E##M##L##_FTYPE,				\
+		vector),
+
+#define VUNDEFINED_FLOAT(E, L, MLEN, MODE, SUBMODE)			\
+  DIRECT_NAMED (vundefined_##MODE,					\
+		vundefined_f##E##m##L,					\
+		RISCV_VF##E##M##L##_FTYPE,				\
+		vector),						\
+
+#define VUNDEFINED_VT_INT(SEW, LMUL, NF, MLEN,				\
+			  SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			  VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER)	\
+  DIRECT_NAMED (							\
+    vundefined_##VMODE_PREFIX_LOWER##i,					\
+    vundefined_i##SEW##m##LMUL##x##NF,					\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE,				\
+    vector),								\
+  DIRECT_NAMED (							\
+    vundefined_##VMODE_PREFIX_LOWER##i,					\
+    vundefined_u##SEW##m##LMUL##x##NF,					\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE,				\
+    vector),								\
+
+#define VUNDEFINED_VT_FLOAT(SEW, LMUL, NF, MLEN,			\
+			    SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			    VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER)	\
+  DIRECT_NAMED (							\
+    vundefined_##VMODE_PREFIX_LOWER##f,					\
+    vundefined_f##SEW##m##LMUL##x##NF,					\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE,				\
+    vector),
+
+#define VREINTERPRET_INT(E, L, MLEN, IMODE, ISUBMODE)			\
+  DIRECT_NAMED (mov##IMODE,						\
+		vreinterpret_v_u##E##m##L##_i##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VUI##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (mov##IMODE,						\
+		vreinterpret_v_i##E##m##L##_u##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VI##E##M##L,			\
+		vector),
+
+#define VREINTERPRET_XSEW_INT(E, L, MLEN, IMODE, ISUBMODE, XE, XL, XMODE)\
+  DIRECT_NAMED (reinterpret_##IMODE##XMODE,				\
+		vreinterpret_v_u##XE##m##XL##_i##E##m##L,		\
+		RISCV_VI##E##M##L##_FTYPE_VUI##XE##M##XL,		\
+		vector),						\
+  DIRECT_NAMED (reinterpret_##IMODE##XMODE,				\
+		vreinterpret_v_i##XE##m##XL##_u##E##m##L,		\
+		RISCV_VUI##E##M##L##_FTYPE_VI##XE##M##XL,		\
+		vector),						\
+  DIRECT_NAMED (reinterpret_##IMODE##XMODE,				\
+		vreinterpret_v_u##XE##m##XL##_u##E##m##L,		\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##XE##M##XL,		\
+		vector),						\
+  DIRECT_NAMED (reinterpret_##IMODE##XMODE,				\
+		vreinterpret_v_i##XE##m##XL##_i##E##m##L,		\
+		RISCV_VI##E##M##L##_FTYPE_VI##XE##M##XL,		\
+		vector),
+
+#define VREINTERPRET_XSEW_FLOAT(E, L, MLEN, IMODE, ISUBMODE, XE, XL, XMODE)\
+  DIRECT_NAMED (reinterpret_##IMODE##XMODE,				\
+		vreinterpret_v_f##XE##m##XL##_f##E##m##L,		\
+		RISCV_VF##E##M##L##_FTYPE_VF##XE##M##XL,		\
+		vector),
+
+#define VREINTERPRET(E, L, MLEN, FMODE, FSUBMODE, IMODE, ISUBMODE)	\
+  DIRECT_NAMED (reinterpret_##FMODE##IMODE,				\
+		vreinterpret_v_i##E##m##L##_f##E##m##L,			\
+		RISCV_VF##E##M##L##_FTYPE_VI##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (reinterpret_##FMODE##IMODE,				\
+		vreinterpret_v_u##E##m##L##_f##E##m##L,			\
+		RISCV_VF##E##M##L##_FTYPE_VUI##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (reinterpret_##IMODE##FMODE,				\
+		vreinterpret_v_f##E##m##L##_i##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VF##E##M##L,			\
+		vector),						\
+  DIRECT_NAMED (reinterpret_##IMODE##FMODE,				\
+		vreinterpret_v_f##E##m##L##_u##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VF##E##M##L,			\
+		vector),
+
+#define VINT_SHIFT_BUILTINS_NOMASK(E, L, MLEN, MODE, OP)		\
+  DIRECT_NAMED (OP##MODE##3, v##OP##int##E##m##L,			\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  SHIFT_NAMED (OP##MODE##3_scalar, v##OP##int##E##m##L##_scalar,	\
+	       RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_UQI,		\
+	       vector),
+
+#define VINT_SHIFT_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VINT_SHIFT_BUILTINS_NOMASK(E, L, MLEN, MODE, OP)			\
+  DIRECT_NAMED (OP##MODE##3_mask, v##OP##int##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  SHIFT_MASK_NAMED (OP##MODE##3_scalar_mask, v##OP##int##E##m##L##_scalar_mask,\
+		    RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_UQI,\
+		    vector),
+
+#define VUINT_SHIFT_BUILTINS_NOMASK(E, L, MLEN, MODE, OP)		\
+  DIRECT_NAMED (OP##MODE##3, v##OP##uint##E##m##L,			\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  SHIFT_NAMED (OP##MODE##3_scalar, v##OP##uint##E##m##L##_scalar,	\
+	       RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_UQI,		\
+	       vector),
+
+#define VUINT_SHIFT_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  VUINT_SHIFT_BUILTINS_NOMASK(E, L, MLEN, MODE, OP)			\
+  DIRECT_NAMED (OP##MODE##3_mask, v##OP##uint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  SHIFT_MASK_NAMED (OP##MODE##3_scalar_mask, v##OP##uint##E##m##L##_scalar_mask,\
+		    RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_UQI,\
+		    vector),
+
+#define VINT_NARROWING_SHIFT_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,	\
+					     WE, WL, WVMODE, WSMODE, OP)	\
+  DIRECT_NAMED (OP##VMODE##3_nv, OP##int##WE##m##WL,				\
+		RISCV_VI##E##M##L##_FTYPE_VI##WE##M##WL##_VUI##E##M##L,		\
+		vector),							\
+  SHIFT_NAMED (OP##VMODE##3_nv_scalar, OP##int##WE##m##WL##_scalar,		\
+	       RISCV_VI##E##M##L##_FTYPE_VI##WE##M##WL##_UQI,			\
+	       vector),
+
+#define VINT_NARROWING_SHIFT_BUILTINS(E, L, MLEN, VMODE, SDEMODE,		\
+				      WE, WL, WVMODE, WSMODE, OP)		\
+  VINT_NARROWING_SHIFT_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,		\
+				       WE, WL, WVMODE, WSMODE, OP)		\
+  DIRECT_NAMED (OP##VMODE##3_nv_mask, OP##int##WE##m##WL##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##WE##M##WL##_VUI##E##M##L,\
+		vector),							\
+  SHIFT_MASK_NAMED (OP##VMODE##3_nv_scalar_mask, OP##int##WE##m##WL##_scalar_mask,\
+		    RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##WE##M##WL##_UQI,\
+		    vector),
+
+#define VUINT_NARROWING_SHIFT_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,	\
+					      WE, WL, WVMODE, WSMODE, OP)	\
+  DIRECT_NAMED (OP##VMODE##3_nv, OP##uint##WE##m##WL,				\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##WE##M##WL##_VUI##E##M##L,	\
+		vector),							\
+  SHIFT_NAMED (OP##VMODE##3_nv_scalar, OP##uint##WE##m##WL##_scalar,		\
+	       RISCV_VUI##E##M##L##_FTYPE_VUI##WE##M##WL##_UQI,			\
+	       vector),
+
+#define VUINT_NARROWING_SHIFT_BUILTINS(E, L, MLEN, VMODE, SDEMODE,	\
+				       WE, WL, WVMODE, WSMODE, OP)	\
+  VUINT_NARROWING_SHIFT_BUILTINS_NOMASK(E, L, MLEN, VMODE, SDEMODE,	\
+					WE, WL, WVMODE, WSMODE, OP)	\
+  DIRECT_NAMED (OP##VMODE##3_nv_mask, OP##uint##WE##m##WL##_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##WE##M##WL##_VUI##E##M##L,\
+		vector),						\
+  SHIFT_MASK_NAMED (OP##VMODE##3_nv_scalar_mask, OP##uint##WE##m##WL##_scalar_mask,\
+		    RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##WE##M##WL##_UQI,\
+		    vector),
+
+#define VINT_MERGE_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##MODE##cc, v##NAME##i##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##cc, v##NAME##u##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##cc_scalar, v##NAME##i##E##m##L##_scalar_mask,	\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_##SUBMODE,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##cc_scalar, v##NAME##u##E##m##L##_scalar_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_U##SUBMODE,\
+		vector),
+
+#define VFLOAT_MERGE_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP, NAME)	\
+  DIRECT_NAMED (OP##MODE##cc, v##NAME##f##E##m##L##_mask,		\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##cc_scalar, v##NAME##f##E##m##L##_scalar_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_##SUBMODE,\
+		vector),
+
+#define VINT_SLIDE_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##si, OP##int##E##m##L##_si,		\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L##_SIZE,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di, OP##int##E##m##L##_di,		\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VI##E##M##L##_SIZE,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##si, OP##u##E##m##L##_si,		\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L##_SIZE,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di, OP##u##E##m##L##_di,		\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L##_SIZE,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##si_mask, OP##int##E##m##L##_si_mask,	\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_SIZE,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di_mask, OP##int##E##m##L##_di_mask,	\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_SIZE,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##si_mask, OP##u##E##m##L##_si_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_SIZE,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di_mask, OP##u##E##m##L##_di_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_SIZE,\
+		vector),
+
+#define VINT_SLIDE1_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##si, OP##int##E##m##L##_si,		\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_LONG,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di, OP##int##E##m##L##_di,		\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_LONG,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##si, OP##u##E##m##L##_si,		\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_LONG,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di, OP##u##E##m##L##_di,		\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_LONG,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##si_mask, OP##int##E##m##L##_si_mask,	\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_LONG,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di_mask, OP##int##E##m##L##_di_mask,	\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_LONG,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##si_mask, OP##u##E##m##L##_si_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_LONG,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di_mask, OP##u##E##m##L##_di_mask,	\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_LONG,\
+		vector),
+
+#define VFLOAT_SLIDE_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##si, OP##f##E##m##L##_si,		\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_VF##E##M##L##_SIZE,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di, OP##f##E##m##L##_di,		\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_VF##E##M##L##_SIZE,	\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##si_mask, OP##f##E##m##L##_si_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_SIZE,\
+		vector),					\
+  DIRECT_NAMED (OP##MODE##di_mask, OP##f##E##m##L##_di_mask,	\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_SIZE,\
+		vector),
+
+#define VFLOAT_SLIDE1_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE, OP##f##E##m##L,				\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_##SUBMODE,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_mask, OP##f##E##m##L##_mask,			\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_##SUBMODE,\
+		vector),
+
+#define VINT_VRGATHER_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE, v##OP##int##E##m##L,				\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE, v##OP##uint##E##m##L,				\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##si_scalar, v##OP##int##E##m##L##_si_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_SIZE,		\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##si_scalar, v##OP##uint##E##m##L##_si_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_SIZE,		\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##di_scalar, v##OP##int##E##m##L##_di_scalar,	\
+		RISCV_VI##E##M##L##_FTYPE_VI##E##M##L##_SIZE,		\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##di_scalar, v##OP##uint##E##m##L##_di_scalar,	\
+		RISCV_VUI##E##M##L##_FTYPE_VUI##E##M##L##_SIZE,		\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_mask, v##OP##int##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_mask, v##OP##uint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##si_scalar_mask, v##OP##int##E##m##L##_si_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_SIZE,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##si_scalar_mask, v##OP##uint##E##m##L##_si_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_SIZE,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##di_scalar_mask, v##OP##int##E##m##L##_di_scalar_mask,\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L##_SIZE,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##di_scalar_mask, v##OP##uint##E##m##L##_di_scalar_mask,\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L##_SIZE,\
+		vector),
+
+#define VFLOAT_VRGATHER_BUILTINS(E, L, MLEN, FMODE, FSMODE,		\
+				 IMODE, ISMODE, OP)			\
+  DIRECT_NAMED (OP##FMODE, v##OP##f##E##m##L,				\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_VUI##E##M##L,	\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##si_scalar, v##OP##f##E##m##L##_si_scalar,	\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_SIZE,		\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##di_scalar, v##OP##f##E##m##L##_di_scalar,	\
+		RISCV_VF##E##M##L##_FTYPE_VF##E##M##L##_SIZE,		\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##_mask, v##OP##f##E##m##L##_mask,		\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_VUI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##si_scalar_mask, v##OP##f##E##m##L##_si_scalar_mask,\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_SIZE,\
+		vector),						\
+  DIRECT_NAMED (OP##FMODE##di_scalar_mask, v##OP##f##E##m##L##_di_scalar_mask,\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L##_SIZE,\
+		vector),
+
+#define VINT_VCOMPRESS_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)		\
+  DIRECT_NAMED (OP##MODE##_mask, v##OP##int##E##m##L##_mask,		\
+		RISCV_VI##E##M##L##_FTYPE_VB##MLEN##_VI##E##M##L##_VI##E##M##L,\
+		vector),						\
+  DIRECT_NAMED (OP##MODE##_mask, v##OP##uint##E##m##L##_mask,		\
+		RISCV_VUI##E##M##L##_FTYPE_VB##MLEN##_VUI##E##M##L##_VUI##E##M##L,\
+		vector),
+
+#define VFLOAT_VCOMPRESS_BUILTINS(E, L, MLEN, MODE, SUBMODE, OP)	\
+  DIRECT_NAMED (OP##MODE##_mask, v##OP##f##E##m##L##_mask,		\
+		RISCV_VF##E##M##L##_FTYPE_VB##MLEN##_VF##E##M##L##_VF##E##M##L,\
+		vector),
+
+#define _VSEG_LOAD_STORE(E, L, NF, MLEN, MODE, SUBMODE,	\
+			     PNAME, PMODE, SUBTYPE, VCLASS)\
+  DIRECT_NAMED (							\
+    vseg_load##MODE##_##PNAME,				\
+    vseg_load##SUBTYPE##E##m##L##x##NF##_##PNAME,			\
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_C_##SUBMODE##_PTR,		\
+    vector),								\
+  DIRECT_NAMED (							\
+    vseg_load##MODE##_##PNAME##_mask,					\
+    vseg_load##SUBTYPE##E##m##L##x##NF##_##PNAME##_mask,		\
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##X##NF##_C_##SUBMODE##_PTR,	\
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vseg_store##MODE##_##PNAME,				\
+    vseg_store##SUBTYPE##E##m##L##x##NF##_##PNAME,			\
+    RISCV_VOID_FTYPE_##VCLASS##E##M##L##X##NF##_##SUBMODE##_PTR,	\
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vseg_store##MODE##_##PNAME##_mask,					\
+    vseg_store##SUBTYPE##E##m##L##x##NF##_##PNAME##_mask,		\
+    RISCV_VOID_FTYPE_VB##MLEN##_##VCLASS##E##M##L##X##NF##_##SUBMODE##_PTR,	\
+    vector),								\
+  DIRECT_NAMED (							\
+    vseg_strided_load##MODE##_##PNAME,					\
+    vseg_strided_load##SUBTYPE##E##m##L##x##NF##_##PNAME,		\
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_C_##SUBMODE##_PTR_PTRDIFF,	\
+    vector),								\
+  DIRECT_NAMED (							\
+    vseg_strided_load##MODE##_##PNAME##_mask,				\
+    vseg_strided_load##SUBTYPE##E##m##L##x##NF##_##PNAME##_mask,	\
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##X##NF##_C_##SUBMODE##_PTR_PTRDIFF,	\
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vseg_strided_store##MODE##_##PNAME,					\
+    vseg_strided_store##SUBTYPE##E##m##L##x##NF##_##PNAME,		\
+    RISCV_VOID_FTYPE_##VCLASS##E##M##L##X##NF##_##SUBMODE##_PTR_PTRDIFF,\
+    vector),								\
+  DIRECT_NAMED_NO_TARGET (						\
+    vseg_strided_store##MODE##_##PNAME##_mask,				\
+    vseg_strided_store##SUBTYPE##E##m##L##x##NF##_##PNAME##_mask,	\
+    RISCV_VOID_FTYPE_VB##MLEN##_##VCLASS##E##M##L##X##NF##_##SUBMODE##_PTR_PTRDIFF,	\
+    vector),								\
+  DIRECT_NAMED (							\
+    vseg_ff_load##MODE##_##PNAME,					\
+    vseg_ff_load##SUBTYPE##E##m##L##x##NF##_##PNAME,			\
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_C_##SUBMODE##_PTR,		\
+    vector),								\
+  DIRECT_NAMED (							\
+    vseg_ff_load##MODE##_##PNAME##_mask,				\
+    vseg_ff_load##SUBTYPE##E##m##L##x##NF##_##PNAME##_mask,		\
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##X##NF##_C_##SUBMODE##_PTR,	\
+    vector),
+
+#define VINT_SEG_LOAD_STORE(SEW, LMUL, NF, MLEN,			\
+			    SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			    VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  _VSEG_LOAD_STORE(SEW, LMUL, NF, MLEN, VMODE_PREFIX_LOWER##i, SMODE_PREFIX_UPPER##I, si, SI,\
+		       int, VI)						\
+  _VSEG_LOAD_STORE(SEW, LMUL, NF, MLEN, VMODE_PREFIX_LOWER##i, SMODE_PREFIX_UPPER##I, di, DI,\
+		       int, VI)						\
+  _VSEG_LOAD_STORE(SEW, LMUL, NF, MLEN, VMODE_PREFIX_LOWER##i, U##SMODE_PREFIX_UPPER##I, si, SI,\
+		       uint, VUI)					\
+  _VSEG_LOAD_STORE(SEW, LMUL, NF, MLEN, VMODE_PREFIX_LOWER##i, U##SMODE_PREFIX_UPPER##I, di, DI,\
+		       uint, VUI)
+
+#define _VINT_INDEX_SEG_LOAD_STORE_BUILTINS(E, L, NF, MLEN, MODE, SUBMODE,\
+					    IE, IL, IMODE, ISUBMODE,	\
+					    PNAME, TYPE_US, VCLASS)	\
+  DIRECT_NAMED (							\
+    vseg_idx_load##MODE##IMODE##_##PNAME,                               \
+    vseg_idx_load##TYPE_US##E##m##L##x##NF##_##IE##m##IL##_##PNAME,     \
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_C_##SUBMODE##_PTR_VUI##IE##M##IL,\
+    vector),                                                            \
+  DIRECT_NAMED (                                                       \
+    vseg_idx_load##MODE##IMODE##_##PNAME##_mask,                              \
+    vseg_idx_load##TYPE_US##E##m##L##x##NF##_##IE##m##IL##_##PNAME##_mask,           \
+    RISCV_##VCLASS##E##M##L##X##NF##_FTYPE_VB##MLEN##_##VCLASS##E##M##L##X##NF##_C_##SUBMODE##_PTR_VUI##IE##M##IL,       \
+    vector),                                                           \
+  DIRECT_NAMED_NO_TARGET (                                             \
+    vseg_idx_store##MODE##IMODE##_##PNAME,                            \
+    vseg_idx_store##TYPE_US##E##m##L##x##NF##_##IE##m##IL##_##PNAME,                 \
+    RISCV_VOID_FTYPE_##VCLASS##E##M##L##X##NF##_##SUBMODE##_PTR_VUI##IE##M##IL,  \
+    vector),                                                           \
+  DIRECT_NAMED_NO_TARGET (                                             \
+    vseg_idx_store##MODE##IMODE##_##PNAME##_mask,                                     \
+    vseg_idx_store##TYPE_US##E##m##L##x##NF##_##IE##m##IL##_##PNAME##_mask,          \
+    RISCV_VOID_FTYPE_VB##MLEN##_##VCLASS##E##M##L##X##NF##_##SUBMODE##_PTR_VUI##IE##M##IL,       \
+    vector),
+
+
+#define VINT_INDEX_SEG_LOAD_STORE_BUILTINS(E, L, NF, MLEN, MODE, SUBMODE,\
+					   IE, IL, IMODE, ISUBMODE)	\
+  _VINT_INDEX_SEG_LOAD_STORE_BUILTINS (E, L, NF, MLEN, MODE, SUBMODE,	\
+				       IE, IL, IMODE, ISUBMODE, si, i, VI)\
+  _VINT_INDEX_SEG_LOAD_STORE_BUILTINS (E, L, NF, MLEN, MODE, SUBMODE,	\
+				       IE, IL, IMODE, ISUBMODE, di, i, VI)\
+  _VINT_INDEX_SEG_LOAD_STORE_BUILTINS (E, L, NF, MLEN, MODE, U##SUBMODE,\
+				       IE, IL, IMODE, ISUBMODE, si, u, VUI)\
+  _VINT_INDEX_SEG_LOAD_STORE_BUILTINS (E, L, NF, MLEN, MODE, U##SUBMODE,\
+				       IE, IL, IMODE, ISUBMODE, di, u, VUI)
+
+#define VFLOAT_SEG_LOAD_STORE(SEW, LMUL, NF, MLEN,			\
+			      SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			      VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)\
+  _VSEG_LOAD_STORE(SEW, LMUL, NF, MLEN, VMODE_PREFIX_LOWER##f, SMODE_PREFIX_UPPER##F, si, SI,\
+			 float, VF)						\
+  _VSEG_LOAD_STORE(SEW, LMUL, NF, MLEN, VMODE_PREFIX_LOWER##f, SMODE_PREFIX_UPPER##F, di, DI,\
+			 float, VF)
+
+#define VFLOAT_INDEX_SEG_LOAD_STORE_BUILTINS(E, L, NF, MLEN, MODE, SUBMODE,\
+					   IE, IL, IMODE, ISUBMODE)	\
+  _VINT_INDEX_SEG_LOAD_STORE_BUILTINS (E, L, NF, MLEN, MODE, SUBMODE,	\
+				       IE, IL, IMODE, ISUBMODE, si, f, VF)\
+  _VINT_INDEX_SEG_LOAD_STORE_BUILTINS (E, L, NF, MLEN, MODE, SUBMODE,	\
+				       IE, IL, IMODE, ISUBMODE, di, f, VF)\
+
+#define VINT_SEG_INSERT(SEW, LMUL, NF, MLEN,				\
+			SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,		\
+			VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_insert##VMODE_PREFIX_LOWER##i,				\
+    vtuple_insertint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##X##NF##_VI##SEW##M##LMUL##_SI,\
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_insert##VMODE_PREFIX_LOWER##i,				\
+    vtuple_insertuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##X##NF##_VUI##SEW##M##LMUL##_SI,\
+    vector),
+
+#define VFLOAT_SEG_INSERT(SEW, LMUL, NF, MLEN,				\
+			  SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			  VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_insert##VMODE_PREFIX_LOWER##f,				\
+    vtuple_insertfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##X##NF##	_VF##SEW##M##LMUL##_SI,\
+    vector),								\
+
+#define VINT_SEG_EXTRACT(SEW, LMUL, NF, MLEN,				\
+			SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,		\
+			VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_extract##VMODE_PREFIX_LOWER##i,				\
+    vtuple_extractint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##_FTYPE_VI##SEW##M##LMUL##X##NF##_SI,	\
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_extract##VMODE_PREFIX_LOWER##i,				\
+    vtuple_extractuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##_FTYPE_VUI##SEW##M##LMUL##X##NF##_SI,	\
+    vector),
+
+#define VFLOAT_SEG_EXTRACT(SEW, LMUL, NF, MLEN,				\
+			SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,		\
+			VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_extract##VMODE_PREFIX_LOWER##f,				\
+    vtuple_extractfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##_FTYPE_VF##SEW##M##LMUL##X##NF##_SI,	\
+    vector),
+
+#define VINT_SEG_CREATE2(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##_VI##SEW##M##LMUL, \
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL, \
+    vector),
+
+#define VINT_SEG_CREATE3(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL, \
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL, \
+    vector),
+
+#define VINT_SEG_CREATE4(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL, \
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL, \
+    vector),
+
+#define VINT_SEG_CREATE5(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL, \
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL, \
+    vector),
+
+#define VINT_SEG_CREATE6(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL, \
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL, \
+    vector),
+
+#define VINT_SEG_CREATE7(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL, \
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL, \
+    vector),
+
+#define VINT_SEG_CREATE8(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createint##SEW##m##LMUL##x##NF,				\
+    RISCV_VI##SEW##M##LMUL##X##NF##_FTYPE_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL##_VI##SEW##M##LMUL, \
+    vector),								\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##i,				\
+    vtuple_createuint##SEW##m##LMUL##x##NF,				\
+    RISCV_VUI##SEW##M##LMUL##X##NF##_FTYPE_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL##_VUI##SEW##M##LMUL, \
+    vector),
+
+#define VFLOAT_SEG_CREATE2(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##f,				\
+    vtuple_createfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##_VF##SEW##M##LMUL, \
+    vector),
+
+#define VFLOAT_SEG_CREATE3(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##f,				\
+    vtuple_createfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL, \
+    vector),
+
+#define VFLOAT_SEG_CREATE4(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##f,				\
+    vtuple_createfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL, \
+    vector),
+
+#define VFLOAT_SEG_CREATE5(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##f,				\
+    vtuple_createfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL, \
+    vector),
+
+#define VFLOAT_SEG_CREATE6(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##f,				\
+    vtuple_createfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL, \
+    vector),
+
+#define VFLOAT_SEG_CREATE7(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##f,				\
+    vtuple_createfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL, \
+    vector),
+
+#define VFLOAT_SEG_CREATE8(SEW, LMUL, NF, MLEN,				\
+			 SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			 VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER, XX)	\
+  DIRECT_NAMED (							\
+    vtuple_create##VMODE_PREFIX_LOWER##f,				\
+    vtuple_createfloat##SEW##m##LMUL##x##NF,				\
+    RISCV_VF##SEW##M##LMUL##X##NF##_FTYPE_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL##_VF##SEW##M##LMUL, \
+    vector),
 
 static const struct riscv_builtin_description riscv_builtins[] = {
   DIRECT_BUILTIN (frflags, RISCV_USI_FTYPE, hard_float),
-  DIRECT_NO_TARGET_BUILTIN (fsflags, RISCV_VOID_FTYPE_USI, hard_float)
+  DIRECT_NO_TARGET_BUILTIN (fsflags, RISCV_VOID_FTYPE_USI, hard_float),
+
+  DIRECT_BUILTIN (vreadvlsi, RISCV_SIZE_FTYPE, vector),
+  DIRECT_BUILTIN (vreadvldi, RISCV_SIZE_FTYPE, vector),
+
+  _RVV_INT_ITERATOR (SETVL_BUILTINS)
+  _RVV_INT_ITERATOR (SETVTYPE_BUILTINS)
+
+  _RVV_INT_ITERATOR (VINT_STRIDED_LOAD_STORE_BUILTINS)
+  _RVV_FLOAT_ITERATOR (VFLOAT_STRIDED_LOAD_STORE_BUILTINS)
+
+  _RVV_INT_ITERATOR (VINT_LOAD_STORE_BUILTINS)
+  _RVV_FLOAT_ITERATOR (VFLOAT_LOAD_STORE_BUILTINS)
+
+  _RVV_INT_ITERATOR (VINT_LOAD_FF_BUILTINS)
+  _RVV_FLOAT_ITERATOR (VFLOAT_LOAD_FF_BUILTINS)
+
+  _RVV_INT_INDEX_ITERATOR (VINT_INDEX_LOAD_STORE_BUILTINS)
+  _RVV_FLOAT_INDEX_ITERATOR (VFLOAT_INDEX_LOAD_STORE_BUILTINS)
+
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_BUILTINS, add)
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_BUILTINS, sub)
+  _RVV_INT_ITERATOR_ARG (VINT_SCALAR_ONLY_BIN_OP_BUILTINS, rsub)
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_BUILTINS, mul)
+
+  _RVV_INT_ITERATOR_ARG (VINT_SAT_BIN_BUILTINS, ssadd)
+  _RVV_INT_ITERATOR_ARG (VINT_SAT_BIN_BUILTINS, sssub)
+  _RVV_INT_ITERATOR_ARG (VUINT_SAT_BIN_BUILTINS, usadd)
+  _RVV_INT_ITERATOR_ARG (VUINT_SAT_BIN_BUILTINS, ussub)
+  _RVV_INT_ITERATOR_ARG (VINT_SAT_BIN_BUILTINS, vaadd)
+  _RVV_INT_ITERATOR_ARG (VINT_SAT_BIN_BUILTINS, vasub)
+  _RVV_INT_ITERATOR_ARG (VUINT_SAT_BIN_BUILTINS, vaaddu)
+  _RVV_INT_ITERATOR_ARG (VUINT_SAT_BIN_BUILTINS, vasubu)
+  _RVV_INT_ITERATOR_ARG (VINT_SAT_BIN_BUILTINS, vsmul)
+
+  _RVV_INT_ITERATOR_ARG (VINT_MERGE_BUILTINS, mov, merge)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MERGE_BUILTINS, mov, merge)
+
+  _RVV_INT_ITERATOR_ARG (VINT_SHIFT_BUILTINS, vashl)
+  _RVV_INT_ITERATOR_ARG (VINT_SHIFT_BUILTINS, vashr)
+  _RVV_INT_ITERATOR_ARG (VUINT_SHIFT_BUILTINS, vashl)
+  _RVV_INT_ITERATOR_ARG (VUINT_SHIFT_BUILTINS, vlshr)
+
+  _RVV_INT_ITERATOR_ARG (VINT_SHIFT_BUILTINS, vssra)
+  _RVV_INT_ITERATOR_ARG (VUINT_SHIFT_BUILTINS, vssrl)
+
+  _RVV_WINT_ITERATOR_ARG (VUINT_NARROWING_SHIFT_BUILTINS, vnsrl)
+  _RVV_WINT_ITERATOR_ARG (VINT_NARROWING_SHIFT_BUILTINS, vnsra)
+  _RVV_WINT_ITERATOR_ARG (VUINT_NARROWING_SHIFT_BUILTINS, vnclipu)
+  _RVV_WINT_ITERATOR_ARG (VINT_NARROWING_SHIFT_BUILTINS, vnclip)
+
+  _RVV_INT_ITERATOR_ARG (VINT_ADC_SBC_BUILTINS, adc)
+  _RVV_INT_ITERATOR_ARG (VINT_ADC_SBC_BUILTINS, sbc)
+
+  _RVV_WINT_ITERATOR_ARG (VINT_WIDENING_ADD_SUB_BUILTINS, wadd)
+  _RVV_WINT_ITERATOR_ARG (VINT_WIDENING_ADD_SUB_BUILTINS, wsub)
+  _RVV_WINT_ITERATOR_ARG (VINT_WMUL_BUILTINS, wmul)
+  _RVV_WINT_ITERATOR_ARG (VINT_WIDENING_MAC_OP_BUILTINS, madd, wmacc)
+  _RVV_WINT_ITERATOR_ARG (VINT_WIDENING_MAC_OP_BUILTINS_SCALAR, madd, wmacc)
+  _RVV_QINT_ITERATOR_ARG (VINT_WIDENING_MAC_OP_BUILTINS, madd, qmacc)
+  _RVV_QINT_ITERATOR_ARG (VINT_WIDENING_MAC_OP_BUILTINS_SCALAR, madd, qmacc)
+
+  _RVV_WINT_ITERATOR_ARG (VINT_WIDENING_CVT_BUILTINS,
+			  wcvt, wcvtu, wcvt)
+  _RVV_WINT_ITERATOR_ARG (VINT_WIDENING_CVT_BUILTINS,
+			  extend, zero_extend, extend)
+  _RVV_QINT_ITERATOR_ARG (VINT_WIDENING_CVT_BUILTINS,
+			  extend, zero_extend, extend_q)
+  _RVV_EINT_ITERATOR_ARG (VINT_WIDENING_CVT_BUILTINS,
+			  extend, zero_extend, extend_e)
+
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_BUILTINS, and)
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_BUILTINS, ior)
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_BUILTINS, xor)
+  _RVV_INT_ITERATOR_ARG (VINT_UNARY_OP_BUILTINS, one_cmpl)
+  _RVV_INT_ITERATOR_ARG (VINT_UNARY_OP_BUILTINS_SCALAR, vec_duplicate)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_UNARY_OP_BUILTINS_SCALAR, vec_duplicate)
+
+  _RVV_INT_ITERATOR_ARG (ICMP_BUILTINS, eq, eq)
+  _RVV_INT_ITERATOR_ARG (ICMP_BUILTINS, ne, ne)
+  _RVV_INT_ITERATOR_ARG (ICMP_BUILTINS, lt, ltu)
+  _RVV_INT_ITERATOR_ARG (ICMP_BUILTINS, le, leu)
+  _RVV_INT_ITERATOR_ARG (ICMP_BUILTINS, gt, gtu)
+  _RVV_INT_ITERATOR_ARG (ICMP_BUILTINS, ge, geu)
+
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_OPU_BUILTINS, smax, umax)
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_OPU_BUILTINS, smin, umin)
+
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_OPU_BUILTINS, div, udiv)
+  _RVV_INT_ITERATOR_ARG (VINT_BIN_OP_OPU_BUILTINS, mod, umod)
+
+  _RVV_INT_ITERATOR_ARG (VINT_MAC_OP_BUILTINS, macc)
+  _RVV_INT_ITERATOR_ARG (VINT_MAC_OP_BUILTINS, madd)
+  _RVV_INT_ITERATOR_ARG (VINT_MAC_OP_BUILTINS, nmsac)
+  _RVV_INT_ITERATOR_ARG (VINT_MAC_OP_BUILTINS, nmsub)
+
+  _RVV_INT_ITERATOR_ARG (VINT_SLIDE_BUILTINS, vslideup)
+  _RVV_INT_ITERATOR_ARG (VINT_SLIDE_BUILTINS, vslidedown)
+  _RVV_INT_ITERATOR_ARG (VINT_SLIDE1_BUILTINS, vslide1up)
+  _RVV_INT_ITERATOR_ARG (VINT_SLIDE1_BUILTINS, vslide1down)
+
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_SLIDE_BUILTINS, vslideup)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_SLIDE_BUILTINS, vslidedown)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_SLIDE1_BUILTINS, vfslide1up)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_SLIDE1_BUILTINS, vfslide1down)
+
+  _RVV_INT_ITERATOR_ARG (VINT_VRGATHER_BUILTINS, vrgather)
+  _RVV_FLOAT_INT_ITERATOR_ARG (VFLOAT_VRGATHER_BUILTINS, vrgather)
+
+  _RVV_INT_ITERATOR_ARG (VINT_VCOMPRESS_BUILTINS, vcompress)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_VCOMPRESS_BUILTINS, vcompress)
+
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, add)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, sub)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_SCALAR_ONLY_BIN_OP_BUILTINS, rsub)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, mul)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, div)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_SCALAR_ONLY_BIN_OP_BUILTINS, rdiv)
+  _RVV_INT_ITERATOR_ARG (VINT_MULH_BUILTINS, mulh)
+
+  _RVV_INT_ITERATOR_ARG (VINT_MV_X_S_BUILTINS, vec_extract, mv_xs)
+  _RVV_INT_ITERATOR_ARG (VINT_MV_S_X_BUILTINS, vec_set, mv_sx)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MV_F_S_BUILTINS, vec_extract_fext, mv_fs)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MV_S_F_BUILTINS, vec_set, mv_sf)
+
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, max)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, min)
+
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, copysign)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, ncopysign)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_BIN_OP_BUILTINS, xorsign)
+
+  _RVV_FLOAT_ITERATOR_ARG (FCMP_BUILTINS, feq)
+  _RVV_FLOAT_ITERATOR_ARG (FCMP_BUILTINS, fne)
+  _RVV_FLOAT_ITERATOR_ARG (FCMP_BUILTINS, flt)
+  _RVV_FLOAT_ITERATOR_ARG (FCMP_BUILTINS, fle)
+  _RVV_FLOAT_ITERATOR_ARG (FCMP_BUILTINS, fgt)
+  _RVV_FLOAT_ITERATOR_ARG (FCMP_BUILTINS, fge)
+
+  _RVV_FLOAT_INT_ITERATOR_ARG (VFLOAT_CVT_FX_BUILTINS, float, fcvt_fx)
+  _RVV_FLOAT_INT_ITERATOR_ARG (VFLOAT_CVT_FXU_BUILTINS, floatuns, fcvt_fxu)
+  _RVV_FLOAT_INT_ITERATOR_ARG (VFLOAT_CVT_XF_BUILTINS, lrint, fcvt_xf)
+  _RVV_FLOAT_INT_ITERATOR_ARG (VFLOAT_CVT_XF_BUILTINS, fix_trunc, fcvt_rtz_xf)
+  _RVV_FLOAT_INT_ITERATOR_ARG (VFLOAT_CVT_XUF_BUILTINS, fcvt_xuf, fcvt_xuf)
+  _RVV_FLOAT_INT_ITERATOR_ARG (VFLOAT_CVT_XUF_BUILTINS,
+			       fixuns_trunc, fcvt_rtz_xuf)
+  _RVV_WFLOAT_INT_ITERATOR_ARG (VFLOAT_WCVT_FX_BUILTINS, float, wfcvt_fx)
+  _RVV_WFLOAT_INT_ITERATOR_ARG (VFLOAT_WCVT_FXU_BUILTINS, floatuns, wfcvt_fxu)
+  _RVV_FLOAT_WINT_ITERATOR_ARG (VFLOAT_WCVT_XF_BUILTINS, lrint, wfcvt_xf)
+  _RVV_FLOAT_WINT_ITERATOR_ARG (VFLOAT_WCVT_XF_BUILTINS, fix_trunc, wfcvt_rtz_xf)
+  _RVV_FLOAT_WINT_ITERATOR_ARG (VFLOAT_WCVT_XUF_BUILTINS, fcvt_xuf, wfcvt_xuf)
+  _RVV_FLOAT_WINT_ITERATOR_ARG (VFLOAT_WCVT_XUF_BUILTINS,
+				fixuns_trunc, wfcvt_rtz_xuf)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WCVT_FF_BUILTINS, extend, wfcvt_ff)
+
+  _RVV_FLOAT_WINT_ITERATOR_ARG (VFLOAT_NCVT_FX_BUILTINS, float, nfcvt_fx)
+  _RVV_FLOAT_WINT_ITERATOR_ARG (VFLOAT_NCVT_FXU_BUILTINS, floatuns, nfcvt_fxu)
+  _RVV_WFLOAT_INT_ITERATOR_ARG (VFLOAT_NCVT_XF_BUILTINS, lrint, nfcvt_xf)
+  _RVV_WFLOAT_INT_ITERATOR_ARG (VFLOAT_NCVT_XF_BUILTINS, fix_trunc, nfcvt_rtz_xf)
+  _RVV_WFLOAT_INT_ITERATOR_ARG (VFLOAT_NCVT_XUF_BUILTINS, fcvt_xuf, nfcvt_xuf)
+  _RVV_WFLOAT_INT_ITERATOR_ARG (VFLOAT_NCVT_XUF_BUILTINS,
+				fixuns_trunc, nfcvt_rtz_xuf)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_NCVT_FF_BUILTINS, trunc, nfcvt_ff)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_NCVT_FF_BUILTINS, trunc_rod, nfcvt_rod_ff)
+
+  _RVV_MASK_ITERATOR_ARG (MASK_NULLARY_BUILTINS, clr)
+  _RVV_MASK_ITERATOR_ARG (MASK_NULLARY_BUILTINS, set)
+  _RVV_MASK_ITERATOR_ARG (MASK_UNARY_BUILTINS, sbf)
+  _RVV_MASK_ITERATOR_ARG (MASK_UNARY_BUILTINS, sof)
+  _RVV_MASK_ITERATOR_ARG (MASK_UNARY_BUILTINS, sif)
+  _RVV_MASK_ITERATOR_ARG (UNMAKED_MASK_UNARY_BUILTINS, not)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, and)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, xor)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, or)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, nand)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, nor)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, xnor)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, andnot)
+  _RVV_MASK_ITERATOR_ARG (MASK_LOGICAL_BUILTINS, ornot)
+  _RVV_MASK_ITERATOR_ARG (MASK_SCALAR_UNARY_BUILTINS, popc)
+  _RVV_MASK_ITERATOR_ARG (MASK_SCALAR_UNARY_BUILTINS, first)
+  _RVV_INT_ITERATOR (IOTA_BUILTINS)
+  _RVV_INT_ITERATOR (VID_BUILTINS)
+
+  _RVV_INT_ITERATOR_ARG (VINT_REDUC_OP_BUILTINS, sum, sum)
+  _RVV_INT_ITERATOR_ARG (VINT_REDUC_OP_BUILTINS, max, maxu)
+  _RVV_INT_ITERATOR_ARG (VINT_REDUC_OP_BUILTINS, min, minu)
+  _RVV_INT_ITERATOR_ARG (VINT_REDUC_OP_BUILTINS, and, and)
+  _RVV_INT_ITERATOR_ARG (VINT_REDUC_OP_BUILTINS, or, or)
+  _RVV_INT_ITERATOR_ARG (VINT_REDUC_OP_BUILTINS, xor, xor)
+
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_UNARY_OP_BUILTINS, sqrt)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_REDUC_OP_BUILTINS, sum)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_REDUC_OP_BUILTINS, osum)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_REDUC_OP_BUILTINS, max)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_REDUC_OP_BUILTINS, min)
+
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, macc)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, nmacc)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, msac)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, nmsac)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, madd)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, nmadd)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, msub)
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_MAC_OP_BUILTINS, nmsub)
+
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WIDENING_MAC_OP_BUILTINS, wmacc)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WIDENING_MAC_OP_BUILTINS, wnmacc)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WIDENING_MAC_OP_BUILTINS, wmsac)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WIDENING_MAC_OP_BUILTINS, wnmsac)
+
+  _RVV_WRED_INT_ITERATOR_ARG (VINT_WREDUC_OP_BUILTINS, sum, sumu)
+  _RVV_WRED_FLOAT_ITERATOR_ARG (VFLOAT_WREDUC_OP_BUILTINS, sum)
+  _RVV_WRED_FLOAT_ITERATOR_ARG (VFLOAT_WREDUC_OP_BUILTINS, osum)
+
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WIDENING_ADD_SUB_BUILTINS, fwadd)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WIDENING_ADD_SUB_BUILTINS, fwsub)
+  _RVV_WFLOAT_ITERATOR_ARG (VFLOAT_WMUL_BUILTINS, fwmul)
+
+  _RVV_FLOAT_ITERATOR_ARG (VFLOAT_VFCLASS_BUILTINS, vfclass)
+
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamoswapei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamoaddei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamoxorei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamoandei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamoorei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamominei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamomaxei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamominuei)
+  _RVV_INT_INDEX_ITERATOR_ARG (VINT_AMO_BUILTINS, vamomaxuei)
+
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamoswapei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamoaddei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamoxorei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamoandei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamoorei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamominei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamomaxei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamominuei)
+  _RVV_FLOAT_INDEX_ITERATOR_ARG (VFLOAT_AMO_BUILTINS, vamomaxuei)
+
+  _RVV_FLOAT_INT_ITERATOR (VREINTERPRET)
+  _RVV_INT_ITERATOR (VREINTERPRET_INT)
+  _RVV_FLOAT_REINT_ITERATOR (VREINTERPRET_XSEW_FLOAT)
+  _RVV_INT_REINT_ITERATOR (VREINTERPRET_XSEW_INT)
+
+  /* Segment load/store.  */
+  _RVV_SEG_ARG(VINT_SEG_LOAD_STORE, )
+  _RVV_SEG_NO_SEW8_ARG(VFLOAT_SEG_LOAD_STORE, )
+
+  _RVV_SEG_INT_INDEX_ITERATOR(VINT_INDEX_SEG_LOAD_STORE_BUILTINS)
+  _RVV_SEG_FLOAT_INDEX_ITERATOR(VFLOAT_INDEX_SEG_LOAD_STORE_BUILTINS)
+
+  _RVV_SEG_ARG(VINT_SEG_INSERT, )
+  _RVV_SEG_NO_SEW8_ARG(VFLOAT_SEG_INSERT, )
+
+  _RVV_SEG_ARG(VINT_SEG_EXTRACT, )
+  _RVV_SEG_NO_SEW8_ARG(VFLOAT_SEG_EXTRACT, )
+
+  _RVV_SEG_NF2_ARG(VINT_SEG_CREATE2, )
+  _RVV_SEG_NF3_ARG(VINT_SEG_CREATE3, )
+  _RVV_SEG_NF4_ARG(VINT_SEG_CREATE4, )
+  _RVV_SEG_NF5_ARG(VINT_SEG_CREATE5, )
+  _RVV_SEG_NF6_ARG(VINT_SEG_CREATE6, )
+  _RVV_SEG_NF7_ARG(VINT_SEG_CREATE7, )
+  _RVV_SEG_NF8_ARG(VINT_SEG_CREATE8, )
+
+  _RVV_SEG_NF2_NO_SEW8_ARG(VFLOAT_SEG_CREATE2, )
+  _RVV_SEG_NF3_NO_SEW8_ARG(VFLOAT_SEG_CREATE3, )
+  _RVV_SEG_NF4_NO_SEW8_ARG(VFLOAT_SEG_CREATE4, )
+  _RVV_SEG_NF5_NO_SEW8_ARG(VFLOAT_SEG_CREATE5, )
+  _RVV_SEG_NF6_NO_SEW8_ARG(VFLOAT_SEG_CREATE6, )
+  _RVV_SEG_NF7_NO_SEW8_ARG(VFLOAT_SEG_CREATE7, )
+  _RVV_SEG_NF8_NO_SEW8_ARG(VFLOAT_SEG_CREATE8, )
+
+  _RVV_INT_ITERATOR(VUNDEFINED_INT)
+  _RVV_FLOAT_ITERATOR(VUNDEFINED_FLOAT)
+
+  _RVV_SEG (VUNDEFINED_VT_INT)
+  _RVV_SEG_NO_SEW8 (VUNDEFINED_VT_FLOAT)
 };
 
 /* Index I is the function declaration for riscv_builtins[I], or null if the
@@ -171,11 +2597,108 @@ riscv_build_function_type (enum riscv_function_type type)
   return types[(int) type];
 }
 
+/* Create a builtin vector type with a name.  Taking care not to give
+   the canonical type a name.  */
+
+static tree
+riscv_vector_type (const char *name, tree elt_type, enum machine_mode mode)
+{
+  tree result = build_vector_type_for_mode (elt_type, mode);
+
+  /* Copy so we don't give the canonical type a name.  */
+  result = build_distinct_type_copy (result);
+
+  (*lang_hooks.types.register_builtin_type) (result, name);
+
+  return result;
+}
+
+static tree
+riscv_vector_tuple_type (const char *name,
+			 tree vector_type,
+			 enum machine_mode mode,
+			 size_t nelt)
+{
+  /* The contents of the type are opaque, so we can define them in any
+     way that maps to the correct ABI type.
+
+     Here we choose to use the same layout as for arm_neon.h, but with
+     "__val" instead of "val":
+
+	struct vxxxSEWmLMULxNF_t { svfoo_t __val[N]; };
+
+     (It wouldn't be possible to write that directly in C or C++ for
+     sizeless types, but that's not a problem for this function.)
+
+     Using arrays simplifies the handling of svget and svset for variable
+     arguments.  */
+  tree tuple_type = lang_hooks.types.make_type (RECORD_TYPE);
+  tree array_type = build_array_type_nelts (vector_type, nelt);
+
+  tree field = build_decl (input_location, FIELD_DECL,
+			   get_identifier ("__val"), array_type);
+
+  DECL_FIELD_CONTEXT (field) = tuple_type;
+  TYPE_FIELDS (tuple_type) = field;
+
+  layout_type (tuple_type);
+  SET_TYPE_MODE (tuple_type, mode);
+
+  tree decl = build_decl (input_location, TYPE_DECL,
+			  get_identifier (name), tuple_type);
+  TYPE_NAME (tuple_type) = decl;
+  TYPE_STUB_DECL (tuple_type) = decl;
+  lang_hooks.decls.pushdecl (decl);
+
+  //(*lang_hooks.types.register_builtin_type) (tuple_type, name);
+  /* ??? Undo the effect of set_underlying_type for C.  The C frontend
+     doesn't recognize DECL as a built-in because (as intended) the decl has
+     a real location instead of BUILTINS_LOCATION.  The frontend therefore
+     treats the decl like a normal C "typedef struct foo foo;", expecting
+     the type for tag "struct foo" to have a dummy unnamed TYPE_DECL instead
+     of the named one we attached above.  It then sets DECL_ORIGINAL_TYPE
+     on the supposedly unnamed decl, creating a circularity that upsets
+     dwarf2out.
+
+     We don't want to follow the normal C model and create "struct foo"
+     tags for tuple types since (a) the types are supposed to be opaque
+     and (b) they couldn't be defined as a real struct anyway.  Treating
+     the TYPE_DECLs as "typedef struct foo foo;" without creating
+     "struct foo" would lead to confusing error messages.  */
+  DECL_ORIGINAL_TYPE (decl) = NULL_TREE;
+
+  return tuple_type;
+}
+
+
 /* Implement TARGET_INIT_BUILTINS.  */
 
 void
 riscv_init_builtins (void)
 {
+  int8_type_node = intQI_type_node;
+  unsigned_int8_type_node = unsigned_intQI_type_node;
+  int16_type_node = intHI_type_node;
+  unsigned_int16_type_node = unsigned_intHI_type_node;
+
+  if (TARGET_64BIT)
+    {
+      int32_type_node = intSI_type_node;
+      unsigned_int32_type_node = unsigned_intSI_type_node;
+    }
+  else
+    {
+      /* int32_t/uint32_t defined as `long`/`unsigned long` in RV32,
+	  but intSI_type_node/unsigned_intSI_type_node is
+	  `int` and `unsigned int`, so use long_integer_type_node and
+	  long_unsigned_type_node here for type consistent.  */
+      int32_type_node = long_integer_type_node;
+      unsigned_int32_type_node = long_unsigned_type_node;
+    }
+
+  int64_type_node = intDI_type_node;
+  unsigned_int64_type_node = unsigned_intDI_type_node;
+
   /* _Float16 is C specific.  So we need a language independent type for
      half floats.  Use __fp16 same as the arm/aarch64 ports.  */
   fp16_type_node = make_node (REAL_TYPE);
@@ -183,6 +2706,191 @@ riscv_init_builtins (void)
   layout_type (fp16_type_node);
   (*lang_hooks.types.register_builtin_type) (fp16_type_node, "__fp16");
 
+  if (TARGET_VECTOR)
+    {
+      /* These types exist only for the ld/st intrinsics.  */
+      const_float_ptr_type_node
+	= build_pointer_type (build_type_variant (float_type_node, 1, 0));
+      const_double_ptr_type_node
+	= build_pointer_type (build_type_variant (double_type_node, 1, 0));
+      float16_ptr_type_node = build_pointer_type (fp16_type_node);
+      const_float16_type_node = build_type_variant (fp16_type_node, 1, 0);
+      const_float16_ptr_type_node
+	= build_pointer_type (const_float16_type_node);
+
+      #define DEFINE_SCALAR_PTR_TYPE_NODE(WIDTH, MODE)			\
+        int##MODE##_ptr_type_node					\
+	  = build_pointer_type (int##WIDTH##_type_node); 		\
+        unsigned_int##MODE##_ptr_type_node				\
+	  = build_pointer_type (unsigned_int##WIDTH##_type_node);	\
+        const_int##MODE##_ptr_type_node					\
+	  = build_pointer_type (					\
+	      build_type_variant (int##WIDTH##_type_node, 1, 0));	\
+	const_unsigned_int##MODE##_ptr_type_node			\
+	  = build_pointer_type (					\
+	      build_type_variant (unsigned_int##WIDTH##_type_node, 1, 0));
+
+      _SCALAR_INT_ITERATOR(DEFINE_SCALAR_PTR_TYPE_NODE);
+
+      rvvint8m1_t_node
+	= riscv_vector_type ("vint8m1_t", intQI_type_node, VNx16QImode);
+      rvvint8m2_t_node
+	= riscv_vector_type ("vint8m2_t", intQI_type_node, VNx32QImode);
+      rvvint8m4_t_node
+	= riscv_vector_type ("vint8m4_t", intQI_type_node, VNx64QImode);
+      rvvint8m8_t_node
+	= riscv_vector_type ("vint8m8_t", intQI_type_node, VNx128QImode);
+
+      rvvint16m1_t_node
+	= riscv_vector_type ("vint16m1_t", intHI_type_node, VNx8HImode);
+      rvvint16m2_t_node
+	= riscv_vector_type ("vint16m2_t", intHI_type_node, VNx16HImode);
+      rvvint16m4_t_node
+	= riscv_vector_type ("vint16m4_t", intHI_type_node, VNx32HImode);
+      rvvint16m8_t_node
+	= riscv_vector_type ("vint16m8_t", intHI_type_node, VNx64HImode);
+
+      rvvint32m1_t_node
+	= riscv_vector_type ("vint32m1_t", intSI_type_node, VNx4SImode);
+      rvvint32m2_t_node
+	= riscv_vector_type ("vint32m2_t", intSI_type_node, VNx8SImode);
+      rvvint32m4_t_node
+	= riscv_vector_type ("vint32m4_t", intSI_type_node, VNx16SImode);
+      rvvint32m8_t_node
+	= riscv_vector_type ("vint32m8_t", intSI_type_node, VNx32SImode);
+
+      rvvint64m1_t_node
+	= riscv_vector_type ("vint64m1_t", intDI_type_node, VNx2DImode);
+      rvvint64m2_t_node
+	= riscv_vector_type ("vint64m2_t", intDI_type_node, VNx4DImode);
+      rvvint64m4_t_node
+	= riscv_vector_type ("vint64m4_t", intDI_type_node, VNx8DImode);
+      rvvint64m8_t_node
+	= riscv_vector_type ("vint64m8_t", intDI_type_node, VNx16DImode);
+
+      rvvuint8m1_t_node
+	= riscv_vector_type ("vuint8m1_t", unsigned_intQI_type_node,
+			     VNx16QImode);
+      rvvuint8m2_t_node
+	= riscv_vector_type ("vuint8m2_t", unsigned_intQI_type_node,
+			     VNx32QImode);
+      rvvuint8m4_t_node
+	= riscv_vector_type ("vuint8m4_t", unsigned_intQI_type_node,
+			     VNx64QImode);
+      rvvuint8m8_t_node
+	= riscv_vector_type ("vuint8m8_t", unsigned_intQI_type_node,
+			     VNx128QImode);
+
+      rvvuint16m1_t_node
+	= riscv_vector_type ("vuint16m1_t", unsigned_intHI_type_node,
+			     VNx8HImode);
+      rvvuint16m2_t_node
+	= riscv_vector_type ("vuint16m2_t", unsigned_intHI_type_node,
+			     VNx16HImode);
+      rvvuint16m4_t_node
+	= riscv_vector_type ("vuint16m4_t", unsigned_intHI_type_node,
+			     VNx32HImode);
+      rvvuint16m8_t_node
+	= riscv_vector_type ("vuint16m8_t", unsigned_intHI_type_node,
+			     VNx64HImode);
+
+      rvvuint32m1_t_node
+	= riscv_vector_type ("vuint32m1_t", unsigned_intSI_type_node,
+			     VNx4SImode);
+      rvvuint32m2_t_node
+	= riscv_vector_type ("vuint32m2_t", unsigned_intSI_type_node,
+			     VNx8SImode);
+      rvvuint32m4_t_node
+	= riscv_vector_type ("vuint32m4_t", unsigned_intSI_type_node,
+			     VNx16SImode);
+      rvvuint32m8_t_node
+	= riscv_vector_type ("vuint32m8_t", unsigned_intSI_type_node,
+			     VNx32SImode);
+
+      rvvuint64m1_t_node
+	= riscv_vector_type ("vuint64m1_t", unsigned_intDI_type_node,
+			     VNx2DImode);
+      rvvuint64m2_t_node
+	= riscv_vector_type ("vuint64m2_t", unsigned_intDI_type_node,
+			     VNx4DImode);
+      rvvuint64m4_t_node
+	= riscv_vector_type ("vuint64m4_t", unsigned_intDI_type_node,
+			     VNx8DImode);
+      rvvuint64m8_t_node
+	= riscv_vector_type ("vuint64m8_t", unsigned_intDI_type_node,
+			     VNx16DImode);
+
+      rvvfloat16m1_t_node
+	= riscv_vector_type ("vfloat16m1_t", fp16_type_node, VNx8HFmode);
+      rvvfloat16m2_t_node
+	= riscv_vector_type ("vfloat16m2_t", fp16_type_node, VNx16HFmode);
+      rvvfloat16m4_t_node
+	= riscv_vector_type ("vfloat16m4_t", fp16_type_node, VNx32HFmode);
+      rvvfloat16m8_t_node
+	= riscv_vector_type ("vfloat16m8_t", fp16_type_node, VNx64HFmode);
+
+      rvvfloat32m1_t_node
+	= riscv_vector_type ("vfloat32m1_t", float_type_node, VNx4SFmode);
+      rvvfloat32m2_t_node
+	= riscv_vector_type ("vfloat32m2_t", float_type_node, VNx8SFmode);
+      rvvfloat32m4_t_node
+	= riscv_vector_type ("vfloat32m4_t", float_type_node, VNx16SFmode);
+      rvvfloat32m8_t_node
+	= riscv_vector_type ("vfloat32m8_t", float_type_node, VNx32SFmode);
+
+      rvvfloat64m1_t_node
+	= riscv_vector_type ("vfloat64m1_t", double_type_node, VNx2DFmode);
+      rvvfloat64m2_t_node
+	= riscv_vector_type ("vfloat64m2_t", double_type_node, VNx4DFmode);
+      rvvfloat64m4_t_node
+	= riscv_vector_type ("vfloat64m4_t", double_type_node, VNx8DFmode);
+      rvvfloat64m8_t_node
+	= riscv_vector_type ("vfloat64m8_t", double_type_node, VNx16DFmode);
+
+      rvvbool1_t_node
+	= riscv_vector_type ("vbool1_t", boolean_type_node, VNx128BImode);
+      rvvbool2_t_node
+	= riscv_vector_type ("vbool2_t", boolean_type_node, VNx64BImode);
+      rvvbool4_t_node
+	= riscv_vector_type ("vbool4_t", boolean_type_node, VNx32BImode);
+      rvvbool8_t_node
+	= riscv_vector_type ("vbool8_t", boolean_type_node, VNx16BImode);
+      rvvbool16_t_node
+	= riscv_vector_type ("vbool16_t", boolean_type_node, VNx8BImode);
+      rvvbool32_t_node
+	= riscv_vector_type ("vbool32_t", boolean_type_node, VNx4BImode);
+      rvvbool64_t_node
+	= riscv_vector_type ("vbool64_t", boolean_type_node, VNx2BImode);
+
+#define RISCV_DEFINE_FSEG_TYPES(SEW, LMUL, NR, MLEN,			\
+				SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+				VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER,X)\
+      rvvfloat##SEW##m##LMUL##x##NR##_t_node				\
+        = riscv_vector_tuple_type ("vfloat" #SEW "m" #LMUL "x" #NR "_t",\
+				   rvvfloat##SEW##m##LMUL##_t_node,	\
+				   VMODE_PREFIX_UPPER##Fmode,		\
+				   NR);
+
+_RVV_SEG_NO_SEW8_ARG (RISCV_DEFINE_FSEG_TYPES, X)
+
+#define RISCV_DEFINE_SEG_TYPES(SEW, LMUL, NR, MLEN,			\
+			       SMODE_PREFIX_UPPER, SMODE_PREFIX_LOWER,	\
+			       VMODE_PREFIX_UPPER, VMODE_PREFIX_LOWER,X)\
+      rvvint##SEW##m##LMUL##x##NR##_t_node				\
+        = riscv_vector_tuple_type ("vint" #SEW "m" #LMUL "x" #NR "_t",	\
+				   rvvint##SEW##m##LMUL##_t_node,	\
+				   VMODE_PREFIX_UPPER##Imode,		\
+				   NR);					\
+      rvvuint##SEW##m##LMUL##x##NR##_t_node				\
+        = riscv_vector_tuple_type ("vuint" #SEW "m" #LMUL "x" #NR "_t",	\
+				   rvvuint##SEW##m##LMUL##_t_node,	\
+				   VMODE_PREFIX_UPPER##Imode,		\
+				   NR);
+
+_RVV_SEG_ARG (RISCV_DEFINE_SEG_TYPES, X)
+
+    }
+
   for (size_t i = 0; i < ARRAY_SIZE (riscv_builtins); i++)
     {
       const struct riscv_builtin_description *d = &riscv_builtins[i];
@@ -227,12 +2935,25 @@ static rtx
 riscv_expand_builtin_insn (enum insn_code icode, unsigned int n_ops,
 			   struct expand_operand *ops, bool has_target_p)
 {
-  if (!maybe_expand_insn (icode, n_ops, ops))
+  switch (icode)
     {
-      error ("invalid argument to built-in function");
-      return has_target_p ? gen_reg_rtx (ops[0].mode) : const0_rtx;
-    }
+#define EXPAND_SETVTYPE_BUILTINS(E, L, MLEN, MODE, SUBMODE)	\
+      case CODE_FOR_vsetvli_x0_##MODE:				\
+	emit_insn (gen_vsetvli_x0_##MODE () );			\
+	break;
+
+_RVV_INT_ITERATOR (EXPAND_SETVTYPE_BUILTINS)
+
+#undef EXPAND_SETVTYPE_BUILTINS
 
+    default:
+      if (!maybe_expand_insn (icode, n_ops, ops))
+	{
+	  error ("invalid argument to built-in function");
+	  return has_target_p ? gen_reg_rtx (ops[0].mode) : const0_rtx;
+	}
+      break;
+    }
   return has_target_p ? ops[0].value : const0_rtx;
 }
 
@@ -261,6 +2982,113 @@ riscv_expand_builtin_direct (enum insn_code icode, rtx target, tree exp,
   return riscv_expand_builtin_insn (icode, opno, ops, has_target_p);
 }
 
+/* Return a legitimate rtx for instruction ICODE's return value.  Use TARGET
+   if it's not null, has the right mode, and satisfies operand 0's
+   predicate.  */
+static rtx
+riscv_legitimize_target (enum insn_code icode, rtx target)
+{
+  enum machine_mode mode = insn_data[icode].operand[0].mode;
+
+  if (! target
+      || GET_MODE (target) != mode
+      || ! (*insn_data[icode].operand[0].predicate) (target, mode))
+    return gen_reg_rtx (mode);
+  else
+    return target;
+}
+
+/* Given that ARG is being passed as operand OPNUM to instruction ICODE,
+   check whether ARG satisfies the operand's constraints.  If it doesn't,
+   copy ARG to a temporary register and return that.  Otherwise return ARG
+   itself.  */
+static rtx
+riscv_legitimize_argument (enum insn_code icode, int opnum, rtx arg)
+{
+  enum machine_mode mode = insn_data[icode].operand[opnum].mode;
+
+  if ((*insn_data[icode].operand[opnum].predicate) (arg, mode))
+    return arg;
+  else
+    {
+      rtx tmp_rtx = gen_reg_rtx (mode);
+      convert_move (tmp_rtx, arg, false);
+      return tmp_rtx;
+    }
+}
+
+static rtx
+riscv_expand_unary_builtin (enum insn_code icode, rtx target, tree exp)
+{
+  rtx pat;
+  rtx op0 = expand_normal (CALL_EXPR_ARG (exp, 0));
+
+  /* Map any target to operand 0.  */
+  target = riscv_legitimize_target (icode, target);
+
+  op0 = riscv_legitimize_argument (icode, 1, op0);
+
+  /* Emit and return the new instruction. */
+  pat = GEN_FCN (icode) (target, op0);
+
+  if (! pat)
+    return NULL_RTX;
+
+  emit_insn (pat);
+  return target;
+}
+
+static rtx
+riscv_expand_builtin_shift_scalar (enum insn_code icode, rtx target, tree exp)
+{
+  rtx pat;
+  rtx op0 = expand_normal (CALL_EXPR_ARG (exp, 0));
+  rtx op1 = expand_normal (CALL_EXPR_ARG (exp, 1));
+
+  /* Map any target to operand 0.  */
+  target = riscv_legitimize_target (icode, target);
+
+  op0 = riscv_legitimize_argument (icode, 1, op0);
+  op1 = riscv_legitimize_argument (icode, 2, op1);
+
+  /* Emit and return the new instruction. */
+  pat = GEN_FCN (icode) (target, op0, op1);
+
+  if (! pat)
+    return NULL_RTX;
+
+  emit_insn (pat);
+  return target;
+}
+
+static rtx
+riscv_expand_builtin_shift_mask_scalar (enum insn_code icode,
+					rtx target, tree exp)
+{
+  rtx pat;
+  rtx op0 = expand_normal (CALL_EXPR_ARG (exp, 0));
+  rtx op1 = expand_normal (CALL_EXPR_ARG (exp, 1));
+  rtx op2 = expand_normal (CALL_EXPR_ARG (exp, 2));
+  rtx op3 = expand_normal (CALL_EXPR_ARG (exp, 3));
+
+  /* Map any target to operand 0.  */
+  target = riscv_legitimize_target (icode, target);
+
+  op0 = riscv_legitimize_argument (icode, 1, op0);
+  op1 = riscv_legitimize_argument (icode, 2, op1);
+  op2 = riscv_legitimize_argument (icode, 3, op2);
+  op3 = riscv_legitimize_argument (icode, 4, op3);
+
+  /* Emit and return the new instruction. */
+  pat = GEN_FCN (icode) (target, op0, op1, op2, op3);
+
+  if (! pat)
+    return NULL_RTX;
+
+  emit_insn (pat);
+  return target;
+}
+
 /* Implement TARGET_EXPAND_BUILTIN.  */
 
 rtx
@@ -274,6 +3102,12 @@ riscv_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,
 
   switch (d->builtin_type)
     {
+    case RISCV_BUILTIN_SHIFT_SCALAR:
+      return riscv_expand_builtin_shift_scalar (d->icode, target, exp);
+    case RISCV_BUILTIN_SHIFT_MASK_SCALAR:
+      return riscv_expand_builtin_shift_mask_scalar (d->icode, target, exp);
+    case RISCV_BUILTIN_MV_XS:
+      return riscv_expand_unary_builtin (d->icode, target, exp);
     case RISCV_BUILTIN_DIRECT:
       return riscv_expand_builtin_direct (d->icode, target, exp, true);
 
diff --git a/gcc/config/riscv/riscv-c.c b/gcc/config/riscv/riscv-c.c
index 262b2d27057..08b1100c9a1 100644
--- a/gcc/config/riscv/riscv-c.c
+++ b/gcc/config/riscv/riscv-c.c
@@ -104,4 +104,7 @@ riscv_cpu_cpp_builtins (cpp_reader *pfile)
       break;
 
     }
+
+  if (TARGET_VECTOR)
+    builtin_define ("__riscv_vector");
 }
diff --git a/gcc/config/riscv/riscv-ftypes.def b/gcc/config/riscv/riscv-ftypes.def
index 1c6bc4e9dce..f75e34e8af0 100644
--- a/gcc/config/riscv/riscv-ftypes.def
+++ b/gcc/config/riscv/riscv-ftypes.def
@@ -24,7 +24,47 @@ along with GCC; see the file COPYING3.  If not see
 
       NARGS is the number of arguments.
       LIST contains the return-type code followed by the codes for each
-        argument type.  */
+        argument type.
 
+   Invoke _RVV_INT_ITERATOR and _RVV_MASK_ITERATOR with DEF_RISCV_FTYPE to
+   create bunch of prototype used by RISC-V vector built-in functions.  */
+
+#define DEF_NULLARY_VECTOR_MASKING_FTYPE(MLEN, N)	\
+  DEF_RISCV_FTYPE (0, (VB##MLEN))
+
+_RVV_MASK_ITERATOR(DEF_NULLARY_VECTOR_MASKING_FTYPE)
+
+DEF_RISCV_FTYPE (0, (VOID))
 DEF_RISCV_FTYPE (0, (USI))
+DEF_RISCV_FTYPE (0, (SIZE))
+
 DEF_RISCV_FTYPE (1, (VOID, USI))
+
+DEF_RISCV_FTYPE (1, (SI, SI))
+DEF_RISCV_FTYPE (1, (DI, DI))
+
+#define DEF_UNARY_VECTOR_MASKING_FTYPE(MLEN, N)	\
+  DEF_RISCV_FTYPE (1, (VB##MLEN, VB##MLEN))	\
+  DEF_RISCV_FTYPE (1, (SI, VB##MLEN))		\
+  DEF_RISCV_FTYPE (1, (DI, VB##MLEN))
+
+_RVV_MASK_ITERATOR(DEF_UNARY_VECTOR_MASKING_FTYPE)
+
+#define DEF_BINARY_VECTOR_MASKING_FTYPE(MLEN, N)	\
+  DEF_RISCV_FTYPE (2, (VB##MLEN, VB##MLEN, VB##MLEN))	\
+  DEF_RISCV_FTYPE (2, (SI, VB##MLEN, VB##MLEN))		\
+  DEF_RISCV_FTYPE (2, (DI, VB##MLEN, VB##MLEN))
+
+_RVV_MASK_ITERATOR(DEF_BINARY_VECTOR_MASKING_FTYPE)
+
+#define DEF_MASKED_UNARY_VECTOR_MASKING_FTYPE(MLEN, N) \
+  DEF_RISCV_FTYPE (3, (VB##MLEN, VB##MLEN, VB##MLEN, VB##MLEN))
+
+_RVV_MASK_ITERATOR(DEF_MASKED_UNARY_VECTOR_MASKING_FTYPE)
+
+#define DEF_MASKED_BINARY_VECTOR_MASKING_FTYPE(MLEN, N) \
+  DEF_RISCV_FTYPE (4, (VB##MLEN, VB##MLEN, VB##MLEN, VB##MLEN, VB##MLEN))
+
+_RVV_MASK_ITERATOR(DEF_MASKED_BINARY_VECTOR_MASKING_FTYPE)
+
+#include "riscv-vector-ftypes.def"
diff --git a/gcc/config/riscv/riscv-modes.def b/gcc/config/riscv/riscv-modes.def
index 4afc8998f0a..e3cf392aa2d 100644
--- a/gcc/config/riscv/riscv-modes.def
+++ b/gcc/config/riscv/riscv-modes.def
@@ -24,3 +24,106 @@ FLOAT_MODE (TF, 16, ieee_quad_format);
 /* Half-precision floating point for __fp16.  */
 FLOAT_MODE (HF, 2, 0);
 ADJUST_FLOAT_FORMAT (HF, &ieee_half_format);
+
+/* Vector modes.  */
+
+/* Define vector modes for NVECS vectors.  VB, VH, VS and VD are the prefixes
+   for 8-bit, 16-bit, 32-bit and 64-bit elements respectively.  It isn't
+   strictly necessary to set the alignment here, since the default would
+   be clamped to BIGGEST_ALIGNMENT anyhow, but it seems clearer.  */
+#define RVV_MODES(NVECS, VB, VH, VS, VD) \
+  VECTOR_MODES_WITH_PREFIX (VNx, INT, 16 * NVECS, 0); \
+  VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 16 * NVECS, 0); \
+  \
+  ADJUST_NUNITS (VB##QI, riscv_rvv_chunks * NVECS * 8); \
+  ADJUST_NUNITS (VH##HI, riscv_rvv_chunks * NVECS * 4); \
+  ADJUST_NUNITS (VS##SI, riscv_rvv_chunks * NVECS * 2); \
+  ADJUST_NUNITS (VD##DI, riscv_rvv_chunks * NVECS); \
+  ADJUST_NUNITS (VH##HF, riscv_rvv_chunks * NVECS * 4); \
+  ADJUST_NUNITS (VS##SF, riscv_rvv_chunks * NVECS * 2); \
+  ADJUST_NUNITS (VD##DF, riscv_rvv_chunks * NVECS); \
+  \
+  ADJUST_ALIGNMENT (VB##QI, 1); \
+  ADJUST_ALIGNMENT (VH##HI, 2); \
+  ADJUST_ALIGNMENT (VS##SI, 4); \
+  ADJUST_ALIGNMENT (VD##DI, 8); \
+  ADJUST_ALIGNMENT (VH##HF, 2); \
+  ADJUST_ALIGNMENT (VS##SF, 4); \
+  ADJUST_ALIGNMENT (VD##DF, 8);
+
+/* Give vectors the names normally used for 128-bit vectors.
+   The actual number depends on command-line flags.  */
+RVV_MODES (1, VNx16, VNx8, VNx4, VNx2)
+RVV_MODES (2, VNx32, VNx16, VNx8, VNx4)
+RVV_MODES (4, VNx64, VNx32, VNx16, VNx8)
+RVV_MODES (8, VNx128, VNx64, VNx32, VNx16)
+RVV_MODES (16, VNx256, VNx128, VNx64, VNx32)
+
+#define RVV_TUPLE_MODES(NVECS, NSUBPART, VB, VH, VS, VD) \
+  VECTOR_TUPLE_MODES_WITH_PREFIX (VNx, INT, 16 * NVECS, NSUBPART); \
+  VECTOR_TUPLE_MODES_WITH_PREFIX (VNx, FLOAT, 16 * NVECS, NSUBPART); \
+  \
+  ADJUST_NUNITS (VB##QI, riscv_rvv_chunks * NVECS * 8); \
+  ADJUST_NUNITS (VH##HI, riscv_rvv_chunks * NVECS * 4); \
+  ADJUST_NUNITS (VS##SI, riscv_rvv_chunks * NVECS * 2); \
+  ADJUST_NUNITS (VD##DI, riscv_rvv_chunks * NVECS); \
+  ADJUST_NUNITS (VH##HF, riscv_rvv_chunks * NVECS * 4); \
+  ADJUST_NUNITS (VS##SF, riscv_rvv_chunks * NVECS * 2); \
+  ADJUST_NUNITS (VD##DF, riscv_rvv_chunks * NVECS); \
+  \
+  ADJUST_ALIGNMENT (VB##QI, 1); \
+  ADJUST_ALIGNMENT (VH##HI, 2); \
+  ADJUST_ALIGNMENT (VS##SI, 4); \
+  ADJUST_ALIGNMENT (VD##DI, 8); \
+  ADJUST_ALIGNMENT (VH##HF, 2); \
+  ADJUST_ALIGNMENT (VS##SF, 4); \
+  ADJUST_ALIGNMENT (VD##DF, 8);
+
+/* Modes for vector tuple.  */
+RVV_TUPLE_MODES (2, 2, VNx2x16, VNx2x8, VNx2x4, VNx2x2)
+RVV_TUPLE_MODES (3, 3, VNx3x16, VNx3x8, VNx3x4, VNx3x2)
+RVV_TUPLE_MODES (4, 4, VNx4x16, VNx4x8, VNx4x4, VNx4x2)
+RVV_TUPLE_MODES (5, 5, VNx5x16, VNx5x8, VNx5x4, VNx5x2)
+RVV_TUPLE_MODES (6, 6, VNx6x16, VNx6x8, VNx6x4, VNx6x2)
+RVV_TUPLE_MODES (7, 7, VNx7x16, VNx7x8, VNx7x4, VNx7x2)
+RVV_TUPLE_MODES (8, 8, VNx8x16, VNx8x8, VNx8x4, VNx8x2)
+
+RVV_TUPLE_MODES (4, 2, VNx2x32, VNx2x16, VNx2x8, VNx2x4)
+RVV_TUPLE_MODES (6, 3, VNx3x32, VNx3x16, VNx3x8, VNx3x4)
+RVV_TUPLE_MODES (8, 4, VNx4x32, VNx4x16, VNx4x8, VNx4x4)
+
+RVV_TUPLE_MODES (8, 2, VNx2x64, VNx2x32, VNx2x16, VNx2x8)
+
+#define RVV_MASK_MODES(NCOUNT) \
+  VECTOR_BOOL_MODE (VNx ## NCOUNT ## BI, NCOUNT, 16); \
+  ADJUST_NUNITS (VNx ## NCOUNT ## BI, riscv_rvv_chunks * (NCOUNT / 2)); \
+  ADJUST_BYTESIZE (VNx ## NCOUNT ## BI, riscv_rvv_chunks * 8); \
+  ADJUST_ALIGNMENT (VNx ## NCOUNT ## BI, 1);
+
+/* Mapping table between MLEN and masking type.
+   | Type     | MLEN |
+   | VNx2BI   | 64   |
+   | VNx4BI   | 32   |
+   | VNx8BI   | 16   |
+   | VNx16BI  | 8    |
+   | VNx32BI  | 4    |
+   | VNx64BI  | 2    |
+   | VNx128BI | 1    |  */
+
+RVV_MASK_MODES(2)
+RVV_MASK_MODES(4)
+RVV_MASK_MODES(8)
+RVV_MASK_MODES(16)
+RVV_MASK_MODES(32)
+RVV_MASK_MODES(64)
+RVV_MASK_MODES(128)
+
+/* A 8-tuple of RVV vectors with the maximum -mrvv-vector-bits= setting.
+   Note that this is a limit only on the compile-time sizes of modes;
+   it is not a limit on the runtime sizes, since VL-agnostic code
+   must work with arbitary vector lengths.  */
+#define MAX_BITSIZE_MODE_ANY_MODE (1024 * 8)
+
+/* Coefficient 1 is multiplied by the number of 64-bit chunks in a vector
+   minus one.  */
+#define NUM_POLY_INT_COEFFS 2
diff --git a/gcc/config/riscv/riscv-opts.h b/gcc/config/riscv/riscv-opts.h
index 8c9bebe08ef..9ad29f0b77e 100644
--- a/gcc/config/riscv/riscv-opts.h
+++ b/gcc/config/riscv/riscv-opts.h
@@ -67,4 +67,15 @@ enum riscv_align_data {
 #define TARGET_ZICSR    ((riscv_zi_subext & MASK_ZICSR) != 0)
 #define TARGET_ZIFENCEI ((riscv_zi_subext & MASK_ZIFENCEI) != 0)
 
+/* RVV vector register sizes.  */
+enum riscv_rvv_vector_bits_enum {
+  RVV_SCALABLE,
+  RVV_NOT_IMPLEMENTED = RVV_SCALABLE,
+  RVV_64 = 64,
+  RVV_128 = 128,
+  RVV_256 = 256,
+  RVV_512 = 512,
+  RVV_1024 = 1024
+};
+
 #endif /* ! GCC_RISCV_OPTS_H */
diff --git a/gcc/config/riscv/riscv-protos.h b/gcc/config/riscv/riscv-protos.h
index 256dab1d0cf..897ef06c37a 100644
--- a/gcc/config/riscv/riscv-protos.h
+++ b/gcc/config/riscv/riscv-protos.h
@@ -64,7 +64,7 @@ extern rtx riscv_legitimize_call_address (rtx);
 extern void riscv_set_return_address (rtx, rtx);
 extern bool riscv_expand_block_move (rtx, rtx, rtx);
 extern rtx riscv_return_addr (int, rtx);
-extern HOST_WIDE_INT riscv_initial_elimination_offset (int, int);
+extern poly_int64 riscv_initial_elimination_offset (int, int);
 extern void riscv_expand_prologue (void);
 extern void riscv_expand_epilogue (int);
 extern bool riscv_epilogue_uses (unsigned int);
@@ -75,6 +75,11 @@ extern bool riscv_store_data_bypass_p (rtx_insn *, rtx_insn *);
 extern rtx riscv_gen_gpr_save_insn (struct riscv_frame_info *);
 extern bool riscv_gpr_save_operation_p (rtx);
 
+/* Routines for vector support.  */
+bool riscv_const_vec_all_same_in_range_p (rtx, HOST_WIDE_INT, HOST_WIDE_INT);
+extern poly_uint64 riscv_regmode_natural_size (machine_mode);
+extern void riscv_expand_vtuple_create (rtx *);
+
 /* Routines implemented in riscv-c.c.  */
 void riscv_cpu_cpp_builtins (cpp_reader *);
 
@@ -108,4 +113,7 @@ struct riscv_cpu_info {
 
 extern const riscv_cpu_info *riscv_find_cpu (const char *);
 
+/* Routines for vector tuple types.  */
+extern int riscv_get_nf (machine_mode);
+
 #endif /* ! GCC_RISCV_PROTOS_H */
diff --git a/gcc/config/riscv/riscv-sr.c b/gcc/config/riscv/riscv-sr.c
index 694f90c1583..8adb7e4a8b6 100644
--- a/gcc/config/riscv/riscv-sr.c
+++ b/gcc/config/riscv/riscv-sr.c
@@ -247,7 +247,7 @@ riscv_remove_unneeded_save_restore_calls (void)
   /* We'll adjust stack size after this optimization, that require update every
      sp use site, which could be unsafe, so we decide to turn off this
      optimization if there are any arguments put on stack.  */
-  if (crtl->args.size != 0)
+  if (!crtl->args.size.is_constant () || crtl->args.size.to_constant () != 0)
     return;
 
   /* Will point to the first instruction of the function body, after the
diff --git a/gcc/config/riscv/riscv-vector-ftypes.def b/gcc/config/riscv/riscv-vector-ftypes.def
new file mode 100644
index 00000000000..1357d27cec4
--- /dev/null
+++ b/gcc/config/riscv/riscv-vector-ftypes.def
@@ -0,0 +1,7444 @@
+/* DO NOT EDIT, please edit generator instead.
+   This file was generated by gen-vector-iterator with the command:
+   $ ./gen-vector-iterator -ftype > riscv-vector-ftypes.def  */
+
+DEF_RISCV_FTYPE (1, (DF, VF64M1))
+DEF_RISCV_FTYPE (1, (DF, VF64M2))
+DEF_RISCV_FTYPE (1, (DF, VF64M4))
+DEF_RISCV_FTYPE (1, (DF, VF64M8))
+DEF_RISCV_FTYPE (1, (DI, VI64M1))
+DEF_RISCV_FTYPE (1, (DI, VI64M2))
+DEF_RISCV_FTYPE (1, (DI, VI64M4))
+DEF_RISCV_FTYPE (1, (DI, VI64M8))
+DEF_RISCV_FTYPE (1, (HF, VF16M1))
+DEF_RISCV_FTYPE (1, (HF, VF16M2))
+DEF_RISCV_FTYPE (1, (HF, VF16M4))
+DEF_RISCV_FTYPE (1, (HF, VF16M8))
+DEF_RISCV_FTYPE (1, (HI, VI16M1))
+DEF_RISCV_FTYPE (1, (HI, VI16M2))
+DEF_RISCV_FTYPE (1, (HI, VI16M4))
+DEF_RISCV_FTYPE (1, (HI, VI16M8))
+DEF_RISCV_FTYPE (1, (QI, VI8M1))
+DEF_RISCV_FTYPE (1, (QI, VI8M2))
+DEF_RISCV_FTYPE (1, (QI, VI8M4))
+DEF_RISCV_FTYPE (1, (QI, VI8M8))
+DEF_RISCV_FTYPE (1, (SF, VF32M1))
+DEF_RISCV_FTYPE (1, (SF, VF32M2))
+DEF_RISCV_FTYPE (1, (SF, VF32M4))
+DEF_RISCV_FTYPE (1, (SF, VF32M8))
+DEF_RISCV_FTYPE (1, (SI, VI32M1))
+DEF_RISCV_FTYPE (1, (SI, VI32M2))
+DEF_RISCV_FTYPE (1, (SI, VI32M4))
+DEF_RISCV_FTYPE (1, (SI, VI32M8))
+DEF_RISCV_FTYPE (1, (UDI, VUI64M1))
+DEF_RISCV_FTYPE (1, (UDI, VUI64M2))
+DEF_RISCV_FTYPE (1, (UDI, VUI64M4))
+DEF_RISCV_FTYPE (1, (UDI, VUI64M8))
+DEF_RISCV_FTYPE (1, (UHI, VUI16M1))
+DEF_RISCV_FTYPE (1, (UHI, VUI16M2))
+DEF_RISCV_FTYPE (1, (UHI, VUI16M4))
+DEF_RISCV_FTYPE (1, (UHI, VUI16M8))
+DEF_RISCV_FTYPE (1, (UQI, VUI8M1))
+DEF_RISCV_FTYPE (1, (UQI, VUI8M2))
+DEF_RISCV_FTYPE (1, (UQI, VUI8M4))
+DEF_RISCV_FTYPE (1, (UQI, VUI8M8))
+DEF_RISCV_FTYPE (1, (USI, VUI32M1))
+DEF_RISCV_FTYPE (1, (USI, VUI32M2))
+DEF_RISCV_FTYPE (1, (USI, VUI32M4))
+DEF_RISCV_FTYPE (1, (USI, VUI32M8))
+DEF_RISCV_FTYPE (4, (VB1, VB1, VB1, VI8M8, QI))
+DEF_RISCV_FTYPE (4, (VB1, VB1, VB1, VI8M8, UQI))
+DEF_RISCV_FTYPE (4, (VB1, VB1, VB1, VI8M8, VI8M8))
+DEF_RISCV_FTYPE (4, (VB1, VB1, VB1, VI8M8, VUI8M8))
+DEF_RISCV_FTYPE (4, (VB1, VB1, VB1, VUI8M8, UQI))
+DEF_RISCV_FTYPE (4, (VB1, VB1, VB1, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (2, (VB1, VI8M8, QI))
+DEF_RISCV_FTYPE (3, (VB1, VI8M8, QI, VB1))
+DEF_RISCV_FTYPE (2, (VB1, VI8M8, UQI))
+DEF_RISCV_FTYPE (2, (VB1, VI8M8, VB1))
+DEF_RISCV_FTYPE (2, (VB1, VI8M8, VI8M8))
+DEF_RISCV_FTYPE (3, (VB1, VI8M8, VI8M8, VB1))
+DEF_RISCV_FTYPE (2, (VB1, VI8M8, VUI8M8))
+DEF_RISCV_FTYPE (2, (VB1, VUI8M8, UQI))
+DEF_RISCV_FTYPE (3, (VB1, VUI8M8, UQI, VB1))
+DEF_RISCV_FTYPE (2, (VB1, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (3, (VB1, VUI8M8, VUI8M8, VB1))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VF16M1, HF))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VF32M2, SF))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VF64M4, DF))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI16M1, HI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI16M1, UQI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI32M2, SI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI32M2, UQI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI64M4, DI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI64M4, UQI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VI64M4, VUI64M4))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VUI16M1, UHI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VUI32M2, USI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VUI64M4, UDI))
+DEF_RISCV_FTYPE (4, (VB16, VB16, VB16, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VB16, VF16M1, HF))
+DEF_RISCV_FTYPE (2, (VB16, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (2, (VB16, VF32M2, SF))
+DEF_RISCV_FTYPE (2, (VB16, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (2, (VB16, VF64M4, DF))
+DEF_RISCV_FTYPE (2, (VB16, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (2, (VB16, VI16M1, HI))
+DEF_RISCV_FTYPE (3, (VB16, VI16M1, HI, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI16M1, UQI))
+DEF_RISCV_FTYPE (2, (VB16, VI16M1, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VB16, VI16M1, VI16M1, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VB16, VI32M2, SI))
+DEF_RISCV_FTYPE (3, (VB16, VI32M2, SI, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI32M2, UQI))
+DEF_RISCV_FTYPE (2, (VB16, VI32M2, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VB16, VI32M2, VI32M2, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VB16, VI64M4, DI))
+DEF_RISCV_FTYPE (3, (VB16, VI64M4, DI, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI64M4, UQI))
+DEF_RISCV_FTYPE (2, (VB16, VI64M4, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (3, (VB16, VI64M4, VI64M4, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VI64M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VB16, VUI16M1, UHI))
+DEF_RISCV_FTYPE (3, (VB16, VUI16M1, UHI, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VB16, VUI16M1, VUI16M1, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VUI32M2, USI))
+DEF_RISCV_FTYPE (3, (VB16, VUI32M2, USI, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VB16, VUI32M2, VUI32M2, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VUI64M4, UDI))
+DEF_RISCV_FTYPE (3, (VB16, VUI64M4, UDI, VB16))
+DEF_RISCV_FTYPE (2, (VB16, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (3, (VB16, VUI64M4, VUI64M4, VB16))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VF16M8, HF))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VF16M8, VF16M8))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI16M8, HI))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI16M8, UQI))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI16M8, VI16M8))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI16M8, VUI16M8))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI8M4, QI))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI8M4, UQI))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VUI16M8, UHI))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VUI8M4, UQI))
+DEF_RISCV_FTYPE (4, (VB2, VB2, VB2, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (2, (VB2, VF16M8, HF))
+DEF_RISCV_FTYPE (2, (VB2, VF16M8, VF16M8))
+DEF_RISCV_FTYPE (2, (VB2, VI16M8, HI))
+DEF_RISCV_FTYPE (3, (VB2, VI16M8, HI, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VI16M8, UQI))
+DEF_RISCV_FTYPE (2, (VB2, VI16M8, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VI16M8, VI16M8))
+DEF_RISCV_FTYPE (3, (VB2, VI16M8, VI16M8, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VI16M8, VUI16M8))
+DEF_RISCV_FTYPE (2, (VB2, VI8M4, QI))
+DEF_RISCV_FTYPE (3, (VB2, VI8M4, QI, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VI8M4, UQI))
+DEF_RISCV_FTYPE (2, (VB2, VI8M4, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (3, (VB2, VI8M4, VI8M4, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (2, (VB2, VUI16M8, UHI))
+DEF_RISCV_FTYPE (3, (VB2, VUI16M8, UHI, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (3, (VB2, VUI16M8, VUI16M8, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VUI8M4, UQI))
+DEF_RISCV_FTYPE (3, (VB2, VUI8M4, UQI, VB2))
+DEF_RISCV_FTYPE (2, (VB2, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (3, (VB2, VUI8M4, VUI8M4, VB2))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VF32M1, SF))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VF64M2, DF))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI32M1, SI))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI32M1, UQI))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI64M2, DI))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI64M2, UQI))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VI64M2, VUI64M2))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VUI32M1, USI))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VUI64M2, UDI))
+DEF_RISCV_FTYPE (4, (VB32, VB32, VB32, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VB32, VF32M1, SF))
+DEF_RISCV_FTYPE (2, (VB32, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (2, (VB32, VF64M2, DF))
+DEF_RISCV_FTYPE (2, (VB32, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (2, (VB32, VI32M1, SI))
+DEF_RISCV_FTYPE (3, (VB32, VI32M1, SI, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VI32M1, UQI))
+DEF_RISCV_FTYPE (2, (VB32, VI32M1, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VB32, VI32M1, VI32M1, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VB32, VI64M2, DI))
+DEF_RISCV_FTYPE (3, (VB32, VI64M2, DI, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VI64M2, UQI))
+DEF_RISCV_FTYPE (2, (VB32, VI64M2, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VB32, VI64M2, VI64M2, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VI64M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VB32, VUI32M1, USI))
+DEF_RISCV_FTYPE (3, (VB32, VUI32M1, USI, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VB32, VUI32M1, VUI32M1, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VUI64M2, UDI))
+DEF_RISCV_FTYPE (3, (VB32, VUI64M2, UDI, VB32))
+DEF_RISCV_FTYPE (2, (VB32, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VB32, VUI64M2, VUI64M2, VB32))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VF16M4, HF))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VF32M8, SF))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VF32M8, VF32M8))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI16M4, HI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI16M4, UQI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI32M8, SI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI32M8, UQI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI32M8, VI32M8))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI32M8, VUI32M8))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI8M2, QI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI8M2, UQI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VUI16M4, UHI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VUI32M8, USI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VUI8M2, UQI))
+DEF_RISCV_FTYPE (4, (VB4, VB4, VB4, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VB4, VF16M4, HF))
+DEF_RISCV_FTYPE (2, (VB4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (2, (VB4, VF32M8, SF))
+DEF_RISCV_FTYPE (2, (VB4, VF32M8, VF32M8))
+DEF_RISCV_FTYPE (2, (VB4, VI16M4, HI))
+DEF_RISCV_FTYPE (3, (VB4, VI16M4, HI, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI16M4, UQI))
+DEF_RISCV_FTYPE (2, (VB4, VI16M4, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (3, (VB4, VI16M4, VI16M4, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VB4, VI32M8, SI))
+DEF_RISCV_FTYPE (3, (VB4, VI32M8, SI, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI32M8, UQI))
+DEF_RISCV_FTYPE (2, (VB4, VI32M8, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI32M8, VI32M8))
+DEF_RISCV_FTYPE (3, (VB4, VI32M8, VI32M8, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI32M8, VUI32M8))
+DEF_RISCV_FTYPE (2, (VB4, VI8M2, QI))
+DEF_RISCV_FTYPE (3, (VB4, VI8M2, QI, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI8M2, UQI))
+DEF_RISCV_FTYPE (2, (VB4, VI8M2, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VB4, VI8M2, VI8M2, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VB4, VUI16M4, UHI))
+DEF_RISCV_FTYPE (3, (VB4, VUI16M4, UHI, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VB4, VUI16M4, VUI16M4, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VUI32M8, USI))
+DEF_RISCV_FTYPE (3, (VB4, VUI32M8, USI, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (3, (VB4, VUI32M8, VUI32M8, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VUI8M2, UQI))
+DEF_RISCV_FTYPE (3, (VB4, VUI8M2, UQI, VB4))
+DEF_RISCV_FTYPE (2, (VB4, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VB4, VUI8M2, VUI8M2, VB4))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VF64M1, DF))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VI64M1, DI))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VI64M1, UQI))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VUI64M1, UDI))
+DEF_RISCV_FTYPE (4, (VB64, VB64, VB64, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (2, (VB64, VF64M1, DF))
+DEF_RISCV_FTYPE (2, (VB64, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (2, (VB64, VI64M1, DI))
+DEF_RISCV_FTYPE (3, (VB64, VI64M1, DI, VB64))
+DEF_RISCV_FTYPE (2, (VB64, VI64M1, UQI))
+DEF_RISCV_FTYPE (2, (VB64, VI64M1, VB64))
+DEF_RISCV_FTYPE (2, (VB64, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VB64, VI64M1, VI64M1, VB64))
+DEF_RISCV_FTYPE (2, (VB64, VI64M1, VUI64M1))
+DEF_RISCV_FTYPE (2, (VB64, VUI64M1, UDI))
+DEF_RISCV_FTYPE (3, (VB64, VUI64M1, UDI, VB64))
+DEF_RISCV_FTYPE (2, (VB64, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VB64, VUI64M1, VUI64M1, VB64))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VF16M2, HF))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VF32M4, SF))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VF64M8, DF))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VF64M8, VF64M8))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI16M2, HI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI16M2, UQI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI32M4, SI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI32M4, UQI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI64M8, DI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI64M8, UQI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI64M8, VI64M8))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI64M8, VUI64M8))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI8M1, QI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI16M2, UHI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI32M4, USI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI64M8, UDI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VB8, VB8, VB8, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VB8, VF16M2, HF))
+DEF_RISCV_FTYPE (2, (VB8, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (2, (VB8, VF32M4, SF))
+DEF_RISCV_FTYPE (2, (VB8, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (2, (VB8, VF64M8, DF))
+DEF_RISCV_FTYPE (2, (VB8, VF64M8, VF64M8))
+DEF_RISCV_FTYPE (2, (VB8, VI16M2, HI))
+DEF_RISCV_FTYPE (3, (VB8, VI16M2, HI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI16M2, UQI))
+DEF_RISCV_FTYPE (2, (VB8, VI16M2, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VB8, VI16M2, VI16M2, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VB8, VI32M4, SI))
+DEF_RISCV_FTYPE (3, (VB8, VI32M4, SI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI32M4, UQI))
+DEF_RISCV_FTYPE (2, (VB8, VI32M4, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VB8, VI32M4, VI32M4, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VB8, VI64M8, DI))
+DEF_RISCV_FTYPE (3, (VB8, VI64M8, DI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI64M8, UQI))
+DEF_RISCV_FTYPE (2, (VB8, VI64M8, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI64M8, VI64M8))
+DEF_RISCV_FTYPE (3, (VB8, VI64M8, VI64M8, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI64M8, VUI64M8))
+DEF_RISCV_FTYPE (2, (VB8, VI8M1, QI))
+DEF_RISCV_FTYPE (3, (VB8, VI8M1, QI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI8M1, UQI))
+DEF_RISCV_FTYPE (2, (VB8, VI8M1, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VB8, VI8M1, VI8M1, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VB8, VUI16M2, UHI))
+DEF_RISCV_FTYPE (3, (VB8, VUI16M2, UHI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VB8, VUI16M2, VUI16M2, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VUI32M4, USI))
+DEF_RISCV_FTYPE (3, (VB8, VUI32M4, USI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VB8, VUI32M4, VUI32M4, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VUI64M8, UDI))
+DEF_RISCV_FTYPE (3, (VB8, VUI64M8, UDI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (3, (VB8, VUI64M8, VUI64M8, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VUI8M1, UQI))
+DEF_RISCV_FTYPE (3, (VB8, VUI8M1, UQI, VB8))
+DEF_RISCV_FTYPE (2, (VB8, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VB8, VUI8M1, VUI8M1, VB8))
+DEF_RISCV_FTYPE (0, (VF16M1))
+DEF_RISCV_FTYPE (1, (VF16M1, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF16M1, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF16M1, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, C_HF_PTR, VUI16M1, VF16M1))
+DEF_RISCV_FTYPE (2, (VF16M1, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VF16M1, C_HF_PTR, VUI32M2, VF16M1))
+DEF_RISCV_FTYPE (2, (VF16M1, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1, C_HF_PTR, VUI64M4, VF16M1))
+DEF_RISCV_FTYPE (1, (VF16M1, HF))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, C_HF_PTR, VUI16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, C_HF_PTR, VUI32M2, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, C_HF_PTR, VUI64M4, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, HF))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, HF, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, VF16M1, HF))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, VF16M1, SIZE))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, VF16M1, UHI))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1, VB16, VF16M1, VF16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, VF32M2))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, VI32M2))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, VB16, VF16M1, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1, VB2, VF16M1, VF16M1, VF16M8))
+DEF_RISCV_FTYPE (4, (VF16M1, VB4, VF16M1, VF16M1, VF16M4))
+DEF_RISCV_FTYPE (4, (VF16M1, VB8, VF16M1, VF16M1, VF16M2))
+DEF_RISCV_FTYPE (1, (VF16M1, VF16M1))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1, HF))
+DEF_RISCV_FTYPE (3, (VF16M1, VF16M1, HF, VF16M1))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1, SIZE))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1, UHI))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, VF16M1, VF16M1, SIZE))
+DEF_RISCV_FTYPE (3, (VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1, VF16M1, VF16M1, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M1, VF16M1, VF16M1, VF16M4))
+DEF_RISCV_FTYPE (3, (VF16M1, VF16M1, VF16M1, VF16M8))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1X2, SI))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1X3, SI))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1X4, SI))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1X5, SI))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1X6, SI))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1X7, SI))
+DEF_RISCV_FTYPE (2, (VF16M1, VF16M1X8, SI))
+DEF_RISCV_FTYPE (1, (VF16M1, VF16M2))
+DEF_RISCV_FTYPE (1, (VF16M1, VF16M4))
+DEF_RISCV_FTYPE (1, (VF16M1, VF16M8))
+DEF_RISCV_FTYPE (1, (VF16M1, VF32M2))
+DEF_RISCV_FTYPE (1, (VF16M1, VI16M1))
+DEF_RISCV_FTYPE (1, (VF16M1, VI32M2))
+DEF_RISCV_FTYPE (1, (VF16M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VF16M1, VUI32M2))
+DEF_RISCV_FTYPE (0, (VF16M1X2))
+DEF_RISCV_FTYPE (1, (VF16M1X2, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1X2, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M1X2, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1X2, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF16M1X2, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1X2, VB16, VF16M1X2, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1X2, VB16, VF16M1X2, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M1X2, VB16, VF16M1X2, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X2, VB16, VF16M1X2, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1X2, VB16, VF16M1X2, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF16M1X2, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X2, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X2, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (5, (VF16M1X2, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (6, (VF16M1X2, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (7, (VF16M1X2, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (8, (VF16M1X2, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X2, VF16M1X2, VF16M1, SI))
+DEF_RISCV_FTYPE (0, (VF16M1X3))
+DEF_RISCV_FTYPE (1, (VF16M1X3, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1X3, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M1X3, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1X3, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF16M1X3, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1X3, VB16, VF16M1X3, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1X3, VB16, VF16M1X3, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M1X3, VB16, VF16M1X3, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X3, VB16, VF16M1X3, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1X3, VB16, VF16M1X3, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF16M1X3, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X3, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X3, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (5, (VF16M1X3, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (6, (VF16M1X3, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (7, (VF16M1X3, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (8, (VF16M1X3, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X3, VF16M1X3, VF16M1, SI))
+DEF_RISCV_FTYPE (0, (VF16M1X4))
+DEF_RISCV_FTYPE (1, (VF16M1X4, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1X4, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M1X4, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1X4, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF16M1X4, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1X4, VB16, VF16M1X4, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1X4, VB16, VF16M1X4, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M1X4, VB16, VF16M1X4, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X4, VB16, VF16M1X4, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1X4, VB16, VF16M1X4, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF16M1X4, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X4, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X4, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (5, (VF16M1X4, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (6, (VF16M1X4, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (7, (VF16M1X4, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (8, (VF16M1X4, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X4, VF16M1X4, VF16M1, SI))
+DEF_RISCV_FTYPE (0, (VF16M1X5))
+DEF_RISCV_FTYPE (1, (VF16M1X5, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1X5, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M1X5, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1X5, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF16M1X5, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1X5, VB16, VF16M1X5, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1X5, VB16, VF16M1X5, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M1X5, VB16, VF16M1X5, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X5, VB16, VF16M1X5, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1X5, VB16, VF16M1X5, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF16M1X5, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X5, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X5, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (5, (VF16M1X5, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (6, (VF16M1X5, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (7, (VF16M1X5, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (8, (VF16M1X5, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X5, VF16M1X5, VF16M1, SI))
+DEF_RISCV_FTYPE (0, (VF16M1X6))
+DEF_RISCV_FTYPE (1, (VF16M1X6, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1X6, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M1X6, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1X6, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF16M1X6, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1X6, VB16, VF16M1X6, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1X6, VB16, VF16M1X6, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M1X6, VB16, VF16M1X6, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X6, VB16, VF16M1X6, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1X6, VB16, VF16M1X6, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF16M1X6, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X6, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X6, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (5, (VF16M1X6, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (6, (VF16M1X6, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (7, (VF16M1X6, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (8, (VF16M1X6, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X6, VF16M1X6, VF16M1, SI))
+DEF_RISCV_FTYPE (0, (VF16M1X7))
+DEF_RISCV_FTYPE (1, (VF16M1X7, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1X7, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M1X7, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1X7, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF16M1X7, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1X7, VB16, VF16M1X7, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1X7, VB16, VF16M1X7, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M1X7, VB16, VF16M1X7, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X7, VB16, VF16M1X7, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1X7, VB16, VF16M1X7, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF16M1X7, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X7, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X7, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (5, (VF16M1X7, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (6, (VF16M1X7, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (7, (VF16M1X7, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (8, (VF16M1X7, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X7, VF16M1X7, VF16M1, SI))
+DEF_RISCV_FTYPE (0, (VF16M1X8))
+DEF_RISCV_FTYPE (1, (VF16M1X8, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M1X8, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M1X8, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF16M1X8, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF16M1X8, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF16M1X8, VB16, VF16M1X8, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M1X8, VB16, VF16M1X8, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M1X8, VB16, VF16M1X8, C_HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X8, VB16, VF16M1X8, C_HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF16M1X8, VB16, VF16M1X8, C_HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF16M1X8, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X8, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF16M1X8, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (5, (VF16M1X8, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (6, (VF16M1X8, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (7, (VF16M1X8, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (8, (VF16M1X8, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF16M1X8, VF16M1X8, VF16M1, SI))
+DEF_RISCV_FTYPE (0, (VF16M2))
+DEF_RISCV_FTYPE (1, (VF16M2, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M2, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF16M2, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF16M2, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VF16M2, C_HF_PTR, VUI16M2, VF16M2))
+DEF_RISCV_FTYPE (2, (VF16M2, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VF16M2, C_HF_PTR, VUI32M4, VF16M2))
+DEF_RISCV_FTYPE (2, (VF16M2, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VF16M2, C_HF_PTR, VUI64M8, VF16M2))
+DEF_RISCV_FTYPE (2, (VF16M2, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF16M2, C_HF_PTR, VUI8M1, VF16M2))
+DEF_RISCV_FTYPE (1, (VF16M2, HF))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, C_HF_PTR, VUI16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, C_HF_PTR, VUI32M4, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, C_HF_PTR, VUI64M8, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, C_HF_PTR, VUI8M1, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, HF))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, HF, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, VF16M2, HF))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, VF16M2, SIZE))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, VF16M2, UHI))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2, VB8, VF16M2, VF16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, VF32M4))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, VI32M4))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VF16M2, VB8, VF16M2, VUI32M4))
+DEF_RISCV_FTYPE (1, (VF16M2, VF16M1))
+DEF_RISCV_FTYPE (1, (VF16M2, VF16M2))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2, HF))
+DEF_RISCV_FTYPE (3, (VF16M2, VF16M2, HF, VF16M2))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2, SIZE))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2, UHI))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2, VF16M2, VF16M2, SIZE))
+DEF_RISCV_FTYPE (3, (VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2X2, SI))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2X3, SI))
+DEF_RISCV_FTYPE (2, (VF16M2, VF16M2X4, SI))
+DEF_RISCV_FTYPE (1, (VF16M2, VF16M4))
+DEF_RISCV_FTYPE (1, (VF16M2, VF16M8))
+DEF_RISCV_FTYPE (1, (VF16M2, VF32M4))
+DEF_RISCV_FTYPE (1, (VF16M2, VI16M2))
+DEF_RISCV_FTYPE (1, (VF16M2, VI32M4))
+DEF_RISCV_FTYPE (1, (VF16M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VF16M2, VUI32M4))
+DEF_RISCV_FTYPE (0, (VF16M2X2))
+DEF_RISCV_FTYPE (1, (VF16M2X2, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M2X2, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M2X2, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VF16M2X2, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VF16M2X2, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VF16M2X2, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF16M2X2, VB8, VF16M2X2, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M2X2, VB8, VF16M2X2, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M2X2, VB8, VF16M2X2, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VF16M2X2, VB8, VF16M2X2, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VF16M2X2, VB8, VF16M2X2, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VF16M2X2, VB8, VF16M2X2, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VF16M2X2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2X2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2X2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (5, (VF16M2X2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (6, (VF16M2X2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (7, (VF16M2X2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (8, (VF16M2X2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2X2, VF16M2X2, VF16M2, SI))
+DEF_RISCV_FTYPE (0, (VF16M2X3))
+DEF_RISCV_FTYPE (1, (VF16M2X3, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M2X3, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M2X3, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VF16M2X3, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VF16M2X3, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VF16M2X3, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF16M2X3, VB8, VF16M2X3, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M2X3, VB8, VF16M2X3, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M2X3, VB8, VF16M2X3, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VF16M2X3, VB8, VF16M2X3, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VF16M2X3, VB8, VF16M2X3, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VF16M2X3, VB8, VF16M2X3, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VF16M2X3, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2X3, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2X3, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (5, (VF16M2X3, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (6, (VF16M2X3, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (7, (VF16M2X3, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (8, (VF16M2X3, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2X3, VF16M2X3, VF16M2, SI))
+DEF_RISCV_FTYPE (0, (VF16M2X4))
+DEF_RISCV_FTYPE (1, (VF16M2X4, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M2X4, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M2X4, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VF16M2X4, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VF16M2X4, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VF16M2X4, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF16M2X4, VB8, VF16M2X4, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M2X4, VB8, VF16M2X4, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M2X4, VB8, VF16M2X4, C_HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VF16M2X4, VB8, VF16M2X4, C_HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VF16M2X4, VB8, VF16M2X4, C_HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VF16M2X4, VB8, VF16M2X4, C_HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VF16M2X4, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2X4, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VF16M2X4, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (5, (VF16M2X4, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (6, (VF16M2X4, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (7, (VF16M2X4, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (8, (VF16M2X4, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF16M2X4, VF16M2X4, VF16M2, SI))
+DEF_RISCV_FTYPE (0, (VF16M4))
+DEF_RISCV_FTYPE (1, (VF16M4, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M4, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF16M4, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF16M4, C_HF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VF16M4, C_HF_PTR, VUI16M4, VF16M4))
+DEF_RISCV_FTYPE (2, (VF16M4, C_HF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VF16M4, C_HF_PTR, VUI32M8, VF16M4))
+DEF_RISCV_FTYPE (2, (VF16M4, C_HF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VF16M4, C_HF_PTR, VUI8M2, VF16M4))
+DEF_RISCV_FTYPE (1, (VF16M4, HF))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, C_HF_PTR, VUI16M4, VF16M4))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, C_HF_PTR, VUI32M8, VF16M4))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, C_HF_PTR, VUI8M2, VF16M4))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, C_HF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, C_HF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, C_HF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, HF))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, HF, VF16M4))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, VF16M4, HF))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, VF16M4, SIZE))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, VF16M4, UHI))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (4, (VF16M4, VB4, VF16M4, VF16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, VF32M8))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, VI16M4))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, VI32M8))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VF16M4, VB4, VF16M4, VUI32M8))
+DEF_RISCV_FTYPE (1, (VF16M4, VF16M1))
+DEF_RISCV_FTYPE (1, (VF16M4, VF16M2))
+DEF_RISCV_FTYPE (1, (VF16M4, VF16M4))
+DEF_RISCV_FTYPE (2, (VF16M4, VF16M4, HF))
+DEF_RISCV_FTYPE (3, (VF16M4, VF16M4, HF, VF16M4))
+DEF_RISCV_FTYPE (2, (VF16M4, VF16M4, SIZE))
+DEF_RISCV_FTYPE (2, (VF16M4, VF16M4, UHI))
+DEF_RISCV_FTYPE (2, (VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (3, (VF16M4, VF16M4, VF16M4, SIZE))
+DEF_RISCV_FTYPE (3, (VF16M4, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (2, (VF16M4, VF16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VF16M4, VF16M4X2, SI))
+DEF_RISCV_FTYPE (1, (VF16M4, VF16M8))
+DEF_RISCV_FTYPE (1, (VF16M4, VF32M8))
+DEF_RISCV_FTYPE (1, (VF16M4, VI16M4))
+DEF_RISCV_FTYPE (1, (VF16M4, VI32M8))
+DEF_RISCV_FTYPE (1, (VF16M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VF16M4, VUI32M8))
+DEF_RISCV_FTYPE (0, (VF16M4X2))
+DEF_RISCV_FTYPE (1, (VF16M4X2, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M4X2, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF16M4X2, C_HF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VF16M4X2, C_HF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VF16M4X2, C_HF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VF16M4X2, VB4, VF16M4X2, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M4X2, VB4, VF16M4X2, C_HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF16M4X2, VB4, VF16M4X2, C_HF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VF16M4X2, VB4, VF16M4X2, C_HF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VF16M4X2, VB4, VF16M4X2, C_HF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VF16M4X2, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (3, (VF16M4X2, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (4, (VF16M4X2, VF16M4, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (5, (VF16M4X2, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (6, (VF16M4X2, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (7, (VF16M4X2, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (8, (VF16M4X2, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (3, (VF16M4X2, VF16M4X2, VF16M4, SI))
+DEF_RISCV_FTYPE (0, (VF16M8))
+DEF_RISCV_FTYPE (1, (VF16M8, C_HF_PTR))
+DEF_RISCV_FTYPE (2, (VF16M8, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF16M8, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF16M8, C_HF_PTR, VUI16M8))
+DEF_RISCV_FTYPE (3, (VF16M8, C_HF_PTR, VUI16M8, VF16M8))
+DEF_RISCV_FTYPE (2, (VF16M8, C_HF_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VF16M8, C_HF_PTR, VUI8M4, VF16M8))
+DEF_RISCV_FTYPE (1, (VF16M8, HF))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, C_HF_PTR, VUI16M8, VF16M8))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, C_HF_PTR, VUI8M4, VF16M8))
+DEF_RISCV_FTYPE (3, (VF16M8, VB2, VF16M8, C_HF_PTR))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, C_HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, C_HF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, C_HF_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, C_HF_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VF16M8, VB2, VF16M8, HF))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, HF, VF16M8))
+DEF_RISCV_FTYPE (3, (VF16M8, VB2, VF16M8, VF16M8))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, VF16M8, HF))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, VF16M8, SIZE))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, VF16M8, UHI))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, VF16M8, VF16M8))
+DEF_RISCV_FTYPE (4, (VF16M8, VB2, VF16M8, VF16M8, VUI16M8))
+DEF_RISCV_FTYPE (3, (VF16M8, VB2, VF16M8, VI16M8))
+DEF_RISCV_FTYPE (3, (VF16M8, VB2, VF16M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VF16M8, VF16M1))
+DEF_RISCV_FTYPE (1, (VF16M8, VF16M2))
+DEF_RISCV_FTYPE (1, (VF16M8, VF16M4))
+DEF_RISCV_FTYPE (1, (VF16M8, VF16M8))
+DEF_RISCV_FTYPE (2, (VF16M8, VF16M8, HF))
+DEF_RISCV_FTYPE (3, (VF16M8, VF16M8, HF, VF16M8))
+DEF_RISCV_FTYPE (2, (VF16M8, VF16M8, SIZE))
+DEF_RISCV_FTYPE (2, (VF16M8, VF16M8, UHI))
+DEF_RISCV_FTYPE (2, (VF16M8, VF16M8, VF16M8))
+DEF_RISCV_FTYPE (3, (VF16M8, VF16M8, VF16M8, SIZE))
+DEF_RISCV_FTYPE (3, (VF16M8, VF16M8, VF16M8, VF16M8))
+DEF_RISCV_FTYPE (2, (VF16M8, VF16M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VF16M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VF16M8, VUI16M8))
+DEF_RISCV_FTYPE (0, (VF32M1))
+DEF_RISCV_FTYPE (1, (VF32M1, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF32M1, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF32M1, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, C_SF_PTR, VUI32M1, VF32M1))
+DEF_RISCV_FTYPE (2, (VF32M1, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1, C_SF_PTR, VUI64M2, VF32M1))
+DEF_RISCV_FTYPE (1, (VF32M1, SF))
+DEF_RISCV_FTYPE (4, (VF32M1, VB16, VF32M1, VF32M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VF32M1, VB16, VF32M1, VF32M1, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M1, VB2, VF32M1, VF32M1, VF16M8))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, C_SF_PTR, VUI32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, C_SF_PTR, VUI64M2, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, SF))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, SF, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, VF32M1, SF))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, VF32M1, SIZE))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, VF32M1, USI))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1, VB32, VF32M1, VF32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, VF64M2))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, VI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VB32, VF32M1, VUI64M2))
+DEF_RISCV_FTYPE (4, (VF32M1, VB4, VF32M1, VF32M1, VF16M4))
+DEF_RISCV_FTYPE (4, (VF32M1, VB4, VF32M1, VF32M1, VF32M8))
+DEF_RISCV_FTYPE (4, (VF32M1, VB8, VF32M1, VF32M1, VF16M2))
+DEF_RISCV_FTYPE (4, (VF32M1, VB8, VF32M1, VF32M1, VF32M4))
+DEF_RISCV_FTYPE (1, (VF32M1, VF32M1))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1, SF))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, SF, VF32M1))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1, SIZE))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1, USI))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, SIZE))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF16M2))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF16M4))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF16M8))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF32M4))
+DEF_RISCV_FTYPE (3, (VF32M1, VF32M1, VF32M1, VF32M8))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1X2, SI))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1X3, SI))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1X4, SI))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1X5, SI))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1X6, SI))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1X7, SI))
+DEF_RISCV_FTYPE (2, (VF32M1, VF32M1X8, SI))
+DEF_RISCV_FTYPE (1, (VF32M1, VF32M2))
+DEF_RISCV_FTYPE (1, (VF32M1, VF32M4))
+DEF_RISCV_FTYPE (1, (VF32M1, VF32M8))
+DEF_RISCV_FTYPE (1, (VF32M1, VF64M2))
+DEF_RISCV_FTYPE (1, (VF32M1, VI32M1))
+DEF_RISCV_FTYPE (1, (VF32M1, VI64M2))
+DEF_RISCV_FTYPE (1, (VF32M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VF32M1, VUI64M2))
+DEF_RISCV_FTYPE (0, (VF32M1X2))
+DEF_RISCV_FTYPE (1, (VF32M1X2, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1X2, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M1X2, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1X2, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1X2, VB32, VF32M1X2, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1X2, VB32, VF32M1X2, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M1X2, VB32, VF32M1X2, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X2, VB32, VF32M1X2, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF32M1X2, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X2, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X2, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (5, (VF32M1X2, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (6, (VF32M1X2, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (7, (VF32M1X2, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (8, (VF32M1X2, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X2, VF32M1X2, VF32M1, SI))
+DEF_RISCV_FTYPE (0, (VF32M1X3))
+DEF_RISCV_FTYPE (1, (VF32M1X3, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1X3, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M1X3, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1X3, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1X3, VB32, VF32M1X3, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1X3, VB32, VF32M1X3, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M1X3, VB32, VF32M1X3, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X3, VB32, VF32M1X3, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF32M1X3, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X3, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X3, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (5, (VF32M1X3, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (6, (VF32M1X3, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (7, (VF32M1X3, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (8, (VF32M1X3, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X3, VF32M1X3, VF32M1, SI))
+DEF_RISCV_FTYPE (0, (VF32M1X4))
+DEF_RISCV_FTYPE (1, (VF32M1X4, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1X4, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M1X4, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1X4, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1X4, VB32, VF32M1X4, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1X4, VB32, VF32M1X4, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M1X4, VB32, VF32M1X4, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X4, VB32, VF32M1X4, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF32M1X4, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X4, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X4, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (5, (VF32M1X4, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (6, (VF32M1X4, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (7, (VF32M1X4, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (8, (VF32M1X4, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X4, VF32M1X4, VF32M1, SI))
+DEF_RISCV_FTYPE (0, (VF32M1X5))
+DEF_RISCV_FTYPE (1, (VF32M1X5, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1X5, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M1X5, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1X5, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1X5, VB32, VF32M1X5, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1X5, VB32, VF32M1X5, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M1X5, VB32, VF32M1X5, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X5, VB32, VF32M1X5, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF32M1X5, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X5, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X5, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (5, (VF32M1X5, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (6, (VF32M1X5, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (7, (VF32M1X5, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (8, (VF32M1X5, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X5, VF32M1X5, VF32M1, SI))
+DEF_RISCV_FTYPE (0, (VF32M1X6))
+DEF_RISCV_FTYPE (1, (VF32M1X6, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1X6, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M1X6, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1X6, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1X6, VB32, VF32M1X6, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1X6, VB32, VF32M1X6, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M1X6, VB32, VF32M1X6, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X6, VB32, VF32M1X6, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF32M1X6, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X6, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X6, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (5, (VF32M1X6, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (6, (VF32M1X6, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (7, (VF32M1X6, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (8, (VF32M1X6, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X6, VF32M1X6, VF32M1, SI))
+DEF_RISCV_FTYPE (0, (VF32M1X7))
+DEF_RISCV_FTYPE (1, (VF32M1X7, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1X7, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M1X7, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1X7, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1X7, VB32, VF32M1X7, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1X7, VB32, VF32M1X7, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M1X7, VB32, VF32M1X7, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X7, VB32, VF32M1X7, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF32M1X7, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X7, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X7, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (5, (VF32M1X7, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (6, (VF32M1X7, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (7, (VF32M1X7, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (8, (VF32M1X7, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X7, VF32M1X7, VF32M1, SI))
+DEF_RISCV_FTYPE (0, (VF32M1X8))
+DEF_RISCV_FTYPE (1, (VF32M1X8, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M1X8, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M1X8, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF32M1X8, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF32M1X8, VB32, VF32M1X8, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M1X8, VB32, VF32M1X8, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M1X8, VB32, VF32M1X8, C_SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X8, VB32, VF32M1X8, C_SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF32M1X8, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X8, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF32M1X8, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (5, (VF32M1X8, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (6, (VF32M1X8, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (7, (VF32M1X8, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (8, (VF32M1X8, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF32M1X8, VF32M1X8, VF32M1, SI))
+DEF_RISCV_FTYPE (0, (VF32M2))
+DEF_RISCV_FTYPE (1, (VF32M2, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M2, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF32M2, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF32M2, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VF32M2, C_SF_PTR, VUI16M1, VF32M2))
+DEF_RISCV_FTYPE (2, (VF32M2, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VF32M2, C_SF_PTR, VUI32M2, VF32M2))
+DEF_RISCV_FTYPE (2, (VF32M2, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF32M2, C_SF_PTR, VUI64M4, VF32M2))
+DEF_RISCV_FTYPE (1, (VF32M2, SF))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, C_SF_PTR, VUI16M1, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, C_SF_PTR, VUI32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, C_SF_PTR, VUI64M4, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, HF, VF16M1))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, SF))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, SF, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VF16M1))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF16M1, HF))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF32M2, HF))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF32M2, SF))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF32M2, SIZE))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF32M2, USI))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF32M2, VF16M1))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M2, VB16, VF32M2, VF32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VF64M4))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VI16M1))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VI64M4))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VUI16M1))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VF32M2, VB16, VF32M2, VUI64M4))
+DEF_RISCV_FTYPE (1, (VF32M2, VF16M1))
+DEF_RISCV_FTYPE (2, (VF32M2, VF16M1, HF))
+DEF_RISCV_FTYPE (2, (VF32M2, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (1, (VF32M2, VF32M1))
+DEF_RISCV_FTYPE (1, (VF32M2, VF32M2))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2, HF))
+DEF_RISCV_FTYPE (3, (VF32M2, VF32M2, HF, VF16M1))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2, SF))
+DEF_RISCV_FTYPE (3, (VF32M2, VF32M2, SF, VF32M2))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2, SIZE))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2, USI))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2, VF16M1))
+DEF_RISCV_FTYPE (3, (VF32M2, VF32M2, VF16M1, VF16M1))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2, VF32M2, VF32M2, SIZE))
+DEF_RISCV_FTYPE (3, (VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2X2, SI))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2X3, SI))
+DEF_RISCV_FTYPE (2, (VF32M2, VF32M2X4, SI))
+DEF_RISCV_FTYPE (1, (VF32M2, VF32M4))
+DEF_RISCV_FTYPE (1, (VF32M2, VF32M8))
+DEF_RISCV_FTYPE (1, (VF32M2, VF64M4))
+DEF_RISCV_FTYPE (1, (VF32M2, VI16M1))
+DEF_RISCV_FTYPE (1, (VF32M2, VI32M2))
+DEF_RISCV_FTYPE (1, (VF32M2, VI64M4))
+DEF_RISCV_FTYPE (1, (VF32M2, VUI16M1))
+DEF_RISCV_FTYPE (1, (VF32M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VF32M2, VUI64M4))
+DEF_RISCV_FTYPE (0, (VF32M2X2))
+DEF_RISCV_FTYPE (1, (VF32M2X2, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M2X2, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M2X2, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF32M2X2, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF32M2X2, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF32M2X2, VB16, VF32M2X2, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M2X2, VB16, VF32M2X2, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M2X2, VB16, VF32M2X2, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF32M2X2, VB16, VF32M2X2, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF32M2X2, VB16, VF32M2X2, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF32M2X2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2X2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M2X2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (5, (VF32M2X2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (6, (VF32M2X2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (7, (VF32M2X2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (8, (VF32M2X2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2X2, VF32M2X2, VF32M2, SI))
+DEF_RISCV_FTYPE (0, (VF32M2X3))
+DEF_RISCV_FTYPE (1, (VF32M2X3, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M2X3, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M2X3, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF32M2X3, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF32M2X3, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF32M2X3, VB16, VF32M2X3, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M2X3, VB16, VF32M2X3, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M2X3, VB16, VF32M2X3, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF32M2X3, VB16, VF32M2X3, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF32M2X3, VB16, VF32M2X3, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF32M2X3, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2X3, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M2X3, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (5, (VF32M2X3, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (6, (VF32M2X3, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (7, (VF32M2X3, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (8, (VF32M2X3, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2X3, VF32M2X3, VF32M2, SI))
+DEF_RISCV_FTYPE (0, (VF32M2X4))
+DEF_RISCV_FTYPE (1, (VF32M2X4, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M2X4, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M2X4, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF32M2X4, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF32M2X4, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF32M2X4, VB16, VF32M2X4, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M2X4, VB16, VF32M2X4, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M2X4, VB16, VF32M2X4, C_SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF32M2X4, VB16, VF32M2X4, C_SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF32M2X4, VB16, VF32M2X4, C_SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF32M2X4, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2X4, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VF32M2X4, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (5, (VF32M2X4, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (6, (VF32M2X4, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (7, (VF32M2X4, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (8, (VF32M2X4, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF32M2X4, VF32M2X4, VF32M2, SI))
+DEF_RISCV_FTYPE (0, (VF32M4))
+DEF_RISCV_FTYPE (1, (VF32M4, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M4, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF32M4, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF32M4, C_SF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VF32M4, C_SF_PTR, VUI16M2, VF32M4))
+DEF_RISCV_FTYPE (2, (VF32M4, C_SF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VF32M4, C_SF_PTR, VUI32M4, VF32M4))
+DEF_RISCV_FTYPE (2, (VF32M4, C_SF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VF32M4, C_SF_PTR, VUI64M8, VF32M4))
+DEF_RISCV_FTYPE (2, (VF32M4, C_SF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF32M4, C_SF_PTR, VUI8M1, VF32M4))
+DEF_RISCV_FTYPE (1, (VF32M4, SF))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, C_SF_PTR, VUI16M2, VF32M4))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, C_SF_PTR, VUI32M4, VF32M4))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, C_SF_PTR, VUI64M8, VF32M4))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, C_SF_PTR, VUI8M1, VF32M4))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, C_SF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, C_SF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, C_SF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, C_SF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, HF, VF16M2))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, SF))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, SF, VF32M4))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VF16M2))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF16M2, HF))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF32M4, HF))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF32M4, SF))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF32M4, SIZE))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF32M4, USI))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF32M4, VF16M2))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (4, (VF32M4, VB8, VF32M4, VF32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VF64M8))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VI16M2))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VI64M8))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VUI16M2))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VF32M4, VB8, VF32M4, VUI64M8))
+DEF_RISCV_FTYPE (1, (VF32M4, VF16M2))
+DEF_RISCV_FTYPE (2, (VF32M4, VF16M2, HF))
+DEF_RISCV_FTYPE (2, (VF32M4, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (1, (VF32M4, VF32M1))
+DEF_RISCV_FTYPE (1, (VF32M4, VF32M2))
+DEF_RISCV_FTYPE (1, (VF32M4, VF32M4))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4, HF))
+DEF_RISCV_FTYPE (3, (VF32M4, VF32M4, HF, VF16M2))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4, SF))
+DEF_RISCV_FTYPE (3, (VF32M4, VF32M4, SF, VF32M4))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4, SIZE))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4, USI))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4, VF16M2))
+DEF_RISCV_FTYPE (3, (VF32M4, VF32M4, VF16M2, VF16M2))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (3, (VF32M4, VF32M4, VF32M4, SIZE))
+DEF_RISCV_FTYPE (3, (VF32M4, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VF32M4, VF32M4X2, SI))
+DEF_RISCV_FTYPE (1, (VF32M4, VF32M8))
+DEF_RISCV_FTYPE (1, (VF32M4, VF64M8))
+DEF_RISCV_FTYPE (1, (VF32M4, VI16M2))
+DEF_RISCV_FTYPE (1, (VF32M4, VI32M4))
+DEF_RISCV_FTYPE (1, (VF32M4, VI64M8))
+DEF_RISCV_FTYPE (1, (VF32M4, VUI16M2))
+DEF_RISCV_FTYPE (1, (VF32M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VF32M4, VUI64M8))
+DEF_RISCV_FTYPE (0, (VF32M4X2))
+DEF_RISCV_FTYPE (1, (VF32M4X2, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M4X2, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF32M4X2, C_SF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VF32M4X2, C_SF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VF32M4X2, C_SF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VF32M4X2, C_SF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF32M4X2, VB8, VF32M4X2, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M4X2, VB8, VF32M4X2, C_SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF32M4X2, VB8, VF32M4X2, C_SF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VF32M4X2, VB8, VF32M4X2, C_SF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VF32M4X2, VB8, VF32M4X2, C_SF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VF32M4X2, VB8, VF32M4X2, C_SF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VF32M4X2, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (3, (VF32M4X2, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (4, (VF32M4X2, VF32M4, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (5, (VF32M4X2, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (6, (VF32M4X2, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (7, (VF32M4X2, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (8, (VF32M4X2, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (3, (VF32M4X2, VF32M4X2, VF32M4, SI))
+DEF_RISCV_FTYPE (0, (VF32M8))
+DEF_RISCV_FTYPE (1, (VF32M8, C_SF_PTR))
+DEF_RISCV_FTYPE (2, (VF32M8, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF32M8, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF32M8, C_SF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VF32M8, C_SF_PTR, VUI16M4, VF32M8))
+DEF_RISCV_FTYPE (2, (VF32M8, C_SF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VF32M8, C_SF_PTR, VUI32M8, VF32M8))
+DEF_RISCV_FTYPE (2, (VF32M8, C_SF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VF32M8, C_SF_PTR, VUI8M2, VF32M8))
+DEF_RISCV_FTYPE (1, (VF32M8, SF))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, C_SF_PTR, VUI16M4, VF32M8))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, C_SF_PTR, VUI32M8, VF32M8))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, C_SF_PTR, VUI8M2, VF32M8))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, C_SF_PTR))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, C_SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, C_SF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, C_SF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, C_SF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, C_SF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, HF, VF16M4))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, SF))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, SF, VF32M8))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, VF16M4))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF16M4, HF))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, VF32M8))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF32M8, HF))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF32M8, SF))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF32M8, SIZE))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF32M8, USI))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF32M8, VF16M4))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF32M8, VF32M8))
+DEF_RISCV_FTYPE (4, (VF32M8, VB4, VF32M8, VF32M8, VUI32M8))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, VI16M4))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, VI32M8))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, VUI16M4))
+DEF_RISCV_FTYPE (3, (VF32M8, VB4, VF32M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VF32M8, VF16M4))
+DEF_RISCV_FTYPE (2, (VF32M8, VF16M4, HF))
+DEF_RISCV_FTYPE (2, (VF32M8, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (1, (VF32M8, VF32M1))
+DEF_RISCV_FTYPE (1, (VF32M8, VF32M2))
+DEF_RISCV_FTYPE (1, (VF32M8, VF32M4))
+DEF_RISCV_FTYPE (1, (VF32M8, VF32M8))
+DEF_RISCV_FTYPE (2, (VF32M8, VF32M8, HF))
+DEF_RISCV_FTYPE (3, (VF32M8, VF32M8, HF, VF16M4))
+DEF_RISCV_FTYPE (2, (VF32M8, VF32M8, SF))
+DEF_RISCV_FTYPE (3, (VF32M8, VF32M8, SF, VF32M8))
+DEF_RISCV_FTYPE (2, (VF32M8, VF32M8, SIZE))
+DEF_RISCV_FTYPE (2, (VF32M8, VF32M8, USI))
+DEF_RISCV_FTYPE (2, (VF32M8, VF32M8, VF16M4))
+DEF_RISCV_FTYPE (3, (VF32M8, VF32M8, VF16M4, VF16M4))
+DEF_RISCV_FTYPE (2, (VF32M8, VF32M8, VF32M8))
+DEF_RISCV_FTYPE (3, (VF32M8, VF32M8, VF32M8, SIZE))
+DEF_RISCV_FTYPE (3, (VF32M8, VF32M8, VF32M8, VF32M8))
+DEF_RISCV_FTYPE (2, (VF32M8, VF32M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VF32M8, VI16M4))
+DEF_RISCV_FTYPE (1, (VF32M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VF32M8, VUI16M4))
+DEF_RISCV_FTYPE (1, (VF32M8, VUI32M8))
+DEF_RISCV_FTYPE (0, (VF64M1))
+DEF_RISCV_FTYPE (1, (VF64M1, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF64M1, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF64M1, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, C_DF_PTR, VUI64M1, VF64M1))
+DEF_RISCV_FTYPE (1, (VF64M1, DF))
+DEF_RISCV_FTYPE (4, (VF64M1, VB16, VF64M1, VF64M1, VF32M2))
+DEF_RISCV_FTYPE (4, (VF64M1, VB16, VF64M1, VF64M1, VF64M4))
+DEF_RISCV_FTYPE (4, (VF64M1, VB32, VF64M1, VF64M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VF64M1, VB32, VF64M1, VF64M1, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M1, VB4, VF64M1, VF64M1, VF32M8))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, C_DF_PTR, VUI64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VB64, VF64M1, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VB64, VF64M1, DF))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, DF, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VB64, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, VF64M1, DF))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, VF64M1, SIZE))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, VF64M1, UDI))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1, VB64, VF64M1, VF64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VB64, VF64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VB64, VF64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VF64M1, VB8, VF64M1, VF64M1, VF32M4))
+DEF_RISCV_FTYPE (4, (VF64M1, VB8, VF64M1, VF64M1, VF64M8))
+DEF_RISCV_FTYPE (1, (VF64M1, VF64M1))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1, DF))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, DF, VF64M1))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1, SIZE))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1, UDI))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, SIZE))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF32M2))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF32M4))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF32M8))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF64M4))
+DEF_RISCV_FTYPE (3, (VF64M1, VF64M1, VF64M1, VF64M8))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1X2, SI))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1X3, SI))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1X4, SI))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1X5, SI))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1X6, SI))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1X7, SI))
+DEF_RISCV_FTYPE (2, (VF64M1, VF64M1X8, SI))
+DEF_RISCV_FTYPE (1, (VF64M1, VF64M2))
+DEF_RISCV_FTYPE (1, (VF64M1, VF64M4))
+DEF_RISCV_FTYPE (1, (VF64M1, VF64M8))
+DEF_RISCV_FTYPE (1, (VF64M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VF64M1, VUI64M1))
+DEF_RISCV_FTYPE (0, (VF64M1X2))
+DEF_RISCV_FTYPE (1, (VF64M1X2, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1X2, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M1X2, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X2, VB64, VF64M1X2, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1X2, VB64, VF64M1X2, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M1X2, VB64, VF64M1X2, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1X2, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X2, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1X2, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (5, (VF64M1X2, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (6, (VF64M1X2, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (7, (VF64M1X2, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (8, (VF64M1X2, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X2, VF64M1X2, VF64M1, SI))
+DEF_RISCV_FTYPE (0, (VF64M1X3))
+DEF_RISCV_FTYPE (1, (VF64M1X3, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1X3, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M1X3, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X3, VB64, VF64M1X3, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1X3, VB64, VF64M1X3, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M1X3, VB64, VF64M1X3, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1X3, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X3, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1X3, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (5, (VF64M1X3, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (6, (VF64M1X3, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (7, (VF64M1X3, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (8, (VF64M1X3, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X3, VF64M1X3, VF64M1, SI))
+DEF_RISCV_FTYPE (0, (VF64M1X4))
+DEF_RISCV_FTYPE (1, (VF64M1X4, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1X4, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M1X4, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X4, VB64, VF64M1X4, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1X4, VB64, VF64M1X4, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M1X4, VB64, VF64M1X4, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1X4, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X4, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1X4, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (5, (VF64M1X4, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (6, (VF64M1X4, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (7, (VF64M1X4, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (8, (VF64M1X4, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X4, VF64M1X4, VF64M1, SI))
+DEF_RISCV_FTYPE (0, (VF64M1X5))
+DEF_RISCV_FTYPE (1, (VF64M1X5, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1X5, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M1X5, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X5, VB64, VF64M1X5, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1X5, VB64, VF64M1X5, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M1X5, VB64, VF64M1X5, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1X5, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X5, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1X5, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (5, (VF64M1X5, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (6, (VF64M1X5, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (7, (VF64M1X5, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (8, (VF64M1X5, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X5, VF64M1X5, VF64M1, SI))
+DEF_RISCV_FTYPE (0, (VF64M1X6))
+DEF_RISCV_FTYPE (1, (VF64M1X6, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1X6, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M1X6, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X6, VB64, VF64M1X6, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1X6, VB64, VF64M1X6, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M1X6, VB64, VF64M1X6, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1X6, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X6, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1X6, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (5, (VF64M1X6, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (6, (VF64M1X6, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (7, (VF64M1X6, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (8, (VF64M1X6, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X6, VF64M1X6, VF64M1, SI))
+DEF_RISCV_FTYPE (0, (VF64M1X7))
+DEF_RISCV_FTYPE (1, (VF64M1X7, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1X7, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M1X7, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X7, VB64, VF64M1X7, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1X7, VB64, VF64M1X7, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M1X7, VB64, VF64M1X7, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1X7, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X7, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1X7, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (5, (VF64M1X7, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (6, (VF64M1X7, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (7, (VF64M1X7, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (8, (VF64M1X7, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X7, VF64M1X7, VF64M1, SI))
+DEF_RISCV_FTYPE (0, (VF64M1X8))
+DEF_RISCV_FTYPE (1, (VF64M1X8, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M1X8, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M1X8, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X8, VB64, VF64M1X8, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M1X8, VB64, VF64M1X8, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M1X8, VB64, VF64M1X8, C_DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VF64M1X8, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X8, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VF64M1X8, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (5, (VF64M1X8, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (6, (VF64M1X8, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (7, (VF64M1X8, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (8, (VF64M1X8, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VF64M1X8, VF64M1X8, VF64M1, SI))
+DEF_RISCV_FTYPE (0, (VF64M2))
+DEF_RISCV_FTYPE (1, (VF64M2, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M2, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF64M2, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF64M2, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VF64M2, C_DF_PTR, VUI32M1, VF64M2))
+DEF_RISCV_FTYPE (2, (VF64M2, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF64M2, C_DF_PTR, VUI64M2, VF64M2))
+DEF_RISCV_FTYPE (1, (VF64M2, DF))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, C_DF_PTR, VUI32M1, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, C_DF_PTR, VUI64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, DF))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, DF, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, SF, VF32M1))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, VF32M1))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF32M1, SF))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF64M2, DF))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF64M2, SF))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF64M2, SIZE))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF64M2, UDI))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF64M2, VF32M1))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M2, VB32, VF64M2, VF64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, VI32M1))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, VUI32M1))
+DEF_RISCV_FTYPE (3, (VF64M2, VB32, VF64M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VF64M2, VF32M1))
+DEF_RISCV_FTYPE (2, (VF64M2, VF32M1, SF))
+DEF_RISCV_FTYPE (2, (VF64M2, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (1, (VF64M2, VF64M1))
+DEF_RISCV_FTYPE (1, (VF64M2, VF64M2))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2, DF))
+DEF_RISCV_FTYPE (3, (VF64M2, VF64M2, DF, VF64M2))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2, SF))
+DEF_RISCV_FTYPE (3, (VF64M2, VF64M2, SF, VF32M1))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2, SIZE))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2, UDI))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2, VF32M1))
+DEF_RISCV_FTYPE (3, (VF64M2, VF64M2, VF32M1, VF32M1))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2, VF64M2, VF64M2, SIZE))
+DEF_RISCV_FTYPE (3, (VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2X2, SI))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2X3, SI))
+DEF_RISCV_FTYPE (2, (VF64M2, VF64M2X4, SI))
+DEF_RISCV_FTYPE (1, (VF64M2, VF64M4))
+DEF_RISCV_FTYPE (1, (VF64M2, VF64M8))
+DEF_RISCV_FTYPE (1, (VF64M2, VI32M1))
+DEF_RISCV_FTYPE (1, (VF64M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VF64M2, VUI32M1))
+DEF_RISCV_FTYPE (1, (VF64M2, VUI64M2))
+DEF_RISCV_FTYPE (0, (VF64M2X2))
+DEF_RISCV_FTYPE (1, (VF64M2X2, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M2X2, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M2X2, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF64M2X2, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X2, VB32, VF64M2X2, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M2X2, VB32, VF64M2X2, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M2X2, VB32, VF64M2X2, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF64M2X2, VB32, VF64M2X2, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF64M2X2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M2X2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (5, (VF64M2X2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (6, (VF64M2X2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (7, (VF64M2X2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (8, (VF64M2X2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X2, VF64M2X2, VF64M2, SI))
+DEF_RISCV_FTYPE (0, (VF64M2X3))
+DEF_RISCV_FTYPE (1, (VF64M2X3, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M2X3, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M2X3, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF64M2X3, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X3, VB32, VF64M2X3, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M2X3, VB32, VF64M2X3, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M2X3, VB32, VF64M2X3, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF64M2X3, VB32, VF64M2X3, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF64M2X3, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X3, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M2X3, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (5, (VF64M2X3, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (6, (VF64M2X3, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (7, (VF64M2X3, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (8, (VF64M2X3, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X3, VF64M2X3, VF64M2, SI))
+DEF_RISCV_FTYPE (0, (VF64M2X4))
+DEF_RISCV_FTYPE (1, (VF64M2X4, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M2X4, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M2X4, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VF64M2X4, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X4, VB32, VF64M2X4, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M2X4, VB32, VF64M2X4, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M2X4, VB32, VF64M2X4, C_DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VF64M2X4, VB32, VF64M2X4, C_DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VF64M2X4, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X4, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (4, (VF64M2X4, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (5, (VF64M2X4, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (6, (VF64M2X4, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (7, (VF64M2X4, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (8, (VF64M2X4, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VF64M2X4, VF64M2X4, VF64M2, SI))
+DEF_RISCV_FTYPE (0, (VF64M4))
+DEF_RISCV_FTYPE (1, (VF64M4, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M4, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF64M4, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF64M4, C_DF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VF64M4, C_DF_PTR, VUI16M1, VF64M4))
+DEF_RISCV_FTYPE (2, (VF64M4, C_DF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VF64M4, C_DF_PTR, VUI32M2, VF64M4))
+DEF_RISCV_FTYPE (2, (VF64M4, C_DF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF64M4, C_DF_PTR, VUI64M4, VF64M4))
+DEF_RISCV_FTYPE (1, (VF64M4, DF))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, C_DF_PTR, VUI16M1, VF64M4))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, C_DF_PTR, VUI32M2, VF64M4))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, C_DF_PTR, VUI64M4, VF64M4))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, C_DF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, C_DF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, C_DF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, DF))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, DF, VF64M4))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, SF, VF32M2))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, VF32M2))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF32M2, SF))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF64M4, DF))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF64M4, SF))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF64M4, SIZE))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF64M4, UDI))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF64M4, VF32M2))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (4, (VF64M4, VB16, VF64M4, VF64M4, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, VI32M2))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, VI64M4))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, VUI32M2))
+DEF_RISCV_FTYPE (3, (VF64M4, VB16, VF64M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VF64M4, VF32M2))
+DEF_RISCV_FTYPE (2, (VF64M4, VF32M2, SF))
+DEF_RISCV_FTYPE (2, (VF64M4, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (1, (VF64M4, VF64M1))
+DEF_RISCV_FTYPE (1, (VF64M4, VF64M2))
+DEF_RISCV_FTYPE (1, (VF64M4, VF64M4))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4, DF))
+DEF_RISCV_FTYPE (3, (VF64M4, VF64M4, DF, VF64M4))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4, SF))
+DEF_RISCV_FTYPE (3, (VF64M4, VF64M4, SF, VF32M2))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4, SIZE))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4, UDI))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4, VF32M2))
+DEF_RISCV_FTYPE (3, (VF64M4, VF64M4, VF32M2, VF32M2))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (3, (VF64M4, VF64M4, VF64M4, SIZE))
+DEF_RISCV_FTYPE (3, (VF64M4, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF64M4, VF64M4X2, SI))
+DEF_RISCV_FTYPE (1, (VF64M4, VF64M8))
+DEF_RISCV_FTYPE (1, (VF64M4, VI32M2))
+DEF_RISCV_FTYPE (1, (VF64M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VF64M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VF64M4, VUI64M4))
+DEF_RISCV_FTYPE (0, (VF64M4X2))
+DEF_RISCV_FTYPE (1, (VF64M4X2, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M4X2, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VF64M4X2, C_DF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VF64M4X2, C_DF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VF64M4X2, C_DF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VF64M4X2, VB16, VF64M4X2, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M4X2, VB16, VF64M4X2, C_DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VF64M4X2, VB16, VF64M4X2, C_DF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VF64M4X2, VB16, VF64M4X2, C_DF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VF64M4X2, VB16, VF64M4X2, C_DF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VF64M4X2, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (3, (VF64M4X2, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (4, (VF64M4X2, VF64M4, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (5, (VF64M4X2, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (6, (VF64M4X2, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (7, (VF64M4X2, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (8, (VF64M4X2, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4, VF64M4))
+DEF_RISCV_FTYPE (3, (VF64M4X2, VF64M4X2, VF64M4, SI))
+DEF_RISCV_FTYPE (0, (VF64M8))
+DEF_RISCV_FTYPE (1, (VF64M8, C_DF_PTR))
+DEF_RISCV_FTYPE (2, (VF64M8, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (2, (VF64M8, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VF64M8, C_DF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VF64M8, C_DF_PTR, VUI16M2, VF64M8))
+DEF_RISCV_FTYPE (2, (VF64M8, C_DF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VF64M8, C_DF_PTR, VUI32M4, VF64M8))
+DEF_RISCV_FTYPE (2, (VF64M8, C_DF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VF64M8, C_DF_PTR, VUI64M8, VF64M8))
+DEF_RISCV_FTYPE (2, (VF64M8, C_DF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF64M8, C_DF_PTR, VUI8M1, VF64M8))
+DEF_RISCV_FTYPE (1, (VF64M8, DF))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, C_DF_PTR, VUI16M2, VF64M8))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, C_DF_PTR, VUI32M4, VF64M8))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, C_DF_PTR, VUI64M8, VF64M8))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, C_DF_PTR, VUI8M1, VF64M8))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, C_DF_PTR))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, C_DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, C_DF_PTR, SI))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, C_DF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, C_DF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, C_DF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, C_DF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, DF))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, DF, VF64M8))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, SF, VF32M4))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, VF32M4))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF32M4, SF))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, VF64M8))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF64M8, DF))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF64M8, SF))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF64M8, SIZE))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF64M8, UDI))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF64M8, VF32M4))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF64M8, VF64M8))
+DEF_RISCV_FTYPE (4, (VF64M8, VB8, VF64M8, VF64M8, VUI64M8))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, VI32M4))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, VI64M8))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, VUI32M4))
+DEF_RISCV_FTYPE (3, (VF64M8, VB8, VF64M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VF64M8, VF32M4))
+DEF_RISCV_FTYPE (2, (VF64M8, VF32M4, SF))
+DEF_RISCV_FTYPE (2, (VF64M8, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (1, (VF64M8, VF64M1))
+DEF_RISCV_FTYPE (1, (VF64M8, VF64M2))
+DEF_RISCV_FTYPE (1, (VF64M8, VF64M4))
+DEF_RISCV_FTYPE (1, (VF64M8, VF64M8))
+DEF_RISCV_FTYPE (2, (VF64M8, VF64M8, DF))
+DEF_RISCV_FTYPE (3, (VF64M8, VF64M8, DF, VF64M8))
+DEF_RISCV_FTYPE (2, (VF64M8, VF64M8, SF))
+DEF_RISCV_FTYPE (3, (VF64M8, VF64M8, SF, VF32M4))
+DEF_RISCV_FTYPE (2, (VF64M8, VF64M8, SIZE))
+DEF_RISCV_FTYPE (2, (VF64M8, VF64M8, UDI))
+DEF_RISCV_FTYPE (2, (VF64M8, VF64M8, VF32M4))
+DEF_RISCV_FTYPE (3, (VF64M8, VF64M8, VF32M4, VF32M4))
+DEF_RISCV_FTYPE (2, (VF64M8, VF64M8, VF64M8))
+DEF_RISCV_FTYPE (3, (VF64M8, VF64M8, VF64M8, SIZE))
+DEF_RISCV_FTYPE (3, (VF64M8, VF64M8, VF64M8, VF64M8))
+DEF_RISCV_FTYPE (2, (VF64M8, VF64M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VF64M8, VI32M4))
+DEF_RISCV_FTYPE (1, (VF64M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VF64M8, VUI32M4))
+DEF_RISCV_FTYPE (1, (VF64M8, VUI64M8))
+DEF_RISCV_FTYPE (0, (VI16M1))
+DEF_RISCV_FTYPE (1, (VI16M1, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI16M1, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI16M1, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1, C_HI_PTR, VUI16M1, VI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VI16M1, C_HI_PTR, VUI32M2, VI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1, C_HI_PTR, VUI64M4, VI16M1))
+DEF_RISCV_FTYPE (1, (VI16M1, HI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB1, VI16M1, VI16M1, VI8M8))
+DEF_RISCV_FTYPE (1, (VI16M1, VB16))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, C_HI_PTR, VUI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, C_HI_PTR, VUI32M2, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, C_HI_PTR, VUI64M4, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1, VB16, VI16M1, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1, VB16, VI16M1, HI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, HI, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1, VB16, VI16M1, VB16))
+DEF_RISCV_FTYPE (3, (VI16M1, VB16, VI16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VI16M1, VB16, VI16M1, VF32M2))
+DEF_RISCV_FTYPE (3, (VI16M1, VB16, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI16M1, HI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI16M1, LONG))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI16M1, SIZE))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI16M1, UHI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI16M1, UQI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI32M2, UQI))
+DEF_RISCV_FTYPE (4, (VI16M1, VB16, VI16M1, VI32M2, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1, VB2, VI16M1, VI16M1, VI16M8))
+DEF_RISCV_FTYPE (4, (VI16M1, VB2, VI16M1, VI16M1, VI8M4))
+DEF_RISCV_FTYPE (4, (VI16M1, VB4, VI16M1, VI16M1, VI16M4))
+DEF_RISCV_FTYPE (4, (VI16M1, VB4, VI16M1, VI16M1, VI8M2))
+DEF_RISCV_FTYPE (4, (VI16M1, VB8, VI16M1, VI16M1, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M1, VB8, VI16M1, VI16M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VF16M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VF32M2))
+DEF_RISCV_FTYPE (1, (VI16M1, VI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1, HI))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, HI, VB16))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, HI, VI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1, LONG))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1, SIZE))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1, UHI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1, UQI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, SIZE))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VB16))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI16M4))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI16M8))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI8M2))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI8M4))
+DEF_RISCV_FTYPE (3, (VI16M1, VI16M1, VI16M1, VI8M8))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1X2, SI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1X3, SI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1X4, SI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1X5, SI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1X6, SI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1X7, SI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI16M1X8, SI))
+DEF_RISCV_FTYPE (1, (VI16M1, VI16M2))
+DEF_RISCV_FTYPE (1, (VI16M1, VI16M4))
+DEF_RISCV_FTYPE (1, (VI16M1, VI16M8))
+DEF_RISCV_FTYPE (1, (VI16M1, VI32M1))
+DEF_RISCV_FTYPE (2, (VI16M1, VI32M2, UQI))
+DEF_RISCV_FTYPE (2, (VI16M1, VI32M2, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI16M1, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI16M1, VUI16M8))
+DEF_RISCV_FTYPE (1, (VI16M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VUI64M1))
+DEF_RISCV_FTYPE (1, (VI16M1, VUI8M1))
+DEF_RISCV_FTYPE (0, (VI16M1X2))
+DEF_RISCV_FTYPE (1, (VI16M1X2, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1X2, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M1X2, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1X2, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI16M1X2, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1X2, VB16, VI16M1X2, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1X2, VB16, VI16M1X2, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M1X2, VB16, VI16M1X2, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X2, VB16, VI16M1X2, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1X2, VB16, VI16M1X2, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI16M1X2, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X2, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X2, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (5, (VI16M1X2, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (6, (VI16M1X2, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (7, (VI16M1X2, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (8, (VI16M1X2, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X2, VI16M1X2, VI16M1, SI))
+DEF_RISCV_FTYPE (0, (VI16M1X3))
+DEF_RISCV_FTYPE (1, (VI16M1X3, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1X3, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M1X3, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1X3, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI16M1X3, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1X3, VB16, VI16M1X3, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1X3, VB16, VI16M1X3, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M1X3, VB16, VI16M1X3, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X3, VB16, VI16M1X3, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1X3, VB16, VI16M1X3, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI16M1X3, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X3, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X3, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (5, (VI16M1X3, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (6, (VI16M1X3, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (7, (VI16M1X3, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (8, (VI16M1X3, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X3, VI16M1X3, VI16M1, SI))
+DEF_RISCV_FTYPE (0, (VI16M1X4))
+DEF_RISCV_FTYPE (1, (VI16M1X4, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1X4, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M1X4, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1X4, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI16M1X4, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1X4, VB16, VI16M1X4, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1X4, VB16, VI16M1X4, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M1X4, VB16, VI16M1X4, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X4, VB16, VI16M1X4, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1X4, VB16, VI16M1X4, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI16M1X4, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X4, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X4, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (5, (VI16M1X4, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (6, (VI16M1X4, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (7, (VI16M1X4, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (8, (VI16M1X4, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X4, VI16M1X4, VI16M1, SI))
+DEF_RISCV_FTYPE (0, (VI16M1X5))
+DEF_RISCV_FTYPE (1, (VI16M1X5, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1X5, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M1X5, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1X5, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI16M1X5, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1X5, VB16, VI16M1X5, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1X5, VB16, VI16M1X5, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M1X5, VB16, VI16M1X5, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X5, VB16, VI16M1X5, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1X5, VB16, VI16M1X5, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI16M1X5, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X5, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X5, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (5, (VI16M1X5, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (6, (VI16M1X5, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (7, (VI16M1X5, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (8, (VI16M1X5, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X5, VI16M1X5, VI16M1, SI))
+DEF_RISCV_FTYPE (0, (VI16M1X6))
+DEF_RISCV_FTYPE (1, (VI16M1X6, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1X6, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M1X6, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1X6, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI16M1X6, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1X6, VB16, VI16M1X6, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1X6, VB16, VI16M1X6, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M1X6, VB16, VI16M1X6, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X6, VB16, VI16M1X6, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1X6, VB16, VI16M1X6, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI16M1X6, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X6, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X6, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (5, (VI16M1X6, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (6, (VI16M1X6, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (7, (VI16M1X6, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (8, (VI16M1X6, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X6, VI16M1X6, VI16M1, SI))
+DEF_RISCV_FTYPE (0, (VI16M1X7))
+DEF_RISCV_FTYPE (1, (VI16M1X7, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1X7, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M1X7, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1X7, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI16M1X7, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1X7, VB16, VI16M1X7, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1X7, VB16, VI16M1X7, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M1X7, VB16, VI16M1X7, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X7, VB16, VI16M1X7, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1X7, VB16, VI16M1X7, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI16M1X7, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X7, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X7, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (5, (VI16M1X7, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (6, (VI16M1X7, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (7, (VI16M1X7, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (8, (VI16M1X7, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X7, VI16M1X7, VI16M1, SI))
+DEF_RISCV_FTYPE (0, (VI16M1X8))
+DEF_RISCV_FTYPE (1, (VI16M1X8, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M1X8, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M1X8, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI16M1X8, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI16M1X8, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI16M1X8, VB16, VI16M1X8, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M1X8, VB16, VI16M1X8, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M1X8, VB16, VI16M1X8, C_HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X8, VB16, VI16M1X8, C_HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI16M1X8, VB16, VI16M1X8, C_HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI16M1X8, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X8, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI16M1X8, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (5, (VI16M1X8, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (6, (VI16M1X8, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (7, (VI16M1X8, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (8, (VI16M1X8, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI16M1X8, VI16M1X8, VI16M1, SI))
+DEF_RISCV_FTYPE (0, (VI16M2))
+DEF_RISCV_FTYPE (1, (VI16M2, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M2, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI16M2, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI16M2, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2, C_HI_PTR, VUI16M2, VI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VI16M2, C_HI_PTR, VUI32M4, VI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VI16M2, C_HI_PTR, VUI64M8, VI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2, C_HI_PTR, VUI8M1, VI16M2))
+DEF_RISCV_FTYPE (1, (VI16M2, HI))
+DEF_RISCV_FTYPE (1, (VI16M2, VB8))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, C_HI_PTR, VUI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, C_HI_PTR, VUI32M4, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, C_HI_PTR, VUI64M8, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, C_HI_PTR, VUI8M1, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2, VB8, VI16M2, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2, VB8, VI16M2, HI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, HI, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, QI, VI8M1))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, QI, VUI8M1))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, UQI, VI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2, VB8, VI16M2, VB8))
+DEF_RISCV_FTYPE (3, (VI16M2, VB8, VI16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VI16M2, VB8, VI16M2, VF32M4))
+DEF_RISCV_FTYPE (3, (VI16M2, VB8, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, HI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, LONG))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, QI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, SIZE))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, UHI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, UQI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, VI8M1))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI32M4, UQI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI32M4, VUI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2, VB8, VI16M2, VI8M1))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI8M1, QI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI16M2, VB8, VI16M2, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI16M2, VF16M2))
+DEF_RISCV_FTYPE (1, (VI16M2, VF32M4))
+DEF_RISCV_FTYPE (1, (VI16M2, VI16M1))
+DEF_RISCV_FTYPE (1, (VI16M2, VI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, HI))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, HI, VB8))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, HI, VI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, LONG))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, QI))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, QI, VI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, QI, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, SIZE))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, UHI))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, UQI))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, UQI, VI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, VI16M2, SIZE))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, VI16M2, VB8))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, VI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2, VI16M2, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2X2, SI))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2X3, SI))
+DEF_RISCV_FTYPE (2, (VI16M2, VI16M2X4, SI))
+DEF_RISCV_FTYPE (1, (VI16M2, VI16M4))
+DEF_RISCV_FTYPE (1, (VI16M2, VI16M8))
+DEF_RISCV_FTYPE (1, (VI16M2, VI32M2))
+DEF_RISCV_FTYPE (2, (VI16M2, VI32M4, UQI))
+DEF_RISCV_FTYPE (2, (VI16M2, VI32M4, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI16M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VI16M2, VI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2, VI8M1, QI))
+DEF_RISCV_FTYPE (2, (VI16M2, VI8M1, UQI))
+DEF_RISCV_FTYPE (2, (VI16M2, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI16M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VI16M2, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI16M2, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI16M2, VUI16M8))
+DEF_RISCV_FTYPE (1, (VI16M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI16M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI16M2, VUI8M2))
+DEF_RISCV_FTYPE (0, (VI16M2X2))
+DEF_RISCV_FTYPE (1, (VI16M2X2, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M2X2, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M2X2, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2X2, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI16M2X2, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI16M2X2, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2X2, VB8, VI16M2X2, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M2X2, VB8, VI16M2X2, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M2X2, VB8, VI16M2X2, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2X2, VB8, VI16M2X2, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI16M2X2, VB8, VI16M2X2, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI16M2X2, VB8, VI16M2X2, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2X2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2X2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2X2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (5, (VI16M2X2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (6, (VI16M2X2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (7, (VI16M2X2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (8, (VI16M2X2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2X2, VI16M2X2, VI16M2, SI))
+DEF_RISCV_FTYPE (0, (VI16M2X3))
+DEF_RISCV_FTYPE (1, (VI16M2X3, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M2X3, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M2X3, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2X3, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI16M2X3, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI16M2X3, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2X3, VB8, VI16M2X3, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M2X3, VB8, VI16M2X3, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M2X3, VB8, VI16M2X3, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2X3, VB8, VI16M2X3, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI16M2X3, VB8, VI16M2X3, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI16M2X3, VB8, VI16M2X3, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2X3, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2X3, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2X3, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (5, (VI16M2X3, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (6, (VI16M2X3, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (7, (VI16M2X3, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (8, (VI16M2X3, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2X3, VI16M2X3, VI16M2, SI))
+DEF_RISCV_FTYPE (0, (VI16M2X4))
+DEF_RISCV_FTYPE (1, (VI16M2X4, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M2X4, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M2X4, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI16M2X4, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI16M2X4, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI16M2X4, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI16M2X4, VB8, VI16M2X4, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M2X4, VB8, VI16M2X4, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M2X4, VB8, VI16M2X4, C_HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2X4, VB8, VI16M2X4, C_HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI16M2X4, VB8, VI16M2X4, C_HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI16M2X4, VB8, VI16M2X4, C_HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI16M2X4, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2X4, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI16M2X4, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (5, (VI16M2X4, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (6, (VI16M2X4, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (7, (VI16M2X4, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (8, (VI16M2X4, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI16M2X4, VI16M2X4, VI16M2, SI))
+DEF_RISCV_FTYPE (0, (VI16M4))
+DEF_RISCV_FTYPE (1, (VI16M4, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M4, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI16M4, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI16M4, C_HI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VI16M4, C_HI_PTR, VUI16M4, VI16M4))
+DEF_RISCV_FTYPE (2, (VI16M4, C_HI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VI16M4, C_HI_PTR, VUI32M8, VI16M4))
+DEF_RISCV_FTYPE (2, (VI16M4, C_HI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI16M4, C_HI_PTR, VUI8M2, VI16M4))
+DEF_RISCV_FTYPE (1, (VI16M4, HI))
+DEF_RISCV_FTYPE (1, (VI16M4, VB4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, C_HI_PTR, VUI16M4, VI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, C_HI_PTR, VUI32M8, VI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, C_HI_PTR, VUI8M2, VI16M4))
+DEF_RISCV_FTYPE (3, (VI16M4, VB4, VI16M4, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, C_HI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, C_HI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, C_HI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI16M4, VB4, VI16M4, HI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, HI, VI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, QI, VI8M2))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, QI, VUI8M2))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, UQI, VI8M2))
+DEF_RISCV_FTYPE (3, (VI16M4, VB4, VI16M4, VB4))
+DEF_RISCV_FTYPE (3, (VI16M4, VB4, VI16M4, VF16M4))
+DEF_RISCV_FTYPE (3, (VI16M4, VB4, VI16M4, VF32M8))
+DEF_RISCV_FTYPE (3, (VI16M4, VB4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, HI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, LONG))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, QI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, SIZE))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, UHI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, UQI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, VI8M2))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI32M8, UQI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI32M8, VUI16M4))
+DEF_RISCV_FTYPE (3, (VI16M4, VB4, VI16M4, VI8M2))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI8M2, QI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI8M2, UQI))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VI16M4, VB4, VI16M4, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI16M4, VF16M4))
+DEF_RISCV_FTYPE (1, (VI16M4, VF32M8))
+DEF_RISCV_FTYPE (1, (VI16M4, VI16M1))
+DEF_RISCV_FTYPE (1, (VI16M4, VI16M2))
+DEF_RISCV_FTYPE (1, (VI16M4, VI16M4))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, HI))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, HI, VB4))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, HI, VI16M4))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, LONG))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, QI))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, QI, VI8M2))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, QI, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, SIZE))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, UHI))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, UQI))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, UQI, VI8M2))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, VI16M4, SIZE))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, VI16M4, VB4))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, VI8M2))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI16M4, VI16M4, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI16M4, VI16M4X2, SI))
+DEF_RISCV_FTYPE (1, (VI16M4, VI16M8))
+DEF_RISCV_FTYPE (1, (VI16M4, VI32M4))
+DEF_RISCV_FTYPE (2, (VI16M4, VI32M8, UQI))
+DEF_RISCV_FTYPE (2, (VI16M4, VI32M8, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI16M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VI16M4, VI8M2))
+DEF_RISCV_FTYPE (2, (VI16M4, VI8M2, QI))
+DEF_RISCV_FTYPE (2, (VI16M4, VI8M2, UQI))
+DEF_RISCV_FTYPE (2, (VI16M4, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (2, (VI16M4, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI16M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VI16M4, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI16M4, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI16M4, VUI16M8))
+DEF_RISCV_FTYPE (1, (VI16M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI16M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI16M4, VUI8M4))
+DEF_RISCV_FTYPE (0, (VI16M4X2))
+DEF_RISCV_FTYPE (1, (VI16M4X2, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M4X2, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI16M4X2, C_HI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI16M4X2, C_HI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VI16M4X2, C_HI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI16M4X2, VB4, VI16M4X2, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M4X2, VB4, VI16M4X2, C_HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI16M4X2, VB4, VI16M4X2, C_HI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4X2, VB4, VI16M4X2, C_HI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VI16M4X2, VB4, VI16M4X2, C_HI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI16M4X2, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (3, (VI16M4X2, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (4, (VI16M4X2, VI16M4, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (5, (VI16M4X2, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (6, (VI16M4X2, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (7, (VI16M4X2, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (8, (VI16M4X2, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (3, (VI16M4X2, VI16M4X2, VI16M4, SI))
+DEF_RISCV_FTYPE (0, (VI16M8))
+DEF_RISCV_FTYPE (1, (VI16M8, C_HI_PTR))
+DEF_RISCV_FTYPE (2, (VI16M8, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI16M8, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI16M8, C_HI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (3, (VI16M8, C_HI_PTR, VUI16M8, VI16M8))
+DEF_RISCV_FTYPE (2, (VI16M8, C_HI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VI16M8, C_HI_PTR, VUI8M4, VI16M8))
+DEF_RISCV_FTYPE (1, (VI16M8, HI))
+DEF_RISCV_FTYPE (1, (VI16M8, VB2))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, C_HI_PTR, VUI16M8, VI16M8))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, C_HI_PTR, VUI8M4, VI16M8))
+DEF_RISCV_FTYPE (3, (VI16M8, VB2, VI16M8, C_HI_PTR))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, C_HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, C_HI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, C_HI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, C_HI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VI16M8, VB2, VI16M8, HI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, HI, VI16M8))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, QI, VI8M4))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, QI, VUI8M4))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, UQI, VI8M4))
+DEF_RISCV_FTYPE (3, (VI16M8, VB2, VI16M8, VB2))
+DEF_RISCV_FTYPE (3, (VI16M8, VB2, VI16M8, VF16M8))
+DEF_RISCV_FTYPE (3, (VI16M8, VB2, VI16M8, VI16M8))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, HI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, LONG))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, QI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, SIZE))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, UHI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, UQI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, VI16M8))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, VI8M4))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI16M8, VUI16M8))
+DEF_RISCV_FTYPE (3, (VI16M8, VB2, VI16M8, VI8M4))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI8M4, QI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI8M4, UQI))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (4, (VI16M8, VB2, VI16M8, VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI16M8, VF16M8))
+DEF_RISCV_FTYPE (1, (VI16M8, VI16M1))
+DEF_RISCV_FTYPE (1, (VI16M8, VI16M2))
+DEF_RISCV_FTYPE (1, (VI16M8, VI16M4))
+DEF_RISCV_FTYPE (1, (VI16M8, VI16M8))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, HI))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, HI, VB2))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, HI, VI16M8))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, LONG))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, QI))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, QI, VI8M4))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, QI, VUI8M4))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, SIZE))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, UHI))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, UQI))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, UQI, VI8M4))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, VI16M8))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, VI16M8, SIZE))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, VI16M8, VB2))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, VI16M8, VI16M8))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, VI8M4))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (3, (VI16M8, VI16M8, VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, VUI16M8))
+DEF_RISCV_FTYPE (2, (VI16M8, VI16M8, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI16M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VI16M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VI16M8, VI8M4))
+DEF_RISCV_FTYPE (2, (VI16M8, VI8M4, QI))
+DEF_RISCV_FTYPE (2, (VI16M8, VI8M4, UQI))
+DEF_RISCV_FTYPE (2, (VI16M8, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (2, (VI16M8, VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI16M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VI16M8, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI16M8, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI16M8, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI16M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VI16M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VI16M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI16M8, VUI8M8))
+DEF_RISCV_FTYPE (0, (VI32M1))
+DEF_RISCV_FTYPE (1, (VI32M1, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI32M1, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI32M1, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1, C_SI_PTR, VUI32M1, VI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1, C_SI_PTR, VUI64M2, VI32M1))
+DEF_RISCV_FTYPE (1, (VI32M1, SI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB16, VI32M1, VI32M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI32M1, VB16, VI32M1, VI32M1, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M1, VB2, VI32M1, VI32M1, VI16M8))
+DEF_RISCV_FTYPE (1, (VI32M1, VB32))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, C_SI_PTR, VUI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, C_SI_PTR, VUI64M2, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1, VB32, VI32M1, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1, VB32, VI32M1, SI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, SI, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1, VB32, VI32M1, VB32))
+DEF_RISCV_FTYPE (3, (VI32M1, VB32, VI32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VI32M1, VB32, VI32M1, VF64M2))
+DEF_RISCV_FTYPE (3, (VI32M1, VB32, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI32M1, LONG))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI32M1, SI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI32M1, SIZE))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI32M1, UQI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI32M1, USI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI64M2, UQI))
+DEF_RISCV_FTYPE (4, (VI32M1, VB32, VI32M1, VI64M2, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1, VB4, VI32M1, VI32M1, VI16M4))
+DEF_RISCV_FTYPE (4, (VI32M1, VB4, VI32M1, VI32M1, VI32M8))
+DEF_RISCV_FTYPE (4, (VI32M1, VB8, VI32M1, VI32M1, VI16M2))
+DEF_RISCV_FTYPE (4, (VI32M1, VB8, VI32M1, VI32M1, VI32M4))
+DEF_RISCV_FTYPE (1, (VI32M1, VF32M1))
+DEF_RISCV_FTYPE (1, (VI32M1, VF64M2))
+DEF_RISCV_FTYPE (1, (VI32M1, VI16M1))
+DEF_RISCV_FTYPE (1, (VI32M1, VI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1, LONG))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1, SI))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, SI, VB32))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, SI, VI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1, SIZE))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1, UQI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1, USI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, SIZE))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VB32))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI16M2))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI16M4))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI16M8))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI32M4))
+DEF_RISCV_FTYPE (3, (VI32M1, VI32M1, VI32M1, VI32M8))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1X2, SI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1X3, SI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1X4, SI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1X5, SI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1X6, SI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1X7, SI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI32M1X8, SI))
+DEF_RISCV_FTYPE (1, (VI32M1, VI32M2))
+DEF_RISCV_FTYPE (1, (VI32M1, VI32M4))
+DEF_RISCV_FTYPE (1, (VI32M1, VI32M8))
+DEF_RISCV_FTYPE (1, (VI32M1, VI64M1))
+DEF_RISCV_FTYPE (2, (VI32M1, VI64M2, UQI))
+DEF_RISCV_FTYPE (2, (VI32M1, VI64M2, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI32M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VI32M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI32M1, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI32M1, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI32M1, VUI32M8))
+DEF_RISCV_FTYPE (1, (VI32M1, VUI64M1))
+DEF_RISCV_FTYPE (1, (VI32M1, VUI8M1))
+DEF_RISCV_FTYPE (0, (VI32M1X2))
+DEF_RISCV_FTYPE (1, (VI32M1X2, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1X2, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M1X2, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1X2, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1X2, VB32, VI32M1X2, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1X2, VB32, VI32M1X2, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M1X2, VB32, VI32M1X2, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X2, VB32, VI32M1X2, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI32M1X2, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X2, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X2, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (5, (VI32M1X2, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (6, (VI32M1X2, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (7, (VI32M1X2, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (8, (VI32M1X2, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X2, VI32M1X2, VI32M1, SI))
+DEF_RISCV_FTYPE (0, (VI32M1X3))
+DEF_RISCV_FTYPE (1, (VI32M1X3, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1X3, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M1X3, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1X3, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1X3, VB32, VI32M1X3, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1X3, VB32, VI32M1X3, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M1X3, VB32, VI32M1X3, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X3, VB32, VI32M1X3, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI32M1X3, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X3, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X3, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (5, (VI32M1X3, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (6, (VI32M1X3, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (7, (VI32M1X3, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (8, (VI32M1X3, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X3, VI32M1X3, VI32M1, SI))
+DEF_RISCV_FTYPE (0, (VI32M1X4))
+DEF_RISCV_FTYPE (1, (VI32M1X4, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1X4, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M1X4, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1X4, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1X4, VB32, VI32M1X4, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1X4, VB32, VI32M1X4, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M1X4, VB32, VI32M1X4, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X4, VB32, VI32M1X4, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI32M1X4, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X4, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X4, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (5, (VI32M1X4, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (6, (VI32M1X4, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (7, (VI32M1X4, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (8, (VI32M1X4, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X4, VI32M1X4, VI32M1, SI))
+DEF_RISCV_FTYPE (0, (VI32M1X5))
+DEF_RISCV_FTYPE (1, (VI32M1X5, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1X5, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M1X5, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1X5, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1X5, VB32, VI32M1X5, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1X5, VB32, VI32M1X5, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M1X5, VB32, VI32M1X5, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X5, VB32, VI32M1X5, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI32M1X5, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X5, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X5, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (5, (VI32M1X5, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (6, (VI32M1X5, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (7, (VI32M1X5, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (8, (VI32M1X5, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X5, VI32M1X5, VI32M1, SI))
+DEF_RISCV_FTYPE (0, (VI32M1X6))
+DEF_RISCV_FTYPE (1, (VI32M1X6, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1X6, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M1X6, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1X6, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1X6, VB32, VI32M1X6, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1X6, VB32, VI32M1X6, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M1X6, VB32, VI32M1X6, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X6, VB32, VI32M1X6, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI32M1X6, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X6, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X6, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (5, (VI32M1X6, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (6, (VI32M1X6, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (7, (VI32M1X6, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (8, (VI32M1X6, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X6, VI32M1X6, VI32M1, SI))
+DEF_RISCV_FTYPE (0, (VI32M1X7))
+DEF_RISCV_FTYPE (1, (VI32M1X7, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1X7, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M1X7, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1X7, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1X7, VB32, VI32M1X7, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1X7, VB32, VI32M1X7, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M1X7, VB32, VI32M1X7, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X7, VB32, VI32M1X7, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI32M1X7, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X7, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X7, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (5, (VI32M1X7, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (6, (VI32M1X7, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (7, (VI32M1X7, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (8, (VI32M1X7, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X7, VI32M1X7, VI32M1, SI))
+DEF_RISCV_FTYPE (0, (VI32M1X8))
+DEF_RISCV_FTYPE (1, (VI32M1X8, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M1X8, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M1X8, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI32M1X8, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI32M1X8, VB32, VI32M1X8, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M1X8, VB32, VI32M1X8, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M1X8, VB32, VI32M1X8, C_SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X8, VB32, VI32M1X8, C_SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI32M1X8, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X8, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI32M1X8, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (5, (VI32M1X8, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (6, (VI32M1X8, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (7, (VI32M1X8, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (8, (VI32M1X8, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI32M1X8, VI32M1X8, VI32M1, SI))
+DEF_RISCV_FTYPE (0, (VI32M2))
+DEF_RISCV_FTYPE (1, (VI32M2, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M2, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI32M2, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI32M2, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, C_SI_PTR, VUI16M1, VI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2, C_SI_PTR, VUI32M2, VI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI32M2, C_SI_PTR, VUI64M4, VI32M2))
+DEF_RISCV_FTYPE (1, (VI32M2, SI))
+DEF_RISCV_FTYPE (1, (VI32M2, VB16))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, C_SI_PTR, VUI16M1, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, C_SI_PTR, VUI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, C_SI_PTR, VUI64M4, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, HI, VI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, HI, VUI16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, SI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, SI, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, UHI, VI16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, VB16))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, VF16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, VF64M4))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, VI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI16M1, HI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI16M1, UHI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, VB16, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, HI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, LONG))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, SI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, SIZE))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, UQI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, USI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, VI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI64M4, UQI))
+DEF_RISCV_FTYPE (4, (VI32M2, VB16, VI32M2, VI64M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VF16M1))
+DEF_RISCV_FTYPE (1, (VI32M2, VF32M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VF64M4))
+DEF_RISCV_FTYPE (1, (VI32M2, VI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2, VI16M1, HI))
+DEF_RISCV_FTYPE (2, (VI32M2, VI16M1, UHI))
+DEF_RISCV_FTYPE (2, (VI32M2, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI32M2, VI16M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VI32M1))
+DEF_RISCV_FTYPE (1, (VI32M2, VI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, HI))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, HI, VI16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, HI, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, LONG))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, SI))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, SI, VB16))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, SI, VI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, SIZE))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, UHI))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, UHI, VI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, UQI))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, USI))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, VI16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, VI32M2, SIZE))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, VI32M2, VB16))
+DEF_RISCV_FTYPE (3, (VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2X2, SI))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2X3, SI))
+DEF_RISCV_FTYPE (2, (VI32M2, VI32M2X4, SI))
+DEF_RISCV_FTYPE (1, (VI32M2, VI32M4))
+DEF_RISCV_FTYPE (1, (VI32M2, VI32M8))
+DEF_RISCV_FTYPE (1, (VI32M2, VI64M2))
+DEF_RISCV_FTYPE (2, (VI32M2, VI64M4, UQI))
+DEF_RISCV_FTYPE (2, (VI32M2, VI64M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI32M2, VUI32M8))
+DEF_RISCV_FTYPE (1, (VI32M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI32M2, VUI8M2))
+DEF_RISCV_FTYPE (0, (VI32M2X2))
+DEF_RISCV_FTYPE (1, (VI32M2X2, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M2X2, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M2X2, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2X2, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2X2, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI32M2X2, VB16, VI32M2X2, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M2X2, VB16, VI32M2X2, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M2X2, VB16, VI32M2X2, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2X2, VB16, VI32M2X2, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2X2, VB16, VI32M2X2, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI32M2X2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2X2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2X2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (5, (VI32M2X2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (6, (VI32M2X2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (7, (VI32M2X2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (8, (VI32M2X2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2X2, VI32M2X2, VI32M2, SI))
+DEF_RISCV_FTYPE (0, (VI32M2X3))
+DEF_RISCV_FTYPE (1, (VI32M2X3, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M2X3, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M2X3, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2X3, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2X3, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI32M2X3, VB16, VI32M2X3, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M2X3, VB16, VI32M2X3, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M2X3, VB16, VI32M2X3, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2X3, VB16, VI32M2X3, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2X3, VB16, VI32M2X3, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI32M2X3, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2X3, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2X3, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (5, (VI32M2X3, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (6, (VI32M2X3, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (7, (VI32M2X3, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (8, (VI32M2X3, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2X3, VI32M2X3, VI32M2, SI))
+DEF_RISCV_FTYPE (0, (VI32M2X4))
+DEF_RISCV_FTYPE (1, (VI32M2X4, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M2X4, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M2X4, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI32M2X4, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI32M2X4, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI32M2X4, VB16, VI32M2X4, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M2X4, VB16, VI32M2X4, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M2X4, VB16, VI32M2X4, C_SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI32M2X4, VB16, VI32M2X4, C_SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2X4, VB16, VI32M2X4, C_SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI32M2X4, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2X4, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VI32M2X4, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (5, (VI32M2X4, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (6, (VI32M2X4, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (7, (VI32M2X4, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (8, (VI32M2X4, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI32M2X4, VI32M2X4, VI32M2, SI))
+DEF_RISCV_FTYPE (0, (VI32M4))
+DEF_RISCV_FTYPE (1, (VI32M4, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M4, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI32M4, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI32M4, C_SI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VI32M4, C_SI_PTR, VUI16M2, VI32M4))
+DEF_RISCV_FTYPE (2, (VI32M4, C_SI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VI32M4, C_SI_PTR, VUI32M4, VI32M4))
+DEF_RISCV_FTYPE (2, (VI32M4, C_SI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VI32M4, C_SI_PTR, VUI64M8, VI32M4))
+DEF_RISCV_FTYPE (2, (VI32M4, C_SI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI32M4, C_SI_PTR, VUI8M1, VI32M4))
+DEF_RISCV_FTYPE (1, (VI32M4, SI))
+DEF_RISCV_FTYPE (1, (VI32M4, VB8))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, C_SI_PTR, VUI16M2, VI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, C_SI_PTR, VUI32M4, VI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, C_SI_PTR, VUI64M8, VI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, C_SI_PTR, VUI8M1, VI32M4))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, C_SI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, C_SI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, C_SI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, C_SI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, HI, VI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, HI, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, QI, VI8M1))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, QI, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, SI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, SI, VI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, UHI, VI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, UQI, VI8M1))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, VB8))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, VF16M2))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, VF32M4))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, VF64M8))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, VI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI16M2, HI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI16M2, UHI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, HI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, LONG))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, SI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, SIZE))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, UQI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, USI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, VI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI64M8, UQI))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI64M8, VUI32M4))
+DEF_RISCV_FTYPE (3, (VI32M4, VB8, VI32M4, VI8M1))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI32M4, VB8, VI32M4, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI32M4, VF16M2))
+DEF_RISCV_FTYPE (1, (VI32M4, VF32M4))
+DEF_RISCV_FTYPE (1, (VI32M4, VF64M8))
+DEF_RISCV_FTYPE (1, (VI32M4, VI16M2))
+DEF_RISCV_FTYPE (2, (VI32M4, VI16M2, HI))
+DEF_RISCV_FTYPE (2, (VI32M4, VI16M2, UHI))
+DEF_RISCV_FTYPE (2, (VI32M4, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (2, (VI32M4, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI32M4, VI16M4))
+DEF_RISCV_FTYPE (1, (VI32M4, VI32M1))
+DEF_RISCV_FTYPE (1, (VI32M4, VI32M2))
+DEF_RISCV_FTYPE (1, (VI32M4, VI32M4))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, HI))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, HI, VI16M2))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, HI, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, LONG))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, QI, VI8M1))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, QI, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, SI))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, SI, VB8))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, SI, VI32M4))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, SIZE))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, UHI))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, UHI, VI16M2))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, UQI))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, UQI, VI8M1))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, USI))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, VI16M2))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, VI32M4, SIZE))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, VI32M4, VB8))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI32M4, VI32M4, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI32M4, VI32M4X2, SI))
+DEF_RISCV_FTYPE (1, (VI32M4, VI32M8))
+DEF_RISCV_FTYPE (1, (VI32M4, VI64M4))
+DEF_RISCV_FTYPE (2, (VI32M4, VI64M8, UQI))
+DEF_RISCV_FTYPE (2, (VI32M4, VI64M8, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI32M4, VI8M1))
+DEF_RISCV_FTYPE (2, (VI32M4, VI8M1, UQI))
+DEF_RISCV_FTYPE (2, (VI32M4, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI32M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VI32M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI32M4, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI32M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI32M4, VUI32M8))
+DEF_RISCV_FTYPE (1, (VI32M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI32M4, VUI8M4))
+DEF_RISCV_FTYPE (0, (VI32M4X2))
+DEF_RISCV_FTYPE (1, (VI32M4X2, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M4X2, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI32M4X2, C_SI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI32M4X2, C_SI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI32M4X2, C_SI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI32M4X2, C_SI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI32M4X2, VB8, VI32M4X2, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M4X2, VB8, VI32M4X2, C_SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI32M4X2, VB8, VI32M4X2, C_SI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI32M4X2, VB8, VI32M4X2, C_SI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4X2, VB8, VI32M4X2, C_SI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI32M4X2, VB8, VI32M4X2, C_SI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI32M4X2, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VI32M4X2, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (4, (VI32M4X2, VI32M4, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (5, (VI32M4X2, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (6, (VI32M4X2, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (7, (VI32M4X2, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (8, (VI32M4X2, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VI32M4X2, VI32M4X2, VI32M4, SI))
+DEF_RISCV_FTYPE (0, (VI32M8))
+DEF_RISCV_FTYPE (1, (VI32M8, C_SI_PTR))
+DEF_RISCV_FTYPE (2, (VI32M8, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI32M8, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI32M8, C_SI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VI32M8, C_SI_PTR, VUI16M4, VI32M8))
+DEF_RISCV_FTYPE (2, (VI32M8, C_SI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VI32M8, C_SI_PTR, VUI32M8, VI32M8))
+DEF_RISCV_FTYPE (2, (VI32M8, C_SI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI32M8, C_SI_PTR, VUI8M2, VI32M8))
+DEF_RISCV_FTYPE (1, (VI32M8, SI))
+DEF_RISCV_FTYPE (1, (VI32M8, VB4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, C_SI_PTR, VUI16M4, VI32M8))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, C_SI_PTR, VUI32M8, VI32M8))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, C_SI_PTR, VUI8M2, VI32M8))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, C_SI_PTR))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, C_SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, C_SI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, C_SI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, C_SI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, C_SI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, HI, VI16M4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, HI, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, QI, VI8M2))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, QI, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, SI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, SI, VI32M8))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, UHI, VI16M4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, UQI, VI8M2))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, VB4))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, VF16M4))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, VF32M8))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, VI16M4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI16M4, HI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI16M4, UHI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, VI32M8))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, HI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, LONG))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, SI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, SIZE))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, UQI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, USI))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, VI16M4))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, VI32M8))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI32M8, VUI32M8))
+DEF_RISCV_FTYPE (3, (VI32M8, VB4, VI32M8, VI8M2))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VI32M8, VB4, VI32M8, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI32M8, VF16M4))
+DEF_RISCV_FTYPE (1, (VI32M8, VF32M8))
+DEF_RISCV_FTYPE (1, (VI32M8, VI16M4))
+DEF_RISCV_FTYPE (2, (VI32M8, VI16M4, HI))
+DEF_RISCV_FTYPE (2, (VI32M8, VI16M4, UHI))
+DEF_RISCV_FTYPE (2, (VI32M8, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (2, (VI32M8, VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI32M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VI32M8, VI32M1))
+DEF_RISCV_FTYPE (1, (VI32M8, VI32M2))
+DEF_RISCV_FTYPE (1, (VI32M8, VI32M4))
+DEF_RISCV_FTYPE (1, (VI32M8, VI32M8))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, HI))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, HI, VI16M4))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, HI, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, LONG))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, QI, VI8M2))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, QI, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, SI))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, SI, VB4))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, SI, VI32M8))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, SIZE))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, UHI))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, UHI, VI16M4))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, UQI))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, UQI, VI8M2))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, USI))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, VI16M4))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, VI16M4, VI16M4))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, VI16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, VI32M8))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, VI32M8, SIZE))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, VI32M8, VB4))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, VI32M8, VI32M8))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI32M8, VI32M8, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, VUI32M8))
+DEF_RISCV_FTYPE (2, (VI32M8, VI32M8, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI32M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VI32M8, VI8M2))
+DEF_RISCV_FTYPE (2, (VI32M8, VI8M2, UQI))
+DEF_RISCV_FTYPE (2, (VI32M8, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI32M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VI32M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VI32M8, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI32M8, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI32M8, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI32M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VI32M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI32M8, VUI8M8))
+DEF_RISCV_FTYPE (0, (VI64M1))
+DEF_RISCV_FTYPE (1, (VI64M1, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI64M1, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI64M1, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1, C_DI_PTR, VUI64M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VI64M1, DI))
+DEF_RISCV_FTYPE (4, (VI64M1, VB16, VI64M1, VI64M1, VI32M2))
+DEF_RISCV_FTYPE (4, (VI64M1, VB16, VI64M1, VI64M1, VI64M4))
+DEF_RISCV_FTYPE (4, (VI64M1, VB32, VI64M1, VI64M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI64M1, VB32, VI64M1, VI64M1, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M1, VB4, VI64M1, VI64M1, VI32M8))
+DEF_RISCV_FTYPE (1, (VI64M1, VB64))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, C_DI_PTR, VUI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1, VB64, VI64M1, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1, VB64, VI64M1, DI))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, DI, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1, VB64, VI64M1, VB64))
+DEF_RISCV_FTYPE (3, (VI64M1, VB64, VI64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VI64M1, VB64, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, VI64M1, DI))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, VI64M1, LONG))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, VI64M1, SIZE))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, VI64M1, UDI))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, VI64M1, UQI))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1, VB64, VI64M1, VI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1, VB8, VI64M1, VI64M1, VI32M4))
+DEF_RISCV_FTYPE (4, (VI64M1, VB8, VI64M1, VI64M1, VI64M8))
+DEF_RISCV_FTYPE (1, (VI64M1, VF64M1))
+DEF_RISCV_FTYPE (1, (VI64M1, VI16M1))
+DEF_RISCV_FTYPE (1, (VI64M1, VI32M1))
+DEF_RISCV_FTYPE (1, (VI64M1, VI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1, DI))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, DI, VB64))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, DI, VI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1, LONG))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1, SIZE))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1, UDI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1, UQI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, SIZE))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VB64))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI32M2))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI32M4))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI32M8))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI64M4))
+DEF_RISCV_FTYPE (3, (VI64M1, VI64M1, VI64M1, VI64M8))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1X2, SI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1X3, SI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1X4, SI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1X5, SI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1X6, SI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1X7, SI))
+DEF_RISCV_FTYPE (2, (VI64M1, VI64M1X8, SI))
+DEF_RISCV_FTYPE (1, (VI64M1, VI64M2))
+DEF_RISCV_FTYPE (1, (VI64M1, VI64M4))
+DEF_RISCV_FTYPE (1, (VI64M1, VI64M8))
+DEF_RISCV_FTYPE (1, (VI64M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VI64M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI64M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI64M1, VUI64M1))
+DEF_RISCV_FTYPE (1, (VI64M1, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI64M1, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI64M1, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI64M1, VUI8M1))
+DEF_RISCV_FTYPE (0, (VI64M1X2))
+DEF_RISCV_FTYPE (1, (VI64M1X2, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1X2, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M1X2, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X2, VB64, VI64M1X2, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1X2, VB64, VI64M1X2, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M1X2, VB64, VI64M1X2, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1X2, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X2, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1X2, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (5, (VI64M1X2, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (6, (VI64M1X2, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (7, (VI64M1X2, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (8, (VI64M1X2, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X2, VI64M1X2, VI64M1, SI))
+DEF_RISCV_FTYPE (0, (VI64M1X3))
+DEF_RISCV_FTYPE (1, (VI64M1X3, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1X3, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M1X3, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X3, VB64, VI64M1X3, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1X3, VB64, VI64M1X3, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M1X3, VB64, VI64M1X3, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1X3, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X3, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1X3, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (5, (VI64M1X3, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (6, (VI64M1X3, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (7, (VI64M1X3, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (8, (VI64M1X3, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X3, VI64M1X3, VI64M1, SI))
+DEF_RISCV_FTYPE (0, (VI64M1X4))
+DEF_RISCV_FTYPE (1, (VI64M1X4, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1X4, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M1X4, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X4, VB64, VI64M1X4, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1X4, VB64, VI64M1X4, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M1X4, VB64, VI64M1X4, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1X4, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X4, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1X4, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (5, (VI64M1X4, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (6, (VI64M1X4, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (7, (VI64M1X4, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (8, (VI64M1X4, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X4, VI64M1X4, VI64M1, SI))
+DEF_RISCV_FTYPE (0, (VI64M1X5))
+DEF_RISCV_FTYPE (1, (VI64M1X5, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1X5, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M1X5, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X5, VB64, VI64M1X5, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1X5, VB64, VI64M1X5, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M1X5, VB64, VI64M1X5, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1X5, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X5, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1X5, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (5, (VI64M1X5, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (6, (VI64M1X5, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (7, (VI64M1X5, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (8, (VI64M1X5, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X5, VI64M1X5, VI64M1, SI))
+DEF_RISCV_FTYPE (0, (VI64M1X6))
+DEF_RISCV_FTYPE (1, (VI64M1X6, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1X6, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M1X6, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X6, VB64, VI64M1X6, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1X6, VB64, VI64M1X6, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M1X6, VB64, VI64M1X6, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1X6, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X6, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1X6, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (5, (VI64M1X6, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (6, (VI64M1X6, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (7, (VI64M1X6, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (8, (VI64M1X6, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X6, VI64M1X6, VI64M1, SI))
+DEF_RISCV_FTYPE (0, (VI64M1X7))
+DEF_RISCV_FTYPE (1, (VI64M1X7, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1X7, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M1X7, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X7, VB64, VI64M1X7, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1X7, VB64, VI64M1X7, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M1X7, VB64, VI64M1X7, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1X7, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X7, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1X7, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (5, (VI64M1X7, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (6, (VI64M1X7, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (7, (VI64M1X7, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (8, (VI64M1X7, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X7, VI64M1X7, VI64M1, SI))
+DEF_RISCV_FTYPE (0, (VI64M1X8))
+DEF_RISCV_FTYPE (1, (VI64M1X8, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M1X8, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M1X8, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X8, VB64, VI64M1X8, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M1X8, VB64, VI64M1X8, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M1X8, VB64, VI64M1X8, C_DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VI64M1X8, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X8, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VI64M1X8, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (5, (VI64M1X8, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (6, (VI64M1X8, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (7, (VI64M1X8, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (8, (VI64M1X8, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VI64M1X8, VI64M1X8, VI64M1, SI))
+DEF_RISCV_FTYPE (0, (VI64M2))
+DEF_RISCV_FTYPE (1, (VI64M2, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M2, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI64M2, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI64M2, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VI64M2, C_DI_PTR, VUI32M1, VI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2, C_DI_PTR, VUI64M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VI64M2, DI))
+DEF_RISCV_FTYPE (1, (VI64M2, VB32))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, C_DI_PTR, VUI32M1, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, C_DI_PTR, VUI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2, VB32, VI64M2, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2, VB32, VI64M2, DI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, DI, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, SI, VI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, SI, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, USI, VI32M1))
+DEF_RISCV_FTYPE (3, (VI64M2, VB32, VI64M2, VB32))
+DEF_RISCV_FTYPE (3, (VI64M2, VB32, VI64M2, VF32M1))
+DEF_RISCV_FTYPE (3, (VI64M2, VB32, VI64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VI64M2, VB32, VI64M2, VI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI32M1, SI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI32M1, USI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VI64M2, VB32, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, DI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, LONG))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, SI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, SIZE))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, UDI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, UQI))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, VI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M2, VB32, VI64M2, VI64M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VF32M1))
+DEF_RISCV_FTYPE (1, (VI64M2, VF64M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VI16M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2, VI32M1, SI))
+DEF_RISCV_FTYPE (2, (VI64M2, VI32M1, USI))
+DEF_RISCV_FTYPE (2, (VI64M2, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2, VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI64M2, VI32M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VI64M1))
+DEF_RISCV_FTYPE (1, (VI64M2, VI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, DI))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, DI, VB32))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, DI, VI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, LONG))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, SI))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, SI, VI32M1))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, SI, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, SIZE))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, UDI))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, UQI))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, USI))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, USI, VI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, VI32M1))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, VI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, VI32M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, VI64M2, SIZE))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, VI64M2, VB32))
+DEF_RISCV_FTYPE (3, (VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2X2, SI))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2X3, SI))
+DEF_RISCV_FTYPE (2, (VI64M2, VI64M2X4, SI))
+DEF_RISCV_FTYPE (1, (VI64M2, VI64M4))
+DEF_RISCV_FTYPE (1, (VI64M2, VI64M8))
+DEF_RISCV_FTYPE (1, (VI64M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VUI64M1))
+DEF_RISCV_FTYPE (1, (VI64M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI64M2, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI64M2, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI64M2, VUI8M2))
+DEF_RISCV_FTYPE (0, (VI64M2X2))
+DEF_RISCV_FTYPE (1, (VI64M2X2, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M2X2, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M2X2, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2X2, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X2, VB32, VI64M2X2, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M2X2, VB32, VI64M2X2, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M2X2, VB32, VI64M2X2, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2X2, VB32, VI64M2X2, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2X2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M2X2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (5, (VI64M2X2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (6, (VI64M2X2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (7, (VI64M2X2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (8, (VI64M2X2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X2, VI64M2X2, VI64M2, SI))
+DEF_RISCV_FTYPE (0, (VI64M2X3))
+DEF_RISCV_FTYPE (1, (VI64M2X3, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M2X3, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M2X3, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2X3, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X3, VB32, VI64M2X3, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M2X3, VB32, VI64M2X3, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M2X3, VB32, VI64M2X3, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2X3, VB32, VI64M2X3, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2X3, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X3, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M2X3, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (5, (VI64M2X3, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (6, (VI64M2X3, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (7, (VI64M2X3, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (8, (VI64M2X3, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X3, VI64M2X3, VI64M2, SI))
+DEF_RISCV_FTYPE (0, (VI64M2X4))
+DEF_RISCV_FTYPE (1, (VI64M2X4, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M2X4, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M2X4, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VI64M2X4, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X4, VB32, VI64M2X4, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M2X4, VB32, VI64M2X4, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M2X4, VB32, VI64M2X4, C_DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VI64M2X4, VB32, VI64M2X4, C_DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VI64M2X4, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X4, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (4, (VI64M2X4, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (5, (VI64M2X4, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (6, (VI64M2X4, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (7, (VI64M2X4, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (8, (VI64M2X4, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VI64M2X4, VI64M2X4, VI64M2, SI))
+DEF_RISCV_FTYPE (0, (VI64M4))
+DEF_RISCV_FTYPE (1, (VI64M4, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M4, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI64M4, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI64M4, C_DI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VI64M4, C_DI_PTR, VUI16M1, VI64M4))
+DEF_RISCV_FTYPE (2, (VI64M4, C_DI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, C_DI_PTR, VUI32M2, VI64M4))
+DEF_RISCV_FTYPE (2, (VI64M4, C_DI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4, C_DI_PTR, VUI64M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VI64M4, DI))
+DEF_RISCV_FTYPE (1, (VI64M4, VB16))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, C_DI_PTR, VUI16M1, VI64M4))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, C_DI_PTR, VUI32M2, VI64M4))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, C_DI_PTR, VUI64M4, VI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, C_DI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, C_DI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, C_DI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, DI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, DI, VI64M4))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, HI, VI16M1))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, HI, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, SI, VI32M2))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, SI, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, UHI, VI16M1))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, USI, VI32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, VB16))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, VF32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, VF64M4))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, VI16M1))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, VI32M2))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI32M2, SI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI32M2, USI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, VB16, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, DI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, LONG))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, SI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, SIZE))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, UDI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, UQI))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, VI32M2))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (4, (VI64M4, VB16, VI64M4, VI64M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VF32M2))
+DEF_RISCV_FTYPE (1, (VI64M4, VF64M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VI16M1))
+DEF_RISCV_FTYPE (2, (VI64M4, VI16M1, UHI))
+DEF_RISCV_FTYPE (2, (VI64M4, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI64M4, VI16M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VI32M2))
+DEF_RISCV_FTYPE (2, (VI64M4, VI32M2, SI))
+DEF_RISCV_FTYPE (2, (VI64M4, VI32M2, USI))
+DEF_RISCV_FTYPE (2, (VI64M4, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (2, (VI64M4, VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI64M4, VI32M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VI64M1))
+DEF_RISCV_FTYPE (1, (VI64M4, VI64M2))
+DEF_RISCV_FTYPE (1, (VI64M4, VI64M4))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, DI))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, DI, VB16))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, DI, VI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, HI, VI16M1))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, HI, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, LONG))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, SI))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, SI, VI32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, SI, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, SIZE))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, UDI))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, UHI))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, UHI, VI16M1))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, UQI))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, USI))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, USI, VI32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, VI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, VI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, VI32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, VI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, VI32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, VI64M4, SIZE))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, VI64M4, VB16))
+DEF_RISCV_FTYPE (3, (VI64M4, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI64M4, VI64M4X2, SI))
+DEF_RISCV_FTYPE (1, (VI64M4, VI64M8))
+DEF_RISCV_FTYPE (1, (VI64M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VUI64M1))
+DEF_RISCV_FTYPE (1, (VI64M4, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI64M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI64M4, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI64M4, VUI8M4))
+DEF_RISCV_FTYPE (0, (VI64M4X2))
+DEF_RISCV_FTYPE (1, (VI64M4X2, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M4X2, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI64M4X2, C_DI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VI64M4X2, C_DI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VI64M4X2, C_DI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4X2, VB16, VI64M4X2, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M4X2, VB16, VI64M4X2, C_DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI64M4X2, VB16, VI64M4X2, C_DI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VI64M4X2, VB16, VI64M4X2, C_DI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VI64M4X2, VB16, VI64M4X2, C_DI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VI64M4X2, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4X2, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (4, (VI64M4X2, VI64M4, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (5, (VI64M4X2, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (6, (VI64M4X2, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (7, (VI64M4X2, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (8, (VI64M4X2, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4, VI64M4))
+DEF_RISCV_FTYPE (3, (VI64M4X2, VI64M4X2, VI64M4, SI))
+DEF_RISCV_FTYPE (0, (VI64M8))
+DEF_RISCV_FTYPE (1, (VI64M8, C_DI_PTR))
+DEF_RISCV_FTYPE (2, (VI64M8, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI64M8, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI64M8, C_DI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VI64M8, C_DI_PTR, VUI16M2, VI64M8))
+DEF_RISCV_FTYPE (2, (VI64M8, C_DI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, C_DI_PTR, VUI32M4, VI64M8))
+DEF_RISCV_FTYPE (2, (VI64M8, C_DI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VI64M8, C_DI_PTR, VUI64M8, VI64M8))
+DEF_RISCV_FTYPE (2, (VI64M8, C_DI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI64M8, C_DI_PTR, VUI8M1, VI64M8))
+DEF_RISCV_FTYPE (1, (VI64M8, DI))
+DEF_RISCV_FTYPE (1, (VI64M8, VB8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, C_DI_PTR, VUI16M2, VI64M8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, C_DI_PTR, VUI32M4, VI64M8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, C_DI_PTR, VUI64M8, VI64M8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, C_DI_PTR, VUI8M1, VI64M8))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, C_DI_PTR))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, C_DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, C_DI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, C_DI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, C_DI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, C_DI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, C_DI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, DI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, DI, VI64M8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, HI, VI16M2))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, HI, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, SI, VI32M4))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, SI, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, UHI, VI16M2))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, USI, VI32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, VB8))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, VF32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, VF64M8))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, VI16M2))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, VI32M4))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI32M4, SI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI32M4, USI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, VI64M8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, DI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, LONG))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, SI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, SIZE))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, UDI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, UQI))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, VI32M4))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, VI64M8))
+DEF_RISCV_FTYPE (4, (VI64M8, VB8, VI64M8, VI64M8, VUI64M8))
+DEF_RISCV_FTYPE (3, (VI64M8, VB8, VI64M8, VI8M1))
+DEF_RISCV_FTYPE (1, (VI64M8, VF32M4))
+DEF_RISCV_FTYPE (1, (VI64M8, VF64M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VI16M2))
+DEF_RISCV_FTYPE (2, (VI64M8, VI16M2, UHI))
+DEF_RISCV_FTYPE (2, (VI64M8, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI64M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VI32M4))
+DEF_RISCV_FTYPE (2, (VI64M8, VI32M4, SI))
+DEF_RISCV_FTYPE (2, (VI64M8, VI32M4, USI))
+DEF_RISCV_FTYPE (2, (VI64M8, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (2, (VI64M8, VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI64M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VI64M1))
+DEF_RISCV_FTYPE (1, (VI64M8, VI64M2))
+DEF_RISCV_FTYPE (1, (VI64M8, VI64M4))
+DEF_RISCV_FTYPE (1, (VI64M8, VI64M8))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, DI))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, DI, VB8))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, DI, VI64M8))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, HI, VI16M2))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, HI, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, LONG))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, SI))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, SI, VI32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, SI, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, SIZE))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, UDI))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, UHI))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, UHI, VI16M2))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, UQI))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, USI))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, USI, VI32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, VI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, VI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, VI32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, VI32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, VI32M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, VI64M8))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, VI64M8, SIZE))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, VI64M8, VB8))
+DEF_RISCV_FTYPE (3, (VI64M8, VI64M8, VI64M8, VI64M8))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI64M8, VI64M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VI8M1))
+DEF_RISCV_FTYPE (1, (VI64M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VUI64M1))
+DEF_RISCV_FTYPE (1, (VI64M8, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI64M8, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI64M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI64M8, VUI8M8))
+DEF_RISCV_FTYPE (0, (VI8M1))
+DEF_RISCV_FTYPE (1, (VI8M1, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI8M1, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI8M1, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VI8M1, C_QI_PTR, VUI16M2, VI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VI8M1, C_QI_PTR, VUI32M4, VI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VI8M1, C_QI_PTR, VUI64M8, VI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1, C_QI_PTR, VUI8M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VI8M1, QI))
+DEF_RISCV_FTYPE (4, (VI8M1, VB1, VI8M1, VI8M1, VI8M8))
+DEF_RISCV_FTYPE (4, (VI8M1, VB2, VI8M1, VI8M1, VI8M4))
+DEF_RISCV_FTYPE (4, (VI8M1, VB4, VI8M1, VI8M1, VI8M2))
+DEF_RISCV_FTYPE (1, (VI8M1, VB8))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, C_QI_PTR, VUI16M2, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, C_QI_PTR, VUI32M4, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, C_QI_PTR, VUI64M8, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, C_QI_PTR, VUI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1, VB8, VI8M1, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1, VB8, VI8M1, QI))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, QI, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1, VB8, VI8M1, VB8))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI16M2, UQI))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI16M2, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1, VB8, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI8M1, LONG))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI8M1, QI))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI8M1, SIZE))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1, VB8, VI8M1, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VI16M1))
+DEF_RISCV_FTYPE (2, (VI8M1, VI16M2, UQI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI16M2, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VI32M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1, LONG))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1, QI))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, QI, VB8))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, QI, VI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1, SIZE))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1, UQI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, VI8M1, SIZE))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, VI8M1, VB8))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, VI8M1, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, VI8M1, VI8M4))
+DEF_RISCV_FTYPE (3, (VI8M1, VI8M1, VI8M1, VI8M8))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1X2, SI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1X3, SI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1X4, SI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1X5, SI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1X6, SI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1X7, SI))
+DEF_RISCV_FTYPE (2, (VI8M1, VI8M1X8, SI))
+DEF_RISCV_FTYPE (1, (VI8M1, VI8M2))
+DEF_RISCV_FTYPE (1, (VI8M1, VI8M4))
+DEF_RISCV_FTYPE (1, (VI8M1, VI8M8))
+DEF_RISCV_FTYPE (1, (VI8M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VUI64M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI8M1, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI8M1, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI8M1, VUI8M8))
+DEF_RISCV_FTYPE (0, (VI8M1X2))
+DEF_RISCV_FTYPE (1, (VI8M1X2, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1X2, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M1X2, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI8M1X2, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI8M1X2, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI8M1X2, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X2, VB8, VI8M1X2, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1X2, VB8, VI8M1X2, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M1X2, VB8, VI8M1X2, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1X2, VB8, VI8M1X2, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1X2, VB8, VI8M1X2, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1X2, VB8, VI8M1X2, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1X2, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X2, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1X2, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (5, (VI8M1X2, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (6, (VI8M1X2, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (7, (VI8M1X2, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (8, (VI8M1X2, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X2, VI8M1X2, VI8M1, SI))
+DEF_RISCV_FTYPE (0, (VI8M1X3))
+DEF_RISCV_FTYPE (1, (VI8M1X3, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1X3, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M1X3, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI8M1X3, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI8M1X3, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI8M1X3, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X3, VB8, VI8M1X3, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1X3, VB8, VI8M1X3, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M1X3, VB8, VI8M1X3, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1X3, VB8, VI8M1X3, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1X3, VB8, VI8M1X3, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1X3, VB8, VI8M1X3, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1X3, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X3, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1X3, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (5, (VI8M1X3, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (6, (VI8M1X3, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (7, (VI8M1X3, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (8, (VI8M1X3, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X3, VI8M1X3, VI8M1, SI))
+DEF_RISCV_FTYPE (0, (VI8M1X4))
+DEF_RISCV_FTYPE (1, (VI8M1X4, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1X4, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M1X4, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI8M1X4, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI8M1X4, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI8M1X4, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X4, VB8, VI8M1X4, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1X4, VB8, VI8M1X4, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M1X4, VB8, VI8M1X4, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1X4, VB8, VI8M1X4, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1X4, VB8, VI8M1X4, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1X4, VB8, VI8M1X4, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1X4, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X4, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1X4, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (5, (VI8M1X4, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (6, (VI8M1X4, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (7, (VI8M1X4, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (8, (VI8M1X4, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X4, VI8M1X4, VI8M1, SI))
+DEF_RISCV_FTYPE (0, (VI8M1X5))
+DEF_RISCV_FTYPE (1, (VI8M1X5, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1X5, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M1X5, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI8M1X5, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI8M1X5, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI8M1X5, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X5, VB8, VI8M1X5, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1X5, VB8, VI8M1X5, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M1X5, VB8, VI8M1X5, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1X5, VB8, VI8M1X5, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1X5, VB8, VI8M1X5, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1X5, VB8, VI8M1X5, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1X5, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X5, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1X5, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (5, (VI8M1X5, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (6, (VI8M1X5, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (7, (VI8M1X5, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (8, (VI8M1X5, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X5, VI8M1X5, VI8M1, SI))
+DEF_RISCV_FTYPE (0, (VI8M1X6))
+DEF_RISCV_FTYPE (1, (VI8M1X6, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1X6, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M1X6, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI8M1X6, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI8M1X6, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI8M1X6, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X6, VB8, VI8M1X6, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1X6, VB8, VI8M1X6, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M1X6, VB8, VI8M1X6, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1X6, VB8, VI8M1X6, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1X6, VB8, VI8M1X6, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1X6, VB8, VI8M1X6, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1X6, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X6, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1X6, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (5, (VI8M1X6, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (6, (VI8M1X6, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (7, (VI8M1X6, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (8, (VI8M1X6, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X6, VI8M1X6, VI8M1, SI))
+DEF_RISCV_FTYPE (0, (VI8M1X7))
+DEF_RISCV_FTYPE (1, (VI8M1X7, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1X7, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M1X7, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI8M1X7, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI8M1X7, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI8M1X7, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X7, VB8, VI8M1X7, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1X7, VB8, VI8M1X7, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M1X7, VB8, VI8M1X7, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1X7, VB8, VI8M1X7, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1X7, VB8, VI8M1X7, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1X7, VB8, VI8M1X7, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1X7, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X7, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1X7, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (5, (VI8M1X7, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (6, (VI8M1X7, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (7, (VI8M1X7, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (8, (VI8M1X7, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X7, VI8M1X7, VI8M1, SI))
+DEF_RISCV_FTYPE (0, (VI8M1X8))
+DEF_RISCV_FTYPE (1, (VI8M1X8, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M1X8, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M1X8, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VI8M1X8, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VI8M1X8, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VI8M1X8, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X8, VB8, VI8M1X8, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M1X8, VB8, VI8M1X8, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M1X8, VB8, VI8M1X8, C_QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VI8M1X8, VB8, VI8M1X8, C_QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VI8M1X8, VB8, VI8M1X8, C_QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VI8M1X8, VB8, VI8M1X8, C_QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VI8M1X8, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X8, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VI8M1X8, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (5, (VI8M1X8, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (6, (VI8M1X8, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (7, (VI8M1X8, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (8, (VI8M1X8, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VI8M1X8, VI8M1X8, VI8M1, SI))
+DEF_RISCV_FTYPE (0, (VI8M2))
+DEF_RISCV_FTYPE (1, (VI8M2, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M2, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI8M2, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI8M2, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VI8M2, C_QI_PTR, VUI16M4, VI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VI8M2, C_QI_PTR, VUI32M8, VI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2, C_QI_PTR, VUI8M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VI8M2, QI))
+DEF_RISCV_FTYPE (1, (VI8M2, VB4))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, C_QI_PTR, VUI16M4, VI8M2))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, C_QI_PTR, VUI32M8, VI8M2))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, C_QI_PTR, VUI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2, VB4, VI8M2, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2, VB4, VI8M2, QI))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, QI, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2, VB4, VI8M2, VB4))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI16M4, UQI))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI16M4, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2, VB4, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI8M2, LONG))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI8M2, QI))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI8M2, SIZE))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI8M2, UQI))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VI8M2, VB4, VI8M2, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VI16M2))
+DEF_RISCV_FTYPE (2, (VI8M2, VI16M4, UQI))
+DEF_RISCV_FTYPE (2, (VI8M2, VI16M4, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VI32M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VI8M1))
+DEF_RISCV_FTYPE (1, (VI8M2, VI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2, LONG))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2, QI))
+DEF_RISCV_FTYPE (3, (VI8M2, VI8M2, QI, VB4))
+DEF_RISCV_FTYPE (3, (VI8M2, VI8M2, QI, VI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2, SIZE))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2, UQI))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2, VI8M2, VI8M2, SIZE))
+DEF_RISCV_FTYPE (3, (VI8M2, VI8M2, VI8M2, VB4))
+DEF_RISCV_FTYPE (3, (VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2X2, SI))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2X3, SI))
+DEF_RISCV_FTYPE (2, (VI8M2, VI8M2X4, SI))
+DEF_RISCV_FTYPE (1, (VI8M2, VI8M4))
+DEF_RISCV_FTYPE (1, (VI8M2, VI8M8))
+DEF_RISCV_FTYPE (1, (VI8M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI8M2, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI8M2, VUI8M8))
+DEF_RISCV_FTYPE (0, (VI8M2X2))
+DEF_RISCV_FTYPE (1, (VI8M2X2, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M2X2, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M2X2, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI8M2X2, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VI8M2X2, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X2, VB4, VI8M2X2, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M2X2, VB4, VI8M2X2, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M2X2, VB4, VI8M2X2, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI8M2X2, VB4, VI8M2X2, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VI8M2X2, VB4, VI8M2X2, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2X2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VI8M2X2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (5, (VI8M2X2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (6, (VI8M2X2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (7, (VI8M2X2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (8, (VI8M2X2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X2, VI8M2X2, VI8M2, SI))
+DEF_RISCV_FTYPE (0, (VI8M2X3))
+DEF_RISCV_FTYPE (1, (VI8M2X3, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M2X3, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M2X3, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI8M2X3, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VI8M2X3, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X3, VB4, VI8M2X3, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M2X3, VB4, VI8M2X3, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M2X3, VB4, VI8M2X3, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI8M2X3, VB4, VI8M2X3, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VI8M2X3, VB4, VI8M2X3, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2X3, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X3, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VI8M2X3, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (5, (VI8M2X3, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (6, (VI8M2X3, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (7, (VI8M2X3, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (8, (VI8M2X3, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X3, VI8M2X3, VI8M2, SI))
+DEF_RISCV_FTYPE (0, (VI8M2X4))
+DEF_RISCV_FTYPE (1, (VI8M2X4, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M2X4, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M2X4, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VI8M2X4, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VI8M2X4, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X4, VB4, VI8M2X4, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M2X4, VB4, VI8M2X4, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M2X4, VB4, VI8M2X4, C_QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VI8M2X4, VB4, VI8M2X4, C_QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VI8M2X4, VB4, VI8M2X4, C_QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VI8M2X4, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X4, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VI8M2X4, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (5, (VI8M2X4, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (6, (VI8M2X4, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (7, (VI8M2X4, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (8, (VI8M2X4, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VI8M2X4, VI8M2X4, VI8M2, SI))
+DEF_RISCV_FTYPE (0, (VI8M4))
+DEF_RISCV_FTYPE (1, (VI8M4, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M4, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI8M4, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI8M4, C_QI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (3, (VI8M4, C_QI_PTR, VUI16M8, VI8M4))
+DEF_RISCV_FTYPE (2, (VI8M4, C_QI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4, C_QI_PTR, VUI8M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VI8M4, QI))
+DEF_RISCV_FTYPE (1, (VI8M4, VB2))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, C_QI_PTR, VUI16M8, VI8M4))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, C_QI_PTR, VUI8M4, VI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4, VB2, VI8M4, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, C_QI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, C_QI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4, VB2, VI8M4, QI))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, QI, VI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4, VB2, VI8M4, VB2))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI16M8, UQI))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI16M8, VUI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4, VB2, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI8M4, LONG))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI8M4, QI))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI8M4, SIZE))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI8M4, UQI))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (4, (VI8M4, VB2, VI8M4, VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VI16M4))
+DEF_RISCV_FTYPE (2, (VI8M4, VI16M8, UQI))
+DEF_RISCV_FTYPE (2, (VI8M4, VI16M8, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VI32M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VI8M1))
+DEF_RISCV_FTYPE (1, (VI8M4, VI8M2))
+DEF_RISCV_FTYPE (1, (VI8M4, VI8M4))
+DEF_RISCV_FTYPE (2, (VI8M4, VI8M4, LONG))
+DEF_RISCV_FTYPE (2, (VI8M4, VI8M4, QI))
+DEF_RISCV_FTYPE (3, (VI8M4, VI8M4, QI, VB2))
+DEF_RISCV_FTYPE (3, (VI8M4, VI8M4, QI, VI8M4))
+DEF_RISCV_FTYPE (2, (VI8M4, VI8M4, SIZE))
+DEF_RISCV_FTYPE (2, (VI8M4, VI8M4, UQI))
+DEF_RISCV_FTYPE (2, (VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4, VI8M4, VI8M4, SIZE))
+DEF_RISCV_FTYPE (3, (VI8M4, VI8M4, VI8M4, VB2))
+DEF_RISCV_FTYPE (3, (VI8M4, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (2, (VI8M4, VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (2, (VI8M4, VI8M4X2, SI))
+DEF_RISCV_FTYPE (1, (VI8M4, VI8M8))
+DEF_RISCV_FTYPE (1, (VI8M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI8M4, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI8M4, VUI8M8))
+DEF_RISCV_FTYPE (0, (VI8M4X2))
+DEF_RISCV_FTYPE (1, (VI8M4X2, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M4X2, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VI8M4X2, C_QI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (2, (VI8M4X2, C_QI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4X2, VB2, VI8M4X2, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M4X2, VB2, VI8M4X2, C_QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VI8M4X2, VB2, VI8M4X2, C_QI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VI8M4X2, VB2, VI8M4X2, C_QI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (2, (VI8M4X2, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4X2, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (4, (VI8M4X2, VI8M4, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (5, (VI8M4X2, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (6, (VI8M4X2, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (7, (VI8M4X2, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (8, (VI8M4X2, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4, VI8M4))
+DEF_RISCV_FTYPE (3, (VI8M4X2, VI8M4X2, VI8M4, SI))
+DEF_RISCV_FTYPE (0, (VI8M8))
+DEF_RISCV_FTYPE (1, (VI8M8, C_QI_PTR))
+DEF_RISCV_FTYPE (2, (VI8M8, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VI8M8, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VI8M8, C_QI_PTR, VUI8M8))
+DEF_RISCV_FTYPE (3, (VI8M8, C_QI_PTR, VUI8M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VI8M8, QI))
+DEF_RISCV_FTYPE (1, (VI8M8, VB1))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, C_QI_PTR, VUI8M8, VI8M8))
+DEF_RISCV_FTYPE (3, (VI8M8, VB1, VI8M8, C_QI_PTR))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, C_QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, C_QI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, C_QI_PTR, VUI8M8))
+DEF_RISCV_FTYPE (3, (VI8M8, VB1, VI8M8, QI))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, QI, VI8M8))
+DEF_RISCV_FTYPE (3, (VI8M8, VB1, VI8M8, VB1))
+DEF_RISCV_FTYPE (3, (VI8M8, VB1, VI8M8, VI8M8))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, VI8M8, LONG))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, VI8M8, QI))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, VI8M8, SIZE))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, VI8M8, UQI))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, VI8M8, VI8M8))
+DEF_RISCV_FTYPE (4, (VI8M8, VB1, VI8M8, VI8M8, VUI8M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VI8M1))
+DEF_RISCV_FTYPE (1, (VI8M8, VI8M2))
+DEF_RISCV_FTYPE (1, (VI8M8, VI8M4))
+DEF_RISCV_FTYPE (1, (VI8M8, VI8M8))
+DEF_RISCV_FTYPE (2, (VI8M8, VI8M8, LONG))
+DEF_RISCV_FTYPE (2, (VI8M8, VI8M8, QI))
+DEF_RISCV_FTYPE (3, (VI8M8, VI8M8, QI, VB1))
+DEF_RISCV_FTYPE (3, (VI8M8, VI8M8, QI, VI8M8))
+DEF_RISCV_FTYPE (2, (VI8M8, VI8M8, SIZE))
+DEF_RISCV_FTYPE (2, (VI8M8, VI8M8, UQI))
+DEF_RISCV_FTYPE (2, (VI8M8, VI8M8, VI8M8))
+DEF_RISCV_FTYPE (3, (VI8M8, VI8M8, VI8M8, SIZE))
+DEF_RISCV_FTYPE (3, (VI8M8, VI8M8, VI8M8, VB1))
+DEF_RISCV_FTYPE (3, (VI8M8, VI8M8, VI8M8, VI8M8))
+DEF_RISCV_FTYPE (2, (VI8M8, VI8M8, VUI8M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VI8M8, VUI8M1))
+DEF_RISCV_FTYPE (1, (VI8M8, VUI8M2))
+DEF_RISCV_FTYPE (1, (VI8M8, VUI8M4))
+DEF_RISCV_FTYPE (1, (VI8M8, VUI8M8))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI16M1, VF64M4))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI16M2, VF64M8))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI32M1, VF64M2))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI32M2, VF64M4))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI32M4, VF64M8))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI64M4, VF64M4))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI64M8, VF64M8))
+DEF_RISCV_FTYPE (3, (VOID, DF_PTR, VUI8M1, VF64M8))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI16M1, VI64M4))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI16M2, VI64M8))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI32M1, VI64M2))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI32M2, VI64M4))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI32M4, VI64M8))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI64M1, VI64M1))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI64M2, VI64M2))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI64M4, VI64M4))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI64M8, VI64M8))
+DEF_RISCV_FTYPE (3, (VOID, DI_PTR, VUI8M1, VI64M8))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI16M4, VF16M4))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI16M8, VF16M8))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI32M2, VF16M1))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI32M4, VF16M2))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI32M8, VF16M4))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI64M4, VF16M1))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI64M8, VF16M2))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI8M1, VF16M2))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI8M2, VF16M4))
+DEF_RISCV_FTYPE (3, (VOID, HF_PTR, VUI8M4, VF16M8))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI16M1, VI16M1))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI16M2, VI16M2))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI16M4, VI16M4))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI16M8, VI16M8))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI32M2, VI16M1))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI32M4, VI16M2))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI32M8, VI16M4))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI64M4, VI16M1))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI64M8, VI16M2))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI8M1, VI16M2))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI8M2, VI16M4))
+DEF_RISCV_FTYPE (3, (VOID, HI_PTR, VUI8M4, VI16M8))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI16M2, VI8M1))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI16M4, VI8M2))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI16M8, VI8M4))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI32M4, VI8M1))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI32M8, VI8M2))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI64M8, VI8M1))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI8M1, VI8M1))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI8M2, VI8M2))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI8M4, VI8M4))
+DEF_RISCV_FTYPE (3, (VOID, QI_PTR, VUI8M8, VI8M8))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI16M1, VF32M2))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI16M2, VF32M4))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI16M4, VF32M8))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI32M4, VF32M4))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI32M8, VF32M8))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI64M2, VF32M1))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI64M4, VF32M2))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI64M8, VF32M4))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI8M1, VF32M4))
+DEF_RISCV_FTYPE (3, (VOID, SF_PTR, VUI8M2, VF32M8))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI16M1, VI32M2))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI16M2, VI32M4))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI16M4, VI32M8))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI32M1, VI32M1))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI32M2, VI32M2))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI32M4, VI32M4))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI32M8, VI32M8))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI64M2, VI32M1))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI64M4, VI32M2))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI64M8, VI32M4))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI8M1, VI32M4))
+DEF_RISCV_FTYPE (3, (VOID, SI_PTR, VUI8M2, VI32M8))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI16M1, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI16M2, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI32M1, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI32M2, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI32M4, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, UDI_PTR, VUI8M1, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI64M4, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI64M8, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI8M1, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI8M2, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, UHI_PTR, VUI8M4, VUI16M8))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI32M4, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI32M8, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI64M8, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (3, (VOID, UQI_PTR, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI16M1, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI16M2, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI16M4, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI8M1, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, USI_PTR, VUI8M2, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB1, QI_PTR, VUI8M8, VI8M8))
+DEF_RISCV_FTYPE (4, (VOID, VB1, UQI_PTR, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (3, (VOID, VB1, VI8M8, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB1, VI8M8, QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB1, VI8M8, QI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB1, VUI8M8, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB1, VUI8M8, UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB1, VUI8M8, UQI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, DF_PTR, VUI16M1, VF64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, DF_PTR, VUI32M2, VF64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, DF_PTR, VUI64M4, VF64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, DI_PTR, VUI16M1, VI64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, DI_PTR, VUI32M2, VI64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, DI_PTR, VUI64M4, VI64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, HF_PTR, VUI16M1, VF16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, HF_PTR, VUI32M2, VF16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, HF_PTR, VUI64M4, VF16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, HI_PTR, VUI16M1, VI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, HI_PTR, VUI32M2, VI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, HI_PTR, VUI64M4, VI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, SF_PTR, VUI16M1, VF32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, SF_PTR, VUI32M2, VF32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, SF_PTR, VUI64M4, VF32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, SI_PTR, VUI16M1, VI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, SI_PTR, VUI32M2, VI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, SI_PTR, VUI64M4, VI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, UDI_PTR, VUI16M1, VUI64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, UDI_PTR, VUI32M2, VUI64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, UDI_PTR, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB16, UHI_PTR, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, UHI_PTR, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, UHI_PTR, VUI64M4, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, USI_PTR, VUI16M1, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, USI_PTR, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, USI_PTR, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1, HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1, HF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1X2, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X2, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X2, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X2, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X2, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1X3, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X3, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X3, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X3, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X3, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1X4, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X4, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X4, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X4, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X4, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1X5, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X5, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X5, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X5, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X5, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1X6, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X6, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X6, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X6, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X6, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1X7, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X7, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X7, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X7, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X7, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF16M1X8, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X8, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X8, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X8, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF16M1X8, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF32M2, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2, SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2, SF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF32M2X2, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X2, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X2, SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X2, SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X2, SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF32M2X3, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X3, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X3, SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X3, SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X3, SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF32M2X4, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X4, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X4, SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X4, SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF32M2X4, SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF64M4, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF64M4, DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF64M4, DF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VF64M4X2, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF64M4X2, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF64M4X2, DF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF64M4X2, DF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VF64M4X2, DF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1, HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1, HI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1X2, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X2, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X2, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X2, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X2, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1X3, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X3, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X3, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X3, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X3, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1X4, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X4, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X4, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X4, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X4, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1X5, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X5, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X5, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X5, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X5, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1X6, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X6, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X6, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X6, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X6, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1X7, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X7, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X7, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X7, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X7, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI16M1X8, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X8, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X8, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X8, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI16M1X8, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI32M2, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2, SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2, SI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI32M2X2, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X2, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X2, SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X2, SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X2, SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI32M2X3, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X3, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X3, SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X3, SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X3, SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI32M2X4, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X4, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X4, SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X4, SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI32M2X4, SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI64M4, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI64M4, DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI64M4, DI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VI64M4X2, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI64M4X2, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI64M4X2, DI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI64M4X2, DI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VI64M4X2, DI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1, UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1, UHI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1X2, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X2, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X2, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X2, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X2, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1X3, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X3, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X3, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X3, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X3, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1X4, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X4, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X4, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X4, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X4, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1X5, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X5, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X5, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X5, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X5, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1X6, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X6, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X6, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X6, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X6, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1X7, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X7, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X7, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X7, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X7, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI16M1X8, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X8, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X8, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X8, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI16M1X8, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI32M2, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2, USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2, USI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI32M2X2, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X2, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X2, USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X2, USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X2, USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI32M2X3, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X3, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X3, USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X3, USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X3, USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI32M2X4, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X4, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X4, USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X4, USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI32M2X4, USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI64M4, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI64M4, UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI64M4, UDI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB16, VUI64M4X2, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI64M4X2, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI64M4X2, UDI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI64M4X2, UDI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VOID, VB16, VUI64M4X2, UDI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (4, (VOID, VB2, HF_PTR, VUI16M8, VF16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, HF_PTR, VUI8M4, VF16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, HI_PTR, VUI16M8, VI16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, HI_PTR, VUI8M4, VI16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, QI_PTR, VUI16M8, VI8M4))
+DEF_RISCV_FTYPE (4, (VOID, VB2, QI_PTR, VUI8M4, VI8M4))
+DEF_RISCV_FTYPE (4, (VOID, VB2, UHI_PTR, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, UHI_PTR, VUI8M4, VUI16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, UQI_PTR, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (4, (VOID, VB2, UQI_PTR, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (3, (VOID, VB2, VF16M8, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VF16M8, HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VF16M8, HF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB2, VI16M8, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VI16M8, HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VI16M8, HI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB2, VI8M4, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VI8M4, QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VI8M4, QI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB2, VI8M4X2, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VI8M4X2, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VI8M4X2, QI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VI8M4X2, QI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VOID, VB2, VUI16M8, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VUI16M8, UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VUI16M8, UHI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB2, VUI8M4, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VUI8M4, UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VUI8M4, UQI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB2, VUI8M4X2, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VUI8M4X2, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VUI8M4X2, UQI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VOID, VB2, VUI8M4X2, UQI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (4, (VOID, VB32, DF_PTR, VUI32M1, VF64M2))
+DEF_RISCV_FTYPE (4, (VOID, VB32, DF_PTR, VUI64M2, VF64M2))
+DEF_RISCV_FTYPE (4, (VOID, VB32, DI_PTR, VUI32M1, VI64M2))
+DEF_RISCV_FTYPE (4, (VOID, VB32, DI_PTR, VUI64M2, VI64M2))
+DEF_RISCV_FTYPE (4, (VOID, VB32, SF_PTR, VUI32M1, VF32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, SF_PTR, VUI64M2, VF32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, SI_PTR, VUI32M1, VI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, SI_PTR, VUI64M2, VI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, UDI_PTR, VUI32M1, VUI64M2))
+DEF_RISCV_FTYPE (4, (VOID, VB32, UDI_PTR, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (4, (VOID, VB32, USI_PTR, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, USI_PTR, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1, SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1, SF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1X2, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X2, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X2, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X2, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1X3, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X3, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X3, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X3, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1X4, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X4, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X4, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X4, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1X5, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X5, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X5, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X5, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1X6, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X6, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X6, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X6, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1X7, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X7, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X7, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X7, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF32M1X8, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X8, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X8, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF32M1X8, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF64M2, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2, DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2, DF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF64M2X2, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X2, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X2, DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X2, DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF64M2X3, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X3, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X3, DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X3, DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VF64M2X4, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X4, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X4, DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VF64M2X4, DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1, SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1, SI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1X2, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X2, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X2, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X2, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1X3, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X3, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X3, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X3, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1X4, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X4, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X4, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X4, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1X5, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X5, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X5, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X5, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1X6, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X6, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X6, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X6, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1X7, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X7, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X7, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X7, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI32M1X8, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X8, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X8, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI32M1X8, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI64M2, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2, DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2, DI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI64M2X2, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X2, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X2, DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X2, DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI64M2X3, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X3, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X3, DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X3, DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VI64M2X4, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X4, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X4, DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VI64M2X4, DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1, USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1, USI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1X2, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X2, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X2, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X2, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1X3, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X3, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X3, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X3, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1X4, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X4, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X4, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X4, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1X5, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X5, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X5, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X5, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1X6, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X6, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X6, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X6, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1X7, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X7, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X7, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X7, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI32M1X8, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X8, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X8, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI32M1X8, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI64M2, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2, UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2, UDI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI64M2X2, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X2, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X2, UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X2, UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI64M2X3, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X3, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X3, UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X3, UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VOID, VB32, VUI64M2X4, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X4, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X4, UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VOID, VB32, VUI64M2X4, UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (4, (VOID, VB4, HF_PTR, VUI16M4, VF16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, HF_PTR, VUI32M8, VF16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, HF_PTR, VUI8M2, VF16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, HI_PTR, VUI16M4, VI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, HI_PTR, VUI32M8, VI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, HI_PTR, VUI8M2, VI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, QI_PTR, VUI16M4, VI8M2))
+DEF_RISCV_FTYPE (4, (VOID, VB4, QI_PTR, VUI32M8, VI8M2))
+DEF_RISCV_FTYPE (4, (VOID, VB4, QI_PTR, VUI8M2, VI8M2))
+DEF_RISCV_FTYPE (4, (VOID, VB4, SF_PTR, VUI16M4, VF32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, SF_PTR, VUI32M8, VF32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, SF_PTR, VUI8M2, VF32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, SI_PTR, VUI16M4, VI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, SI_PTR, VUI32M8, VI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, SI_PTR, VUI8M2, VI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, UHI_PTR, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, UHI_PTR, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, UHI_PTR, VUI8M2, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, UQI_PTR, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (4, (VOID, VB4, UQI_PTR, VUI32M8, VUI8M2))
+DEF_RISCV_FTYPE (4, (VOID, VB4, UQI_PTR, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (4, (VOID, VB4, USI_PTR, VUI16M4, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, USI_PTR, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, USI_PTR, VUI8M2, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VF16M4, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF16M4, HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF16M4, HF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VF16M4X2, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF16M4X2, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF16M4X2, HF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF16M4X2, HF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF16M4X2, HF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VF32M8, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF32M8, SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VF32M8, SF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VI16M4, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI16M4, HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI16M4, HI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VI16M4X2, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI16M4X2, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI16M4X2, HI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI16M4X2, HI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI16M4X2, HI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VI32M8, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI32M8, SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI32M8, SI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VI8M2, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2, QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2, QI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VI8M2X2, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X2, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X2, QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X2, QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X2, QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VI8M2X3, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X3, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X3, QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X3, QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X3, QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VI8M2X4, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X4, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X4, QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X4, QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VI8M2X4, QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VUI16M4, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI16M4, UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI16M4, UHI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VUI16M4X2, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI16M4X2, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI16M4X2, UHI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI16M4X2, UHI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI16M4X2, UHI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VUI32M8, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI32M8, USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI32M8, USI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VUI8M2, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2, UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2, UQI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VUI8M2X2, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X2, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X2, UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X2, UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X2, UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VUI8M2X3, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X3, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X3, UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X3, UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X3, UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VOID, VB4, VUI8M2X4, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X4, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X4, UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X4, UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VOID, VB4, VUI8M2X4, UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (4, (VOID, VB64, DF_PTR, VUI64M1, VF64M1))
+DEF_RISCV_FTYPE (4, (VOID, VB64, DI_PTR, VUI64M1, VI64M1))
+DEF_RISCV_FTYPE (4, (VOID, VB64, UDI_PTR, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1, DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1, DF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1X2, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X2, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X2, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1X3, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X3, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X3, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1X4, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X4, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X4, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1X5, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X5, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X5, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1X6, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X6, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X6, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1X7, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X7, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X7, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VF64M1X8, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X8, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VF64M1X8, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1, DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1, DI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1X2, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X2, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X2, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1X3, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X3, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X3, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1X4, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X4, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X4, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1X5, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X5, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X5, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1X6, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X6, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X6, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1X7, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X7, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X7, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VI64M1X8, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X8, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VI64M1X8, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1, UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1, UDI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1X2, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X2, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X2, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1X3, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X3, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X3, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1X4, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X4, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X4, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1X5, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X5, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X5, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1X6, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X6, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X6, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1X7, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X7, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X7, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VOID, VB64, VUI64M1X8, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X8, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB64, VUI64M1X8, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DF_PTR, VUI16M2, VF64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DF_PTR, VUI32M4, VF64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DF_PTR, VUI64M8, VF64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DF_PTR, VUI8M1, VF64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DI_PTR, VUI16M2, VI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DI_PTR, VUI32M4, VI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DI_PTR, VUI64M8, VI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, DI_PTR, VUI8M1, VI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HF_PTR, VUI16M2, VF16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HF_PTR, VUI32M4, VF16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HF_PTR, VUI64M8, VF16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HF_PTR, VUI8M1, VF16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HI_PTR, VUI16M2, VI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HI_PTR, VUI32M4, VI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HI_PTR, VUI64M8, VI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, HI_PTR, VUI8M1, VI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, QI_PTR, VUI16M2, VI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, QI_PTR, VUI32M4, VI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, QI_PTR, VUI64M8, VI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, QI_PTR, VUI8M1, VI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SF_PTR, VUI16M2, VF32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SF_PTR, VUI32M4, VF32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SF_PTR, VUI64M8, VF32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SF_PTR, VUI8M1, VF32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SI_PTR, VUI16M2, VI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SI_PTR, VUI32M4, VI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SI_PTR, VUI64M8, VI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, SI_PTR, VUI8M1, VI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UDI_PTR, VUI16M2, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UDI_PTR, VUI32M4, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UDI_PTR, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UDI_PTR, VUI8M1, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UHI_PTR, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UHI_PTR, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UHI_PTR, VUI64M8, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UHI_PTR, VUI8M1, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UQI_PTR, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UQI_PTR, VUI32M4, VUI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UQI_PTR, VUI64M8, VUI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, UQI_PTR, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VOID, VB8, USI_PTR, VUI16M2, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, USI_PTR, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, USI_PTR, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, USI_PTR, VUI8M1, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VF16M2, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2, HF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2, HF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VF16M2X2, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X2, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X2, HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X2, HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X2, HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X2, HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VF16M2X3, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X3, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X3, HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X3, HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X3, HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X3, HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VF16M2X4, HF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X4, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X4, HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X4, HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X4, HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF16M2X4, HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VF32M4, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF32M4, SF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF32M4, SF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VF32M4X2, SF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF32M4X2, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF32M4X2, SF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF32M4X2, SF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF32M4X2, SF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF32M4X2, SF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VF64M8, DF_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF64M8, DF_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VF64M8, DF_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI16M2, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2, HI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2, HI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI16M2X2, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X2, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X2, HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X2, HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X2, HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X2, HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI16M2X3, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X3, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X3, HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X3, HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X3, HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X3, HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI16M2X4, HI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X4, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X4, HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X4, HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X4, HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI16M2X4, HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI32M4, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI32M4, SI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI32M4, SI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI32M4X2, SI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI32M4X2, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI32M4X2, SI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI32M4X2, SI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI32M4X2, SI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI32M4X2, SI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI64M8, DI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI64M8, DI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI64M8, DI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1, QI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1, QI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1X2, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X2, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X2, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X2, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X2, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X2, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1X3, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X3, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X3, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X3, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X3, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X3, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1X4, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X4, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X4, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X4, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X4, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X4, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1X5, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X5, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X5, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X5, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X5, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X5, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1X6, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X6, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X6, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X6, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X6, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X6, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1X7, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X7, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X7, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X7, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X7, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X7, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VI8M1X8, QI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X8, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X8, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X8, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X8, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VI8M1X8, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI16M2, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2, UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2, UHI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI16M2X2, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X2, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X2, UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X2, UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X2, UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X2, UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI16M2X3, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X3, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X3, UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X3, UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X3, UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X3, UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI16M2X4, UHI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X4, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X4, UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X4, UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X4, UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI16M2X4, UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI32M4, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI32M4, USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI32M4, USI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI32M4X2, USI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI32M4X2, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI32M4X2, USI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI32M4X2, USI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI32M4X2, USI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI32M4X2, USI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI64M8, UDI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI64M8, UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI64M8, UDI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1, UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1, UQI_PTR, SI))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1X2, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X2, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X2, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X2, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X2, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X2, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1X3, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X3, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X3, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X3, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X3, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X3, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1X4, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X4, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X4, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X4, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X4, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X4, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1X5, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X5, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X5, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X5, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X5, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X5, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1X6, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X6, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X6, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X6, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X6, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X6, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1X7, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X7, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X7, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X7, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X7, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X7, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VOID, VB8, VUI8M1X8, UQI_PTR))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X8, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X8, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X8, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X8, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VOID, VB8, VUI8M1X8, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1, HF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1, HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1X2, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X2, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X2, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X2, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X2, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1X3, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X3, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X3, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X3, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X3, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1X4, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X4, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X4, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X4, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X4, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1X5, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X5, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X5, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X5, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X5, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1X6, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X6, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X6, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X6, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X6, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1X7, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X7, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X7, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X7, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X7, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF16M1X8, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X8, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X8, HF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X8, HF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M1X8, HF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF16M2, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2, HF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2, HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF16M2X2, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X2, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X2, HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X2, HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X2, HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X2, HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VF16M2X3, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X3, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X3, HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X3, HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X3, HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X3, HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VF16M2X4, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X4, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X4, HF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X4, HF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X4, HF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VF16M2X4, HF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VF16M4, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M4, HF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF16M4, HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF16M4X2, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M4X2, HF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF16M4X2, HF_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VF16M4X2, HF_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VF16M4X2, HF_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VF16M8, HF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF16M8, HF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF16M8, HF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1, SF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1, SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1X2, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X2, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X2, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X2, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1X3, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X3, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X3, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X3, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1X4, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X4, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X4, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X4, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1X5, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X5, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X5, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X5, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1X6, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X6, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X6, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X6, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1X7, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X7, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X7, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X7, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF32M1X8, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X8, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X8, SF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M1X8, SF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF32M2, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2, SF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2, SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF32M2X2, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X2, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X2, SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X2, SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X2, SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF32M2X3, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X3, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X3, SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X3, SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X3, SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF32M2X4, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X4, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X4, SF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X4, SF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF32M2X4, SF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF32M4, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M4, SF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF32M4, SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF32M4X2, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M4X2, SF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF32M4X2, SF_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VF32M4X2, SF_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VF32M4X2, SF_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VF32M4X2, SF_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VF32M8, SF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF32M8, SF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF32M8, SF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1, DF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1, DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1X2, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X2, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X2, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1X3, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X3, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X3, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1X4, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X4, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X4, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1X5, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X5, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X5, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1X6, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X6, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X6, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1X7, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X7, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X7, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VF64M1X8, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X8, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M1X8, DF_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VF64M2, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2, DF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2, DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF64M2X2, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X2, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X2, DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X2, DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF64M2X3, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X3, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X3, DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X3, DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF64M2X4, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X4, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X4, DF_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VF64M2X4, DF_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VF64M4, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M4, DF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF64M4, DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VF64M4X2, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M4X2, DF_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VF64M4X2, DF_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VF64M4X2, DF_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VF64M4X2, DF_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VF64M8, DF_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VF64M8, DF_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VF64M8, DF_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1, HI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1, HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1X2, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X2, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X2, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X2, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X2, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1X3, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X3, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X3, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X3, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X3, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1X4, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X4, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X4, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X4, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X4, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1X5, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X5, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X5, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X5, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X5, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1X6, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X6, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X6, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X6, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X6, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1X7, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X7, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X7, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X7, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X7, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI16M1X8, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X8, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X8, HI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X8, HI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M1X8, HI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI16M2, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2, HI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2, HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI16M2X2, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X2, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X2, HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X2, HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X2, HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X2, HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI16M2X3, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X3, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X3, HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X3, HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X3, HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X3, HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI16M2X4, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X4, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X4, HI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X4, HI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X4, HI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI16M2X4, HI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI16M4, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M4, HI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI16M4, HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI16M4X2, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M4X2, HI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI16M4X2, HI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VI16M4X2, HI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VI16M4X2, HI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VI16M8, HI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI16M8, HI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI16M8, HI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1, SI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1, SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1X2, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X2, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X2, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X2, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1X3, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X3, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X3, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X3, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1X4, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X4, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X4, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X4, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1X5, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X5, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X5, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X5, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1X6, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X6, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X6, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X6, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1X7, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X7, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X7, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X7, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI32M1X8, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X8, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X8, SI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M1X8, SI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI32M2, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2, SI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2, SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI32M2X2, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X2, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X2, SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X2, SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X2, SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI32M2X3, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X3, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X3, SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X3, SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X3, SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI32M2X4, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X4, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X4, SI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X4, SI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI32M2X4, SI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI32M4, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M4, SI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI32M4, SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI32M4X2, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M4X2, SI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI32M4X2, SI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI32M4X2, SI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI32M4X2, SI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI32M4X2, SI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI32M8, SI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI32M8, SI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI32M8, SI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1, DI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1, DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1X2, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X2, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X2, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1X3, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X3, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X3, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1X4, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X4, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X4, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1X5, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X5, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X5, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1X6, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X6, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X6, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1X7, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X7, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X7, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VI64M1X8, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X8, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M1X8, DI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VI64M2, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2, DI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2, DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI64M2X2, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X2, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X2, DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X2, DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI64M2X3, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X3, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X3, DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X3, DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI64M2X4, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X4, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X4, DI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VI64M2X4, DI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VI64M4, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M4, DI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI64M4, DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI64M4X2, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M4X2, DI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI64M4X2, DI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VI64M4X2, DI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VI64M4X2, DI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VI64M8, DI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI64M8, DI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI64M8, DI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1, QI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1, QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1X2, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X2, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X2, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X2, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X2, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X2, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1X3, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X3, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X3, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X3, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X3, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X3, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1X4, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X4, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X4, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X4, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X4, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X4, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1X5, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X5, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X5, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X5, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X5, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X5, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1X6, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X6, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X6, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X6, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X6, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X6, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1X7, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X7, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X7, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X7, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X7, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X7, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI8M1X8, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X8, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X8, QI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X8, QI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X8, QI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M1X8, QI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VI8M2, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2, QI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2, QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI8M2X2, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X2, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X2, QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X2, QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X2, QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VI8M2X3, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X3, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X3, QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X3, QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X3, QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VI8M2X4, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X4, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X4, QI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X4, QI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M2X4, QI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VI8M4, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M4, QI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI8M4, QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VI8M4X2, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M4X2, QI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VI8M4X2, QI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (3, (VOID, VI8M4X2, QI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (2, (VOID, VI8M8, QI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VI8M8, QI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VI8M8, QI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1, UHI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1, UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1X2, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X2, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X2, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X2, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X2, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1X3, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X3, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X3, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X3, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X3, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1X4, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X4, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X4, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X4, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X4, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1X5, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X5, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X5, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X5, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X5, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1X6, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X6, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X6, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X6, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X6, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1X7, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X7, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X7, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X7, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X7, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M1X8, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X8, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X8, UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X8, UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M1X8, UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M2, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2, UHI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2, UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M2X2, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X2, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X2, UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X2, UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X2, UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X2, UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M2X3, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X3, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X3, UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X3, UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X3, UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X3, UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M2X4, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X4, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X4, UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X4, UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X4, UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M2X4, UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M4, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M4, UHI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M4, UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M4X2, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M4X2, UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M4X2, UHI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M4X2, UHI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M4X2, UHI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI16M8, UHI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M8, UHI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI16M8, UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1, USI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1, USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1X2, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X2, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X2, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X2, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1X3, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X3, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X3, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X3, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1X4, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X4, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X4, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X4, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1X5, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X5, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X5, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X5, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1X6, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X6, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X6, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X6, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1X7, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X7, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X7, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X7, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M1X8, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X8, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X8, USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M1X8, USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M2, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2, USI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2, USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M2X2, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X2, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X2, USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X2, USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X2, USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M2X3, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X3, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X3, USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X3, USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X3, USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M2X4, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X4, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X4, USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X4, USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M2X4, USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M4, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M4, USI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M4, USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M4X2, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M4X2, USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M4X2, USI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M4X2, USI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M4X2, USI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M4X2, USI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI32M8, USI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M8, USI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI32M8, USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1, UDI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1, UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1X2, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X2, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X2, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1X3, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X3, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X3, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1X4, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X4, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X4, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1X5, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X5, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X5, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1X6, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X6, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X6, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1X7, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X7, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X7, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M1X8, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X8, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M1X8, UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M2, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2, UDI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2, UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M2X2, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X2, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X2, UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X2, UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M2X3, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X3, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X3, UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X3, UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M2X4, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X4, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X4, UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M2X4, UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M4, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M4, UDI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M4, UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M4X2, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M4X2, UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M4X2, UDI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M4X2, UDI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M4X2, UDI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI64M8, UDI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M8, UDI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI64M8, UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1, UQI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1, UQI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1X2, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X2, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X2, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X2, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X2, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X2, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1X3, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X3, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X3, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X3, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X3, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X3, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1X4, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X4, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X4, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X4, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X4, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X4, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1X5, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X5, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X5, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X5, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X5, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X5, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1X6, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X6, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X6, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X6, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X6, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X6, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1X7, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X7, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X7, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X7, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X7, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X7, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M1X8, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X8, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X8, UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X8, UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X8, UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M1X8, UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M2, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2, UQI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2, UQI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M2X2, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X2, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X2, UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X2, UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X2, UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M2X3, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X3, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X3, UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X3, UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X3, UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M2X4, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X4, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X4, UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X4, UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M2X4, UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M4, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M4, UQI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M4, UQI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M4X2, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M4X2, UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M4X2, UQI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M4X2, UQI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (2, (VOID, VUI8M8, UQI_PTR))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M8, UQI_PTR, DI))
+DEF_RISCV_FTYPE (3, (VOID, VUI8M8, UQI_PTR, SI))
+DEF_RISCV_FTYPE (0, (VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI16M1, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI16M1, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1, C_UHI_PTR, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI16M1, C_UHI_PTR, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1, C_UHI_PTR, VUI64M4, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB1, VUI16M1, VUI16M1, VUI8M8))
+DEF_RISCV_FTYPE (1, (VUI16M1, VB16))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, C_UHI_PTR, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, C_UHI_PTR, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, C_UHI_PTR, VUI64M4, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1, VB16, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1, VB16, VUI16M1, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1, VB16, VUI16M1, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, UHI, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1, VB16, VUI16M1, VB16))
+DEF_RISCV_FTYPE (3, (VUI16M1, VB16, VUI16M1, VF16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1, VB16, VUI16M1, VF32M2))
+DEF_RISCV_FTYPE (3, (VUI16M1, VB16, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, VUI16M1, LONG))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, VUI16M1, SIZE))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, VUI16M1, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, VUI16M1, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, VUI32M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB16, VUI16M1, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB2, VUI16M1, VUI16M1, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB2, VUI16M1, VUI16M1, VUI8M4))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB4, VUI16M1, VUI16M1, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB4, VUI16M1, VUI16M1, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB8, VUI16M1, VUI16M1, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M1, VB8, VUI16M1, VUI16M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VF16M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VF32M2))
+DEF_RISCV_FTYPE (1, (VUI16M1, VI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M1, VI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M1, VI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M1, VI32M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1, LONG))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1, SIZE))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1, UHI))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, UHI, VB16))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, UHI, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1, UQI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, SIZE))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VB16))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI16M8))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI16M1, VUI16M1, VUI16M1, VUI8M8))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1X2, SI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1X3, SI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1X4, SI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1X5, SI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1X6, SI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1X7, SI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI16M1X8, SI))
+DEF_RISCV_FTYPE (1, (VUI16M1, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M1, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M1, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI32M2, UQI))
+DEF_RISCV_FTYPE (2, (VUI16M1, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VUI64M1))
+DEF_RISCV_FTYPE (1, (VUI16M1, VUI8M1))
+DEF_RISCV_FTYPE (0, (VUI16M1X2))
+DEF_RISCV_FTYPE (1, (VUI16M1X2, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1X2, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M1X2, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1X2, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M1X2, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1X2, VB16, VUI16M1X2, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1X2, VB16, VUI16M1X2, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M1X2, VB16, VUI16M1X2, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X2, VB16, VUI16M1X2, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1X2, VB16, VUI16M1X2, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI16M1X2, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X2, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X2, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (5, (VUI16M1X2, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (6, (VUI16M1X2, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (7, (VUI16M1X2, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (8, (VUI16M1X2, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X2, VUI16M1X2, VUI16M1, SI))
+DEF_RISCV_FTYPE (0, (VUI16M1X3))
+DEF_RISCV_FTYPE (1, (VUI16M1X3, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1X3, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M1X3, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1X3, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M1X3, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1X3, VB16, VUI16M1X3, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1X3, VB16, VUI16M1X3, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M1X3, VB16, VUI16M1X3, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X3, VB16, VUI16M1X3, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1X3, VB16, VUI16M1X3, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI16M1X3, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X3, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X3, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (5, (VUI16M1X3, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (6, (VUI16M1X3, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (7, (VUI16M1X3, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (8, (VUI16M1X3, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X3, VUI16M1X3, VUI16M1, SI))
+DEF_RISCV_FTYPE (0, (VUI16M1X4))
+DEF_RISCV_FTYPE (1, (VUI16M1X4, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1X4, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M1X4, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1X4, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M1X4, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1X4, VB16, VUI16M1X4, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1X4, VB16, VUI16M1X4, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M1X4, VB16, VUI16M1X4, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X4, VB16, VUI16M1X4, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1X4, VB16, VUI16M1X4, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI16M1X4, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X4, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X4, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (5, (VUI16M1X4, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (6, (VUI16M1X4, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (7, (VUI16M1X4, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (8, (VUI16M1X4, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X4, VUI16M1X4, VUI16M1, SI))
+DEF_RISCV_FTYPE (0, (VUI16M1X5))
+DEF_RISCV_FTYPE (1, (VUI16M1X5, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1X5, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M1X5, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1X5, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M1X5, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1X5, VB16, VUI16M1X5, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1X5, VB16, VUI16M1X5, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M1X5, VB16, VUI16M1X5, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X5, VB16, VUI16M1X5, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1X5, VB16, VUI16M1X5, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI16M1X5, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X5, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X5, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (5, (VUI16M1X5, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (6, (VUI16M1X5, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (7, (VUI16M1X5, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (8, (VUI16M1X5, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X5, VUI16M1X5, VUI16M1, SI))
+DEF_RISCV_FTYPE (0, (VUI16M1X6))
+DEF_RISCV_FTYPE (1, (VUI16M1X6, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1X6, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M1X6, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1X6, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M1X6, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1X6, VB16, VUI16M1X6, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1X6, VB16, VUI16M1X6, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M1X6, VB16, VUI16M1X6, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X6, VB16, VUI16M1X6, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1X6, VB16, VUI16M1X6, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI16M1X6, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X6, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X6, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (5, (VUI16M1X6, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (6, (VUI16M1X6, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (7, (VUI16M1X6, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (8, (VUI16M1X6, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X6, VUI16M1X6, VUI16M1, SI))
+DEF_RISCV_FTYPE (0, (VUI16M1X7))
+DEF_RISCV_FTYPE (1, (VUI16M1X7, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1X7, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M1X7, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1X7, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M1X7, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1X7, VB16, VUI16M1X7, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1X7, VB16, VUI16M1X7, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M1X7, VB16, VUI16M1X7, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X7, VB16, VUI16M1X7, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1X7, VB16, VUI16M1X7, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI16M1X7, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X7, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X7, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (5, (VUI16M1X7, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (6, (VUI16M1X7, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (7, (VUI16M1X7, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (8, (VUI16M1X7, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X7, VUI16M1X7, VUI16M1, SI))
+DEF_RISCV_FTYPE (0, (VUI16M1X8))
+DEF_RISCV_FTYPE (1, (VUI16M1X8, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M1X8, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M1X8, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI16M1X8, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M1X8, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI16M1X8, VB16, VUI16M1X8, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M1X8, VB16, VUI16M1X8, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M1X8, VB16, VUI16M1X8, C_UHI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X8, VB16, VUI16M1X8, C_UHI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI16M1X8, VB16, VUI16M1X8, C_UHI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI16M1X8, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X8, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI16M1X8, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (5, (VUI16M1X8, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (6, (VUI16M1X8, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (7, (VUI16M1X8, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (8, (VUI16M1X8, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI16M1X8, VUI16M1X8, VUI16M1, SI))
+DEF_RISCV_FTYPE (0, (VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M2, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI16M2, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI16M2, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2, C_UHI_PTR, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI16M2, C_UHI_PTR, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI16M2, C_UHI_PTR, VUI64M8, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M2, C_UHI_PTR, VUI8M1, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, UHI))
+DEF_RISCV_FTYPE (1, (VUI16M2, VB8))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, C_UHI_PTR, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, C_UHI_PTR, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, C_UHI_PTR, VUI64M8, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, C_UHI_PTR, VUI8M1, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, VB8, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2, VB8, VUI16M2, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M2, VB8, VUI16M2, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, UHI, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, UQI, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M2, VB8, VUI16M2, VB8))
+DEF_RISCV_FTYPE (3, (VUI16M2, VB8, VUI16M2, VF16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2, VB8, VUI16M2, VF32M4))
+DEF_RISCV_FTYPE (3, (VUI16M2, VB8, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI16M2, LONG))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI16M2, SIZE))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI16M2, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI16M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI32M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2, VB8, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M2, VB8, VUI16M2, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI16M2, VF16M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, VF32M4))
+DEF_RISCV_FTYPE (1, (VUI16M2, VI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M2, VI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, VI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M2, VI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M2, VI32M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2, LONG))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2, SIZE))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2, UHI))
+DEF_RISCV_FTYPE (3, (VUI16M2, VUI16M2, UHI, VB8))
+DEF_RISCV_FTYPE (3, (VUI16M2, VUI16M2, UHI, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2, UQI))
+DEF_RISCV_FTYPE (3, (VUI16M2, VUI16M2, UQI, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2, VUI16M2, VUI16M2, SIZE))
+DEF_RISCV_FTYPE (3, (VUI16M2, VUI16M2, VUI16M2, VB8))
+DEF_RISCV_FTYPE (3, (VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M2, VUI16M2, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2X2, SI))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2X3, SI))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI16M2X4, SI))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI32M4, UQI))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI8M1, UQI))
+DEF_RISCV_FTYPE (2, (VUI16M2, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI16M2, VUI8M2))
+DEF_RISCV_FTYPE (0, (VUI16M2X2))
+DEF_RISCV_FTYPE (1, (VUI16M2X2, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M2X2, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M2X2, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2X2, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI16M2X2, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI16M2X2, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M2X2, VB8, VUI16M2X2, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M2X2, VB8, VUI16M2X2, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M2X2, VB8, VUI16M2X2, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2X2, VB8, VUI16M2X2, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI16M2X2, VB8, VUI16M2X2, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI16M2X2, VB8, VUI16M2X2, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI16M2X2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2X2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2X2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (5, (VUI16M2X2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (6, (VUI16M2X2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (7, (VUI16M2X2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (8, (VUI16M2X2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2X2, VUI16M2X2, VUI16M2, SI))
+DEF_RISCV_FTYPE (0, (VUI16M2X3))
+DEF_RISCV_FTYPE (1, (VUI16M2X3, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M2X3, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M2X3, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2X3, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI16M2X3, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI16M2X3, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M2X3, VB8, VUI16M2X3, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M2X3, VB8, VUI16M2X3, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M2X3, VB8, VUI16M2X3, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2X3, VB8, VUI16M2X3, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI16M2X3, VB8, VUI16M2X3, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI16M2X3, VB8, VUI16M2X3, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI16M2X3, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2X3, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2X3, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (5, (VUI16M2X3, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (6, (VUI16M2X3, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (7, (VUI16M2X3, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (8, (VUI16M2X3, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2X3, VUI16M2X3, VUI16M2, SI))
+DEF_RISCV_FTYPE (0, (VUI16M2X4))
+DEF_RISCV_FTYPE (1, (VUI16M2X4, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M2X4, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M2X4, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI16M2X4, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI16M2X4, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI16M2X4, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI16M2X4, VB8, VUI16M2X4, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M2X4, VB8, VUI16M2X4, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M2X4, VB8, VUI16M2X4, C_UHI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2X4, VB8, VUI16M2X4, C_UHI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI16M2X4, VB8, VUI16M2X4, C_UHI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI16M2X4, VB8, VUI16M2X4, C_UHI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI16M2X4, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2X4, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI16M2X4, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (5, (VUI16M2X4, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (6, (VUI16M2X4, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (7, (VUI16M2X4, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (8, (VUI16M2X4, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI16M2X4, VUI16M2X4, VUI16M2, SI))
+DEF_RISCV_FTYPE (0, (VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M4, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI16M4, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI16M4, C_UHI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI16M4, C_UHI_PTR, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI16M4, C_UHI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI16M4, C_UHI_PTR, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI16M4, C_UHI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI16M4, C_UHI_PTR, VUI8M2, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, UHI))
+DEF_RISCV_FTYPE (1, (VUI16M4, VB4))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, C_UHI_PTR, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, C_UHI_PTR, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, C_UHI_PTR, VUI8M2, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI16M4, VB4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI16M4, VB4, VUI16M4, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, C_UHI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, C_UHI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, C_UHI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI16M4, VB4, VUI16M4, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, UHI, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, UQI, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI16M4, VB4, VUI16M4, VB4))
+DEF_RISCV_FTYPE (3, (VUI16M4, VB4, VUI16M4, VF16M4))
+DEF_RISCV_FTYPE (3, (VUI16M4, VB4, VUI16M4, VF32M8))
+DEF_RISCV_FTYPE (3, (VUI16M4, VB4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI16M4, LONG))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI16M4, SIZE))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI16M4, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI16M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI32M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI16M4, VB4, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI8M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M4, VB4, VUI16M4, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI16M4, VF16M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, VF32M8))
+DEF_RISCV_FTYPE (1, (VUI16M4, VI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M4, VI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M4, VI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, VI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M4, VI32M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI16M4, LONG))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI16M4, SIZE))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI16M4, UHI))
+DEF_RISCV_FTYPE (3, (VUI16M4, VUI16M4, UHI, VB4))
+DEF_RISCV_FTYPE (3, (VUI16M4, VUI16M4, UHI, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI16M4, UQI))
+DEF_RISCV_FTYPE (3, (VUI16M4, VUI16M4, UQI, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI16M4, VUI16M4, VUI16M4, SIZE))
+DEF_RISCV_FTYPE (3, (VUI16M4, VUI16M4, VUI16M4, VB4))
+DEF_RISCV_FTYPE (3, (VUI16M4, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI16M4, VUI16M4, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI16M4X2, SI))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI32M8, UQI))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI8M2, UQI))
+DEF_RISCV_FTYPE (2, (VUI16M4, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI16M4, VUI8M4))
+DEF_RISCV_FTYPE (0, (VUI16M4X2))
+DEF_RISCV_FTYPE (1, (VUI16M4X2, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M4X2, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI16M4X2, C_UHI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI16M4X2, C_UHI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI16M4X2, C_UHI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI16M4X2, VB4, VUI16M4X2, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M4X2, VB4, VUI16M4X2, C_UHI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI16M4X2, VB4, VUI16M4X2, C_UHI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4X2, VB4, VUI16M4X2, C_UHI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI16M4X2, VB4, VUI16M4X2, C_UHI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI16M4X2, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI16M4X2, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI16M4X2, VUI16M4, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (5, (VUI16M4X2, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (6, (VUI16M4X2, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (7, (VUI16M4X2, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (8, (VUI16M4X2, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI16M4X2, VUI16M4X2, VUI16M4, SI))
+DEF_RISCV_FTYPE (0, (VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, C_UHI_PTR))
+DEF_RISCV_FTYPE (2, (VUI16M8, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI16M8, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI16M8, C_UHI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (3, (VUI16M8, C_UHI_PTR, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (2, (VUI16M8, C_UHI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI16M8, C_UHI_PTR, VUI8M4, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, UHI))
+DEF_RISCV_FTYPE (1, (VUI16M8, VB2))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, C_UHI_PTR, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, C_UHI_PTR, VUI8M4, VUI16M8))
+DEF_RISCV_FTYPE (2, (VUI16M8, VB2, VUI16M8))
+DEF_RISCV_FTYPE (3, (VUI16M8, VB2, VUI16M8, C_UHI_PTR))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, C_UHI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, C_UHI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, C_UHI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, C_UHI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI16M8, VB2, VUI16M8, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, UHI, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, UQI, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI16M8, VB2, VUI16M8, VB2))
+DEF_RISCV_FTYPE (3, (VUI16M8, VB2, VUI16M8, VF16M8))
+DEF_RISCV_FTYPE (3, (VUI16M8, VB2, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI16M8, LONG))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI16M8, SIZE))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI16M8, UHI))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI16M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI16M8, VB2, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI8M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI16M8, VB2, VUI16M8, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI16M8, VF16M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, VI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M8, VI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M8, VI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI16M8, LONG))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI16M8, SIZE))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI16M8, UHI))
+DEF_RISCV_FTYPE (3, (VUI16M8, VUI16M8, UHI, VB2))
+DEF_RISCV_FTYPE (3, (VUI16M8, VUI16M8, UHI, VUI16M8))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI16M8, UQI))
+DEF_RISCV_FTYPE (3, (VUI16M8, VUI16M8, UQI, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (3, (VUI16M8, VUI16M8, VUI16M8, SIZE))
+DEF_RISCV_FTYPE (3, (VUI16M8, VUI16M8, VUI16M8, VB2))
+DEF_RISCV_FTYPE (3, (VUI16M8, VUI16M8, VUI16M8, VUI16M8))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI16M8, VUI16M8, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI8M4, UQI))
+DEF_RISCV_FTYPE (2, (VUI16M8, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI16M8, VUI8M8))
+DEF_RISCV_FTYPE (0, (VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI32M1, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI32M1, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1, C_USI_PTR, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1, C_USI_PTR, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, USI))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB16, VUI32M1, VUI32M1, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB16, VUI32M1, VUI32M1, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB2, VUI32M1, VUI32M1, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI32M1, VB32))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, C_USI_PTR, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, C_USI_PTR, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1, VB32, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1, VB32, VUI32M1, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1, VB32, VUI32M1, USI))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, USI, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1, VB32, VUI32M1, VB32))
+DEF_RISCV_FTYPE (3, (VUI32M1, VB32, VUI32M1, VF32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1, VB32, VUI32M1, VF64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1, VB32, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, VUI32M1, LONG))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, VUI32M1, SIZE))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, VUI32M1, UQI))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, VUI32M1, USI))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, VUI64M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB32, VUI32M1, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB4, VUI32M1, VUI32M1, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB4, VUI32M1, VUI32M1, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB8, VUI32M1, VUI32M1, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI32M1, VB8, VUI32M1, VUI32M1, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M1, VF32M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, VF64M2))
+DEF_RISCV_FTYPE (1, (VUI32M1, VI16M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, VI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, VI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M1, VI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M1, VI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1, LONG))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1, SIZE))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1, UQI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1, USI))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, USI, VB32))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, USI, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, SIZE))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VB32))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI16M8))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M1, VUI32M1, VUI32M1, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1X2, SI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1X3, SI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1X4, SI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1X5, SI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1X6, SI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1X7, SI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI32M1X8, SI))
+DEF_RISCV_FTYPE (1, (VUI32M1, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M1, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M1, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M1, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI64M2, UQI))
+DEF_RISCV_FTYPE (2, (VUI32M1, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M1, VUI8M1))
+DEF_RISCV_FTYPE (0, (VUI32M1X2))
+DEF_RISCV_FTYPE (1, (VUI32M1X2, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1X2, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M1X2, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1X2, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1X2, VB32, VUI32M1X2, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1X2, VB32, VUI32M1X2, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M1X2, VB32, VUI32M1X2, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X2, VB32, VUI32M1X2, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M1X2, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X2, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X2, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (5, (VUI32M1X2, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (6, (VUI32M1X2, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (7, (VUI32M1X2, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (8, (VUI32M1X2, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X2, VUI32M1X2, VUI32M1, SI))
+DEF_RISCV_FTYPE (0, (VUI32M1X3))
+DEF_RISCV_FTYPE (1, (VUI32M1X3, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1X3, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M1X3, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1X3, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1X3, VB32, VUI32M1X3, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1X3, VB32, VUI32M1X3, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M1X3, VB32, VUI32M1X3, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X3, VB32, VUI32M1X3, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M1X3, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X3, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X3, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (5, (VUI32M1X3, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (6, (VUI32M1X3, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (7, (VUI32M1X3, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (8, (VUI32M1X3, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X3, VUI32M1X3, VUI32M1, SI))
+DEF_RISCV_FTYPE (0, (VUI32M1X4))
+DEF_RISCV_FTYPE (1, (VUI32M1X4, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1X4, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M1X4, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1X4, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1X4, VB32, VUI32M1X4, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1X4, VB32, VUI32M1X4, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M1X4, VB32, VUI32M1X4, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X4, VB32, VUI32M1X4, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M1X4, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X4, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X4, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (5, (VUI32M1X4, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (6, (VUI32M1X4, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (7, (VUI32M1X4, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (8, (VUI32M1X4, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X4, VUI32M1X4, VUI32M1, SI))
+DEF_RISCV_FTYPE (0, (VUI32M1X5))
+DEF_RISCV_FTYPE (1, (VUI32M1X5, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1X5, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M1X5, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1X5, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1X5, VB32, VUI32M1X5, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1X5, VB32, VUI32M1X5, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M1X5, VB32, VUI32M1X5, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X5, VB32, VUI32M1X5, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M1X5, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X5, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X5, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (5, (VUI32M1X5, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (6, (VUI32M1X5, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (7, (VUI32M1X5, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (8, (VUI32M1X5, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X5, VUI32M1X5, VUI32M1, SI))
+DEF_RISCV_FTYPE (0, (VUI32M1X6))
+DEF_RISCV_FTYPE (1, (VUI32M1X6, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1X6, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M1X6, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1X6, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1X6, VB32, VUI32M1X6, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1X6, VB32, VUI32M1X6, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M1X6, VB32, VUI32M1X6, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X6, VB32, VUI32M1X6, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M1X6, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X6, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X6, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (5, (VUI32M1X6, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (6, (VUI32M1X6, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (7, (VUI32M1X6, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (8, (VUI32M1X6, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X6, VUI32M1X6, VUI32M1, SI))
+DEF_RISCV_FTYPE (0, (VUI32M1X7))
+DEF_RISCV_FTYPE (1, (VUI32M1X7, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1X7, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M1X7, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1X7, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1X7, VB32, VUI32M1X7, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1X7, VB32, VUI32M1X7, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M1X7, VB32, VUI32M1X7, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X7, VB32, VUI32M1X7, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M1X7, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X7, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X7, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (5, (VUI32M1X7, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (6, (VUI32M1X7, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (7, (VUI32M1X7, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (8, (VUI32M1X7, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X7, VUI32M1X7, VUI32M1, SI))
+DEF_RISCV_FTYPE (0, (VUI32M1X8))
+DEF_RISCV_FTYPE (1, (VUI32M1X8, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M1X8, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M1X8, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI32M1X8, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI32M1X8, VB32, VUI32M1X8, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M1X8, VB32, VUI32M1X8, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M1X8, VB32, VUI32M1X8, C_USI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X8, VB32, VUI32M1X8, C_USI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M1X8, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X8, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI32M1X8, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (5, (VUI32M1X8, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (6, (VUI32M1X8, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (7, (VUI32M1X8, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (8, (VUI32M1X8, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI32M1X8, VUI32M1X8, VUI32M1, SI))
+DEF_RISCV_FTYPE (0, (VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M2, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI32M2, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI32M2, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI32M2, C_USI_PTR, VUI16M1, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2, C_USI_PTR, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI32M2, C_USI_PTR, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, USI))
+DEF_RISCV_FTYPE (1, (VUI32M2, VB16))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, C_USI_PTR, VUI16M1, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, C_USI_PTR, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, C_USI_PTR, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2, VB16, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, UHI, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, USI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, USI, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, VB16))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, VF16M1))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, VF32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, VF64M4))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI16M1, UHI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI32M2, VB16, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI32M2, LONG))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI32M2, SIZE))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI32M2, UHI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI32M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI32M2, USI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI64M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI32M2, VB16, VUI32M2, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VF16M1))
+DEF_RISCV_FTYPE (1, (VUI32M2, VF32M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VF64M4))
+DEF_RISCV_FTYPE (1, (VUI32M2, VI16M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M2, VI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M2, VI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI16M1, UHI))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2, LONG))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2, SIZE))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2, UHI))
+DEF_RISCV_FTYPE (3, (VUI32M2, VUI32M2, UHI, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2, UQI))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2, USI))
+DEF_RISCV_FTYPE (3, (VUI32M2, VUI32M2, USI, VB16))
+DEF_RISCV_FTYPE (3, (VUI32M2, VUI32M2, USI, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI32M2, VUI32M2, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2, VUI32M2, VUI32M2, SIZE))
+DEF_RISCV_FTYPE (3, (VUI32M2, VUI32M2, VUI32M2, VB16))
+DEF_RISCV_FTYPE (3, (VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2X2, SI))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2X3, SI))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI32M2X4, SI))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI64M4, UQI))
+DEF_RISCV_FTYPE (2, (VUI32M2, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M2, VUI8M2))
+DEF_RISCV_FTYPE (0, (VUI32M2X2))
+DEF_RISCV_FTYPE (1, (VUI32M2X2, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M2X2, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M2X2, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI32M2X2, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2X2, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI32M2X2, VB16, VUI32M2X2, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M2X2, VB16, VUI32M2X2, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M2X2, VB16, VUI32M2X2, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI32M2X2, VB16, VUI32M2X2, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2X2, VB16, VUI32M2X2, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI32M2X2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2X2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2X2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (5, (VUI32M2X2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (6, (VUI32M2X2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (7, (VUI32M2X2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (8, (VUI32M2X2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2X2, VUI32M2X2, VUI32M2, SI))
+DEF_RISCV_FTYPE (0, (VUI32M2X3))
+DEF_RISCV_FTYPE (1, (VUI32M2X3, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M2X3, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M2X3, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI32M2X3, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2X3, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI32M2X3, VB16, VUI32M2X3, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M2X3, VB16, VUI32M2X3, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M2X3, VB16, VUI32M2X3, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI32M2X3, VB16, VUI32M2X3, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2X3, VB16, VUI32M2X3, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI32M2X3, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2X3, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2X3, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (5, (VUI32M2X3, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (6, (VUI32M2X3, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (7, (VUI32M2X3, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (8, (VUI32M2X3, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2X3, VUI32M2X3, VUI32M2, SI))
+DEF_RISCV_FTYPE (0, (VUI32M2X4))
+DEF_RISCV_FTYPE (1, (VUI32M2X4, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M2X4, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M2X4, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI32M2X4, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI32M2X4, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI32M2X4, VB16, VUI32M2X4, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M2X4, VB16, VUI32M2X4, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M2X4, VB16, VUI32M2X4, C_USI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI32M2X4, VB16, VUI32M2X4, C_USI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2X4, VB16, VUI32M2X4, C_USI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI32M2X4, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2X4, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI32M2X4, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (5, (VUI32M2X4, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (6, (VUI32M2X4, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (7, (VUI32M2X4, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (8, (VUI32M2X4, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI32M2X4, VUI32M2X4, VUI32M2, SI))
+DEF_RISCV_FTYPE (0, (VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M4, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI32M4, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI32M4, C_USI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI32M4, C_USI_PTR, VUI16M2, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI32M4, C_USI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4, C_USI_PTR, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI32M4, C_USI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI32M4, C_USI_PTR, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI32M4, C_USI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI32M4, C_USI_PTR, VUI8M1, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, USI))
+DEF_RISCV_FTYPE (1, (VUI32M4, VB8))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, C_USI_PTR, VUI16M2, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, C_USI_PTR, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, C_USI_PTR, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, C_USI_PTR, VUI8M1, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI32M4, VB8, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, C_USI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, C_USI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, C_USI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, C_USI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, UHI, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, UQI, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, USI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, USI, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, VB8))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, VF16M2))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, VF32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, VF64M8))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI16M2, UHI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI32M4, LONG))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI32M4, SIZE))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI32M4, UHI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI32M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI32M4, USI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI64M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4, VB8, VUI32M4, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI32M4, VB8, VUI32M4, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI32M4, VF16M2))
+DEF_RISCV_FTYPE (1, (VUI32M4, VF32M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, VF64M8))
+DEF_RISCV_FTYPE (1, (VUI32M4, VI16M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, VI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M4, VI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M4, VI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, VI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI16M2, UHI))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4, LONG))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4, SIZE))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4, UHI))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, UHI, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4, UQI))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, UQI, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4, USI))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, USI, VB8))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, USI, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, VUI32M4, SIZE))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, VUI32M4, VB8))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4, VUI32M4, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI32M4X2, SI))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI64M8, UQI))
+DEF_RISCV_FTYPE (2, (VUI32M4, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI32M4, VUI8M4))
+DEF_RISCV_FTYPE (0, (VUI32M4X2))
+DEF_RISCV_FTYPE (1, (VUI32M4X2, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M4X2, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI32M4X2, C_USI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI32M4X2, C_USI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI32M4X2, C_USI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI32M4X2, C_USI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI32M4X2, VB8, VUI32M4X2, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M4X2, VB8, VUI32M4X2, C_USI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI32M4X2, VB8, VUI32M4X2, C_USI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI32M4X2, VB8, VUI32M4X2, C_USI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4X2, VB8, VUI32M4X2, C_USI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI32M4X2, VB8, VUI32M4X2, C_USI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI32M4X2, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4X2, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI32M4X2, VUI32M4, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (5, (VUI32M4X2, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (6, (VUI32M4X2, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (7, (VUI32M4X2, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (8, (VUI32M4X2, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI32M4X2, VUI32M4X2, VUI32M4, SI))
+DEF_RISCV_FTYPE (0, (VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, C_USI_PTR))
+DEF_RISCV_FTYPE (2, (VUI32M8, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI32M8, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI32M8, C_USI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI32M8, C_USI_PTR, VUI16M4, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI32M8, C_USI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI32M8, C_USI_PTR, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI32M8, C_USI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI32M8, C_USI_PTR, VUI8M2, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, USI))
+DEF_RISCV_FTYPE (1, (VUI32M8, VB4))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, C_USI_PTR, VUI16M4, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, C_USI_PTR, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, C_USI_PTR, VUI8M2, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI32M8, VB4, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, C_USI_PTR))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, C_USI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, C_USI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, C_USI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, C_USI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, C_USI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, UHI, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, UQI, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, USI))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, USI, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, VB4))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, VF16M4))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, VF32M8))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI16M4, UHI))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI32M8, LONG))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI32M8, SIZE))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI32M8, UHI))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI32M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI32M8, USI))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI32M8, VB4, VUI32M8, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI32M8, VB4, VUI32M8, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI32M8, VF16M4))
+DEF_RISCV_FTYPE (1, (VUI32M8, VF32M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, VI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M8, VI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M8, VI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI16M4, UHI))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI32M8, LONG))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI32M8, SIZE))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI32M8, UHI))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, UHI, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI32M8, UQI))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, UQI, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI32M8, USI))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, USI, VB4))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, USI, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI32M8, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, VUI16M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI32M8, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, VUI32M8, SIZE))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, VUI32M8, VB4))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, VUI32M8, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI32M8, VUI32M8, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI32M8, VUI8M8))
+DEF_RISCV_FTYPE (0, (VUI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI64M1, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI64M1, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, C_UDI_PTR, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB16, VUI64M1, VUI64M1, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB16, VUI64M1, VUI64M1, VUI64M4))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB32, VUI64M1, VUI64M1, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB32, VUI64M1, VUI64M1, VUI64M2))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB4, VUI64M1, VUI64M1, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI64M1, VB64))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, C_UDI_PTR, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1, VB64, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, VB64, VUI64M1, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, VB64, VUI64M1, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, UDI, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, VB64, VUI64M1, VB64))
+DEF_RISCV_FTYPE (3, (VUI64M1, VB64, VUI64M1, VF64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, VB64, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, VUI64M1, LONG))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, VUI64M1, SIZE))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, VUI64M1, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, VUI64M1, UQI))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB64, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB8, VUI64M1, VUI64M1, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI64M1, VB8, VUI64M1, VUI64M1, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M1, VF64M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, VI16M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, VI32M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, VI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M1, VI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M1, VI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1, LONG))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1, SIZE))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1, UDI))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, UDI, VB64))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, UDI, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1, UQI))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, SIZE))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VB64))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M1, VUI64M1, VUI64M1, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1X2, SI))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1X3, SI))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1X4, SI))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1X5, SI))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1X6, SI))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1X7, SI))
+DEF_RISCV_FTYPE (2, (VUI64M1, VUI64M1X8, SI))
+DEF_RISCV_FTYPE (1, (VUI64M1, VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M1, VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M1, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M1, VUI8M1))
+DEF_RISCV_FTYPE (0, (VUI64M1X2))
+DEF_RISCV_FTYPE (1, (VUI64M1X2, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1X2, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M1X2, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X2, VB64, VUI64M1X2, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1X2, VB64, VUI64M1X2, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M1X2, VB64, VUI64M1X2, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1X2, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X2, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1X2, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (5, (VUI64M1X2, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (6, (VUI64M1X2, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (7, (VUI64M1X2, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (8, (VUI64M1X2, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X2, VUI64M1X2, VUI64M1, SI))
+DEF_RISCV_FTYPE (0, (VUI64M1X3))
+DEF_RISCV_FTYPE (1, (VUI64M1X3, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1X3, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M1X3, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X3, VB64, VUI64M1X3, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1X3, VB64, VUI64M1X3, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M1X3, VB64, VUI64M1X3, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1X3, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X3, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1X3, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (5, (VUI64M1X3, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (6, (VUI64M1X3, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (7, (VUI64M1X3, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (8, (VUI64M1X3, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X3, VUI64M1X3, VUI64M1, SI))
+DEF_RISCV_FTYPE (0, (VUI64M1X4))
+DEF_RISCV_FTYPE (1, (VUI64M1X4, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1X4, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M1X4, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X4, VB64, VUI64M1X4, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1X4, VB64, VUI64M1X4, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M1X4, VB64, VUI64M1X4, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1X4, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X4, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1X4, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (5, (VUI64M1X4, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (6, (VUI64M1X4, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (7, (VUI64M1X4, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (8, (VUI64M1X4, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X4, VUI64M1X4, VUI64M1, SI))
+DEF_RISCV_FTYPE (0, (VUI64M1X5))
+DEF_RISCV_FTYPE (1, (VUI64M1X5, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1X5, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M1X5, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X5, VB64, VUI64M1X5, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1X5, VB64, VUI64M1X5, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M1X5, VB64, VUI64M1X5, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1X5, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X5, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1X5, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (5, (VUI64M1X5, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (6, (VUI64M1X5, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (7, (VUI64M1X5, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (8, (VUI64M1X5, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X5, VUI64M1X5, VUI64M1, SI))
+DEF_RISCV_FTYPE (0, (VUI64M1X6))
+DEF_RISCV_FTYPE (1, (VUI64M1X6, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1X6, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M1X6, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X6, VB64, VUI64M1X6, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1X6, VB64, VUI64M1X6, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M1X6, VB64, VUI64M1X6, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1X6, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X6, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1X6, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (5, (VUI64M1X6, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (6, (VUI64M1X6, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (7, (VUI64M1X6, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (8, (VUI64M1X6, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X6, VUI64M1X6, VUI64M1, SI))
+DEF_RISCV_FTYPE (0, (VUI64M1X7))
+DEF_RISCV_FTYPE (1, (VUI64M1X7, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1X7, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M1X7, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X7, VB64, VUI64M1X7, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1X7, VB64, VUI64M1X7, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M1X7, VB64, VUI64M1X7, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1X7, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X7, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1X7, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (5, (VUI64M1X7, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (6, (VUI64M1X7, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (7, (VUI64M1X7, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (8, (VUI64M1X7, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X7, VUI64M1X7, VUI64M1, SI))
+DEF_RISCV_FTYPE (0, (VUI64M1X8))
+DEF_RISCV_FTYPE (1, (VUI64M1X8, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M1X8, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M1X8, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X8, VB64, VUI64M1X8, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M1X8, VB64, VUI64M1X8, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M1X8, VB64, VUI64M1X8, C_UDI_PTR, VUI64M1))
+DEF_RISCV_FTYPE (2, (VUI64M1X8, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X8, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (4, (VUI64M1X8, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (5, (VUI64M1X8, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (6, (VUI64M1X8, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (7, (VUI64M1X8, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (8, (VUI64M1X8, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1, VUI64M1))
+DEF_RISCV_FTYPE (3, (VUI64M1X8, VUI64M1X8, VUI64M1, SI))
+DEF_RISCV_FTYPE (0, (VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M2, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI64M2, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI64M2, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI64M2, C_UDI_PTR, VUI32M1, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2, C_UDI_PTR, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, UDI))
+DEF_RISCV_FTYPE (1, (VUI64M2, VB32))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, C_UDI_PTR, VUI32M1, VUI64M2))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, C_UDI_PTR, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2, VB32, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2, VB32, VUI64M2, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2, VB32, VUI64M2, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, UDI, VUI64M2))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, USI, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI64M2, VB32, VUI64M2, VB32))
+DEF_RISCV_FTYPE (3, (VUI64M2, VB32, VUI64M2, VF32M1))
+DEF_RISCV_FTYPE (3, (VUI64M2, VB32, VUI64M2, VF64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2, VB32, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI32M1, USI))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI64M2, VB32, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI64M2, LONG))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI64M2, SIZE))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI64M2, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI64M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI64M2, USI))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI64M2, VB32, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VF32M1))
+DEF_RISCV_FTYPE (1, (VUI64M2, VF64M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VI16M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VI32M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M2, VI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI32M1, USI))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2, LONG))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2, SIZE))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2, UDI))
+DEF_RISCV_FTYPE (3, (VUI64M2, VUI64M2, UDI, VB32))
+DEF_RISCV_FTYPE (3, (VUI64M2, VUI64M2, UDI, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2, UQI))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2, USI))
+DEF_RISCV_FTYPE (3, (VUI64M2, VUI64M2, USI, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2, VUI32M1))
+DEF_RISCV_FTYPE (3, (VUI64M2, VUI64M2, VUI32M1, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2, VUI64M2, VUI64M2, SIZE))
+DEF_RISCV_FTYPE (3, (VUI64M2, VUI64M2, VUI64M2, VB32))
+DEF_RISCV_FTYPE (3, (VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2X2, SI))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2X3, SI))
+DEF_RISCV_FTYPE (2, (VUI64M2, VUI64M2X4, SI))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M2, VUI8M2))
+DEF_RISCV_FTYPE (0, (VUI64M2X2))
+DEF_RISCV_FTYPE (1, (VUI64M2X2, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M2X2, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M2X2, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI64M2X2, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X2, VB32, VUI64M2X2, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M2X2, VB32, VUI64M2X2, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M2X2, VB32, VUI64M2X2, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI64M2X2, VB32, VUI64M2X2, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2X2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (4, (VUI64M2X2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (5, (VUI64M2X2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (6, (VUI64M2X2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (7, (VUI64M2X2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (8, (VUI64M2X2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X2, VUI64M2X2, VUI64M2, SI))
+DEF_RISCV_FTYPE (0, (VUI64M2X3))
+DEF_RISCV_FTYPE (1, (VUI64M2X3, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M2X3, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M2X3, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI64M2X3, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X3, VB32, VUI64M2X3, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M2X3, VB32, VUI64M2X3, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M2X3, VB32, VUI64M2X3, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI64M2X3, VB32, VUI64M2X3, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2X3, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X3, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (4, (VUI64M2X3, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (5, (VUI64M2X3, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (6, (VUI64M2X3, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (7, (VUI64M2X3, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (8, (VUI64M2X3, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X3, VUI64M2X3, VUI64M2, SI))
+DEF_RISCV_FTYPE (0, (VUI64M2X4))
+DEF_RISCV_FTYPE (1, (VUI64M2X4, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M2X4, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M2X4, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (2, (VUI64M2X4, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X4, VB32, VUI64M2X4, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M2X4, VB32, VUI64M2X4, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M2X4, VB32, VUI64M2X4, C_UDI_PTR, VUI32M1))
+DEF_RISCV_FTYPE (4, (VUI64M2X4, VB32, VUI64M2X4, C_UDI_PTR, VUI64M2))
+DEF_RISCV_FTYPE (2, (VUI64M2X4, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X4, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (4, (VUI64M2X4, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (5, (VUI64M2X4, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (6, (VUI64M2X4, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (7, (VUI64M2X4, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (8, (VUI64M2X4, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2, VUI64M2))
+DEF_RISCV_FTYPE (3, (VUI64M2X4, VUI64M2X4, VUI64M2, SI))
+DEF_RISCV_FTYPE (0, (VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M4, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI64M4, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI64M4, C_UDI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI64M4, C_UDI_PTR, VUI16M1, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI64M4, C_UDI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI64M4, C_UDI_PTR, VUI32M2, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI64M4, C_UDI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4, C_UDI_PTR, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, UDI))
+DEF_RISCV_FTYPE (1, (VUI64M4, VB16))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, C_UDI_PTR, VUI16M1, VUI64M4))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, C_UDI_PTR, VUI32M2, VUI64M4))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, C_UDI_PTR, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI64M4, VB16, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, C_UDI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, C_UDI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, C_UDI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, UDI, VUI64M4))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, UHI, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, USI, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, VB16))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, VF32M2))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, VF64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI32M2, USI))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI64M4, VB16, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI64M4, LONG))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI64M4, SIZE))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI64M4, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI64M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI64M4, USI))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI64M4, VB16, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VF32M2))
+DEF_RISCV_FTYPE (1, (VUI64M4, VF64M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VI16M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VI32M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M4, VI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI16M1))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI16M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI32M2, USI))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4, LONG))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4, SIZE))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4, UDI))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, UDI, VB16))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, UDI, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, UHI, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4, UQI))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4, USI))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, USI, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, VUI16M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4, VUI32M2))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, VUI32M2, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, VUI64M4, SIZE))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, VUI64M4, VB16))
+DEF_RISCV_FTYPE (3, (VUI64M4, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI64M4, VUI64M4X2, SI))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M4, VUI8M4))
+DEF_RISCV_FTYPE (0, (VUI64M4X2))
+DEF_RISCV_FTYPE (1, (VUI64M4X2, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M4X2, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI64M4X2, C_UDI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI64M4X2, C_UDI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (2, (VUI64M4X2, C_UDI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4X2, VB16, VUI64M4X2, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M4X2, VB16, VUI64M4X2, C_UDI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI64M4X2, VB16, VUI64M4X2, C_UDI_PTR, VUI16M1))
+DEF_RISCV_FTYPE (4, (VUI64M4X2, VB16, VUI64M4X2, C_UDI_PTR, VUI32M2))
+DEF_RISCV_FTYPE (4, (VUI64M4X2, VB16, VUI64M4X2, C_UDI_PTR, VUI64M4))
+DEF_RISCV_FTYPE (2, (VUI64M4X2, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4X2, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (4, (VUI64M4X2, VUI64M4, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (5, (VUI64M4X2, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (6, (VUI64M4X2, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (7, (VUI64M4X2, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (8, (VUI64M4X2, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4, VUI64M4))
+DEF_RISCV_FTYPE (3, (VUI64M4X2, VUI64M4X2, VUI64M4, SI))
+DEF_RISCV_FTYPE (0, (VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, C_UDI_PTR))
+DEF_RISCV_FTYPE (2, (VUI64M8, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI64M8, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI64M8, C_UDI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI64M8, C_UDI_PTR, VUI16M2, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI64M8, C_UDI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI64M8, C_UDI_PTR, VUI32M4, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI64M8, C_UDI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI64M8, C_UDI_PTR, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI64M8, C_UDI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI64M8, C_UDI_PTR, VUI8M1, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, UDI))
+DEF_RISCV_FTYPE (1, (VUI64M8, VB8))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, C_UDI_PTR, VUI16M2, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, C_UDI_PTR, VUI32M4, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, C_UDI_PTR, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, C_UDI_PTR, VUI8M1, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI64M8, VB8, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, C_UDI_PTR))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, C_UDI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, C_UDI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, C_UDI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, C_UDI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, C_UDI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, C_UDI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, UDI, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, UHI, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, USI, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, VB8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, VF32M4))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, VF64M8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI32M4, USI))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI64M8, LONG))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI64M8, SIZE))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI64M8, UDI))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI64M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI64M8, USI))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI64M8, VB8, VUI64M8, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VB8, VUI64M8, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI64M8, VF32M4))
+DEF_RISCV_FTYPE (1, (VUI64M8, VF64M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M8, VI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M8, VI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI16M2))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI32M4, USI))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI64M1))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI64M8, LONG))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI64M8, SIZE))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI64M8, UDI))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, UDI, VB8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, UDI, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, UHI, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI64M8, UQI))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI64M8, USI))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, USI, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, VUI16M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI64M8, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, VUI32M4, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI64M8, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, VUI64M8, SIZE))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, VUI64M8, VB8))
+DEF_RISCV_FTYPE (3, (VUI64M8, VUI64M8, VUI64M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI64M8, VUI8M8))
+DEF_RISCV_FTYPE (0, (VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI8M1, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI8M1, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (3, (VUI8M1, C_UQI_PTR, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (3, (VUI8M1, C_UQI_PTR, VUI32M4, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (3, (VUI8M1, C_UQI_PTR, VUI64M8, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1, C_UQI_PTR, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB1, VUI8M1, VUI8M1, VUI8M8))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB2, VUI8M1, VUI8M1, VUI8M4))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB4, VUI8M1, VUI8M1, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M1, VB8))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, C_UQI_PTR, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, C_UQI_PTR, VUI32M4, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, C_UQI_PTR, VUI64M8, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, C_UQI_PTR, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1, VB8, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1, VB8, VUI8M1, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1, VB8, VUI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, UQI, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1, VB8, VUI8M1, VB8))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, VUI16M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1, VB8, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, VUI8M1, LONG))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, VUI8M1, SIZE))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, VUI8M1, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M1, VB8, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VI16M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VI32M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VI64M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M1, VI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M1, VI8M8))
+DEF_RISCV_FTYPE (1, (VUI8M1, VUI16M1))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI16M2, UQI))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI16M2, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VUI32M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VUI64M1))
+DEF_RISCV_FTYPE (1, (VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1, LONG))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1, SIZE))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1, UQI))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, UQI, VB8))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, UQI, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, VUI8M1, SIZE))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, VUI8M1, VB8))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, VUI8M1, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, VUI8M1, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M1, VUI8M1, VUI8M1, VUI8M8))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1X2, SI))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1X3, SI))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1X4, SI))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1X5, SI))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1X6, SI))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1X7, SI))
+DEF_RISCV_FTYPE (2, (VUI8M1, VUI8M1X8, SI))
+DEF_RISCV_FTYPE (1, (VUI8M1, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M1, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M1, VUI8M8))
+DEF_RISCV_FTYPE (0, (VUI8M1X2))
+DEF_RISCV_FTYPE (1, (VUI8M1X2, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1X2, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M1X2, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M1X2, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI8M1X2, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI8M1X2, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X2, VB8, VUI8M1X2, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1X2, VB8, VUI8M1X2, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M1X2, VB8, VUI8M1X2, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1X2, VB8, VUI8M1X2, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1X2, VB8, VUI8M1X2, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1X2, VB8, VUI8M1X2, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1X2, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X2, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1X2, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (5, (VUI8M1X2, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (6, (VUI8M1X2, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (7, (VUI8M1X2, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (8, (VUI8M1X2, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X2, VUI8M1X2, VUI8M1, SI))
+DEF_RISCV_FTYPE (0, (VUI8M1X3))
+DEF_RISCV_FTYPE (1, (VUI8M1X3, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1X3, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M1X3, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M1X3, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI8M1X3, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI8M1X3, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X3, VB8, VUI8M1X3, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1X3, VB8, VUI8M1X3, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M1X3, VB8, VUI8M1X3, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1X3, VB8, VUI8M1X3, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1X3, VB8, VUI8M1X3, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1X3, VB8, VUI8M1X3, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1X3, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X3, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1X3, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (5, (VUI8M1X3, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (6, (VUI8M1X3, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (7, (VUI8M1X3, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (8, (VUI8M1X3, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X3, VUI8M1X3, VUI8M1, SI))
+DEF_RISCV_FTYPE (0, (VUI8M1X4))
+DEF_RISCV_FTYPE (1, (VUI8M1X4, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1X4, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M1X4, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M1X4, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI8M1X4, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI8M1X4, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X4, VB8, VUI8M1X4, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1X4, VB8, VUI8M1X4, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M1X4, VB8, VUI8M1X4, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1X4, VB8, VUI8M1X4, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1X4, VB8, VUI8M1X4, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1X4, VB8, VUI8M1X4, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1X4, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X4, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1X4, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (5, (VUI8M1X4, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (6, (VUI8M1X4, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (7, (VUI8M1X4, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (8, (VUI8M1X4, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X4, VUI8M1X4, VUI8M1, SI))
+DEF_RISCV_FTYPE (0, (VUI8M1X5))
+DEF_RISCV_FTYPE (1, (VUI8M1X5, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1X5, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M1X5, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M1X5, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI8M1X5, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI8M1X5, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X5, VB8, VUI8M1X5, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1X5, VB8, VUI8M1X5, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M1X5, VB8, VUI8M1X5, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1X5, VB8, VUI8M1X5, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1X5, VB8, VUI8M1X5, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1X5, VB8, VUI8M1X5, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1X5, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X5, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1X5, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (5, (VUI8M1X5, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (6, (VUI8M1X5, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (7, (VUI8M1X5, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (8, (VUI8M1X5, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X5, VUI8M1X5, VUI8M1, SI))
+DEF_RISCV_FTYPE (0, (VUI8M1X6))
+DEF_RISCV_FTYPE (1, (VUI8M1X6, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1X6, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M1X6, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M1X6, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI8M1X6, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI8M1X6, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X6, VB8, VUI8M1X6, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1X6, VB8, VUI8M1X6, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M1X6, VB8, VUI8M1X6, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1X6, VB8, VUI8M1X6, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1X6, VB8, VUI8M1X6, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1X6, VB8, VUI8M1X6, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1X6, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X6, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1X6, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (5, (VUI8M1X6, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (6, (VUI8M1X6, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (7, (VUI8M1X6, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (8, (VUI8M1X6, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X6, VUI8M1X6, VUI8M1, SI))
+DEF_RISCV_FTYPE (0, (VUI8M1X7))
+DEF_RISCV_FTYPE (1, (VUI8M1X7, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1X7, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M1X7, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M1X7, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI8M1X7, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI8M1X7, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X7, VB8, VUI8M1X7, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1X7, VB8, VUI8M1X7, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M1X7, VB8, VUI8M1X7, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1X7, VB8, VUI8M1X7, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1X7, VB8, VUI8M1X7, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1X7, VB8, VUI8M1X7, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1X7, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X7, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1X7, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (5, (VUI8M1X7, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (6, (VUI8M1X7, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (7, (VUI8M1X7, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (8, (VUI8M1X7, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X7, VUI8M1X7, VUI8M1, SI))
+DEF_RISCV_FTYPE (0, (VUI8M1X8))
+DEF_RISCV_FTYPE (1, (VUI8M1X8, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M1X8, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M1X8, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M1X8, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (2, (VUI8M1X8, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (2, (VUI8M1X8, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X8, VB8, VUI8M1X8, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M1X8, VB8, VUI8M1X8, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M1X8, VB8, VUI8M1X8, C_UQI_PTR, VUI16M2))
+DEF_RISCV_FTYPE (4, (VUI8M1X8, VB8, VUI8M1X8, C_UQI_PTR, VUI32M4))
+DEF_RISCV_FTYPE (4, (VUI8M1X8, VB8, VUI8M1X8, C_UQI_PTR, VUI64M8))
+DEF_RISCV_FTYPE (4, (VUI8M1X8, VB8, VUI8M1X8, C_UQI_PTR, VUI8M1))
+DEF_RISCV_FTYPE (2, (VUI8M1X8, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X8, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (4, (VUI8M1X8, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (5, (VUI8M1X8, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (6, (VUI8M1X8, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (7, (VUI8M1X8, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (8, (VUI8M1X8, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1, VUI8M1))
+DEF_RISCV_FTYPE (3, (VUI8M1X8, VUI8M1X8, VUI8M1, SI))
+DEF_RISCV_FTYPE (0, (VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M2, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI8M2, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI8M2, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (3, (VUI8M2, C_UQI_PTR, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (3, (VUI8M2, C_UQI_PTR, VUI32M8, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2, C_UQI_PTR, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, UQI))
+DEF_RISCV_FTYPE (1, (VUI8M2, VB4))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, C_UQI_PTR, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, C_UQI_PTR, VUI32M8, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, C_UQI_PTR, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2, VB4, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2, VB4, VUI8M2, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2, VB4, VUI8M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, UQI, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2, VB4, VUI8M2, VB4))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, VUI16M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2, VB4, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, VUI8M2, LONG))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, VUI8M2, SIZE))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, VUI8M2, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M2, VB4, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VI16M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VI32M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VI64M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M2, VI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M2, VI8M8))
+DEF_RISCV_FTYPE (1, (VUI8M2, VUI16M2))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI16M4, UQI))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI16M4, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VUI32M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VUI64M2))
+DEF_RISCV_FTYPE (1, (VUI8M2, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI8M2, LONG))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI8M2, SIZE))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI8M2, UQI))
+DEF_RISCV_FTYPE (3, (VUI8M2, VUI8M2, UQI, VB4))
+DEF_RISCV_FTYPE (3, (VUI8M2, VUI8M2, UQI, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2, VUI8M2, VUI8M2, SIZE))
+DEF_RISCV_FTYPE (3, (VUI8M2, VUI8M2, VUI8M2, VB4))
+DEF_RISCV_FTYPE (3, (VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI8M2X2, SI))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI8M2X3, SI))
+DEF_RISCV_FTYPE (2, (VUI8M2, VUI8M2X4, SI))
+DEF_RISCV_FTYPE (1, (VUI8M2, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M2, VUI8M8))
+DEF_RISCV_FTYPE (0, (VUI8M2X2))
+DEF_RISCV_FTYPE (1, (VUI8M2X2, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M2X2, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M2X2, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI8M2X2, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI8M2X2, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X2, VB4, VUI8M2X2, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M2X2, VB4, VUI8M2X2, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M2X2, VB4, VUI8M2X2, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI8M2X2, VB4, VUI8M2X2, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI8M2X2, VB4, VUI8M2X2, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2X2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI8M2X2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (5, (VUI8M2X2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (6, (VUI8M2X2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (7, (VUI8M2X2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (8, (VUI8M2X2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X2, VUI8M2X2, VUI8M2, SI))
+DEF_RISCV_FTYPE (0, (VUI8M2X3))
+DEF_RISCV_FTYPE (1, (VUI8M2X3, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M2X3, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M2X3, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI8M2X3, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI8M2X3, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X3, VB4, VUI8M2X3, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M2X3, VB4, VUI8M2X3, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M2X3, VB4, VUI8M2X3, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI8M2X3, VB4, VUI8M2X3, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI8M2X3, VB4, VUI8M2X3, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2X3, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X3, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI8M2X3, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (5, (VUI8M2X3, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (6, (VUI8M2X3, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (7, (VUI8M2X3, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (8, (VUI8M2X3, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X3, VUI8M2X3, VUI8M2, SI))
+DEF_RISCV_FTYPE (0, (VUI8M2X4))
+DEF_RISCV_FTYPE (1, (VUI8M2X4, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M2X4, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M2X4, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI8M2X4, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (2, (VUI8M2X4, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X4, VB4, VUI8M2X4, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M2X4, VB4, VUI8M2X4, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M2X4, VB4, VUI8M2X4, C_UQI_PTR, VUI16M4))
+DEF_RISCV_FTYPE (4, (VUI8M2X4, VB4, VUI8M2X4, C_UQI_PTR, VUI32M8))
+DEF_RISCV_FTYPE (4, (VUI8M2X4, VB4, VUI8M2X4, C_UQI_PTR, VUI8M2))
+DEF_RISCV_FTYPE (2, (VUI8M2X4, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X4, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (4, (VUI8M2X4, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (5, (VUI8M2X4, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (6, (VUI8M2X4, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (7, (VUI8M2X4, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (8, (VUI8M2X4, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2, VUI8M2))
+DEF_RISCV_FTYPE (3, (VUI8M2X4, VUI8M2X4, VUI8M2, SI))
+DEF_RISCV_FTYPE (0, (VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M4, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI8M4, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI8M4, C_UQI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (3, (VUI8M4, C_UQI_PTR, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI8M4, C_UQI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4, C_UQI_PTR, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, UQI))
+DEF_RISCV_FTYPE (1, (VUI8M4, VB2))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, C_UQI_PTR, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, C_UQI_PTR, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI8M4, VB2, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4, VB2, VUI8M4, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, C_UQI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, C_UQI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4, VB2, VUI8M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, UQI, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4, VB2, VUI8M4, VB2))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, VUI16M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4, VB2, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, VUI8M4, LONG))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, VUI8M4, SIZE))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, VUI8M4, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M4, VB2, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VI16M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VI32M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VI64M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M4, VI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M4, VI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VI8M8))
+DEF_RISCV_FTYPE (1, (VUI8M4, VUI16M4))
+DEF_RISCV_FTYPE (2, (VUI8M4, VUI16M8, UQI))
+DEF_RISCV_FTYPE (2, (VUI8M4, VUI16M8, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VUI32M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VUI64M4))
+DEF_RISCV_FTYPE (1, (VUI8M4, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M4, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI8M4, VUI8M4, LONG))
+DEF_RISCV_FTYPE (2, (VUI8M4, VUI8M4, SIZE))
+DEF_RISCV_FTYPE (2, (VUI8M4, VUI8M4, UQI))
+DEF_RISCV_FTYPE (3, (VUI8M4, VUI8M4, UQI, VB2))
+DEF_RISCV_FTYPE (3, (VUI8M4, VUI8M4, UQI, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4, VUI8M4, VUI8M4, SIZE))
+DEF_RISCV_FTYPE (3, (VUI8M4, VUI8M4, VUI8M4, VB2))
+DEF_RISCV_FTYPE (3, (VUI8M4, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI8M4, VUI8M4X2, SI))
+DEF_RISCV_FTYPE (1, (VUI8M4, VUI8M8))
+DEF_RISCV_FTYPE (0, (VUI8M4X2))
+DEF_RISCV_FTYPE (1, (VUI8M4X2, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M4X2, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (2, (VUI8M4X2, C_UQI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (2, (VUI8M4X2, C_UQI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4X2, VB2, VUI8M4X2, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M4X2, VB2, VUI8M4X2, C_UQI_PTR, PTRDIFF))
+DEF_RISCV_FTYPE (4, (VUI8M4X2, VB2, VUI8M4X2, C_UQI_PTR, VUI16M8))
+DEF_RISCV_FTYPE (4, (VUI8M4X2, VB2, VUI8M4X2, C_UQI_PTR, VUI8M4))
+DEF_RISCV_FTYPE (2, (VUI8M4X2, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4X2, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (4, (VUI8M4X2, VUI8M4, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (5, (VUI8M4X2, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (6, (VUI8M4X2, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (7, (VUI8M4X2, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (8, (VUI8M4X2, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4, VUI8M4))
+DEF_RISCV_FTYPE (3, (VUI8M4X2, VUI8M4X2, VUI8M4, SI))
+DEF_RISCV_FTYPE (0, (VUI8M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, C_UQI_PTR))
+DEF_RISCV_FTYPE (2, (VUI8M8, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (2, (VUI8M8, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (2, (VUI8M8, C_UQI_PTR, VUI8M8))
+DEF_RISCV_FTYPE (3, (VUI8M8, C_UQI_PTR, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, UQI))
+DEF_RISCV_FTYPE (1, (VUI8M8, VB1))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, C_UQI_PTR, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (2, (VUI8M8, VB1, VUI8M8))
+DEF_RISCV_FTYPE (3, (VUI8M8, VB1, VUI8M8, C_UQI_PTR))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, C_UQI_PTR, DI))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, C_UQI_PTR, SI))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, C_UQI_PTR, VUI8M8))
+DEF_RISCV_FTYPE (3, (VUI8M8, VB1, VUI8M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, UQI, VUI8M8))
+DEF_RISCV_FTYPE (3, (VUI8M8, VB1, VUI8M8, VB1))
+DEF_RISCV_FTYPE (3, (VUI8M8, VB1, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, VUI8M8, LONG))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, VUI8M8, SIZE))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, VUI8M8, UQI))
+DEF_RISCV_FTYPE (4, (VUI8M8, VB1, VUI8M8, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VI16M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VI32M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VI64M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M8, VI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M8, VI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M8, VI8M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VUI16M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VUI32M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VUI64M8))
+DEF_RISCV_FTYPE (1, (VUI8M8, VUI8M1))
+DEF_RISCV_FTYPE (1, (VUI8M8, VUI8M2))
+DEF_RISCV_FTYPE (1, (VUI8M8, VUI8M4))
+DEF_RISCV_FTYPE (1, (VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (2, (VUI8M8, VUI8M8, LONG))
+DEF_RISCV_FTYPE (2, (VUI8M8, VUI8M8, SIZE))
+DEF_RISCV_FTYPE (2, (VUI8M8, VUI8M8, UQI))
+DEF_RISCV_FTYPE (3, (VUI8M8, VUI8M8, UQI, VB1))
+DEF_RISCV_FTYPE (3, (VUI8M8, VUI8M8, UQI, VUI8M8))
+DEF_RISCV_FTYPE (2, (VUI8M8, VUI8M8, VUI8M8))
+DEF_RISCV_FTYPE (3, (VUI8M8, VUI8M8, VUI8M8, SIZE))
+DEF_RISCV_FTYPE (3, (VUI8M8, VUI8M8, VUI8M8, VB1))
+DEF_RISCV_FTYPE (3, (VUI8M8, VUI8M8, VUI8M8, VUI8M8))
diff --git a/gcc/config/riscv/riscv-vector-iterator.h b/gcc/config/riscv/riscv-vector-iterator.h
new file mode 100644
index 00000000000..9023b53d5ca
--- /dev/null
+++ b/gcc/config/riscv/riscv-vector-iterator.h
@@ -0,0 +1,1571 @@
+/* DO NOT EDIT, please edit generator instead.
+   This file was generated by gen-vector-iterator with the command:
+   $ ./gen-vector-iterator -c > riscv-vector-iterator.h  */
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector and integer modes.  */
+#define _RVV_INT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, vnx16qi, QI) \
+  MACRO (8, 2, 4, vnx32qi, QI) \
+  MACRO (8, 4, 2, vnx64qi, QI) \
+  MACRO (8, 8, 1, vnx128qi, QI) \
+  MACRO (16, 1, 16, vnx8hi, HI) \
+  MACRO (16, 2, 8, vnx16hi, HI) \
+  MACRO (16, 4, 4, vnx32hi, HI) \
+  MACRO (16, 8, 2, vnx64hi, HI) \
+  MACRO (32, 1, 32, vnx4si, SI) \
+  MACRO (32, 2, 16, vnx8si, SI) \
+  MACRO (32, 4, 8, vnx16si, SI) \
+  MACRO (32, 8, 4, vnx32si, SI) \
+  MACRO (64, 1, 64, vnx2di, DI) \
+  MACRO (64, 2, 32, vnx4di, DI) \
+  MACRO (64, 4, 16, vnx8di, DI) \
+  MACRO (64, 8, 8, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer modes, and info for
+   corresponding widening vector type.  */
+#define _RVV_WINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer modes, and info for
+   corresponding widening vector type.  */
+#define _RVV_WRED_INT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi, HI) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 1, vnx8hi, HI) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 1, vnx8hi, HI) \
+  MACRO (8, 8, 1, vnx128qi, QI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si, SI) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 1, vnx4si, SI) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 1, vnx4si, SI) \
+  MACRO (16, 8, 2, vnx64hi, HI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di, DI) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 1, vnx2di, DI) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 1, vnx2di, DI) \
+  MACRO (32, 8, 4, vnx32si, SI, 64, 1, vnx2di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WRED_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer modes, and info for
+   corresponding quad widening vector type.  */
+#define _RVV_QINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI) \
+  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_QINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer modes, and info for
+   corresponding eightfold widening vector type.  */
+#define _RVV_EINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_EINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer point modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_INT_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 2, vnx32qi, QI) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI) \
+  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 4, vnx64qi, QI) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 8, vnx128qi, QI) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 2, 8, vnx16hi, HI, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI) \
+  MACRO (16, 4, 4, vnx32hi, HI, 8, 2, vnx32qi, QI) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 4, vnx32hi, HI) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI) \
+  MACRO (16, 8, 2, vnx64hi, HI, 8, 4, vnx64qi, QI) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 8, vnx64hi, HI) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 2, 16, vnx8si, SI, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI) \
+  MACRO (32, 4, 8, vnx16si, SI, 8, 1, vnx16qi, QI) \
+  MACRO (32, 4, 8, vnx16si, SI, 16, 2, vnx16hi, HI) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 4, vnx16si, SI) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI) \
+  MACRO (32, 8, 4, vnx32si, SI, 8, 2, vnx32qi, QI) \
+  MACRO (32, 8, 4, vnx32si, SI, 16, 4, vnx32hi, HI) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 8, vnx32si, SI) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 2, 32, vnx4di, DI, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 2, vnx4di, DI) \
+  MACRO (64, 4, 16, vnx8di, DI, 16, 1, vnx8hi, HI) \
+  MACRO (64, 4, 16, vnx8di, DI, 32, 2, vnx8si, SI) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 4, vnx8di, DI) \
+  MACRO (64, 8, 8, vnx16di, DI, 8, 1, vnx16qi, QI) \
+  MACRO (64, 8, 8, vnx16di, DI, 16, 2, vnx16hi, HI) \
+  MACRO (64, 8, 8, vnx16di, DI, 32, 4, vnx16si, SI) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 8, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 4, vnx64qi, QI, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 8, vnx128qi, QI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 8, 4, vnx64qi, QI, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer point modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_INT_REINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi) \
+  MACRO (8, 1, 8, vnx16qi, QI, 32, 1, vnx4si) \
+  MACRO (8, 1, 8, vnx16qi, QI, 64, 1, vnx2di) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 2, vnx32qi) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 4, vnx64qi) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 8, vnx128qi) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 2, vnx16hi) \
+  MACRO (8, 2, 4, vnx32qi, QI, 32, 2, vnx8si) \
+  MACRO (8, 2, 4, vnx32qi, QI, 64, 2, vnx4di) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 1, vnx16qi) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 4, vnx64qi) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 8, vnx128qi) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 4, vnx32hi) \
+  MACRO (8, 4, 2, vnx64qi, QI, 32, 4, vnx16si) \
+  MACRO (8, 4, 2, vnx64qi, QI, 64, 4, vnx8di) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 1, vnx16qi) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 2, vnx32qi) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 8, vnx128qi) \
+  MACRO (8, 8, 1, vnx128qi, QI, 16, 8, vnx64hi) \
+  MACRO (8, 8, 1, vnx128qi, QI, 32, 8, vnx32si) \
+  MACRO (8, 8, 1, vnx128qi, QI, 64, 8, vnx16di) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 1, vnx16qi) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 2, vnx32qi) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 4, vnx64qi) \
+  MACRO (16, 1, 16, vnx8hi, HI, 8, 1, vnx16qi) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si) \
+  MACRO (16, 1, 16, vnx8hi, HI, 64, 1, vnx2di) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 2, vnx16hi) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 4, vnx32hi) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 8, vnx64hi) \
+  MACRO (16, 2, 8, vnx16hi, HI, 8, 2, vnx32qi) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 2, vnx8si) \
+  MACRO (16, 2, 8, vnx16hi, HI, 64, 2, vnx4di) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 1, vnx8hi) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 4, vnx32hi) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 8, vnx64hi) \
+  MACRO (16, 4, 4, vnx32hi, HI, 8, 4, vnx64qi) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 4, vnx16si) \
+  MACRO (16, 4, 4, vnx32hi, HI, 64, 4, vnx8di) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 1, vnx8hi) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 2, vnx16hi) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 8, vnx64hi) \
+  MACRO (16, 8, 2, vnx64hi, HI, 8, 8, vnx128qi) \
+  MACRO (16, 8, 2, vnx64hi, HI, 32, 8, vnx32si) \
+  MACRO (16, 8, 2, vnx64hi, HI, 64, 8, vnx16di) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 1, vnx8hi) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 2, vnx16hi) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 4, vnx32hi) \
+  MACRO (32, 1, 32, vnx4si, SI, 8, 1, vnx16qi) \
+  MACRO (32, 1, 32, vnx4si, SI, 16, 1, vnx8hi) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 2, vnx8si) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 4, vnx16si) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 8, vnx32si) \
+  MACRO (32, 2, 16, vnx8si, SI, 8, 2, vnx32qi) \
+  MACRO (32, 2, 16, vnx8si, SI, 16, 2, vnx16hi) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 2, vnx4di) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 1, vnx4si) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 4, vnx16si) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 8, vnx32si) \
+  MACRO (32, 4, 8, vnx16si, SI, 8, 4, vnx64qi) \
+  MACRO (32, 4, 8, vnx16si, SI, 16, 4, vnx32hi) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 4, vnx8di) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 1, vnx4si) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 2, vnx8si) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 8, vnx32si) \
+  MACRO (32, 8, 4, vnx32si, SI, 8, 8, vnx128qi) \
+  MACRO (32, 8, 4, vnx32si, SI, 16, 8, vnx64hi) \
+  MACRO (32, 8, 4, vnx32si, SI, 64, 8, vnx16di) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 1, vnx4si) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 2, vnx8si) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 4, vnx16si) \
+  MACRO (64, 1, 64, vnx2di, DI, 8, 1, vnx16qi) \
+  MACRO (64, 1, 64, vnx2di, DI, 16, 1, vnx8hi) \
+  MACRO (64, 1, 64, vnx2di, DI, 32, 1, vnx4si) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 2, vnx4di) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 4, vnx8di) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 8, vnx16di) \
+  MACRO (64, 2, 32, vnx4di, DI, 8, 2, vnx32qi) \
+  MACRO (64, 2, 32, vnx4di, DI, 16, 2, vnx16hi) \
+  MACRO (64, 2, 32, vnx4di, DI, 32, 2, vnx8si) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 1, vnx2di) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 4, vnx8di) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 8, vnx16di) \
+  MACRO (64, 4, 16, vnx8di, DI, 8, 4, vnx64qi) \
+  MACRO (64, 4, 16, vnx8di, DI, 16, 4, vnx32hi) \
+  MACRO (64, 4, 16, vnx8di, DI, 32, 4, vnx16si) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 1, vnx2di) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 2, vnx4di) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 8, vnx16di) \
+  MACRO (64, 8, 8, vnx16di, DI, 8, 8, vnx128qi) \
+  MACRO (64, 8, 8, vnx16di, DI, 16, 8, vnx64hi) \
+  MACRO (64, 8, 8, vnx16di, DI, 32, 8, vnx32si) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 1, vnx2di) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 2, vnx4di) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 4, vnx8di) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_REINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 32, 1, vnx4si, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 64, 1, vnx2di, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
+  MACRO (8, 1, 8, vnx16qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 16, 2, vnx16hi, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 32, 2, vnx8si, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 64, 2, vnx4di, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
+  MACRO (8, 2, 4, vnx32qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 16, 4, vnx32hi, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 32, 4, vnx16si, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 64, 4, vnx8di, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
+  MACRO (8, 4, 2, vnx64qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 16, 8, vnx64hi, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 32, 8, vnx32si, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 64, 8, vnx16di, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
+  MACRO (8, 8, 1, vnx128qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 8, 1, vnx16qi, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 64, 1, vnx2di, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 8, 2, vnx32qi, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 2, vnx8si, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 64, 2, vnx4di, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 8, 4, vnx64qi, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 4, vnx16si, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 64, 4, vnx8di, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 8, 8, vnx128qi, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 32, 8, vnx32si, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 64, 8, vnx16di, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 8, 1, vnx16qi, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 16, 1, vnx8hi, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 2, vnx8si, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 4, vnx16si, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 32, 8, vnx32si, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 8, 2, vnx32qi, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 16, 2, vnx16hi, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 2, vnx4di, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 1, vnx4si, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 4, vnx16si, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 32, 8, vnx32si, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 8, 4, vnx64qi, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 16, 4, vnx32hi, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 4, vnx8di, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 1, vnx4si, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 2, vnx8si, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 32, 8, vnx32si, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 8, 8, vnx128qi, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 16, 8, vnx64hi, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 64, 8, vnx16di, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 1, vnx4si, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 2, vnx8si, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32si, SI, 32, 4, vnx16si, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, 8, 1, vnx16qi, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, 16, 1, vnx8hi, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, 32, 1, vnx4si, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 2, vnx4di, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 4, vnx8di, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2di, DI, 64, 8, vnx16di, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 8, 2, vnx32qi, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 16, 2, vnx16hi, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 32, 2, vnx8si, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 1, vnx2di, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 4, vnx8di, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4di, DI, 64, 8, vnx16di, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 8, 4, vnx64qi, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 16, 4, vnx32hi, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 32, 4, vnx16si, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 1, vnx2di, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 2, vnx4di, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8di, DI, 64, 8, vnx16di, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 8, 8, vnx128qi, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 16, 8, vnx64hi, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 32, 8, vnx32si, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 1, vnx2di, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 2, vnx4di, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16di, DI, 64, 4, vnx8di, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector and floating point modes.  */
+#define _RVV_FLOAT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hf, HF) \
+  MACRO (16, 2, 8, vnx16hf, HF) \
+  MACRO (16, 4, 4, vnx32hf, HF) \
+  MACRO (16, 8, 2, vnx64hf, HF) \
+  MACRO (32, 1, 32, vnx4sf, SF) \
+  MACRO (32, 2, 16, vnx8sf, SF) \
+  MACRO (32, 4, 8, vnx16sf, SF) \
+  MACRO (32, 8, 4, vnx32sf, SF) \
+  MACRO (64, 1, 64, vnx2df, DF) \
+  MACRO (64, 2, 32, vnx4df, DF) \
+  MACRO (64, 4, 16, vnx8df, DF) \
+  MACRO (64, 8, 8, vnx16df, DF) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hf, HF, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2df, DF, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4df, DF, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating modes, and info for
+   corresponding widening vector type.  */
+#define _RVV_WFLOAT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8sf, SF) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16sf, SF) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32sf, SF) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4df, DF) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8df, DF) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16df, DF) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WFLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8sf, SF, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16sf, SF, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32sf, SF, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4df, DF, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8df, DF, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16df, DF, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating modes, and info for
+   corresponding widening vector type.  */
+#define _RVV_WRED_FLOAT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 1, vnx4sf, SF) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 1, vnx4sf, SF) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 1, vnx4sf, SF) \
+  MACRO (16, 8, 2, vnx64hf, HF, 32, 1, vnx4sf, SF) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 1, vnx2df, DF) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 1, vnx2df, DF) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 1, vnx2df, DF) \
+  MACRO (32, 8, 4, vnx32sf, SF, 64, 1, vnx2df, DF) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WRED_FLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_FLOAT_INT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hf, HF, vnx8hi, HI) \
+  MACRO (16, 2, 8, vnx16hf, HF, vnx16hi, HI) \
+  MACRO (16, 4, 4, vnx32hf, HF, vnx32hi, HI) \
+  MACRO (16, 8, 2, vnx64hf, HF, vnx64hi, HI) \
+  MACRO (32, 1, 32, vnx4sf, SF, vnx4si, SI) \
+  MACRO (32, 2, 16, vnx8sf, SF, vnx8si, SI) \
+  MACRO (32, 4, 8, vnx16sf, SF, vnx16si, SI) \
+  MACRO (32, 8, 4, vnx32sf, SF, vnx32si, SI) \
+  MACRO (64, 1, 64, vnx2df, DF, vnx2di, DI) \
+  MACRO (64, 2, 32, vnx4df, DF, vnx4di, DI) \
+  MACRO (64, 4, 16, vnx8df, DF, vnx8di, DI) \
+  MACRO (64, 8, 8, vnx16df, DF, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hf, HF, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, vnx64hi, HI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, vnx16si, SI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, vnx32si, SI, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2df, DF, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4df, DF, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, vnx8di, DI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_FLOAT_WINT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_WINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector type.  */
+#define _RVV_WFLOAT_INT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8sf, SF) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16sf, SF) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32sf, SF) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4df, DF) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8df, DF) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16df, DF) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WFLOAT_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8sf, SF, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16sf, SF, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32sf, SF, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4df, DF, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8df, DF, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16df, DF, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point point modes, and info for
+   corresponding floating point and vector type.  */
+#define _RVV_FLOAT_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 16, vnx8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 2, 8, vnx16hf, HF, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 8, vnx16hf, HF, 64, 8, vnx16di, DI) \
+  MACRO (16, 4, 4, vnx32hf, HF, 8, 2, vnx32qi, QI) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 4, vnx32hi, HI) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI) \
+  MACRO (16, 8, 2, vnx64hf, HF, 8, 4, vnx64qi, QI) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 8, vnx64hi, HI) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 2, 16, vnx8sf, SF, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI) \
+  MACRO (32, 4, 8, vnx16sf, SF, 8, 1, vnx16qi, QI) \
+  MACRO (32, 4, 8, vnx16sf, SF, 16, 2, vnx16hi, HI) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 4, vnx16si, SI) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI) \
+  MACRO (32, 8, 4, vnx32sf, SF, 8, 2, vnx32qi, QI) \
+  MACRO (32, 8, 4, vnx32sf, SF, 16, 4, vnx32hi, HI) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 8, vnx32si, SI) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 2, 32, vnx4df, DF, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 2, vnx4di, DI) \
+  MACRO (64, 4, 16, vnx8df, DF, 16, 1, vnx8hi, HI) \
+  MACRO (64, 4, 16, vnx8df, DF, 32, 2, vnx8si, SI) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 4, vnx8di, DI) \
+  MACRO (64, 8, 8, vnx16df, DF, 8, 1, vnx16qi, QI) \
+  MACRO (64, 8, 8, vnx16df, DF, 16, 2, vnx16hi, HI) \
+  MACRO (64, 8, 8, vnx16df, DF, 32, 4, vnx16si, SI) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 8, vnx16di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, 8, 4, vnx64qi, QI, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 8, vnx64hi, HI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer point modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_FLOAT_REINT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 2, vnx16hf) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 4, vnx32hf) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 8, vnx64hf) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 1, vnx8hf) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 4, vnx32hf) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 8, vnx64hf) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 1, vnx8hf) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 2, vnx16hf) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 8, vnx64hf) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 1, vnx8hf) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 2, vnx16hf) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 4, vnx32hf) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 2, vnx8sf) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 4, vnx16sf) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 8, vnx32sf) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 1, vnx4sf) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 4, vnx16sf) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 8, vnx32sf) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 1, vnx4sf) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 2, vnx8sf) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 8, vnx32sf) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 1, vnx4sf) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 2, vnx8sf) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 4, vnx16sf) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 2, vnx4df) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 4, vnx8df) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 8, vnx16df) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 1, vnx2df) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 4, vnx8df) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 8, vnx16df) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 1, vnx2df) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 2, vnx4df) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 8, vnx16df) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 1, vnx2df) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 2, vnx4df) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 4, vnx8df) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_REINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
+  MACRO (16, 1, 16, vnx8hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
+  MACRO (16, 2, 8, vnx16hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
+  MACRO (16, 4, 4, vnx32hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
+  MACRO (16, 8, 2, vnx64hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 2, vnx8sf, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 4, vnx16sf, __VA_ARGS__) \
+  MACRO (32, 1, 32, vnx4sf, SF, 32, 8, vnx32sf, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 1, vnx4sf, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 4, vnx16sf, __VA_ARGS__) \
+  MACRO (32, 2, 16, vnx8sf, SF, 32, 8, vnx32sf, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 1, vnx4sf, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 2, vnx8sf, __VA_ARGS__) \
+  MACRO (32, 4, 8, vnx16sf, SF, 32, 8, vnx32sf, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 1, vnx4sf, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 2, vnx8sf, __VA_ARGS__) \
+  MACRO (32, 8, 4, vnx32sf, SF, 32, 4, vnx16sf, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 2, vnx4df, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 4, vnx8df, __VA_ARGS__) \
+  MACRO (64, 1, 64, vnx2df, DF, 64, 8, vnx16df, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 1, vnx2df, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 4, vnx8df, __VA_ARGS__) \
+  MACRO (64, 2, 32, vnx4df, DF, 64, 8, vnx16df, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 1, vnx2df, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 2, vnx4df, __VA_ARGS__) \
+  MACRO (64, 4, 16, vnx8df, DF, 64, 8, vnx16df, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 1, vnx2df, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 2, vnx4df, __VA_ARGS__) \
+  MACRO (64, 8, 8, vnx16df, DF, 64, 4, vnx8df, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG(MACRO) \
+  MACRO (8, 1, 2, 8, Q, q,VNx2x16Q, vnx2x16q) \
+  MACRO (8, 1, 3, 8, Q, q,VNx3x16Q, vnx3x16q) \
+  MACRO (8, 1, 4, 8, Q, q,VNx4x16Q, vnx4x16q) \
+  MACRO (8, 1, 5, 8, Q, q,VNx5x16Q, vnx5x16q) \
+  MACRO (8, 1, 6, 8, Q, q,VNx6x16Q, vnx6x16q) \
+  MACRO (8, 1, 7, 8, Q, q,VNx7x16Q, vnx7x16q) \
+  MACRO (8, 1, 8, 8, Q, q,VNx8x16Q, vnx8x16q) \
+  MACRO (8, 2, 2, 4, Q, q,VNx2x32Q, vnx2x32q) \
+  MACRO (8, 2, 3, 4, Q, q,VNx3x32Q, vnx3x32q) \
+  MACRO (8, 2, 4, 4, Q, q,VNx4x32Q, vnx4x32q) \
+  MACRO (8, 4, 2, 2, Q, q,VNx2x64Q, vnx2x64q) \
+  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h) \
+  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h) \
+  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h) \
+  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h) \
+  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h) \
+  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h) \
+  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h) \
+  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h) \
+  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h) \
+  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h) \
+  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h) \
+  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s) \
+  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s) \
+  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s) \
+  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s) \
+  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s) \
+  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s) \
+  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s) \
+  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s) \
+  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s) \
+  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s) \
+  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s) \
+  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d) \
+  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d) \
+  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d) \
+  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d) \
+  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d) \
+  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d) \
+  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d) \
+  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d) \
+  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d) \
+  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d) \
+  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, Q, q,VNx2x16Q, vnx2x16q, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, Q, q,VNx3x16Q, vnx3x16q, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, Q, q,VNx4x16Q, vnx4x16q, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, Q, q,VNx5x16Q, vnx5x16q, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, Q, q,VNx6x16Q, vnx6x16q, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, Q, q,VNx7x16Q, vnx7x16q, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, Q, q,VNx8x16Q, vnx8x16q, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, Q, q,VNx2x32Q, vnx2x32q, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, Q, q,VNx3x32Q, vnx3x32q, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, Q, q,VNx4x32Q, vnx4x32q, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, Q, q,VNx2x64Q, vnx2x64q, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer tuple modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_SEG_INT_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 8, 1, vnx16qi, QI) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 16, 2, vnx16hi, HI) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 32, 4, vnx16si, SI) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 64, 8, vnx16di, DI) \
+  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 8, 2, vnx32qi, QI) \
+  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 16, 4, vnx32hi, HI) \
+  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 32, 8, vnx32si, SI) \
+  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 8, 2, vnx32qi, QI) \
+  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 16, 4, vnx32hi, HI) \
+  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 32, 8, vnx32si, SI) \
+  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 8, 2, vnx32qi, QI) \
+  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 16, 4, vnx32hi, HI) \
+  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 32, 8, vnx32si, SI) \
+  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 8, 4, vnx64qi, QI) \
+  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 16, 8, vnx64hi, HI) \
+  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 64, 4, vnx8di, DI) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 64, 8, vnx16di, DI) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 64, 8, vnx16di, DI) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 64, 8, vnx16di, DI) \
+  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 8, 2, vnx32qi, QI) \
+  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 16, 4, vnx32hi, HI) \
+  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 32, 8, vnx32si, SI) \
+  MACRO (32, 1, 2, 32, vnx2x4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 2, 32, vnx2x4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 3, 32, vnx3x4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 3, 32, vnx3x4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 4, 32, vnx4x4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 4, 32, vnx4x4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 5, 32, vnx5x4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 5, 32, vnx5x4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 6, 32, vnx6x4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 6, 32, vnx6x4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 7, 32, vnx7x4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 7, 32, vnx7x4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 8, 32, vnx8x4si, SI, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 8, 32, vnx8x4si, SI, 64, 2, vnx4di, DI) \
+  MACRO (32, 2, 2, 16, vnx2x8si, SI, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 2, 16, vnx2x8si, SI, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 2, 16, vnx2x8si, SI, 64, 4, vnx8di, DI) \
+  MACRO (32, 2, 3, 16, vnx3x8si, SI, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 3, 16, vnx3x8si, SI, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 3, 16, vnx3x8si, SI, 64, 4, vnx8di, DI) \
+  MACRO (32, 2, 4, 16, vnx4x8si, SI, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 4, 16, vnx4x8si, SI, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 4, 16, vnx4x8si, SI, 64, 4, vnx8di, DI) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 8, 1, vnx16qi, QI) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 16, 2, vnx16hi, HI) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 32, 4, vnx16si, SI) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 64, 8, vnx16di, DI) \
+  MACRO (64, 1, 2, 64, vnx2x2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 3, 64, vnx3x2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 4, 64, vnx4x2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 5, 64, vnx5x2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 6, 64, vnx6x2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 7, 64, vnx7x2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 8, 64, vnx8x2di, DI, 64, 1, vnx2di, DI) \
+  MACRO (64, 2, 2, 32, vnx2x4di, DI, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 2, 32, vnx2x4di, DI, 64, 2, vnx4di, DI) \
+  MACRO (64, 2, 3, 32, vnx3x4di, DI, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 3, 32, vnx3x4di, DI, 64, 2, vnx4di, DI) \
+  MACRO (64, 2, 4, 32, vnx4x4di, DI, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 4, 32, vnx4x4di, DI, 64, 2, vnx4di, DI) \
+  MACRO (64, 4, 2, 16, vnx2x8di, DI, 16, 1, vnx8hi, HI) \
+  MACRO (64, 4, 2, 16, vnx2x8di, DI, 32, 2, vnx8si, SI) \
+  MACRO (64, 4, 2, 16, vnx2x8di, DI, 64, 4, vnx8di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_INT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 8, 4, vnx64qi, QI, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, vnx2x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, vnx2x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, vnx3x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, vnx3x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, vnx4x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, vnx4x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, vnx5x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, vnx5x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, vnx6x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, vnx6x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, vnx7x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, vnx7x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, vnx8x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, vnx8x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, vnx2x8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, vnx2x8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, vnx2x8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, vnx3x8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, vnx3x8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, vnx3x8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, vnx4x8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, vnx4x8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, vnx4x8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16si, SI, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, vnx2x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, vnx3x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, vnx4x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, vnx5x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, vnx6x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, vnx7x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, vnx8x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, vnx2x4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, vnx2x4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, vnx3x4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, vnx3x4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, vnx4x4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, vnx4x4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, vnx2x8di, DI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, vnx2x8di, DI, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, vnx2x8di, DI, 64, 4, vnx8di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF2(MACRO) \
+  MACRO (8, 1, 2, 8, Q, q, VNx2x16Q, vnx2x16q) \
+  MACRO (8, 2, 2, 4, Q, q, VNx2x32Q, vnx2x32q) \
+  MACRO (8, 4, 2, 2, Q, q, VNx2x64Q, vnx2x64q) \
+  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h) \
+  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h) \
+  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h) \
+  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s) \
+  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s) \
+  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s) \
+  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d) \
+  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d) \
+  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF2_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, Q, q, VNx2x16Q, vnx2x16q, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, Q, q, VNx2x32Q, vnx2x32q, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, Q, q, VNx2x64Q, vnx2x64q, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF3(MACRO) \
+  MACRO (8, 1, 3, 8, Q, q, VNx3x16Q, vnx3x16q) \
+  MACRO (8, 2, 3, 4, Q, q, VNx3x32Q, vnx3x32q) \
+  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h) \
+  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h) \
+  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s) \
+  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s) \
+  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d) \
+  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF3_ARG(MACRO, ...) \
+  MACRO (8, 1, 3, 8, Q, q, VNx3x16Q, vnx3x16q, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, Q, q, VNx3x32Q, vnx3x32q, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF4(MACRO) \
+  MACRO (8, 1, 4, 8, Q, q, VNx4x16Q, vnx4x16q) \
+  MACRO (8, 2, 4, 4, Q, q, VNx4x32Q, vnx4x32q) \
+  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h) \
+  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h) \
+  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s) \
+  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s) \
+  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d) \
+  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF4_ARG(MACRO, ...) \
+  MACRO (8, 1, 4, 8, Q, q, VNx4x16Q, vnx4x16q, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, Q, q, VNx4x32Q, vnx4x32q, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF5(MACRO) \
+  MACRO (8, 1, 5, 8, Q, q, VNx5x16Q, vnx5x16q) \
+  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h) \
+  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s) \
+  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF5_ARG(MACRO, ...) \
+  MACRO (8, 1, 5, 8, Q, q, VNx5x16Q, vnx5x16q, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF6(MACRO) \
+  MACRO (8, 1, 6, 8, Q, q, VNx6x16Q, vnx6x16q) \
+  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h) \
+  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s) \
+  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF6_ARG(MACRO, ...) \
+  MACRO (8, 1, 6, 8, Q, q, VNx6x16Q, vnx6x16q, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF7(MACRO) \
+  MACRO (8, 1, 7, 8, Q, q, VNx7x16Q, vnx7x16q) \
+  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h) \
+  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s) \
+  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF7_ARG(MACRO, ...) \
+  MACRO (8, 1, 7, 8, Q, q, VNx7x16Q, vnx7x16q, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF8(MACRO) \
+  MACRO (8, 1, 8, 8, Q, q, VNx8x16Q, vnx8x16q) \
+  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h) \
+  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s) \
+  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF8_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, 8, Q, q, VNx8x16Q, vnx8x16q, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NO_SEW8(MACRO) \
+  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h) \
+  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h) \
+  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h) \
+  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h) \
+  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h) \
+  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h) \
+  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h) \
+  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h) \
+  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h) \
+  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h) \
+  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h) \
+  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s) \
+  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s) \
+  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s) \
+  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s) \
+  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s) \
+  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s) \
+  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s) \
+  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s) \
+  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s) \
+  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s) \
+  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s) \
+  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d) \
+  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d) \
+  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d) \
+  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d) \
+  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d) \
+  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d) \
+  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d) \
+  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d) \
+  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d) \
+  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d) \
+  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, float-point tuple modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_SEG_FLOAT_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 16, 1, vnx8hi, HI) \
+  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 32, 2, vnx8si, SI) \
+  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 64, 4, vnx8di, DI) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 64, 8, vnx16di, DI) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 64, 8, vnx16di, DI) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 8, 1, vnx16qi, QI) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 16, 2, vnx16hi, HI) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 32, 4, vnx16si, SI) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 64, 8, vnx16di, DI) \
+  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 8, 2, vnx32qi, QI) \
+  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 16, 4, vnx32hi, HI) \
+  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 32, 8, vnx32si, SI) \
+  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 32, 1, vnx4si, SI) \
+  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 64, 2, vnx4di, DI) \
+  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 64, 4, vnx8di, DI) \
+  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 64, 4, vnx8di, DI) \
+  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 16, 1, vnx8hi, HI) \
+  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 32, 2, vnx8si, SI) \
+  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 64, 4, vnx8di, DI) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 8, 1, vnx16qi, QI) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 16, 2, vnx16hi, HI) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 32, 4, vnx16si, SI) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 64, 8, vnx16di, DI) \
+  MACRO (64, 1, 2, 64, vnx2x2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 3, 64, vnx3x2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 4, 64, vnx4x2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 5, 64, vnx5x2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 6, 64, vnx6x2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 7, 64, vnx7x2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 1, 8, 64, vnx8x2df, DF, 64, 1, vnx2di, DI) \
+  MACRO (64, 2, 2, 32, vnx2x4df, DF, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 2, 32, vnx2x4df, DF, 64, 2, vnx4di, DI) \
+  MACRO (64, 2, 3, 32, vnx3x4df, DF, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 3, 32, vnx3x4df, DF, 64, 2, vnx4di, DI) \
+  MACRO (64, 2, 4, 32, vnx4x4df, DF, 32, 1, vnx4si, SI) \
+  MACRO (64, 2, 4, 32, vnx4x4df, DF, 64, 2, vnx4di, DI) \
+  MACRO (64, 4, 2, 16, vnx2x8df, DF, 16, 1, vnx8hi, HI) \
+  MACRO (64, 4, 2, 16, vnx2x8df, DF, 32, 2, vnx8si, SI) \
+  MACRO (64, 4, 2, 16, vnx2x8df, DF, 64, 4, vnx8di, DI) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_FLOAT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 8, 2, vnx32qi, QI, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 16, 4, vnx32hi, HI, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 32, 8, vnx32si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 32, 4, vnx16si, SI, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 64, 8, vnx16di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, vnx2x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, vnx3x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, vnx4x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, vnx5x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, vnx6x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, vnx7x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, vnx8x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, vnx2x4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, vnx2x4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, vnx3x4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, vnx3x4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, vnx4x4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, vnx4x4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, vnx2x8df, DF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, vnx2x8df, DF, 32, 2, vnx8si, SI, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, vnx2x8df, DF, 64, 4, vnx8di, DI, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF2_NO_SEW8(MACRO) \
+  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h) \
+  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h) \
+  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h) \
+  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s) \
+  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s) \
+  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s) \
+  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d) \
+  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d) \
+  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF2_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF3_NO_SEW8(MACRO) \
+  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h) \
+  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h) \
+  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s) \
+  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s) \
+  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d) \
+  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF3_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF4_NO_SEW8(MACRO) \
+  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h) \
+  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h) \
+  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s) \
+  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s) \
+  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d) \
+  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF4_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF5_NO_SEW8(MACRO) \
+  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h) \
+  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s) \
+  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF5_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF6_NO_SEW8(MACRO) \
+  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h) \
+  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s) \
+  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF6_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF7_NO_SEW8(MACRO) \
+  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h) \
+  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s) \
+  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF7_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_SEG_NF8_NO_SEW8(MACRO) \
+  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h) \
+  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s) \
+  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_SEG_NF8_NO_SEW8_ARG(MACRO, ...) \
+  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d, __VA_ARGS__) \
+
diff --git a/gcc/config/riscv/riscv.c b/gcc/config/riscv/riscv.c
index a59f178fbf4..763fa6b1dbc 100644
--- a/gcc/config/riscv/riscv.c
+++ b/gcc/config/riscv/riscv.c
@@ -47,6 +47,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "tm_p.h"
 #include "target.h"
 #include "target-def.h"
+#include "tm-constrs.h"
 #include "basic-block.h"
 #include "expr.h"
 #include "optabs.h"
@@ -56,6 +57,10 @@ along with GCC; see the file COPYING3.  If not see
 #include "builtins.h"
 #include "predict.h"
 #include "tree-pass.h"
+#include "cfg.h"
+#include "cfgrtl.h"
+#include "langhooks.h"
+#include "riscv-vector-iterator.h"
 
 /* True if X is an UNSPEC wrapper around a SYMBOL_REF or LABEL_REF.  */
 #define UNSPEC_ADDRESS_P(X)					\
@@ -99,7 +104,7 @@ enum riscv_address_type {
 /* Information about a function's frame layout.  */
 struct GTY(())  riscv_frame_info {
   /* The size of the frame in bytes.  */
-  HOST_WIDE_INT total_size;
+  poly_int64 total_size;
 
   /* Bit X is set if the function saves or restores GPR X.  */
   unsigned int mask;
@@ -111,17 +116,19 @@ struct GTY(())  riscv_frame_info {
   unsigned save_libcall_adjustment;
 
   /* Offsets of fixed-point and floating-point save areas from frame bottom */
-  HOST_WIDE_INT gp_sp_offset;
-  HOST_WIDE_INT fp_sp_offset;
+  poly_int64 gp_sp_offset;
+  poly_int64 fp_sp_offset;
+
+  HOST_WIDE_INT min_first_step;
 
   /* Offset of virtual frame pointer from stack pointer/frame bottom */
-  HOST_WIDE_INT frame_pointer_offset;
+  poly_int64 frame_pointer_offset;
 
   /* Offset of hard frame pointer from stack pointer/frame bottom */
-  HOST_WIDE_INT hard_frame_pointer_offset;
+  poly_int64 hard_frame_pointer_offset;
 
   /* The offset of arg_pointer_rtx from the bottom of the frame.  */
-  HOST_WIDE_INT arg_pointer_offset;
+  poly_int64 arg_pointer_offset;
 };
 
 enum riscv_privilege_levels {
@@ -271,7 +278,22 @@ const enum reg_class riscv_regno_to_class[FIRST_PSEUDO_REGISTER] = {
   FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
   FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
   FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
-  FRAME_REGS,	FRAME_REGS,
+  FRAME_REGS,	FRAME_REGS,	NO_REGS,	VTYPE_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  VECTOR_MASK_REGS,VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
+  VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
+  VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
+  VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
+  VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
+  VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
+  VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
+  VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,	VECTOR_REGS,
 };
 
 /* Costs to use when optimizing for rocket.  */
@@ -313,6 +335,9 @@ static const struct riscv_tune_param optimize_size_tune_info = {
   false,					/* slow_unaligned_access */
 };
 
+/* The number of 64-bit elements in an RVV vector.  */
+poly_uint16 riscv_rvv_chunks;
+
 static tree riscv_handle_fndecl_attribute (tree *, tree, tree, int, bool *);
 static tree riscv_handle_type_attribute (tree *, tree, tree, int, bool *);
 
@@ -631,7 +656,22 @@ static int riscv_symbol_insns (enum riscv_symbol_type type)
     default: gcc_unreachable ();
     }
 }
+
+/* Vector extension helper functions.  */
 
+/* Return true if X is a const_vector with all duplicate elements, which is in
+   the range between MINVAL and MAXVAL.  */
+
+bool
+riscv_const_vec_all_same_in_range_p (rtx x, HOST_WIDE_INT minval,
+				     HOST_WIDE_INT maxval)
+{
+  rtx elt;
+  return (const_vec_duplicate_p (x, &elt)
+	  && CONST_INT_P (elt)
+	  && IN_RANGE (INTVAL (elt), minval, maxval));
+}
+
 /* Implement TARGET_LEGITIMATE_CONSTANT_P.  */
 
 static bool
@@ -720,10 +760,15 @@ riscv_valid_offset_p (rtx x, machine_mode mode)
   if (!const_arith_operand (x, Pmode))
     return false;
 
+  /* Vector load/store disallow any offset.  */
+  if (TARGET_VECTOR && VECTOR_MODE_P (mode))
+    return false;
+
   /* We may need to split multiword moves, so make sure that every word
      is accessible.  */
-  if (GET_MODE_SIZE (mode) > UNITS_PER_WORD
-      && !SMALL_OPERAND (INTVAL (x) + GET_MODE_SIZE (mode) - UNITS_PER_WORD))
+  if (GET_MODE_SIZE (mode).to_constant () > UNITS_PER_WORD
+      && !SMALL_OPERAND (INTVAL (x) + GET_MODE_SIZE (mode).to_constant ()
+			 - UNITS_PER_WORD))
     return false;
 
   return true;
@@ -787,7 +832,7 @@ riscv_valid_lo_sum_p (enum riscv_symbol_type sym_type, machine_mode mode,
   else
     {
       align = GET_MODE_ALIGNMENT (mode);
-      size = GET_MODE_BITSIZE (mode);
+      size = GET_MODE_BITSIZE (mode).to_constant ();
     }
 
   /* We may need to split multiword moves, so make sure that each word
@@ -824,6 +869,9 @@ riscv_classify_address (struct riscv_address_info *info, rtx x,
 	      && riscv_valid_offset_p (info->offset, mode));
 
     case LO_SUM:
+      /* Vector load/store disallow LO_SUM.  */
+      if (TARGET_VECTOR && VECTOR_MODE_P (mode))
+	return false;
       info->type = ADDRESS_LO_SUM;
       info->reg = XEXP (x, 0);
       info->offset = XEXP (x, 1);
@@ -931,8 +979,9 @@ riscv_address_insns (rtx x, machine_mode mode, bool might_split_p)
 
   /* BLKmode is used for single unaligned loads and stores and should
      not count as a multiword mode. */
-  if (mode != BLKmode && might_split_p)
-    n += (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
+  if (mode != BLKmode && might_split_p && !VECTOR_MODE_P (mode))
+    n += ((GET_MODE_SIZE (mode).to_constant () + UNITS_PER_WORD - 1)
+	  / UNITS_PER_WORD);
 
   if (addr.type == ADDRESS_LO_SUM)
     n += riscv_symbol_insns (addr.symbol_type) - 1;
@@ -990,6 +1039,10 @@ riscv_const_insns (rtx x)
     case LABEL_REF:
       return riscv_symbol_insns (riscv_classify_symbol (x));
 
+
+    case CONST_POLY_INT:
+      return 4;
+
     default:
       return 0;
     }
@@ -1025,9 +1078,9 @@ riscv_load_store_insns (rtx mem, rtx_insn *insn)
 
   /* Try to prove that INSN does not need to be split.  */
   might_split_p = true;
-  if (GET_MODE_BITSIZE (mode) <= 32)
+  if (GET_MODE_BITSIZE (mode).to_constant () <= 32)
     might_split_p = false;
-  else if (GET_MODE_BITSIZE (mode) == 64)
+  else if (GET_MODE_BITSIZE (mode).to_constant () == 64)
     {
       set = single_set (insn);
       if (set && !riscv_split_64bit_move_p (SET_DEST (set), SET_SRC (set)))
@@ -1258,22 +1311,32 @@ riscv_split_symbol (rtx temp, rtx addr, machine_mode mode, rtx *low_out,
    SMALL_OPERAND.  */
 
 static rtx
-riscv_add_offset (rtx temp, rtx reg, HOST_WIDE_INT offset)
+riscv_add_offset (rtx temp, rtx reg, poly_int64 offset)
 {
-  if (!SMALL_OPERAND (offset))
+  if (offset.is_constant ())
+    {
+      if (!SMALL_OPERAND (offset.to_constant ()))
+	{
+	  HOST_WIDE_INT offset_value = offset.to_constant ();
+	  rtx high;
+
+	  /* Leave OFFSET as a 16-bit offset and put the excess in HIGH.
+	     The addition inside the macro CONST_HIGH_PART may cause an
+	     overflow, so we need to force a sign-extension check.  */
+	  high = gen_int_mode (CONST_HIGH_PART (offset_value), Pmode);
+	  offset_value = CONST_LOW_PART (offset_value);
+	  high = riscv_force_temporary (temp, high, FALSE);
+	  reg = riscv_force_temporary (temp, gen_rtx_PLUS (Pmode, high, reg),
+				       FALSE);
+	  return plus_constant (Pmode, reg, offset_value);
+	}
+      return plus_constant (Pmode, reg, offset);
+    }
+  else
     {
-      rtx high;
-
-      /* Leave OFFSET as a 16-bit offset and put the excess in HIGH.
-	 The addition inside the macro CONST_HIGH_PART may cause an
-	 overflow, so we need to force a sign-extension check.  */
-      high = gen_int_mode (CONST_HIGH_PART (offset), Pmode);
-      offset = CONST_LOW_PART (offset);
-      high = riscv_force_temporary (temp, high, FALSE);
-      reg = riscv_force_temporary (temp, gen_rtx_PLUS (Pmode, high, reg),
-				   FALSE);
+      gcc_unreachable ();
+      return NULL_RTX;
     }
-  return plus_constant (Pmode, reg, offset);
 }
 
 /* The __tls_get_attr symbol.  */
@@ -1522,12 +1585,30 @@ riscv_legitimize_const_move (machine_mode mode, rtx dest, rtx src)
   riscv_emit_move (dest, src);
 }
 
+/* TODO: */
+static rtx
+riscv_gen_load_poly_int (rtx target, rtx tmp1, rtx tmp2, poly_int64 value);
+
 /* If (set DEST SRC) is not a valid move instruction, emit an equivalent
    sequence that is valid.  */
 
 bool
 riscv_legitimize_move (machine_mode mode, rtx dest, rtx src)
 {
+  if (GET_CODE (src) == CONST_POLY_INT)
+    {
+      if (satisfies_constraint_vp (src))
+	return false;
+
+      poly_int64 value = rtx_to_poly_int64 (src);
+      rtx tmp0 = gen_reg_rtx (word_mode);
+      rtx tmp1 = gen_reg_rtx (word_mode);
+      rtx tmp2 = gen_reg_rtx (word_mode);
+      emit_insn (riscv_gen_load_poly_int (tmp0, tmp1, tmp2, value));
+      emit_move_insn (dest, tmp0);
+      return true;
+    }
+
   if (!register_operand (dest, mode) && !reg_or_0_operand (src, mode))
     {
       rtx reg;
@@ -1538,7 +1619,7 @@ riscv_legitimize_move (machine_mode mode, rtx dest, rtx src)
 	     improve cse.  */
 	  machine_mode promoted_mode = mode;
 	  if (GET_MODE_CLASS (mode) == MODE_INT
-	      && GET_MODE_SIZE (mode) < UNITS_PER_WORD)
+	      && GET_MODE_SIZE (mode).to_constant () < UNITS_PER_WORD)
 	    promoted_mode = word_mode;
 
 	  if (splittable_const_int_operand (src, mode))
@@ -1636,7 +1717,9 @@ riscv_immediate_operand_p (int code, HOST_WIDE_INT x)
 static int
 riscv_binary_cost (rtx x, int single_insns, int double_insns)
 {
-  if (GET_MODE_SIZE (GET_MODE (x)) == UNITS_PER_WORD * 2)
+  machine_mode mode = GET_MODE (x);
+  if (!VECTOR_MODE_P (mode)
+      && GET_MODE_SIZE (mode).to_constant () == UNITS_PER_WORD * 2)
     return COSTS_N_INSNS (double_insns);
   return COSTS_N_INSNS (single_insns);
 }
@@ -1715,7 +1798,11 @@ riscv_rtx_costs (rtx x, machine_mode mode, int outer_code, int opno ATTRIBUTE_UN
       return false;
 
     case NOT:
-      *total = COSTS_N_INSNS (GET_MODE_SIZE (mode) > UNITS_PER_WORD ? 2 : 1);
+      *total
+	= COSTS_N_INSNS (!VECTOR_MODE_P (mode)
+			 && (GET_MODE_SIZE (mode).to_constant ()
+			     > UNITS_PER_WORD)
+			 ? 2 : 1);
       return false;
 
     case AND:
@@ -1832,7 +1919,10 @@ riscv_rtx_costs (rtx x, machine_mode mode, int outer_code, int opno ATTRIBUTE_UN
       if (float_mode_p)
 	*total = tune_param->fp_add[mode == DFmode];
       else
-	*total = COSTS_N_INSNS (GET_MODE_SIZE (mode) > UNITS_PER_WORD ? 4 : 1);
+	*total = COSTS_N_INSNS (!VECTOR_MODE_P (mode)
+				&& (GET_MODE_SIZE (mode).to_constant ()
+				    > UNITS_PER_WORD)
+				? 4 : 1);
       return false;
 
     case MULT:
@@ -1841,7 +1931,8 @@ riscv_rtx_costs (rtx x, machine_mode mode, int outer_code, int opno ATTRIBUTE_UN
       else if (!TARGET_MUL)
 	/* Estimate the cost of a library call.  */
 	*total = COSTS_N_INSNS (speed ? 32 : 6);
-      else if (GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+      else if (!VECTOR_MODE_P (mode)
+	       && GET_MODE_SIZE (mode).to_constant () > UNITS_PER_WORD)
 	*total = 3 * tune_param->int_mul[0] + COSTS_N_INSNS (2);
       else if (!speed)
 	*total = COSTS_N_INSNS (1);
@@ -2005,8 +2096,8 @@ riscv_output_move (rtx dest, rtx src)
   dest_code = GET_CODE (dest);
   src_code = GET_CODE (src);
   mode = GET_MODE (dest);
-  dbl_p = (GET_MODE_SIZE (mode) == 8);
-  width = GET_MODE_SIZE (mode);
+  dbl_p = (GET_MODE_SIZE (mode).to_constant () == 8);
+  width = GET_MODE_SIZE (mode).to_constant ();
 
   if (dbl_p && riscv_split_64bit_move_p (dest, src))
     return "#";
@@ -2116,6 +2207,10 @@ riscv_output_move (rtx dest, rtx src)
 	  case 8: return "fld\t%0,%1";
 	  }
     }
+  if (dest_code == REG && GP_REG_P (REGNO (dest)) && src_code == CONST_POLY_INT)
+    {
+      return "csrr\t%0,vlenb";
+    }
   gcc_unreachable ();
 }
 
@@ -2262,7 +2357,8 @@ static void
 riscv_extend_comparands (rtx_code code, rtx *op0, rtx *op1)
 {
   /* Comparisons consider all XLEN bits, so extend sub-XLEN values.  */
-  if (GET_MODE_SIZE (word_mode) > GET_MODE_SIZE (GET_MODE (*op0)))
+  if (GET_MODE_SIZE (word_mode)
+      > GET_MODE_SIZE (GET_MODE (*op0)).to_constant ())
     {
       /* It is more profitable to zero-extend QImode values.  But not if the
 	 first operand has already been sign-extended, and the second one is
@@ -2625,7 +2721,8 @@ riscv_flatten_aggregate_field (const_tree type,
 	if (n != 0)
 	  return -1;
 
-	HOST_WIDE_INT elt_size = GET_MODE_SIZE (TYPE_MODE (TREE_TYPE (type)));
+	HOST_WIDE_INT elt_size
+	  = GET_MODE_SIZE (TYPE_MODE (TREE_TYPE (type))).to_constant ();
 
 	if (elt_size <= UNITS_PER_FP_ARG)
 	  {
@@ -2643,9 +2740,11 @@ riscv_flatten_aggregate_field (const_tree type,
     default:
       if (n < 2
 	  && ((SCALAR_FLOAT_TYPE_P (type)
-	       && GET_MODE_SIZE (TYPE_MODE (type)) <= UNITS_PER_FP_ARG)
+	       && (GET_MODE_SIZE (TYPE_MODE (type)).to_constant ()
+		   <= UNITS_PER_FP_ARG))
 	      || (INTEGRAL_TYPE_P (type)
-		  && GET_MODE_SIZE (TYPE_MODE (type)) <= UNITS_PER_WORD)))
+		  && (GET_MODE_SIZE (TYPE_MODE (type)).to_constant ()
+		      <= UNITS_PER_WORD))))
 	{
 	  fields[n].type = type;
 	  fields[n].offset = offset;
@@ -2881,7 +2980,8 @@ riscv_get_arg_info (struct riscv_arg_info *info, const CUMULATIVE_ARGS *cum,
     }
 
   /* Work out the size of the argument.  */
-  num_bytes = type ? int_size_in_bytes (type) : GET_MODE_SIZE (mode);
+  num_bytes = (type ? int_size_in_bytes (type)
+	       : GET_MODE_SIZE (mode).to_constant ());
   num_words = (num_bytes + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
 
   /* Doubleword-aligned varargs start on an even register boundary.  */
@@ -2975,7 +3075,8 @@ riscv_function_value (const_tree type, const_tree func, machine_mode mode)
 static bool
 riscv_pass_by_reference (cumulative_args_t cum_v, const function_arg_info &arg)
 {
-  HOST_WIDE_INT size = arg.type_size_in_bytes ();
+  HOST_WIDE_INT size = (arg.type ? int_size_in_bytes (arg.type)
+			: GET_MODE_SIZE (arg.mode).to_constant ());
   struct riscv_arg_info info;
   CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
 
@@ -3407,7 +3508,10 @@ riscv_memmodel_needs_release_fence (enum memmodel model)
    'A'	Print the atomic operation suffix for memory model OP.
    'F'	Print a FENCE if the memory model requires a release.
    'z'	Print x0 if OP is zero, otherwise print OP normally.
-   'i'	Print i if the operand is not a register.  */
+   'i'	Print i if the operand is not a register.
+   'v'  Print the sole immediate value of a const vec duplicate.
+   'V'  Print the negated sole immediate value of a const vec duplicate.
+   'B'  Print a branch condition.  */
 
 static void
 riscv_print_operand (FILE *file, rtx op, int letter)
@@ -3447,6 +3551,34 @@ riscv_print_operand (FILE *file, rtx op, int letter)
         fputs ("i", file);
       break;
 
+    case 'v':
+      {
+	rtx elt;
+	if (!const_vec_duplicate_p (op, &elt))
+	  output_operand_lossage ("invalid vector constant");
+	else if (GET_MODE_CLASS (GET_MODE (op)) == MODE_VECTOR_INT)
+	  asm_fprintf (file, "%wd", INTVAL (elt));
+	else
+	  output_operand_lossage ("invalid vector constant");
+      }
+      break;
+
+    case 'V':
+      {
+	rtx elt;
+	if (!const_vec_duplicate_p (op, &elt))
+	  output_operand_lossage ("invalid vector constant");
+	else if (GET_MODE_CLASS (GET_MODE (op)) == MODE_VECTOR_INT)
+	  asm_fprintf (file, "%wd", -INTVAL (elt));
+	else
+	  output_operand_lossage ("invalid vector constant");
+      }
+      break;
+
+    case 'B':
+      fputs (GET_RTX_NAME (code), file);
+      break;
+
     default:
       switch (code)
 	{
@@ -3463,6 +3595,20 @@ riscv_print_operand (FILE *file, rtx op, int letter)
 	    output_address (mode, XEXP (op, 0));
 	  break;
 
+	case CONST_VECTOR:
+	  {
+	    rtx imm;
+
+	    if (!const_vec_duplicate_p (op, &imm)) {
+	      output_operand_lossage ("invalid immediate value for vector");
+	      break;
+	    }
+
+	    gcc_assert (CONST_INT_P (imm));
+	    asm_fprintf (file, "%wd", INTVAL (imm));
+	    break;
+	  }
+
 	default:
 	  if (letter == 'z' && op == CONST0_RTX (GET_MODE (op)))
 	    fputs (reg_names[GP_REG_FIRST], file);
@@ -3591,7 +3737,7 @@ riscv_elf_select_rtx_section (machine_mode mode, rtx x,
 {
   section *s = default_elf_select_rtx_section (mode, x, align);
 
-  if (riscv_size_ok_for_small_data_p (GET_MODE_SIZE (mode)))
+  if (riscv_size_ok_for_small_data_p (GET_MODE_SIZE (mode).to_constant ()))
     {
       if (strncmp (s->named.name, ".rodata.cst", strlen (".rodata.cst")) == 0)
 	{
@@ -3699,6 +3845,23 @@ riscv_save_libcall_count (unsigned mask)
   abort ();
 }
 
+/* Handle stack align for poly_int.  */
+static poly_int64
+riscv_stack_align (poly_int64 value)
+{
+  poly_int64 aligned_value;
+  for (unsigned i = 0; i < 2; ++i)
+    aligned_value.coeffs[i] = RISCV_STACK_ALIGN (value.coeffs[i]);
+  return aligned_value;
+}
+
+static HOST_WIDE_INT
+riscv_stack_align (HOST_WIDE_INT value)
+{
+  return RISCV_STACK_ALIGN (value);
+}
+
+
 /* Populate the current function's riscv_frame_info structure.
 
    RISC-V stack frames grown downward.  High addresses are at the top.
@@ -3747,7 +3910,7 @@ static void
 riscv_compute_frame_info (void)
 {
   struct riscv_frame_info *frame;
-  HOST_WIDE_INT offset;
+  poly_int64 offset;
   bool interrupt_save_prologue_temp = false;
   unsigned int regno, i, num_x_saved = 0, num_f_saved = 0;
 
@@ -3758,7 +3921,7 @@ riscv_compute_frame_info (void)
   if (cfun->machine->interrupt_handler_p)
     {
       HOST_WIDE_INT step1 = riscv_first_stack_step (frame);
-      if (! SMALL_OPERAND (frame->total_size - step1))
+      if (! POLY_SMALL_OPERAND_P ((frame->total_size - step1)))
 	interrupt_save_prologue_temp = true;
     }
 
@@ -3788,23 +3951,23 @@ riscv_compute_frame_info (void)
     }
 
   /* At the bottom of the frame are any outgoing stack arguments. */
-  offset = RISCV_STACK_ALIGN (crtl->outgoing_args_size);
+  offset = riscv_stack_align (crtl->outgoing_args_size);
   /* Next are local stack variables. */
-  offset += RISCV_STACK_ALIGN (get_frame_size ());
+  offset += riscv_stack_align (get_frame_size ());
   /* The virtual frame pointer points above the local variables. */
   frame->frame_pointer_offset = offset;
   /* Next are the callee-saved FPRs. */
   if (frame->fmask)
-    offset += RISCV_STACK_ALIGN (num_f_saved * UNITS_PER_FP_REG);
+    offset += riscv_stack_align (num_f_saved * UNITS_PER_FP_REG);
   frame->fp_sp_offset = offset - UNITS_PER_FP_REG;
   /* Next are the callee-saved GPRs. */
   if (frame->mask)
     {
-      unsigned x_save_size = RISCV_STACK_ALIGN (num_x_saved * UNITS_PER_WORD);
+      unsigned x_save_size = riscv_stack_align (num_x_saved * UNITS_PER_WORD);
       unsigned num_save_restore = 1 + riscv_save_libcall_count (frame->mask);
 
       /* Only use save/restore routines if they don't alter the stack size.  */
-      if (RISCV_STACK_ALIGN (num_save_restore * UNITS_PER_WORD) == x_save_size)
+      if (riscv_stack_align (num_save_restore * UNITS_PER_WORD) == x_save_size)
 	{
 	  /* Libcall saves/restores 3 registers at once, so we need to
 	     allocate 12 bytes for callee-saved register.  */
@@ -3820,17 +3983,23 @@ riscv_compute_frame_info (void)
   /* The hard frame pointer points above the callee-saved GPRs. */
   frame->hard_frame_pointer_offset = offset;
   /* Above the hard frame pointer is the callee-allocated varags save area. */
-  offset += RISCV_STACK_ALIGN (cfun->machine->varargs_size);
+  offset += riscv_stack_align (cfun->machine->varargs_size);
   /* Next is the callee-allocated area for pretend stack arguments.  */
-  offset += RISCV_STACK_ALIGN (crtl->args.pretend_args_size);
+  offset += riscv_stack_align (crtl->args.pretend_args_size);
   /* Arg pointer must be below pretend args, but must be above alignment
      padding.  */
   frame->arg_pointer_offset = offset - crtl->args.pretend_args_size;
   frame->total_size = offset;
+
+  frame->min_first_step = riscv_stack_align (crtl->args.pretend_args_size)
+			  + riscv_stack_align (cfun->machine->varargs_size)
+			  + riscv_stack_align (num_f_saved * UNITS_PER_FP_REG)
+			  + riscv_stack_align (num_x_saved * UNITS_PER_WORD);
+
   /* Next points the incoming stack pointer and any incoming arguments. */
 
   /* Only use save/restore routines when the GPRs are atop the frame.  */
-  if (frame->hard_frame_pointer_offset != frame->total_size)
+  if (!known_eq (frame->hard_frame_pointer_offset, frame->total_size))
     frame->save_libcall_adjustment = 0;
 }
 
@@ -3847,10 +4016,10 @@ riscv_can_eliminate (const int from ATTRIBUTE_UNUSED, const int to)
    or argument pointer.  TO is either the stack pointer or hard frame
    pointer.  */
 
-HOST_WIDE_INT
+poly_int64
 riscv_initial_elimination_offset (int from, int to)
 {
-  HOST_WIDE_INT src, dest;
+  poly_int64 src, dest;
 
   riscv_compute_frame_info ();
 
@@ -3894,7 +4063,7 @@ riscv_set_return_address (rtx address, rtx scratch)
 
   gcc_assert (BITSET_P (cfun->machine->frame.mask, RETURN_ADDR_REGNUM));
   slot_address = riscv_add_offset (scratch, stack_pointer_rtx,
-				  cfun->machine->frame.gp_sp_offset);
+				   cfun->machine->frame.gp_sp_offset);
   riscv_emit_move (gen_frame_mem (GET_MODE (address), slot_address), address);
 }
 
@@ -3921,13 +4090,13 @@ riscv_save_restore_reg (machine_mode mode, int regno,
    of the frame.  */
 
 static void
-riscv_for_each_saved_reg (HOST_WIDE_INT sp_offset, riscv_save_restore_fn fn,
+riscv_for_each_saved_reg (poly_int64 sp_offset, riscv_save_restore_fn fn,
 			  bool epilogue, bool maybe_eh_return)
 {
   HOST_WIDE_INT offset;
 
   /* Save the link register and s-registers. */
-  offset = cfun->machine->frame.gp_sp_offset - sp_offset;
+  offset = (cfun->machine->frame.gp_sp_offset - sp_offset).to_constant ();
   for (unsigned int regno = GP_REG_FIRST; regno <= GP_REG_LAST; regno++)
     if (BITSET_P (cfun->machine->frame.mask, regno - GP_REG_FIRST))
       {
@@ -3958,14 +4127,14 @@ riscv_for_each_saved_reg (HOST_WIDE_INT sp_offset, riscv_save_restore_fn fn,
 
   /* This loop must iterate over the same space as its companion in
      riscv_compute_frame_info.  */
-  offset = cfun->machine->frame.fp_sp_offset - sp_offset;
+  offset = (cfun->machine->frame.fp_sp_offset - sp_offset).to_constant ();
   for (unsigned int regno = FP_REG_FIRST; regno <= FP_REG_LAST; regno++)
     if (BITSET_P (cfun->machine->frame.fmask, regno - FP_REG_FIRST))
       {
 	machine_mode mode = TARGET_DOUBLE_FLOAT ? DFmode : SFmode;
 
 	riscv_save_restore_reg (mode, regno, offset, fn);
-	offset -= GET_MODE_SIZE (mode);
+	offset -= GET_MODE_SIZE (mode).to_constant ();
       }
 }
 
@@ -4002,43 +4171,56 @@ riscv_restore_reg (rtx reg, rtx mem)
    compute the best value to initially allocate.  It must at a minimum
    allocate enough space to spill the callee-saved registers.  If TARGET_RVC,
    try to pick a value that will allow compression of the register saves
-   without adding extra instructions.  */
+   without adding extra instructions.
+
+   First stack step always step a costant range.  */
 
 static HOST_WIDE_INT
 riscv_first_stack_step (struct riscv_frame_info *frame)
 {
-  if (SMALL_OPERAND (frame->total_size))
-    return frame->total_size;
-
-  HOST_WIDE_INT min_first_step =
-    RISCV_STACK_ALIGN (frame->total_size - frame->fp_sp_offset);
-  HOST_WIDE_INT max_first_step = IMM_REACH / 2 - PREFERRED_STACK_BOUNDARY / 8;
-  HOST_WIDE_INT min_second_step = frame->total_size - max_first_step;
-  gcc_assert (min_first_step <= max_first_step);
-
-  /* As an optimization, use the least-significant bits of the total frame
-     size, so that the second adjustment step is just LUI + ADD.  */
-  if (!SMALL_OPERAND (min_second_step)
-      && frame->total_size % IMM_REACH < IMM_REACH / 2
-      && frame->total_size % IMM_REACH >= min_first_step)
-    return frame->total_size % IMM_REACH;
-
-  if (TARGET_RVC)
+  if (frame->total_size.is_constant ())
     {
-      /* If we need two subtracts, and one is small enough to allow compressed
-	 loads and stores, then put that one first.  */
-      if (IN_RANGE (min_second_step, 0,
-		    (TARGET_64BIT ? SDSP_REACH : SWSP_REACH)))
-	return MAX (min_second_step, min_first_step);
-
-      /* If we need LUI + ADDI + ADD for the second adjustment step, then start
-	 with the minimum first step, so that we can get compressed loads and
-	 stores.  */
-      else if (!SMALL_OPERAND (min_second_step))
-	return min_first_step;
+      HOST_WIDE_INT min_first_step =
+	riscv_stack_align (frame->total_size
+			   - frame->fp_sp_offset).to_constant ();
+      HOST_WIDE_INT total_size = constant_lower_bound (frame->total_size);
+
+      if (SMALL_OPERAND (total_size))
+	return total_size;
+
+      HOST_WIDE_INT max_first_step = ((IMM_REACH / 2)
+				      - (PREFERRED_STACK_BOUNDARY / 8));
+      HOST_WIDE_INT min_second_step = total_size - max_first_step;
+      gcc_assert (min_first_step <= max_first_step);
+
+      /* As an optimization, use the least-significant bits of the total frame
+	 size, so that the second adjustment step is just LUI + ADD.  */
+      if (!SMALL_OPERAND (min_second_step)
+	  && total_size % IMM_REACH < IMM_REACH / 2
+	  && total_size % IMM_REACH >= min_first_step)
+	return total_size % IMM_REACH;
+
+      if (TARGET_RVC)
+	{
+	  /* If we need two subtracts, and one is small enough to allow
+	     compressed loads and stores, then put that one first.  */
+	  if (IN_RANGE (min_second_step, 0,
+			(TARGET_64BIT ? SDSP_REACH : SWSP_REACH)))
+	    return MAX (min_second_step, min_first_step);
+
+          /* If we need LUI + ADDI + ADD for the second adjustment step, then
+	     start with the minimum first step, so that we can get compressed
+	     loads and stores.  */
+	  else if (!SMALL_OPERAND (min_second_step))
+	    return min_first_step;
+	}
+      return max_first_step;
+    }
+  else
+    {
+      /* There is scalable vector in stack, just stepping min_first_step.  */
+      return frame->min_first_step;
     }
-
-  return max_first_step;
 }
 
 static rtx
@@ -4088,18 +4270,91 @@ riscv_emit_stack_tie (void)
     emit_insn (gen_stack_tiedi (stack_pointer_rtx, hard_frame_pointer_rtx));
 }
 
+static rtx
+riscv_gen_load_poly_int (rtx target, rtx tmp1, rtx tmp2, poly_int64 value)
+{
+  gcc_assert (!value.is_constant ());
+  rtx insn;
+
+  HOST_WIDE_INT scalar_offset = 0;
+
+  if (value.coeffs[0] != value.coeffs[1])
+    scalar_offset = value.coeffs[0] - value.coeffs[1];
+
+  if (scalar_offset)
+    value -= scalar_offset;
+
+  gcc_assert (multiple_p (value, UNITS_PER_V_REG));
+  poly_int64 vlenb_mul = exact_div (value, UNITS_PER_V_REG);
+  emit_insn (gen_read_vlenb (tmp1));
+
+  gcc_assert (vlenb_mul.is_constant ());
+
+  HOST_WIDE_INT vlenb_mul_int = vlenb_mul.to_constant();
+
+  emit_move_insn (tmp2,
+		  gen_int_mode (vlenb_mul_int, Pmode));
+
+  if (TARGET_64BIT)
+    insn = gen_muldi3 (target, tmp1, tmp2);
+  else
+    insn = gen_mulsi3 (target, tmp1, tmp2);
+
+  if (scalar_offset)
+    {
+      emit_insn (insn);
+
+      if (SMALL_OPERAND (scalar_offset))
+	return gen_add3_insn (target, target, GEN_INT (scalar_offset));
+      else
+	{
+	  rtx remainder = riscv_add_offset (target, target, scalar_offset);
+	  return gen_rtx_SET (target, remainder);
+	}
+    }
+  else
+    return insn;
+}
+
+void
+riscv_adjust_frame (rtx target, poly_int64 offset)
+{
+  rtx temp_reg1 = RISCV_PROLOGUE_TEMP (Pmode);
+  rtx temp_reg2 = RISCV_PROLOGUE_TEMP2 (Pmode);
+  rtx insn, dwarf, adjust_frame_rtx;
+
+  emit_insn (riscv_gen_load_poly_int (temp_reg1, temp_reg1, temp_reg2, offset));
+
+  insn = gen_add3_insn (target,
+			target,
+			temp_reg1);
+
+  insn = emit_insn (insn);
+
+  RTX_FRAME_RELATED_P (insn) = 1;
+
+  adjust_frame_rtx =
+    gen_rtx_SET (target,
+		 plus_constant (Pmode, target, offset));
+
+  dwarf = alloc_reg_note (REG_FRAME_RELATED_EXPR,
+			  copy_rtx (adjust_frame_rtx), NULL_RTX);
+
+  REG_NOTES (insn) = dwarf;
+}
+
 /* Expand the "prologue" pattern.  */
 
 void
 riscv_expand_prologue (void)
 {
   struct riscv_frame_info *frame = &cfun->machine->frame;
-  HOST_WIDE_INT size = frame->total_size;
+  poly_int64 size = frame->total_size;
   unsigned mask = frame->mask;
   rtx insn;
 
   if (flag_stack_usage_info)
-    current_function_static_stack_size = size;
+    current_function_static_stack_size = constant_lower_bound (size);
 
   if (cfun->machine->naked_p)
     return;
@@ -4121,13 +4376,18 @@ riscv_expand_prologue (void)
   /* Save the registers.  */
   if ((frame->mask | frame->fmask) != 0)
     {
-      HOST_WIDE_INT step1 = MIN (size, riscv_first_stack_step (frame));
+      HOST_WIDE_INT step1 = riscv_first_stack_step (frame);
+
+      if (size.is_constant ())
+	step1 = MIN (size.to_constant(), step1);
+
+     size -= step1;
 
       insn = gen_add3_insn (stack_pointer_rtx,
 			    stack_pointer_rtx,
 			    GEN_INT (-step1));
       RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
-      size -= step1;
+
       riscv_for_each_saved_reg (size, riscv_save_reg, false, false);
     }
 
@@ -4136,31 +4396,46 @@ riscv_expand_prologue (void)
   /* Set up the frame pointer, if we're using one.  */
   if (frame_pointer_needed)
     {
+      poly_int64 offset = frame->hard_frame_pointer_offset - size;
       insn = gen_add3_insn (hard_frame_pointer_rtx, stack_pointer_rtx,
-			    GEN_INT (frame->hard_frame_pointer_offset - size));
+			    GEN_INT (offset.to_constant ()));
       RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
 
       riscv_emit_stack_tie ();
     }
 
   /* Allocate the rest of the frame.  */
-  if (size > 0)
+  if (known_gt (size, 0))
     {
-      if (SMALL_OPERAND (-size))
+      /* Two step adjustment, first for vector values.  */
+      if (!size.is_constant ())
+	{
+	  poly_int64 adj_offset = size;
+	  adj_offset.coeffs[0] = size.coeffs[1];
+	  riscv_adjust_frame (stack_pointer_rtx, -adj_offset);
+	  size -= adj_offset;
+	}
+
+      /* Second step for reset frame.  */
+      HOST_WIDE_INT size_value = size.to_constant ();
+      if (size_value == 0)
+	return;
+
+      if (SMALL_OPERAND (-size_value))
 	{
 	  insn = gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx,
-				GEN_INT (-size));
+				GEN_INT (-size_value));
 	  RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
 	}
       else
 	{
-	  riscv_emit_move (RISCV_PROLOGUE_TEMP (Pmode), GEN_INT (-size));
+	  riscv_emit_move (RISCV_PROLOGUE_TEMP (Pmode), GEN_INT (-size_value));
 	  emit_insn (gen_add3_insn (stack_pointer_rtx,
 				    stack_pointer_rtx,
 				    RISCV_PROLOGUE_TEMP (Pmode)));
 
 	  /* Describe the effect of the previous instructions.  */
-	  insn = plus_constant (Pmode, stack_pointer_rtx, -size);
+	  insn = plus_constant (Pmode, stack_pointer_rtx, -size_value);
 	  insn = gen_rtx_SET (stack_pointer_rtx, insn);
 	  riscv_set_frame_expr (insn);
 	}
@@ -4203,7 +4478,7 @@ riscv_expand_epilogue (int style)
      Start off by assuming that no registers need to be restored.  */
   struct riscv_frame_info *frame = &cfun->machine->frame;
   unsigned mask = frame->mask;
-  HOST_WIDE_INT step1 = frame->total_size;
+  poly_int64 step1 = frame->total_size;
   HOST_WIDE_INT step2 = 0;
   bool use_restore_libcall = ((style == NORMAL_RETURN)
 			      && riscv_use_save_libcall (frame));
@@ -4211,8 +4486,8 @@ riscv_expand_epilogue (int style)
   rtx insn;
 
   /* We need to add memory barrier to prevent read from deallocated stack.  */
-  bool need_barrier_p = (get_frame_size ()
-			 + cfun->machine->frame.arg_pointer_offset) != 0;
+  bool need_barrier_p = known_gt(get_frame_size ()
+				 + cfun->machine->frame.arg_pointer_offset, 0);
 
   if (cfun->machine->naked_p)
     {
@@ -4239,21 +4514,36 @@ riscv_expand_epilogue (int style)
       riscv_emit_stack_tie ();
       need_barrier_p = false;
 
-      rtx adjust = GEN_INT (-frame->hard_frame_pointer_offset);
-      if (!SMALL_OPERAND (INTVAL (adjust)))
+      poly_int64 adjust = -frame->hard_frame_pointer_offset;
+      rtx adjust_rtx = NULL_RTX;
+
+      if (!adjust.is_constant ())
 	{
-	  riscv_emit_move (RISCV_PROLOGUE_TEMP (Pmode), adjust);
-	  adjust = RISCV_PROLOGUE_TEMP (Pmode);
+	  rtx tmp1 = RISCV_PROLOGUE_TEMP (Pmode);
+	  rtx tmp2 = RISCV_PROLOGUE_TEMP2 (Pmode);
+	  emit_insn (riscv_gen_load_poly_int (tmp1, tmp1, tmp2, adjust));
+	  adjust_rtx = tmp1;
+	}
+      else
+	{
+	  if (!SMALL_OPERAND (adjust.to_constant ()))
+	    {
+	      riscv_emit_move (RISCV_PROLOGUE_TEMP (Pmode),
+			       GEN_INT (adjust.to_constant ()));
+	      adjust_rtx = RISCV_PROLOGUE_TEMP (Pmode);
+	    }
+	  else
+	    adjust_rtx = GEN_INT (adjust.to_constant ());
 	}
 
       insn = emit_insn (
 	       gen_add3_insn (stack_pointer_rtx, hard_frame_pointer_rtx,
-			      adjust));
+			      adjust_rtx));
 
       rtx dwarf = NULL_RTX;
       rtx cfa_adjust_value = gen_rtx_PLUS (
 			       Pmode, hard_frame_pointer_rtx,
-			       GEN_INT (-frame->hard_frame_pointer_offset));
+			       gen_int_mode (-frame->hard_frame_pointer_offset, Pmode));
       rtx cfa_adjust_rtx = gen_rtx_SET (stack_pointer_rtx, cfa_adjust_value);
       dwarf = alloc_reg_note (REG_CFA_ADJUST_CFA, cfa_adjust_rtx, dwarf);
       RTX_FRAME_RELATED_P (insn) = 1;
@@ -4270,31 +4560,41 @@ riscv_expand_epilogue (int style)
     }
 
   /* Set TARGET to BASE + STEP1.  */
-  if (step1 > 0)
+  if (known_gt (step1, 0))
     {
       /* Emit a barrier to prevent loads from a deallocated stack.  */
       riscv_emit_stack_tie ();
       need_barrier_p = false;
 
-      /* Get an rtx for STEP1 that we can add to BASE.  */
-      rtx adjust = GEN_INT (step1);
-      if (!SMALL_OPERAND (step1))
+      if (!step1.is_constant ())
 	{
-	  riscv_emit_move (RISCV_PROLOGUE_TEMP (Pmode), adjust);
-	  adjust = RISCV_PROLOGUE_TEMP (Pmode);
+	  poly_int64 adj_offset = step1;
+	  adj_offset.coeffs[0] = step1.coeffs[1];
+	  riscv_adjust_frame (stack_pointer_rtx, adj_offset);
+	  step1 -= adj_offset;
 	}
+	{
+	  HOST_WIDE_INT step1_val = step1.to_constant ();
+	  /* Get an rtx for STEP1 that we can add to BASE.  */
+	  rtx adjust = GEN_INT (step1_val);
+	  if (!SMALL_OPERAND (step1_val))
+	    {
+	      riscv_emit_move (RISCV_PROLOGUE_TEMP (Pmode), adjust);
+	      adjust = RISCV_PROLOGUE_TEMP (Pmode);
+	    }
 
-      insn = emit_insn (
-	       gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx, adjust));
+	  insn = emit_insn (
+		   gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx, adjust));
 
-      rtx dwarf = NULL_RTX;
-      rtx cfa_adjust_rtx = gen_rtx_PLUS (Pmode, stack_pointer_rtx,
-					 GEN_INT (step2));
+	  rtx dwarf = NULL_RTX;
+	  rtx cfa_adjust_rtx = gen_rtx_PLUS (Pmode, stack_pointer_rtx,
+					     GEN_INT (step2));
 
-      dwarf = alloc_reg_note (REG_CFA_DEF_CFA, cfa_adjust_rtx, dwarf);
-      RTX_FRAME_RELATED_P (insn) = 1;
+	  dwarf = alloc_reg_note (REG_CFA_DEF_CFA, cfa_adjust_rtx, dwarf);
+	  RTX_FRAME_RELATED_P (insn) = 1;
 
-      REG_NOTES (insn) = dwarf;
+	  REG_NOTES (insn) = dwarf;
+	}
     }
   else if (frame_pointer_needed)
     {
@@ -4396,7 +4696,7 @@ riscv_epilogue_uses (unsigned int regno)
 bool
 riscv_can_use_return_insn (void)
 {
-  return (reload_completed && cfun->machine->frame.total_size == 0
+  return (reload_completed && known_eq (cfun->machine->frame.total_size, 0)
 	  && ! cfun->machine->interrupt_handler_p);
 }
 
@@ -4496,7 +4796,8 @@ static bool
 riscv_secondary_memory_needed (machine_mode mode, reg_class_t class1,
 			       reg_class_t class2)
 {
-  return (GET_MODE_SIZE (mode) > UNITS_PER_WORD
+  return (!VECTOR_MODE_P (mode)
+	  && GET_MODE_SIZE (mode).to_constant () > UNITS_PER_WORD
 	  && (class1 == FP_REGS) != (class2 == FP_REGS));
 }
 
@@ -4516,14 +4817,51 @@ riscv_register_move_cost (machine_mode mode,
 
 /* Implement TARGET_HARD_REGNO_NREGS.  */
 
+static bool
+riscv_vector_mode (machine_mode mode)
+{
+  scalar_mode inner = GET_MODE_INNER (mode);
+  if (VECTOR_MODE_P (mode)
+      && (inner == BImode
+	  || inner == QImode
+	  || inner == HImode
+	  || inner == HFmode
+	  || inner == SImode
+	  || inner == SFmode
+	  || inner == DImode
+	  || inner == DFmode))
+    return true;
+
+  return false;
+}
+
 static unsigned int
 riscv_hard_regno_nregs (unsigned int regno, machine_mode mode)
 {
+  /* mode for VL or VTYPE are just a marker, not holding value,
+     so it always consume one register.  */
+  if (riscv_vector_mode (mode) &&
+      (regno == VL_REGNUM || regno == VTYPE_REGNUM))
+    return 1;
+
+  /* riscv_hard_regno_mode_ok calls here first, so we must accept vector
+     modes in any register, but the result won't be used for non-vector
+     registers.  */
+  if (riscv_vector_mode (mode))
+    return exact_div (GET_MODE_SIZE (mode),
+		      BYTES_PER_RVV_VECTOR).to_constant ();
+
+  HOST_WIDE_INT constant_size = GET_MODE_SIZE (mode).to_constant ();
+
   if (FP_REG_P (regno))
-    return (GET_MODE_SIZE (mode) + UNITS_PER_FP_REG - 1) / UNITS_PER_FP_REG;
+    return CEIL (constant_size, UNITS_PER_FP_REG);
+
+  /* Assume every valid non-vector mode fits in one vector register.  */
+  if (VECT_REG_P (regno))
+    return 1;
 
   /* All other registers are word-sized.  */
-  return (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
+  return CEIL (constant_size, UNITS_PER_WORD);
 }
 
 /* Implement TARGET_HARD_REGNO_MODE_OK.  */
@@ -4537,14 +4875,19 @@ riscv_hard_regno_mode_ok (unsigned int regno, machine_mode mode)
     {
       if (!GP_REG_P (regno + nregs - 1))
 	return false;
+
+      if (VECTOR_MODE_P (mode))
+	return false;
     }
   else if (FP_REG_P (regno))
     {
       if (!FP_REG_P (regno + nregs - 1))
 	return false;
 
-      if (GET_MODE_CLASS (mode) != MODE_FLOAT
-	  && GET_MODE_CLASS (mode) != MODE_COMPLEX_FLOAT)
+      if (!FLOAT_MODE_P (mode))
+	return false;
+
+      if (VECTOR_MODE_P (mode))
 	return false;
 
       /* Only use callee-saved registers if a potential callee is guaranteed
@@ -4554,6 +4897,52 @@ riscv_hard_regno_mode_ok (unsigned int regno, machine_mode mode)
 	      && GET_MODE_UNIT_SIZE (mode) > UNITS_PER_FP_ARG))
 	return false;
     }
+  else if (VECT_REG_P (regno))
+   {
+      int align = -1;
+      if (!VECT_REG_P (regno + nregs -1))
+	return false;
+
+      /* Assume only vector modes fit in vector registers.  */
+      if (!VECTOR_MODE_P (mode))
+	return false;
+
+      switch (mode)
+	{
+#define VEC_INT_TUPLE_TYPES(SEW, LMUL, NF, MLEN, SMODE_PREFIX_UPPER,	\
+			    SMODE_PREFIX_LOWER, VMODE_PREFIX_UPPER,	\
+			    VMODE_PREFIX_LOWER, X)			\
+	case E_##VMODE_PREFIX_UPPER##Imode:				\
+	  align = LMUL;							\
+	  break;
+	_RVV_SEG_ARG (VEC_INT_TUPLE_TYPES, X)
+#undef VEC_INT_TUPLE_TYPES
+#define VEC_FLOAT_TUPLE_TYPES(SEW, LMUL, NF, MLEN, SMODE_PREFIX_UPPER,	\
+			      SMODE_PREFIX_LOWER, VMODE_PREFIX_UPPER,	\
+			      VMODE_PREFIX_LOWER, X)			\
+	case E_##VMODE_PREFIX_UPPER##Fmode:				\
+	  align = LMUL;							\
+	  break;
+	_RVV_SEG_NO_SEW8_ARG (VEC_FLOAT_TUPLE_TYPES, X)
+#undef VEC_FLOAT_TUPLE_TYPES
+	default:
+	  align = nregs;
+	}
+
+      /* Check alignment requirement for vector mode.  */
+      if ((regno & (align - 1)) != 0)
+	return false;
+    }
+  else if (regno == VTYPE_REGNUM || regno == VL_REGNUM)
+   {
+      /* Assume only vector modes fit in vector registers.  */
+      if (!VECTOR_MODE_P (mode))
+	return false;
+
+      /* Never hold value using more than 1 reg for VL and VTYPE.  */
+      if (nregs != 1)
+	return false;
+    }
   else
     return false;
 
@@ -4590,6 +4979,9 @@ riscv_class_max_nregs (reg_class_t rclass, machine_mode mode)
   if (reg_class_subset_p (GR_REGS, rclass))
     return riscv_hard_regno_nregs (GP_REG_FIRST, mode);
 
+  if (reg_class_subset_p (VECTOR_REGS, rclass))
+    return riscv_hard_regno_nregs (VECT_REG_FIRST, mode);
+
   return 0;
 }
 
@@ -4829,6 +5221,10 @@ riscv_option_override (void)
 	   " [%<-mriscv-attribute%>]");
 #endif
 
+  if (riscv_rvv_vector_bits == RVV_SCALABLE)
+    riscv_rvv_chunks = poly_uint16 (2, 2);
+  else
+    riscv_rvv_chunks = (int) riscv_rvv_vector_bits / 64;
 }
 
 /* Implement TARGET_CONDITIONAL_REGISTER_USAGE.  */
@@ -4861,6 +5257,12 @@ riscv_conditional_register_usage (void)
       for (int regno = FP_REG_FIRST; regno <= FP_REG_LAST; regno++)
 	call_used_regs[regno] = 1;
     }
+
+  if (!TARGET_VECTOR)
+    {
+      for (int regno = VECT_REG_FIRST; regno <= VECT_REG_LAST; regno++)
+	fixed_regs[regno] = call_used_regs[regno] = 1;
+    }
 }
 
 /* Return a register priority for hard reg REGNO.  */
@@ -5178,15 +5580,14 @@ riscv_promote_function_mode (const_tree type ATTRIBUTE_UNUSED,
 			     const_tree fntype ATTRIBUTE_UNUSED,
 			     int for_return ATTRIBUTE_UNUSED)
 {
-  int unsignedp;
-
   if (type != NULL_TREE)
     return promote_mode (type, mode, punsignedp);
 
-  unsignedp = *punsignedp;
-  PROMOTE_MODE (mode, unsignedp, type);
+  int unsignedp = *punsignedp;
+  scalar_mode smode = as_a <scalar_mode> (mode);
+  PROMOTE_MODE (smode, unsignedp, type);
   *punsignedp = unsignedp;
-  return mode;
+  return smode;
 }
 
 /* Implement TARGET_MACHINE_DEPENDENT_REORG.  */
@@ -5309,6 +5710,188 @@ riscv_mangle_type (const_tree type)
   if (TREE_CODE (type) == REAL_TYPE && TYPE_PRECISION (type) == 16)
     return "Dh";
 
+  /* Mangle all vector type for vector extension.  */
+  /* XXX: Revise this later, we don't write down this into spec yet.  */
+  if (TARGET_VECTOR && VECTOR_MODE_P (TYPE_MODE (type)))
+    switch (TYPE_MODE (type))
+      {
+      case E_VNx16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1_t" : "_vint8m1_t";
+      case E_VNx32QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m2_t" : "_vint8m2_t";
+      case E_VNx64QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m4_t" : "_vint8m4_t";
+      case E_VNx128QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m8_t" : "_vint8m8_t";
+      case E_VNx8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1_t" : "_vint16m1_t";
+      case E_VNx16HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m2_t" : "_vint16m2_t";
+      case E_VNx32HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m4_t" : "_vint16m4_t";
+      case E_VNx64HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m8_t" : "_vint16m8_t";
+      case E_VNx4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1_t" : "_vint32m1_t";
+      case E_VNx8SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m2_t" : "_vint32m2_t";
+      case E_VNx16SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m4_t" : "_vint32m4_t";
+      case E_VNx32SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m8_t" : "_vint32m8_t";
+      case E_VNx2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1_t" : "_vint64m1_t";
+      case E_VNx4DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m2_t" : "_vint64m2_t";
+      case E_VNx8DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m4_t" : "_vint64m4_t";
+      case E_VNx16DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m8_t" : "_vint64m8_t";
+      case E_VNx8HFmode:   return "_vfloat16m1_t";
+      case E_VNx16HFmode:  return "_vfloat16m2_t";
+      case E_VNx32HFmode:  return "_vfloat16m4_t";
+      case E_VNx64HFmode:  return "_vfloat16m8_t";
+      case E_VNx4SFmode:   return "_vfloat32m1_t";
+      case E_VNx8SFmode:   return "_vfloat32m2_t";
+      case E_VNx16SFmode:  return "_vfloat32m4_t";
+      case E_VNx32SFmode:  return "_vfloat32m8_t";
+      case E_VNx2DFmode:   return "_vfloat64m1_t";
+      case E_VNx4DFmode:   return "_vfloat64m2_t";
+      case E_VNx8DFmode:   return "_vfloat64m4_t";
+      case E_VNx16DFmode:  return "_vfloat64m8_t";
+      case E_VNx2BImode:   return "_vbool64_t";
+      case E_VNx4BImode:   return "_vbool32_t";
+      case E_VNx8BImode:   return "_vbool16_t";
+      case E_VNx16BImode:  return "_vbool8_t";
+      case E_VNx32BImode:  return "_vbool4_t";
+      case E_VNx64BImode:  return "_vbool2_t";
+      case E_VNx128BImode: return "_vbool1_t";
+
+      case E_VNx2x16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1x2_t": "_vint8m1x2_t";
+      case E_VNx3x16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1x3_t": "_vint8m1x3_t";
+      case E_VNx4x16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1x4_t": "_vint8m1x4_t";
+      case E_VNx5x16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1x5_t": "_vint8m1x5_t";
+      case E_VNx6x16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1x6_t": "_vint8m1x6_t";
+      case E_VNx7x16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1x7_t": "_vint8m1x7_t";
+      case E_VNx8x16QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m1x8_t": "_vint8m1x8_t";
+      case E_VNx2x32QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m2x2_t": "_vint8m2x2_t";
+      case E_VNx3x32QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m2x3_t": "_vint8m2x3_t";
+      case E_VNx4x32QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m2x4_t": "_vint8m2x4_t";
+      case E_VNx2x64QImode:
+	return TYPE_UNSIGNED(type) ? "_vuint8m4x2_t": "_vint8m4x2_t";
+      case E_VNx2x8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1x2_t": "_vint16m1x2_t";
+      case E_VNx3x8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1x3_t": "_vint16m1x3_t";
+      case E_VNx4x8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1x4_t": "_vint16m1x4_t";
+      case E_VNx5x8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1x5_t": "_vint16m1x5_t";
+      case E_VNx6x8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1x6_t": "_vint16m1x6_t";
+      case E_VNx7x8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1x7_t": "_vint16m1x7_t";
+      case E_VNx8x8HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m1x8_t": "_vint16m1x8_t";
+      case E_VNx2x16HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m2x2_t": "_vint16m2x2_t";
+      case E_VNx3x16HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m2x3_t": "_vint16m2x3_t";
+      case E_VNx4x16HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m2x4_t": "_vint16m2x4_t";
+      case E_VNx2x32HImode:
+	return TYPE_UNSIGNED(type) ? "_vuint16m4x2_t": "_vint16m4x2_t";
+      case E_VNx2x4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1x2_t": "_vint32m1x2_t";
+      case E_VNx3x4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1x3_t": "_vint32m1x3_t";
+      case E_VNx4x4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1x4_t": "_vint32m1x4_t";
+      case E_VNx5x4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1x5_t": "_vint32m1x5_t";
+      case E_VNx6x4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1x6_t": "_vint32m1x6_t";
+      case E_VNx7x4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1x7_t": "_vint32m1x7_t";
+      case E_VNx8x4SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m1x8_t": "_vint32m1x8_t";
+      case E_VNx2x8SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m2x2_t": "_vint32m2x2_t";
+      case E_VNx3x8SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m2x3_t": "_vint32m2x3_t";
+      case E_VNx4x8SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m2x4_t": "_vint32m2x4_t";
+      case E_VNx2x16SImode:
+	return TYPE_UNSIGNED(type) ? "_vuint32m4x2_t": "_vint32m4x2_t";
+      case E_VNx2x2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1x2_t": "_vint64m1x2_t";
+      case E_VNx3x2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1x3_t": "_vint64m1x3_t";
+      case E_VNx4x2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1x4_t": "_vint64m1x4_t";
+      case E_VNx5x2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1x5_t": "_vint64m1x5_t";
+      case E_VNx6x2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1x6_t": "_vint64m1x6_t";
+      case E_VNx7x2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1x7_t": "_vint64m1x7_t";
+      case E_VNx8x2DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m1x8_t": "_vint64m1x8_t";
+      case E_VNx2x4DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m2x2_t": "_vint64m2x2_t";
+      case E_VNx3x4DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m2x3_t": "_vint64m2x3_t";
+      case E_VNx4x4DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m2x4_t": "_vint64m2x4_t";
+      case E_VNx2x8DImode:
+	return TYPE_UNSIGNED(type) ? "_vuint64m4x2_t": "_vint64m4x2_t";
+      case E_VNx2x8HFmode: return "_vfloat16m1x2_t";
+      case E_VNx3x8HFmode: return "_vfloat16m1x3_t";
+      case E_VNx4x8HFmode: return "_vfloat16m1x4_t";
+      case E_VNx5x8HFmode: return "_vfloat16m1x5_t";
+      case E_VNx6x8HFmode: return "_vfloat16m1x6_t";
+      case E_VNx7x8HFmode: return "_vfloat16m1x7_t";
+      case E_VNx8x8HFmode: return "_vfloat16m1x8_t";
+      case E_VNx2x16HFmode: return "_vfloat16m2x2_t";
+      case E_VNx3x16HFmode: return "_vfloat16m2x3_t";
+      case E_VNx4x16HFmode: return "_vfloat16m2x4_t";
+      case E_VNx2x32HFmode: return "_vfloat16m4x2_t";
+      case E_VNx2x4SFmode: return "_vfloat32m1x2_t";
+      case E_VNx3x4SFmode: return "_vfloat32m1x3_t";
+      case E_VNx4x4SFmode: return "_vfloat32m1x4_t";
+      case E_VNx5x4SFmode: return "_vfloat32m1x5_t";
+      case E_VNx6x4SFmode: return "_vfloat32m1x6_t";
+      case E_VNx7x4SFmode: return "_vfloat32m1x7_t";
+      case E_VNx8x4SFmode: return "_vfloat32m1x8_t";
+      case E_VNx2x8SFmode: return "_vfloat32m2x2_t";
+      case E_VNx3x8SFmode: return "_vfloat32m2x3_t";
+      case E_VNx4x8SFmode: return "_vfloat32m2x4_t";
+      case E_VNx2x16SFmode: return "_vfloat32m4x2_t";
+      case E_VNx2x2DFmode: return "_vfloat64m1x2_t";
+      case E_VNx3x2DFmode: return "_vfloat64m1x3_t";
+      case E_VNx4x2DFmode: return "_vfloat64m1x4_t";
+      case E_VNx5x2DFmode: return "_vfloat64m1x5_t";
+      case E_VNx6x2DFmode: return "_vfloat64m1x6_t";
+      case E_VNx7x2DFmode: return "_vfloat64m1x7_t";
+      case E_VNx8x2DFmode: return "_vfloat64m1x8_t";
+      case E_VNx2x4DFmode: return "_vfloat64m2x2_t";
+      case E_VNx3x4DFmode: return "_vfloat64m2x3_t";
+      case E_VNx4x4DFmode: return "_vfloat64m2x4_t";
+      case E_VNx2x8DFmode: return "_vfloat64m4x2_t";
+      default:
+	break;
+      }
+
   /* Use the default mangling.  */
   return NULL;
 }
@@ -5381,6 +5964,240 @@ riscv_floatn_mode (int n, bool extended)
   return default_floatn_mode (n, extended);
 }
 
+/* Implement TARGET_VECTOR_MODE_SUPPORTED_P.  */
+
+static bool
+riscv_vector_mode_supported_p (machine_mode mode)
+{
+  if (TARGET_VECTOR && riscv_vector_mode (mode))
+    return true;
+
+  return false;
+}
+
+/* Implement TARGET_VECTORIZE_PREFERRED_SIMD_MODE.  */
+
+static machine_mode
+riscv_preferred_simd_mode (scalar_mode mode)
+{
+#if 0
+  /* Disable auto vectorization since it's broken for V extension,
+     but we'll fix and enable that later.  */
+  if (TARGET_VECTOR)
+    switch (mode)
+      {
+      case E_DFmode:
+	return VNx2DFmode;
+      case E_SFmode:
+	return VNx4SFmode;
+      case E_HFmode:
+	return VNx8HFmode;
+      case E_DImode:
+	return VNx2DImode;
+      case E_SImode:
+	return VNx4SImode;
+      case E_HImode:
+	return VNx8HImode;
+      case E_QImode:
+	return VNx16QImode;
+      default:
+	return word_mode;
+      }
+#endif
+
+  return word_mode;
+}
+
+/* Implement TARGET_VECTOR_ALIGNMENT.  */
+
+static HOST_WIDE_INT
+riscv_vector_alignment (const_tree type)
+{
+  /* ??? Vectors actually only require element alignment, but that could
+     potentially cause ABI changes when RVV support is off, so use a
+     constant value.  */
+  if (TREE_CODE (TYPE_SIZE (type)) != INTEGER_CST)
+    return 64;
+
+  /* Don't assume that TYPE_SIZE fits in a HOST_WIDE_INT.  */
+  HOST_WIDE_INT align
+    = wi::umin (wi::to_wide (TYPE_SIZE (type)), 128).to_uhwi ();
+  /* The selftest option tests V8HImode, so we have to be able to handle
+     that here.  It becomes TImode, which requires TImode alignment.  */
+  if (VECTOR_MODE_P (TYPE_MODE (type)))
+    return MIN (align, 64);
+  else
+    return MIN (align, 128);
+}
+
+/* Implement the TARGET_DWARF_POLY_INDETERMINATE_VALUE hook.  */
+
+static unsigned int
+riscv_dwarf_poly_indeterminate_value (unsigned int i, unsigned int *factor,
+				      int *offset)
+{
+  /* Polynomial invariant 1 == (VLENB / 8) - 1.  */
+  /* XXX: It's might not correct for ELEN=32 system.  */
+  gcc_assert (i == 1);
+  *factor = 8;
+  *offset = 1;
+  return RISCV_DWARF_VLEN;
+}
+
+/* Implement REGMODE_NATURAL_SIZE.  */
+
+poly_uint64
+riscv_regmode_natural_size (machine_mode mode)
+{
+  if (TARGET_VECTOR && VECTOR_MODE_P (mode))
+    return BYTES_PER_RVV_VECTOR;
+  return UNITS_PER_WORD;
+}
+
+/* Get the number of fields for the mode, MODE should be a machine mode for
+   vector tuple types.  */
+
+int
+riscv_get_nf (machine_mode mode)
+{
+  switch (mode)
+    {
+#define VEC_INT_TUPLE_TYPES(SEW, LMUL, NF, MLEN, SMODE_PREFIX_UPPER,	\
+			    SMODE_PREFIX_LOWER, VMODE_PREFIX_UPPER,	\
+			    VMODE_PREFIX_LOWER, X)			\
+    case E_##VMODE_PREFIX_UPPER##Imode:					\
+      return NF;
+    _RVV_SEG_ARG (VEC_INT_TUPLE_TYPES, X)
+#undef VEC_INT_TUPLE_TYPES
+#define VEC_FLOAT_TUPLE_TYPES(SEW, LMUL, NF, MLEN, SMODE_PREFIX_UPPER,	\
+			      SMODE_PREFIX_LOWER, VMODE_PREFIX_UPPER,	\
+			      VMODE_PREFIX_LOWER, X)			\
+    case E_##VMODE_PREFIX_UPPER##Fmode:				\
+      return NF;
+    _RVV_SEG_NO_SEW8_ARG (VEC_FLOAT_TUPLE_TYPES, X)
+#undef VEC_FLOAT_TUPLE_TYPES
+    default:
+      /* Non-vector tuple type should not call this function.  */
+      gcc_unreachable ();
+      return -1;
+    }
+}
+
+/* Routines for expand vtuple_create pattern.  */
+
+void
+riscv_expand_vtuple_create (rtx *operands)
+{
+  machine_mode vtmode = GET_MODE (operands[0]);
+  int nf = riscv_get_nf (vtmode);
+  int i;
+  gcc_assert (nf != -1);
+
+  /* Made GCC won't 0-initialize register.  */
+  emit_clobber (operands[0]);
+
+  for (i = 0; i <nf; ++i)
+    switch (vtmode)
+      {
+#define VEC_INT_TUPLE_TYPES(SEW, LMUL, NF, MLEN, SMODE_PREFIX_UPPER,	\
+			    SMODE_PREFIX_LOWER, VMODE_PREFIX_UPPER,	\
+			    VMODE_PREFIX_LOWER, X)			\
+	case E_##VMODE_PREFIX_UPPER##Imode:				\
+	  emit_insn (							\
+	    gen_vtuple_insert##VMODE_PREFIX_LOWER##i (			\
+	      operands[0], operands[0],					\
+	      operands[i + 1], GEN_INT (i)));				\
+	  break;
+	_RVV_SEG_ARG (VEC_INT_TUPLE_TYPES, X)
+#undef VEC_INT_TUPLE_TYPES
+#define VEC_FLOAT_TUPLE_TYPES(SEW, LMUL, NF, MLEN, SMODE_PREFIX_UPPER,	\
+			      SMODE_PREFIX_LOWER, VMODE_PREFIX_UPPER,	\
+			      VMODE_PREFIX_LOWER, X)			\
+	case E_##VMODE_PREFIX_UPPER##Fmode:				\
+	  emit_insn (							\
+	    gen_vtuple_insert##VMODE_PREFIX_LOWER##f (			\
+	      operands[0], operands[0],					\
+	      operands[i + 1], GEN_INT (i)));				\
+	  break;
+	_RVV_SEG_NO_SEW8_ARG (VEC_FLOAT_TUPLE_TYPES, X)
+#undef VEC_FLOAT_TUPLE_TYPES
+      default:
+	gcc_unreachable ();
+      }
+}
+
+static bool
+riscv_verify_type_context (location_t loc, type_context_kind context,
+			   const_tree type, bool silent_p)
+{
+  if (type == error_mark_node)
+    return true;
+
+  if (GET_MODE_SIZE (TYPE_MODE (type)).is_constant ())
+    return true;
+
+  switch (context)
+    {
+    case TCTX_SIZEOF:
+    case TCTX_STATIC_STORAGE:
+      if (!silent_p)
+	error_at (loc, "RVV type %qT does not have a fixed size", type);
+      return false;
+
+    case TCTX_ALIGNOF:
+      if (!silent_p)
+	error_at (loc, "RVV type %qT does not have a defined alignment", type);
+      return false;
+
+    case TCTX_THREAD_STORAGE:
+      if (!silent_p)
+	error_at (loc, "variables of type %qT cannot have thread-local"
+		  " storage duration", type);
+      return false;
+
+    case TCTX_POINTER_ARITH:
+      if (!silent_p)
+	error_at (loc, "arithmetic on pointer to RVV type %qT", type);
+      return false;
+
+    case TCTX_FIELD:
+      if (silent_p)
+	;
+      else if (lang_GNU_CXX ())
+	error_at (loc, "member variables cannot have RVV type %qT", type);
+      else
+	error_at (loc, "fields cannot have RVV type %qT", type);
+      return false;
+
+    case TCTX_ARRAY_ELEMENT:
+      if (!silent_p)
+	error_at (loc, "array elements cannot have RVV type %qT", type);
+      return false;
+
+    case TCTX_ALLOCATION:
+      if (!silent_p)
+	error_at (loc, "cannot allocate objects with RVV type %qT", type);
+      return false;
+
+    case TCTX_DEALLOCATION:
+      if (!silent_p)
+	error_at (loc, "cannot delete objects with RVV type %qT", type);
+      return false;
+
+    case TCTX_EXCEPTIONS:
+      if (!silent_p)
+	error_at (loc, "cannot throw or catch RVV type %qT", type);
+      return false;
+
+    case TCTX_CAPTURE_BY_COPY:
+      if (!silent_p)
+	error_at (loc, "capture by copy of RVV type %qT", type);
+      return false;
+    }
+  gcc_unreachable ();
+
+}
+
 /* Initialize the GCC target structure.  */
 #undef TARGET_ASM_ALIGNED_HI_OP
 #define TARGET_ASM_ALIGNED_HI_OP "\t.half\t"
@@ -5578,6 +6395,22 @@ riscv_floatn_mode (int n, bool extended)
 #undef TARGET_FLOATN_MODE
 #define TARGET_FLOATN_MODE riscv_floatn_mode
 
+#undef TARGET_VECTOR_MODE_SUPPORTED_P
+#define TARGET_VECTOR_MODE_SUPPORTED_P riscv_vector_mode_supported_p
+
+#undef TARGET_VECTORIZE_PREFERRED_SIMD_MODE
+#define TARGET_VECTORIZE_PREFERRED_SIMD_MODE riscv_preferred_simd_mode
+
+#undef TARGET_VECTOR_ALIGNMENT
+#define TARGET_VECTOR_ALIGNMENT riscv_vector_alignment
+
+#undef TARGET_DWARF_POLY_INDETERMINATE_VALUE
+#define TARGET_DWARF_POLY_INDETERMINATE_VALUE \
+  riscv_dwarf_poly_indeterminate_value
+
+#undef TARGET_VERIFY_TYPE_CONTEXT
+#define TARGET_VERIFY_TYPE_CONTEXT riscv_verify_type_context
+
 struct gcc_target targetm = TARGET_INITIALIZER;
 
 #include "gt-riscv.h"
diff --git a/gcc/config/riscv/riscv.h b/gcc/config/riscv/riscv.h
index 91eb63cb6f8..41027c58745 100644
--- a/gcc/config/riscv/riscv.h
+++ b/gcc/config/riscv/riscv.h
@@ -113,7 +113,8 @@ ASM_MISA_SPEC
 
 /* The mapping from gcc register number to DWARF 2 CFA column number.  */
 #define DWARF_FRAME_REGNUM(REGNO) \
-  (GP_REG_P (REGNO) || FP_REG_P (REGNO) ? REGNO : INVALID_REGNUM)
+  (GP_REG_P (REGNO) || FP_REG_P (REGNO) || VECT_REG_P (REGNO)	\
+   ? REGNO : INVALID_REGNUM)
 
 /* The DWARF 2 CFA column which tracks the return address.  */
 #define DWARF_FRAME_RETURN_COLUMN RETURN_ADDR_REGNUM
@@ -141,6 +142,7 @@ ASM_MISA_SPEC
 
 /* The `Q' extension is not yet supported.  */
 #define UNITS_PER_FP_REG (TARGET_DOUBLE_FLOAT ? 8 : 4)
+#define UNITS_PER_V_REG (GET_MODE_SIZE (VNx2DImode))
 
 /* The largest type that can be passed in floating-point registers.  */
 #define UNITS_PER_FP_ARG						\
@@ -275,9 +277,13 @@ ASM_MISA_SPEC
    - 32 floating point registers
    - 2 fake registers:
 	- ARG_POINTER_REGNUM
-	- FRAME_POINTER_REGNUM */
+	- FRAME_POINTER_REGNUM
+	- VECTOR_LENGTH_REGNUM
+	- VECTOR_TYPE_REGNUM
+   - 30 unused registers for future expansion
+   - 32 vector registers */
 
-#define FIRST_PSEUDO_REGISTER 66
+#define FIRST_PSEUDO_REGISTER 128
 
 /* x0, sp, gp, and tp are fixed.  */
 
@@ -289,7 +295,11 @@ ASM_MISA_SPEC
   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
   /* Others.  */							\
-  1, 1									\
+  1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,			\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,			\
+  /* Vector registers.  */						\
+  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
+  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0			\
 }
 
 /* a0-a7, t0-t6, fa0-fa7, and ft0-ft11 are volatile across calls.
@@ -303,7 +313,11 @@ ASM_MISA_SPEC
   1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,			\
   1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,			\
   /* Others.  */							\
-  1, 1									\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,			\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,			\
+  /* Vector registers.  */						\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,			\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1			\
 }
 
 /* Select a register mode required for caller save of hard regno REGNO.
@@ -323,6 +337,10 @@ ASM_MISA_SPEC
 #define FP_REG_LAST  63
 #define FP_REG_NUM   (FP_REG_LAST - FP_REG_FIRST + 1)
 
+#define VECT_REG_FIRST 96
+#define VECT_REG_LAST  127
+#define VECT_REG_NUM   (VECT_REG_LAST - VECT_REG_FIRST + 1)
+
 /* The DWARF 2 CFA column which tracks the return address from a
    signal handler context.  This means that to maintain backwards
    compatibility, no hard register can be assigned this column if it
@@ -331,8 +349,10 @@ ASM_MISA_SPEC
 
 #define GP_REG_P(REGNO)	\
   ((unsigned int) ((int) (REGNO) - GP_REG_FIRST) < GP_REG_NUM)
-#define FP_REG_P(REGNO)  \
+#define FP_REG_P(REGNO) \
   ((unsigned int) ((int) (REGNO) - FP_REG_FIRST) < FP_REG_NUM)
+#define VECT_REG_P(REGNO) \
+  ((unsigned int) ((int) (REGNO) - VECT_REG_FIRST) < VECT_REG_NUM)
 
 /* True when REGNO is in SIBCALL_REGS set.  */
 #define SIBCALL_REG_P(REGNO)	\
@@ -349,6 +369,10 @@ ASM_MISA_SPEC
    the stack or hard frame pointer.  */
 #define ARG_POINTER_REGNUM 64
 #define FRAME_POINTER_REGNUM 65
+/* These two registers don't really exist: they are used for vector
+   operations.  */
+#define VECTOR_LENGTH_REGNUM 66
+#define VECTOR_TYPE_REGNUM 67
 
 /* Register in which static-chain is passed to a function.  */
 #define STATIC_CHAIN_REGNUM (GP_TEMP_FIRST + 2)
@@ -362,6 +386,8 @@ ASM_MISA_SPEC
 
 #define RISCV_PROLOGUE_TEMP_REGNUM (GP_TEMP_FIRST)
 #define RISCV_PROLOGUE_TEMP(MODE) gen_rtx_REG (MODE, RISCV_PROLOGUE_TEMP_REGNUM)
+#define RISCV_PROLOGUE_TEMP2_REGNUM (GP_TEMP_FIRST + 2)
+#define RISCV_PROLOGUE_TEMP2(MODE) gen_rtx_REG (MODE, RISCV_PROLOGUE_TEMP2_REGNUM)
 
 #define RISCV_CALL_ADDRESS_TEMP_REGNUM (GP_TEMP_FIRST + 1)
 #define RISCV_CALL_ADDRESS_TEMP(MODE) \
@@ -416,6 +442,10 @@ enum reg_class
   GR_REGS,			/* integer registers */
   FP_REGS,			/* floating-point registers */
   FRAME_REGS,			/* arg pointer and frame pointer */
+  VECTOR_MASK_REGS,		/* vector mask registers */
+  VECTOR_NO_MASK_REGS,		/* vector registers except mask registers */
+  VECTOR_REGS,			/* vector registers */
+  VTYPE_REGS,			/* vype register */
   ALL_REGS,			/* all registers */
   LIM_REG_CLASSES		/* max value + 1 */
 };
@@ -436,6 +466,10 @@ enum reg_class
   "GR_REGS",								\
   "FP_REGS",								\
   "FRAME_REGS",								\
+  "VECTOR_MASK_REGS", 							\
+  "VECTOR_NO_MASK_REGS", 						\
+  "VECTOR_REGS", 							\
+  "VTYPE_REGS", 							\
   "ALL_REGS"								\
 }
 
@@ -452,13 +486,17 @@ enum reg_class
 
 #define REG_CLASS_CONTENTS						\
 {									\
-  { 0x00000000, 0x00000000, 0x00000000 },	/* NO_REGS */		\
-  { 0xf003fcc0, 0x00000000, 0x00000000 },	/* SIBCALL_REGS */	\
-  { 0xffffffc0, 0x00000000, 0x00000000 },	/* JALR_REGS */		\
-  { 0xffffffff, 0x00000000, 0x00000000 },	/* GR_REGS */		\
-  { 0x00000000, 0xffffffff, 0x00000000 },	/* FP_REGS */		\
-  { 0x00000000, 0x00000000, 0x00000003 },	/* FRAME_REGS */	\
-  { 0xffffffff, 0xffffffff, 0x00000003 }	/* ALL_REGS */		\
+  { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	/* NO_REGS */		\
+  { 0xf003fcc0, 0x00000000, 0x00000000, 0x00000000 },	/* SIBCALL_REGS */	\
+  { 0xffffffc0, 0x00000000, 0x00000000, 0x00000000 },	/* JALR_REGS */		\
+  { 0xffffffff, 0x00000000, 0x00000000, 0x00000000 },	/* GR_REGS */		\
+  { 0x00000000, 0xffffffff, 0x00000000, 0x00000000 },	/* FP_REGS */		\
+  { 0x00000000, 0x00000000, 0x00000003, 0x00000000 },	/* FRAME_REGS */	\
+  { 0x00000000, 0x00000000, 0x00000000, 0x00000001 },	/* VECTOR_MASK_REGS */\
+  { 0x00000000, 0x00000000, 0x00000000, 0xfffffffe },	/* VECTOR_REGS */\
+  { 0x00000000, 0x00000000, 0x00000000, 0xffffffff },	/* VECTOR_REGS */\
+  { 0x00000000, 0x00000000, 0x00000008, 0x00000000 },	/* VTYPE_REGS */\
+  { 0xffffffff, 0xffffffff, 0x0000000f, 0xffffffff }	/* ALL_REGS */		\
 }
 
 /* A C expression whose value is a register class containing hard
@@ -498,9 +536,15 @@ enum reg_class
   60, 61, 62, 63,							\
   /* Call-saved FPRs.  */						\
   40, 41, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,			\
+  /* Call-clobbered vector registers.  */				\
+  97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,	\
+  111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,	\
+  124, 125, 126, 127,							\
+  /* The vector mask register.  */					\
+  96,									\
   /* None of the remaining classes have defined call-saved		\
      registers.  */							\
-  64, 65								\
+  64, 65, 66, 67							\
 }
 
 /* True if VALUE is a signed 12-bit number.  */
@@ -508,6 +552,10 @@ enum reg_class
 #define SMALL_OPERAND(VALUE) \
   ((unsigned HOST_WIDE_INT) (VALUE) + IMM_REACH/2 < IMM_REACH)
 
+#define POLY_SMALL_OPERAND_P(POLY_VALUE)		\
+  (POLY_VALUE.is_constant () ?				\
+     SMALL_OPERAND (POLY_VALUE.to_constant ()) : false)
+
 /* True if VALUE can be loaded into a register using LUI.  */
 
 #define LUI_OPERAND(VALUE)						\
@@ -758,7 +806,15 @@ typedef struct {
   "fs0", "fs1", "fa0", "fa1", "fa2", "fa3", "fa4", "fa5",	\
   "fa6", "fa7", "fs2", "fs3", "fs4", "fs5", "fs6", "fs7",	\
   "fs8", "fs9", "fs10","fs11","ft8", "ft9", "ft10","ft11",	\
-  "arg", "frame", }
+  "arg", "frame","vl","vtype","N/A", "N/A", "N/A", "N/A",	\
+  "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", 	\
+  "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", 	\
+  "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", 	\
+  "v0",  "v1",  "v2",  "v3",  "v4",  "v5",  "v6",  "v7",	\
+  "v8",  "v9",  "v10", "v11", "v12", "v13", "v14", "v15",	\
+  "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23",	\
+  "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31" 	\
+}
 
 #define ADDITIONAL_REGISTER_NAMES					\
 {									\
@@ -984,4 +1040,22 @@ extern void riscv_remove_unneeded_save_restore_calls (void);
 
 #define TARGET_FP16 TARGET_RVZFH
 
+#ifndef USED_FOR_TARGET
+extern poly_uint16 riscv_rvv_chunks;
+
+/* The number of bits and bytes in a RVV vector.  */
+#define BITS_PER_RVV_VECTOR (poly_uint16 (riscv_rvv_chunks * 64))
+#define BYTES_PER_RVV_VECTOR (poly_uint16 (riscv_rvv_chunks * 8))
+#endif
+
+/* Minimal value of VLEN in bytes.  */
+#define MIN_VLENB \
+  (MAX (UNITS_PER_WORD, UNITS_PER_FP_REG))
+
+#define TARGET_SUPPORTS_WIDE_INT 1
+
+#define REGMODE_NATURAL_SIZE(MODE) riscv_regmode_natural_size (MODE)
+
+#define RISCV_DWARF_VLEN (4096 + 0xc22)
+
 #endif /* ! GCC_RISCV_H */
diff --git a/gcc/config/riscv/riscv.md b/gcc/config/riscv/riscv.md
index 5271b3e7de8..d7fba80959b 100644
--- a/gcc/config/riscv/riscv.md
+++ b/gcc/config/riscv/riscv.md
@@ -40,11 +40,93 @@
   UNSPEC_FLT_QUIET
   UNSPEC_FLE_QUIET
   UNSPEC_COPYSIGN
+  UNSPEC_NCOPYSIGN
+  UNSPEC_XORSIGN
   UNSPEC_LRINT
   UNSPEC_LROUND
 
   ;; Stack tie
   UNSPEC_TIE
+
+  ;; Vector unspecs.
+  UNSPEC_FIRST
+  UNSPEC_SBF
+  UNSPEC_SIF
+  UNSPEC_SOF
+  UNSPEC_IOTA
+  UNSPEC_VID
+  UNSPEC_MASKED_STORE
+  UNSPEC_STRIDED_LOAD
+  UNSPEC_STRIDED_STORE
+  UNSPEC_REDUC
+  UNSPEC_ORDERED_REDUC
+  UNSPEC_REDUC_SUM
+  UNSPEC_REDUC_USUM
+  UNSPEC_ORDERED_REDUC_SUM
+  UNSPEC_OVERFLOW
+  UNSPEC_VMULHS
+  UNSPEC_VMULHU
+  UNSPEC_VMULHSU
+  UNSPEC_FCVT_XUF
+  UNSPEC_ROD
+  UNSPEC_VFCLASS
+  UNSPEC_VSLIDEUP
+  UNSPEC_VSLIDE1UP
+  UNSPEC_VFSLIDE1UP
+  UNSPEC_VSLIDEDOWN
+  UNSPEC_VSLIDE1DOWN
+  UNSPEC_VFSLIDE1DOWN
+  UNSPEC_VRGATHER
+  UNSPEC_VCOMPRESS
+  UNSPEC_VNCLIP
+  UNSPEC_VNCLIPU
+  UNSPEC_VSSRL
+  UNSPEC_VSSRA
+  UNSPEC_VAADDU
+  UNSPEC_VAADD
+  UNSPEC_VASUBU
+  UNSPEC_VASUB
+  UNSPEC_VSMUL
+  UNSPEC_VLEFF
+  UNSPEC_LOAD_GATHER
+  UNSPEC_INDEXED_LOAD
+  UNSPEC_STORE_SCATTER
+  UNSPEC_ORDERED_INDEXED_STORE
+  UNSPEC_UNORDERED_INDEXED_STORE
+  UNSPEC_VAMO_SWAP
+  UNSPEC_VAMO_ADD
+  UNSPEC_VAMO_XOR
+  UNSPEC_VAMO_AND
+  UNSPEC_VAMO_OR
+  UNSPEC_VAMO_MIN
+  UNSPEC_VAMO_MAX
+  UNSPEC_VAMO_MINU
+  UNSPEC_VAMO_MAXU
+  UNSPEC_READ_VL
+  UNSPEC_WHOLE_MOVE
+  UNSPEC_VPOPCOUNT
+  UNSPEC_VQMAC
+  UNSPEC_VMADD
+  UNSPEC_READ_VTYPE
+  UNSPEC_WRITE_VTYPE
+  UNSPEC_MASK_VMADD
+  UNSPEC_MASK_VMSUB
+  UNSPEC_MASK_VMACC
+  UNSPEC_MASK_VMSAC
+  UNSPEC_MASK_VFMADD
+  UNSPEC_MASK_VFMACC
+  UNSPEC_MASK_VFNMADD
+  UNSPEC_MASK_VFNMACC
+  UNSPEC_MASK_VFWMACC
+  UNSPEC_MASK_VFWNMACC
+  UNSPEC_VCLR
+  UNSPEC_VSET
+  UNSPEC_USEVL
+
+  ;; Segment load/store
+  UNSPEC_SEG_STORE
+  UNSPEC_SEG_LOAD
+  UNSPEC_SEG_LOAD_FIRST_FAULT
 ])
 
 (define_c_enum "unspecv" [
@@ -65,6 +147,11 @@
   UNSPECV_BLOCKAGE
   UNSPECV_FENCE
   UNSPECV_FENCE_I
+
+  ;; Vector unspecs.
+  UNSPECV_VSETVL
+  UNSPECV_VLOAD
+  UNSPECV_VSTORE
 ])
 
 (define_constants
@@ -85,6 +172,9 @@
    (S10_REGNUM			26)
    (S11_REGNUM			27)
 
+   (VL_REGNUM			66)
+   (VTYPE_REGNUM		67)
+
    (NORMAL_RETURN		0)
    (SIBCALL_RETURN		1)
    (EXCEPTION_RETURN		2)
@@ -165,7 +255,7 @@
 (define_attr "type"
   "unknown,branch,jump,call,load,fpload,store,fpstore,
    mtc,mfc,const,arith,logical,shift,slt,imul,idiv,move,fmove,fadd,fmul,
-   fmadd,fdiv,fcmp,fcvt,fsqrt,multi,auipc,sfb_alu,nop,ghost"
+   fmadd,fdiv,fcmp,fcvt,fsqrt,multi,auipc,sfb_alu,nop,ghost,vector"
   (cond [(eq_attr "got" "load") (const_string "load")
 
 	 ;; If a doubleword move uses these expensive instructions,
@@ -385,7 +475,9 @@
 		     (gt "") (gtu "u")
 		     (ge "") (geu "u")
 		     (lt "") (ltu "u")
-		     (le "") (leu "u")])
+		     (le "") (leu "u")
+		     (fix "") (unsigned_fix "u")
+		     (float "") (unsigned_float "u")])
 
 ;; <su> is like <u>, but the signed form expands to "s" rather than "".
 (define_code_attr su [(sign_extend "s") (zero_extend "u")])
@@ -394,6 +486,7 @@
 (define_code_attr optab [(ashift "ashl")
 			 (ashiftrt "ashr")
 			 (lshiftrt "lshr")
+			 (mult "mul")
 			 (div "div")
 			 (mod "mod")
 			 (udiv "udiv")
@@ -406,7 +499,13 @@
 			 (xor "xor")
 			 (and "and")
 			 (plus "add")
-			 (minus "sub")])
+			 (minus "sub")
+			 (smax "max")
+			 (smin "min")
+			 (us_plus "usadd")
+			 (ss_plus "ssadd")
+			 (us_minus "ussub")
+			 (ss_minus "sssub")])
 
 ;; <insn> expands to the name of the insn that implements a particular code.
 (define_code_attr insn [(ashift "sll")
@@ -420,7 +519,15 @@
 			(xor "xor")
 			(and "and")
 			(plus "add")
-			(minus "sub")])
+			(minus "sub")
+			(smax "max")
+			(umax "maxu")
+			(smin "min")
+			(umin "minu")
+			(us_plus "saddu")
+			(ss_plus "sadd")
+			(us_minus "ssubu")
+			(ss_minus "ssub")])
 
 ;; Ghost instructions produce no real code and introduce no hazards.
 ;; They exist purely to express an effect on dataflow.
@@ -454,6 +561,21 @@
   [(set_attr "type" "arith")
    (set_attr "mode" "SI")])
 
+(define_expand "adddi3x"
+  [(set (match_operand:DI          0 "register_operand")
+	(plus:DI (match_operand:DI 1 "register_operand")
+		 (match_operand:DI 2 "move_operand")))]
+  "TARGET_64BIT"
+{
+  if (!arith_operand (operands[2], DImode))
+    {
+      gcc_assert (false);
+    }
+}
+  [(set_attr "type" "arith")
+   (set_attr "mode" "DI")])
+
+
 (define_insn "adddi3"
   [(set (match_operand:DI          0 "register_operand" "=r,r")
 	(plus:DI (match_operand:DI 1 "register_operand" " r,r")
@@ -1359,23 +1481,23 @@
 })
 
 (define_insn "*movdi_32bit"
-  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,m,  *f,*f,*r,*f,*m")
-	(match_operand:DI 1 "move_operand"         " r,i,m,r,*J*r,*m,*f,*f,*f"))]
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r, r,r,m,  *f,*f,*r,*f,*m")
+	(match_operand:DI 1 "move_operand"         " r,i,vp,m,r,*J*r,*m,*f,*f,*f"))]
   "!TARGET_64BIT
    && (register_operand (operands[0], DImode)
        || reg_or_0_operand (operands[1], DImode))"
   { return riscv_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "move,const,load,store,mtc,fpload,mfc,fmove,fpstore")
+  [(set_attr "move_type" "move,const,const,load,store,mtc,fpload,mfc,fmove,fpstore")
    (set_attr "mode" "DI")])
 
 (define_insn "*movdi_64bit"
-  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r, m,  *f,*f,*r,*f,*m")
-	(match_operand:DI 1 "move_operand"         " r,T,m,rJ,*r*J,*m,*f,*f,*f"))]
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r, r,r,  m,  *f,*f,*r,*f,*m")
+	(match_operand:DI 1 "move_operand"         " r,T,vp,m,rJ,*r*J,*m,*f,*f,*f"))]
   "TARGET_64BIT
    && (register_operand (operands[0], DImode)
        || reg_or_0_operand (operands[1], DImode))"
   { return riscv_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "move,const,load,store,mtc,fpload,mfc,fmove,fpstore")
+  [(set_attr "move_type" "move,const,const,load,store,mtc,fpload,mfc,fmove,fpstore")
    (set_attr "mode" "DI")])
 
 ;; 32-bit Integer moves
@@ -1865,7 +1987,7 @@
        (lshiftrt:GPR (match_dup 3) (match_dup 2)))]
 {
   /* Op2 is a VOIDmode constant, so get the mode size from op1.  */
-  operands[2] = GEN_INT (GET_MODE_BITSIZE (GET_MODE (operands[1]))
+  operands[2] = GEN_INT (GET_MODE_BITSIZE (GET_MODE (operands[1])).to_constant ()
 			 - exact_log2 (INTVAL (operands[2]) + 1));
 })
 
@@ -2586,5 +2708,6 @@
 (include "sync.md")
 (include "peephole.md")
 (include "pic.md")
+(include "vector.md")
 (include "generic.md")
 (include "sifive-7.md")
diff --git a/gcc/config/riscv/riscv.opt b/gcc/config/riscv/riscv.opt
index 4d9b4a24ca4..eacb33747ab 100644
--- a/gcc/config/riscv/riscv.opt
+++ b/gcc/config/riscv/riscv.opt
@@ -124,6 +124,32 @@ Target Bool Var(riscv_mrelax) Init(1)
 Take advantage of linker relaxations to reduce the number of instructions
 required to materialize symbol addresses.
 
+Enum
+Name(rvv_vector_bits) Type(enum riscv_rvv_vector_bits_enum)
+The possible RVV vector lengths:
+
+EnumValue
+Enum(rvv_vector_bits) String(scalable) Value(RVV_SCALABLE)
+
+EnumValue
+Enum(rvv_vector_bits) String(64) Value(RVV_64)
+
+EnumValue
+Enum(rvv_vector_bits) String(128) Value(RVV_128)
+
+EnumValue
+Enum(rvv_vector_bits) String(256) Value(RVV_256)
+
+EnumValue
+Enum(rvv_vector_bits) String(512) Value(RVV_512)
+
+EnumValue
+Enum(rvv_vector_bits) String(1024) Value(RVV_1024)
+
+mrvv-vector-bits=
+Target RejectNegative Joined Enum(rvv_vector_bits) Var(riscv_rvv_vector_bits) Init(RVV_SCALABLE)
+-mrvv-vector-bits=<number>	Set the number of bits in an RVV vector register to N.
+
 Mask(64BIT)
 
 Mask(MUL)
@@ -138,6 +164,8 @@ Mask(RVC)
 
 Mask(RVE)
 
+Mask(VECTOR)
+
 Mask(RVZFH)
 
 mriscv-attribute
diff --git a/gcc/config/riscv/riscv_vector.h b/gcc/config/riscv/riscv_vector.h
new file mode 100644
index 00000000000..96751d617c4
--- /dev/null
+++ b/gcc/config/riscv/riscv_vector.h
@@ -0,0 +1,4572 @@
+/* RISC-V Vector extension instructions include file.
+
+   Copyright (C) 2019 Free Software Foundation, Inc.
+   Contributed by SiFive.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+#ifndef _GCC_RISCV_VECTOR_H
+#define _GCC_RISCV_VECTOR_H 1
+
+#include <stdint.h>
+#include <stddef.h>
+
+#ifndef __riscv_vector
+#error "Vector intrinsics require the vector extension."
+#else
+
+/* Uitl type for easier expand floating point functions.  */
+#define _RVV_F16_TYPE float16_t
+#define _RVV_F32_TYPE float
+#define _RVV_F64_TYPE double
+
+typedef int word_type __attribute__ ((mode (__word__)));
+typedef __fp16 float16_t;
+typedef float float32_t;
+typedef double float64_t;
+
+typedef __fp16 __float16_t;
+typedef float __float32_t;
+typedef double __float64_t;
+
+enum RVV_CSR {
+  RVV_VSTART = 0,
+  RVV_VXSAT,
+  RVV_VXRM,
+  RVV_VCSR,
+};
+
+__extension__ extern __inline
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))
+unsigned long vread_csr(enum RVV_CSR csr)
+{
+  unsigned long rv = 0;
+  switch (csr)
+    {
+    case RVV_VSTART:
+      __asm__ __volatile__ ("csrr\t%0,vstart" : "=r"(rv) : : "memory");
+      break;
+    case RVV_VXSAT:
+      __asm__ __volatile__ ("csrr\t%0,vxsat" : "=r"(rv) : : "memory");
+      break;
+    case RVV_VXRM:
+      __asm__ __volatile__ ("csrr\t%0,vxrm" : "=r"(rv) : : "memory");
+      break;
+    case RVV_VCSR:
+      __asm__ __volatile__ ("csrr\t%0,vcsr" : "=r"(rv) : : "memory");
+      break;
+    }
+  return rv;
+}
+
+__extension__ extern __inline
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))
+void vwrite_csr(enum RVV_CSR csr, unsigned long value)
+{
+  switch (csr)
+    {
+    case RVV_VSTART:
+      __asm__ __volatile__ ("csrw\tvstart,%z0" : : "rJ"(value) : "memory");
+      break;
+    case RVV_VXSAT:
+      __asm__ __volatile__ ("csrw\tvxsat,%z0" : : "rJ"(value) : "memory");
+      break;
+    case RVV_VXRM:
+      __asm__ __volatile__ ("csrw\tvxrm,%z0" : : "rJ"(value) : "memory");
+      break;
+    case RVV_VCSR:
+      __asm__ __volatile__ ("csrw\tvcsr,%z0" : : "rJ"(value) : "memory");
+      break;
+    }
+}
+
+/* An iterator to call a macro with every supported MLEN for masking
+   operations.  */
+#define _RVV_MASK_ITERATOR(MACRO, ...)				\
+  MACRO ( 1, __VA_ARGS__)					\
+  MACRO ( 2, __VA_ARGS__)					\
+  MACRO ( 4, __VA_ARGS__)					\
+  MACRO ( 8, __VA_ARGS__)					\
+  MACRO (16, __VA_ARGS__)					\
+  MACRO (32, __VA_ARGS__)					\
+  MACRO (64, __VA_ARGS__)					\
+
+#include <riscv_vector_itr.h>
+
+/* Define the setvl intrinsics.  Use the int iterator because it is a
+   superset of the float one, but ignore the type operand.  */
+
+#define _RVVSETVL(SEW, LMUL, MLEN, T)					\
+__extension__ extern __inline word_type					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsetvl_e##SEW##m##LMUL (word_type a)					\
+{									\
+  word_type vl;								\
+  if (__riscv_xlen == 32)						\
+    vl = __builtin_riscv_vsetvl##SEW##m##LMUL##_si (a);			\
+  else									\
+    vl = __builtin_riscv_vsetvl##SEW##m##LMUL##_di (a);			\
+  return vl;								\
+}									\
+__extension__ extern __inline word_type					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsetvlmax_e##SEW##m##LMUL ()						\
+{									\
+  return vsetvl_e##SEW##m##LMUL (-1);					\
+}
+
+_RVV_INT_ITERATOR (_RVVSETVL)
+
+
+#define _RVV_INT_VEC_MOVE(SEW, LMUL, MLEN, T)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcopy_v_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a)		\
+{									\
+  return a;								\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcopy_v_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a)		\
+{									\
+  return a;								\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_v_v_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a)		\
+{									\
+  return a;								\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_v_v_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a)		\
+{									\
+  return a;								\
+}
+
+
+
+_RVV_INT_ITERATOR (_RVV_INT_VEC_MOVE)
+
+#define _RVVINT_TUPLE_COPY(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcopy_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##x##NF##_t a)	\
+{									\
+  return a;								\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcopy_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##x##NF##_t a)	\
+{									\
+  return a;								\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vundefined_i##SEW##m##LMUL##x##NF ()					\
+{									\
+  return __builtin_riscv_vundefined_i##SEW##m##LMUL##x##NF ();		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vundefined_u##SEW##m##LMUL##x##NF ()					\
+{									\
+  return __builtin_riscv_vundefined_u##SEW##m##LMUL##x##NF ();		\
+}									\
+
+_RVV_INT_TUPLE_ITERATOR_ARG (_RVVINT_TUPLE_COPY, )
+
+#define _RVVFLOAT_TUPLE_COPY(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcopy_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##x##NF##_t a)	\
+{									\
+  return a;								\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vundefined_f##SEW##m##LMUL##x##NF ()					\
+{									\
+  return __builtin_riscv_vundefined_f##SEW##m##LMUL##x##NF ();		\
+}									\
+
+_RVV_FLOAT_TUPLE_ITERATOR_ARG (_RVVFLOAT_TUPLE_COPY, )
+
+/* Define the ld/st intrinsics.  */
+
+#define _RVVINTLD_FF(SEW, LMUL, MLEN, T)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##ff_v_i##SEW##m##LMUL (const int##SEW##_t *a)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleffint##SEW##m##LMUL##_si (a);		\
+  else									\
+    return __builtin_riscv_vleffint##SEW##m##LMUL##_di (a);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##ff_v_u##SEW##m##LMUL (const uint##SEW##_t *a)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleffuint##SEW##m##LMUL##_si (a);		\
+  else									\
+    return __builtin_riscv_vleffuint##SEW##m##LMUL##_di (a);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##ff_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			      vint##SEW##m##LMUL##_t maskedoff,		\
+			      const int##SEW##_t *a)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleffint##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vleffint##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##ff_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				  vuint##SEW##m##LMUL##_t maskedoff,	\
+				  const uint##SEW##_t *a)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleffuint##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vleffuint##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a);					\
+}
+
+_RVV_INT_ITERATOR (_RVVINTLD_FF)
+
+#define _RVVINTLD(SEW, LMUL, MLEN, T)					\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##_v_i##SEW##m##LMUL (const int##SEW##_t *a)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleint##SEW##m##LMUL##_si (a);		\
+  else									\
+    return __builtin_riscv_vleint##SEW##m##LMUL##_di (a);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##_v_u##SEW##m##LMUL (const uint##SEW##_t *a)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleuint##SEW##m##LMUL##_si (a);		\
+  else									\
+    return __builtin_riscv_vleuint##SEW##m##LMUL##_di (a);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				 vint##SEW##m##LMUL##_t maskedoff,	\
+				 const int##SEW##_t *a)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleint##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vleint##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				  vuint##SEW##m##LMUL##_t maskedoff,	\
+				  const uint##SEW##_t *a)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vleuint##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vleuint##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlse##SEW##_v_i##SEW##m##LMUL (const int##SEW##_t *a, word_type stride)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlseint##SEW##m##LMUL##_si (a, stride);	\
+  else									\
+    return __builtin_riscv_vlseint##SEW##m##LMUL##_di (a, stride);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlse##SEW##_v_u##SEW##m##LMUL (const uint##SEW##_t *a, word_type stride)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlseuint##SEW##m##LMUL##_si (a, stride);	\
+  else									\
+    return __builtin_riscv_vlseuint##SEW##m##LMUL##_di (a, stride);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlse##SEW##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				  vint##SEW##m##LMUL##_t maskedoff,	\
+				  const int##SEW##_t *a,		\
+				  word_type stride)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlseint##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a, stride);				\
+  else									\
+    return __builtin_riscv_vlseint##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a, stride);				\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlse##SEW##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,\
+				   const uint##SEW##_t *a,		\
+				   word_type stride)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlseuint##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a, stride);				\
+  else									\
+    return __builtin_riscv_vlseuint##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a, stride);				\
+}									\
+
+#define _RVVINTST(SEW, LMUL, MLEN, T)					\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vse##SEW##_v_i##SEW##m##LMUL (int##SEW##_t *a,			\
+			vint##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseint##SEW##m##LMUL##_si(b, a);			\
+  else									\
+    __builtin_riscv_vseint##SEW##m##LMUL##_di(b, a);			\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vse##SEW##_v_u##SEW##m##LMUL (uint##SEW##_t *a, vuint##SEW##m##LMUL##_t b)\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseuint##SEW##m##LMUL##_si(b, a);			\
+  else									\
+    __builtin_riscv_vseuint##SEW##m##LMUL##_di(b, a);			\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vse##SEW##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			   int##SEW##_t *a,			\
+			   vint##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseint##SEW##m##LMUL##_si_mask (mask, b,		\
+						     a);		\
+  else									\
+    __builtin_riscv_vseint##SEW##m##LMUL##_di_mask (mask, b,		\
+						     a);		\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vse##SEW##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			   uint##SEW##_t *a,			\
+			   vuint##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseuint##SEW##m##LMUL##_si_mask (mask, b,		\
+						      a);		\
+  else									\
+    __builtin_riscv_vseuint##SEW##m##LMUL##_di_mask (mask, b,		\
+						      a);		\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsse##SEW##_v_i##SEW##m##LMUL (int##SEW##_t *a, word_type stride,	\
+			  vint##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vsseint##SEW##m##LMUL##_si (b, a, stride);		\
+  else									\
+    __builtin_riscv_vsseint##SEW##m##LMUL##_di (b, a, stride);		\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsse##SEW##_v_u##SEW##m##LMUL (uint##SEW##_t *a, word_type stride,	\
+			    vuint##SEW##m##LMUL##_t b)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vsseuint##SEW##m##LMUL##_si (b, a, stride);		\
+  else									\
+    __builtin_riscv_vsseuint##SEW##m##LMUL##_di (b, a, stride);		\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsse##SEW##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			    int##SEW##_t *a,			\
+			    word_type stride,				\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vsseint##SEW##m##LMUL##_si_mask (mask, b,		\
+						     a, stride);	\
+  else									\
+    __builtin_riscv_vsseint##SEW##m##LMUL##_di_mask (mask, b,		\
+						     a, stride);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsse##SEW##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			    uint##SEW##_t *a,			\
+			    word_type stride,				\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vsseuint##SEW##m##LMUL##_si_mask (mask, b,		\
+						      a, stride);	\
+  else									\
+    __builtin_riscv_vsseuint##SEW##m##LMUL##_di_mask (mask, b,		\
+						      a, stride);	\
+}
+
+_RVV_INT_ITERATOR (_RVVINTLD)
+_RVV_INT_ITERATOR (_RVVINTST)
+
+#define _RVVINTLD_INDEXED(SEW, LMUL, MLEN, T, ISEW, ILMUL)		\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxei##ISEW##_v_i##SEW##m##LMUL (const T *a,				\
+			      vuint##ISEW##m##ILMUL##_t indexed)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlxeii##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed);\
+  else									\
+    return __builtin_riscv_vlxeii##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxei##ISEW##_v_u##SEW##m##LMUL (const u##T *a,				\
+			      vuint##ISEW##m##ILMUL##_t indexed)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlxeiu##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed);\
+  else									\
+    return __builtin_riscv_vlxeiu##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxei##ISEW##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     const T *a,			\
+				     vuint##ISEW##m##ILMUL##_t indexed)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlxeii##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, maskedoff, a, indexed);				\
+  else									\
+    return __builtin_riscv_vlxeii##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, maskedoff, a, indexed);				\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxei##ISEW##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vuint##SEW##m##LMUL##_t maskedoff,	\
+				     const u##T *a,			\
+				     vuint##ISEW##m##ILMUL##_t indexed)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlxeiu##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, maskedoff, a, indexed);				\
+  else									\
+    return __builtin_riscv_vlxeiu##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, maskedoff, a, indexed);				\
+}
+
+_RVV_INT_INDEX_ITERATOR (_RVVINTLD_INDEXED)
+
+#define _RVVFLOAT_LD_INDEXED(SEW, LMUL, MLEN, T, ISEW, ILMUL)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxei##ISEW##_v_f##SEW##m##LMUL (const _RVV_F##SEW##_TYPE *a,		\
+			      vuint##ISEW##m##ILMUL##_t indexed)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlxeif##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed);\
+  else									\
+    return __builtin_riscv_vlxeif##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxei##ISEW##_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vfloat##SEW##m##LMUL##_t maskedoff,\
+				     const _RVV_F##SEW##_TYPE *a,	\
+				     vuint##ISEW##m##ILMUL##_t indexed)	\
+{									\
+  if (__riscv_xlen == 32)						\
+  return __builtin_riscv_vlxeif##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	   mask, maskedoff, a, indexed);				\
+  else									\
+  return __builtin_riscv_vlxeif##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	   mask, maskedoff, a, indexed);				\
+}
+
+_RVV_FLOAT_INDEX_ITERATOR (_RVVFLOAT_LD_INDEXED)
+
+#define _RVVINTST_INDEXED(SEW, LMUL, MLEN, T, ISEW, ILMUL, NAME)	\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##ISEW##_v_i##SEW##m##LMUL (T *a,				\
+				vuint##ISEW##m##ILMUL##_t indexed,	\
+				vint##SEW##m##LMUL##_t value)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##i##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed, value);\
+  else									\
+    return __builtin_riscv_##NAME##i##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed, value);\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##ISEW##_v_u##SEW##m##LMUL (u##T *a,				\
+				vuint##ISEW##m##ILMUL##_t indexed,	\
+				vuint##SEW##m##LMUL##_t value)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##u##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed, value);\
+  else									\
+    return __builtin_riscv_##NAME##u##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed, value);\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##ISEW##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				    T *a,				\
+				    vuint##ISEW##m##ILMUL##_t indexed,	\
+				    vint##SEW##m##LMUL##_t value)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##i##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, a, indexed, value);					\
+  else									\
+    return __builtin_riscv_##NAME##i##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, a, indexed, value);					\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##ISEW##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				    u##T *a,				\
+				    vuint##ISEW##m##ILMUL##_t indexed,	\
+				    vuint##SEW##m##LMUL##_t value)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##u##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, a, indexed, value);					\
+  else									\
+    return __builtin_riscv_##NAME##u##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, a, indexed, value);					\
+}
+
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINTST_INDEXED, vsxei)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINTST_INDEXED, vsuxei)
+
+#define _RVVFLOAT_ST_INDEXED(SEW, LMUL, MLEN, T, ISEW, ILMUL, NAME)	\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##ISEW##_v_f##SEW##m##LMUL (_RVV_F##SEW##_TYPE *a,		\
+				vuint##ISEW##m##ILMUL##_t indexed,	\
+				vfloat##SEW##m##LMUL##_t value)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##f##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed, value);\
+  else									\
+    return __builtin_riscv_##NAME##f##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed, value);\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##ISEW##_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				    _RVV_F##SEW##_TYPE *a,		\
+				    vuint##ISEW##m##ILMUL##_t indexed,	\
+				    vfloat##SEW##m##LMUL##_t value)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##f##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	   mask, a, indexed, value);					\
+  else									\
+    return __builtin_riscv_##NAME##f##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	   mask, a, indexed, value);					\
+}
+
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_ST_INDEXED, vsxei)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_ST_INDEXED, vsuxei)
+
+#define _RVV_FLOAT_VEC_MOVE(SEW, LMUL, MLEN, T)				\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcopy_v_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a)		\
+{									\
+  return a;								\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_v_v_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a)		\
+{									\
+  return a;								\
+}
+
+_RVV_FLOAT_ITERATOR (_RVV_FLOAT_VEC_MOVE)
+
+#define _RVV_INT_MV_XS(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline T						\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_x_s_i##SEW##m##LMUL##_i##SEW (vint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##i##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline u##T					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_x_s_u##SEW##m##LMUL##_u##SEW (vuint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##u##SEW##m##LMUL (a);			\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_MV_XS, mv_xs)
+
+#define _RVV_INT_MV_SX(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_s_x_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a, T b)			\
+{									\
+  return __builtin_riscv_v##OP##i##SEW##m##LMUL (a, b);			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_s_x_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a, u##T b)		\
+{									\
+  return __builtin_riscv_v##OP##u##SEW##m##LMUL (a, b);			\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_MV_SX, mv_sx)
+
+#define _RVV_INT_MV_FS(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline _RVV_F##SEW##_TYPE			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vfmv_f_s_f##SEW##m##LMUL##_f##SEW (vfloat##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL (a);			\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_INT_MV_FS, mv_fs)
+
+#define _RVV_INT_MV_SF(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vfmv_s_f_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a, _RVV_F##SEW##_TYPE b)\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL (a, b);			\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_INT_MV_SF, mv_sf)
+
+#define _RVVFLOATLD_FF(SEW, LMUL, MLEN, T)				\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##ff_v_f##SEW##m##LMUL (const T *a)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlefffloat##SEW##m##LMUL##_si (a);		\
+  else									\
+    return __builtin_riscv_vlefffloat##SEW##m##LMUL##_di (a);		\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##ff_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+			      vfloat##SEW##m##LMUL##_t maskedoff,	\
+			      const T *a)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlefffloat##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vlefffloat##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a);					\
+}
+
+_RVV_FLOAT_ITERATOR (_RVVFLOATLD_FF)
+
+#define _RVVFLOATLD(SEW, LMUL, MLEN, T)					\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##_v_f##SEW##m##LMUL (const T *a)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlefloat##SEW##m##LMUL##_si (a);		\
+  else									\
+    return __builtin_riscv_vlefloat##SEW##m##LMUL##_di (a);		\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vle##SEW##_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				    vfloat##SEW##m##LMUL##_t maskedoff,\
+				    const T *a)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlefloat##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vlefloat##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlse##SEW##_v_f##SEW##m##LMUL (const T *a, word_type stride)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlsefloat##SEW##m##LMUL##_si (a, stride);	\
+  else									\
+    return __builtin_riscv_vlsefloat##SEW##m##LMUL##_di (a, stride);	\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlse##SEW##_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				    vfloat##SEW##m##LMUL##_t maskedoff,\
+				    const T *a, word_type stride)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vlsefloat##SEW##m##LMUL##_si_mask (		\
+	     mask, maskedoff, a, stride);				\
+  else									\
+    return __builtin_riscv_vlsefloat##SEW##m##LMUL##_di_mask (		\
+	     mask, maskedoff, a, stride);				\
+}
+
+#define _RVVFLOATST(SEW, LMUL, MLEN, T)					\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vse##SEW##_v_f##SEW##m##LMUL (T *a, vfloat##SEW##m##LMUL##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vsefloat##SEW##m##LMUL##_si (b, a);			\
+  else									\
+    __builtin_riscv_vsefloat##SEW##m##LMUL##_di (b, a);			\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vse##SEW##_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			   T *a,					\
+			   vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vsefloat##SEW##m##LMUL##_si_mask (mask, b, a);	\
+  else									\
+    __builtin_riscv_vsefloat##SEW##m##LMUL##_di_mask (mask, b, a);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsse##SEW##_v_f##SEW##m##LMUL (T *a, word_type stride,		\
+			     vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vssefloat##SEW##m##LMUL##_si (b, a, stride);	\
+  else									\
+    __builtin_riscv_vssefloat##SEW##m##LMUL##_di (b, a, stride);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsse##SEW##_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			    T *a, word_type stride,		\
+			    vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vssefloat##SEW##m##LMUL##_si_mask (mask, b,		\
+						       a, stride);	\
+  else									\
+    __builtin_riscv_vssefloat##SEW##m##LMUL##_di_mask (mask, b,		\
+						       a, stride);	\
+}
+
+
+_RVV_FLOAT_ITERATOR (_RVVFLOATLD)
+_RVV_FLOAT_ITERATOR (_RVVFLOATST)
+
+#define _RVV_FLOAT_SPLAT_OP(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vfmv_v_f_f##SEW##m##LMUL (_RVV_F##SEW##_TYPE a)			\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsplat_s_f##SEW##m##LMUL (_RVV_F##SEW##_TYPE a)				\
+{									\
+  return vfmv_v_f_f##SEW##m##LMUL (a);					\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vzero_f##SEW##m##LMUL ()						\
+{									\
+  return vsplat_s_f##SEW##m##LMUL (0.0);				\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vundefined_f##SEW##m##LMUL ()						\
+{									\
+  return __builtin_riscv_vundefined_f##SEW##m##LMUL ();		\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_SPLAT_OP, vec_duplicate)
+
+#define _RVV_INT_UNARY_SPLAT_OP(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_v_x_i##SEW##m##LMUL (T a)					\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmv_v_x_u##SEW##m##LMUL (u##T a)					\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsplat_s_i##SEW##m##LMUL (T a)						\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsplat_s_u##SEW##m##LMUL (u##T a)					\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vzero_i##SEW##m##LMUL ()						\
+{									\
+  return vsplat_s_i##SEW##m##LMUL (0);					\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vundefined_i##SEW##m##LMUL ()						\
+{									\
+  return __builtin_riscv_vundefined_i##SEW##m##LMUL ();			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vzero_u##SEW##m##LMUL ()						\
+{									\
+  return vsplat_s_u##SEW##m##LMUL (0);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vundefined_u##SEW##m##LMUL ()						\
+{									\
+  return __builtin_riscv_vundefined_u##SEW##m##LMUL ();			\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_UNARY_SPLAT_OP, vec_duplicate)
+
+#define _RVV_INT_UNARY_OP(SEW, LMUL, MLEN, T, OP, NAME)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_v_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a)			\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_v_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a)			\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a);						\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vuint##SEW##m##LMUL##_t maskedoff, \
+				     vuint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a);						\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_UNARY_OP, one_cmpl, not)
+
+/* ??? An intrinsic with sizeless type args that doesn't call a builtin fails
+   in ipa because it doesn't handle sizeless types in predicates, as called
+   from will_be_nonconstant_predicate.  So all of these must use a builtin.  */
+
+/* Define the add intrinsics.  */
+
+#define _RVV_INT_BIN_OP(SEW, LMUL, MLEN, T, OP, BOP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,		\
+			    vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+				  vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,	\
+					vint##SEW##m##LMUL##_t maskedoff,\
+					vint##SEW##m##LMUL##_t a,	\
+					vint##SEW##m##LMUL##_t b)	\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,	\
+					 vuint##SEW##m##LMUL##_t maskedoff, \
+					 vuint##SEW##m##LMUL##_t a,	\
+					 vuint##SEW##m##LMUL##_t b)	\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}
+
+#define _RVV_INT_BIN_OP_SCALAR(SEW, LMUL, MLEN, T, OP, BOP)		\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,		\
+				 T b)					\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+					   uint##SEW##_t b)		\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,	\
+					vint##SEW##m##LMUL##_t maskedoff, \
+					vint##SEW##m##LMUL##_t a,	\
+					T b)				\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL##_scalar_mask (	\
+      mask, maskedoff, a, b);						\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,	\
+					 vuint##SEW##m##LMUL##_t maskedoff,	\
+					 vuint##SEW##m##LMUL##_t a,	\
+					 uint##SEW##_t b)		\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL##_scalar_mask (	\
+      mask, maskedoff, a, b);						\
+}
+
+#define _RVV_INT_BIN_OP_NOMASK(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,		\
+				 vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+				  vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL (a, b);		\
+}
+
+#define _RVV_INT_BIN_OP_SCALAR_NOMASK(SEW, LMUL, MLEN, T, OP)		\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,		\
+				 T b)					\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+				  u##T b)				\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+
+#define _RVV_INT_BIN_SHIFT(SEW, LMUL, MLEN, T, OP, BOP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}
+
+#define _RVV_UINT_BIN_SHIFT(SEW, LMUL, MLEN, T, OP, BOP)		\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}
+
+#define _RVV_INT_BIN_SHIFT_SCALAR(SEW, LMUL, MLEN, T, OP, BOP)		\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			    uint8_t b)					\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   uint8_t b)				\
+{									\
+  return __builtin_riscv_v##BOP##int##SEW##m##LMUL##_scalar_mask (	\
+      mask, maskedoff, a, b);						\
+}
+
+#define _RVV_UINT_BIN_SHIFT_SCALAR(SEW, LMUL, MLEN, T, OP, BOP)		\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			    uint8_t b)					\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   uint8_t b)				\
+{									\
+  return __builtin_riscv_v##BOP##uint##SEW##m##LMUL##_scalar_mask (	\
+      mask, maskedoff, a, b);						\
+}
+
+#define _RVV_UINT_BIN_NARROWING(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, NAME, OP)\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##_wv_u##SEW##m##LMUL (vuint##WSEW##m##WLMUL##_t a,		\
+			     vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_##OP##uint##WSEW##m##WLMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##_wv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vuint##SEW##m##LMUL##_t maskedoff,	\
+				     vuint##WSEW##m##WLMUL##_t a,	\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_##OP##uint##WSEW##m##WLMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}
+
+#define _RVV_UINT_BIN_NARROWING_SCALAR(SEW, LMUL, MLEN, T,		\
+				       WSEW, WLMUL, WT, NAME, OP)	\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##_wx_u##SEW##m##LMUL (vuint##WSEW##m##WLMUL##_t a,		\
+			      uint8_t b)				\
+{									\
+  return __builtin_riscv_##OP##uint##WSEW##m##WLMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##_wx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vuint##SEW##m##LMUL##_t maskedoff,	\
+				     vuint##WSEW##m##WLMUL##_t a,	\
+				     uint8_t b)				\
+{									\
+  return __builtin_riscv_##OP##uint##WSEW##m##WLMUL##_scalar_mask (	\
+	   mask, maskedoff, a, b);					\
+}
+
+#define _RVV_INT_BIN_NARROWING(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_wv_i##SEW##m##LMUL (vint##WSEW##m##WLMUL##_t a,			\
+			     vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_##OP##int##WSEW##m##WLMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_wv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##WSEW##m##WLMUL##_t a,	\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_##OP##int##WSEW##m##WLMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}
+
+#define _RVV_INT_BIN_NARROWING_SCALAR(SEW, LMUL, MLEN, T,		\
+					     WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_wx_i##SEW##m##LMUL (vint##WSEW##m##WLMUL##_t a,			\
+			      uint8_t b)				\
+{									\
+  return __builtin_riscv_##OP##int##WSEW##m##WLMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_wx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##WSEW##m##WLMUL##_t a,	\
+				     uint8_t b)				\
+{									\
+  return __builtin_riscv_##OP##int##WSEW##m##WLMUL##_scalar_mask (	\
+	   mask, maskedoff, a, b);					\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP, add, add)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP_SCALAR, add, add)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP, sub, sub)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP_SCALAR, sub, sub)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP_SCALAR, rsub, rsub)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP, mul, mul)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP_SCALAR, mul, mul)
+
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_BIN_SHIFT, sll, vashl)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_BIN_SHIFT_SCALAR, sll, vashl)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_SHIFT, sll, vashl)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_SHIFT_SCALAR, sll, vashl)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_BIN_SHIFT, srl, vlshr)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_BIN_SHIFT_SCALAR, srl, vlshr)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_SHIFT, sra, vashr)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_SHIFT_SCALAR, sra, vashr)
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_SHIFT, ssra, vssra)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_SHIFT_SCALAR, ssra, vssra)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_BIN_SHIFT, ssrl, vssrl)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_BIN_SHIFT_SCALAR, ssrl, vssrl)
+
+_RVV_WINT_ITERATOR_ARG (_RVV_UINT_BIN_NARROWING, vnsrl, vnsrl)
+_RVV_WINT_ITERATOR_ARG (_RVV_UINT_BIN_NARROWING_SCALAR, vnsrl, vnsrl)
+_RVV_WINT_ITERATOR_ARG (_RVV_INT_BIN_NARROWING, vnsra)
+_RVV_WINT_ITERATOR_ARG (_RVV_INT_BIN_NARROWING_SCALAR, vnsra)
+
+_RVV_WINT_ITERATOR_ARG (_RVV_UINT_BIN_NARROWING, vnclipu, vnclipu)
+_RVV_WINT_ITERATOR_ARG (_RVV_UINT_BIN_NARROWING_SCALAR, vnclipu, vnclipu)
+_RVV_WINT_ITERATOR_ARG (_RVV_INT_BIN_NARROWING, vnclip)
+_RVV_WINT_ITERATOR_ARG (_RVV_INT_BIN_NARROWING_SCALAR, vnclip)
+
+#define _RVV_INT_SAT_BIN_OP(SEW, LMUL, MLEN, T, NAME, OP)		\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			      vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##SEW##m##LMUL##_t a,		\
+				     vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}
+
+#define _RVV_UINT_SAT_BIN_OP(SEW, LMUL, MLEN, T, NAME, OPU)		\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+			      vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OPU##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vuint##SEW##m##LMUL##_t maskedoff, \
+				     vuint##SEW##m##LMUL##_t a,		\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OPU##uint##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}
+
+#define _RVV_INT_SAT_BIN_OP_SCALAR(SEW, LMUL, MLEN, T, NAME, OP)	\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a, T b)		\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##SEW##m##LMUL##_t a,		\
+				     T b)				\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_scalar_mask (	\
+      mask, maskedoff, a, b);						\
+}
+
+#define _RVV_UINT_SAT_BIN_OP_SCALAR(SEW, LMUL, MLEN, T, NAME, OPU)	\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+			      uint##SEW##_t b)				\
+{									\
+  return __builtin_riscv_v##OPU##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vuint##SEW##m##LMUL##_t maskedoff,	\
+				     vuint##SEW##m##LMUL##_t a,		\
+				     uint##SEW##_t b)			\
+{									\
+  return __builtin_riscv_v##OPU##uint##SEW##m##LMUL##_scalar_mask (	\
+      mask, maskedoff, a, b);						\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP, sadd, ssadd)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP_SCALAR, sadd, ssadd)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP, ssub, sssub)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP_SCALAR, ssub, sssub)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP, aadd, vaadd)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP_SCALAR, aadd, vaadd)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP, asub, vasub)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP_SCALAR, asub, vasub)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP, smul, vsmul)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SAT_BIN_OP_SCALAR, smul, vsmul)
+
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP, sadd, usadd)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP_SCALAR, sadd, usadd)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP, ssub, ussub)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP_SCALAR, ssub, ussub)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP, aadd, vaaddu)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP_SCALAR, aadd, vaaddu)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP, asub, vasubu)
+_RVV_INT_ITERATOR_ARG (_RVV_UINT_SAT_BIN_OP_SCALAR, asub, vasubu)
+
+#define _RVV_INT_ADC_SBC_OP(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vvm_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,		\
+				  vint##SEW##m##LMUL##_t b,		\
+				  vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL (a, b, carryin);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vvm_i##SEW##m##LMUL##_b##MLEN (vint##SEW##m##LMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b,		\
+				   vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_vm##OP##mint##SEW##m##LMUL (a, b, carryin);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vv_i##SEW##m##LMUL##_b##MLEN (vint##SEW##m##LMUL##_t a,		\
+				  vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vm##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vvm_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,	\
+				   vuint##SEW##m##LMUL##_t b,	\
+				   vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL (a, b, carryin);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vvm_u##SEW##m##LMUL##_b##MLEN (vuint##SEW##m##LMUL##_t a,	\
+				    vuint##SEW##m##LMUL##_t b,	\
+				    vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_vm##OP##muint##SEW##m##LMUL (a, b, carryin);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vv_u##SEW##m##LMUL##_b##MLEN (vuint##SEW##m##LMUL##_t a,	\
+				   vuint##SEW##m##LMUL##_t b)	\
+{									\
+  return __builtin_riscv_vm##OP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vxm_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,		\
+				  int##SEW##_t b,			\
+				  vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_scalar (a, b, carryin);\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vxm_i##SEW##m##LMUL##_b##MLEN (vint##SEW##m##LMUL##_t a,		\
+				   int##SEW##_t b,			\
+				   vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_vm##OP##mint##SEW##m##LMUL##_scalar (a, b, carryin);\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vx_i##SEW##m##LMUL##_b##MLEN (vint##SEW##m##LMUL##_t a,		\
+				  int##SEW##_t b)			\
+{									\
+  return __builtin_riscv_vm##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vxm_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,	\
+				   uint##SEW##_t b,			\
+				   vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_scalar (a, b, carryin);\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vxm_u##SEW##m##LMUL##_b##MLEN (vuint##SEW##m##LMUL##_t a,	\
+				    uint##SEW##_t b,			\
+				    vbool##MLEN##_t carryin)		\
+{									\
+  return __builtin_riscv_vm##OP##muint##SEW##m##LMUL##_scalar (a, b, carryin);\
+}									\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_vx_u##SEW##m##LMUL##_b##MLEN (vuint##SEW##m##LMUL##_t a,	\
+				   uint##SEW##_t b)			\
+{									\
+  return __builtin_riscv_vm##OP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}
+
+
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_ADC_SBC_OP, adc)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_ADC_SBC_OP, sbc)
+
+#define _RVV_WINT_ADD_SUB_MASK(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL##_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a, T b)	\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL##_scalar_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL##_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a, u##T b)	\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL##_scalar_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_wv_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vint##WSEW##m##WLMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_wv_i##SEW##m##LMUL##_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_wx_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vint##WSEW##m##WLMUL##_t a, T b)	\
+{									\
+  return __builtin_riscv_v##OP##_wv_i##SEW##m##LMUL##_scalar_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_wv_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vuint##WSEW##m##WLMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_wv_u##SEW##m##LMUL##_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_wx_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t maskedoff,	\
+				    vuint##WSEW##m##WLMUL##_t a, u##T b)\
+{									\
+  return __builtin_riscv_v##OP##_wv_u##SEW##m##LMUL##_scalar_mask (mask, maskedoff, a, b);\
+}
+
+_RVV_WINT_ITERATOR_ARG (_RVV_WINT_ADD_SUB_MASK, wadd)
+_RVV_WINT_ITERATOR_ARG (_RVV_WINT_ADD_SUB_MASK, wsub)
+
+
+#define _RVV_WINT_ADD_SUB(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a,			\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a, T b)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##WSEW##m##WLMUL (vuint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##WSEW##m##WLMUL (vuint##SEW##m##LMUL##_t a, u##T b)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_wv_i##WSEW##m##WLMUL (vint##WSEW##m##WLMUL##_t a,			\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_wv_i##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_wx_i##WSEW##m##WLMUL (vint##WSEW##m##WLMUL##_t a,	T b)		\
+{									\
+  return __builtin_riscv_v##OP##_wv_i##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_wv_u##WSEW##m##WLMUL (vuint##WSEW##m##WLMUL##_t a,		\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_wv_u##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_wx_u##WSEW##m##WLMUL (vuint##WSEW##m##WLMUL##_t a, u##T b)	\
+{									\
+  return __builtin_riscv_v##OP##_wv_u##SEW##m##LMUL##_scalar (a, b);	\
+}
+
+_RVV_WINT_ITERATOR_ARG (_RVV_WINT_ADD_SUB, wadd)
+_RVV_WINT_ITERATOR_ARG (_RVV_WINT_ADD_SUB, wsub)
+
+#define _RVV_WINT_CVT(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)		\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwcvt_x_x_v_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a)	\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL (a);		\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwcvtu_x_x_v_u##WSEW##m##WLMUL (vuint##SEW##m##LMUL##_t a)	\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL (a);		\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwcvt_x_x_v_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,	\
+				   vint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL##_mask (mask, maskedoff, a);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwcvtu_x_x_v_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,	\
+				   vuint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL##_mask (mask, maskedoff, a);\
+}
+
+_RVV_WINT_ITERATOR_ARG (_RVV_WINT_CVT, wcvt)
+
+#define _RVV_WINT_EXTEND(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)	\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vs##NAME##_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL (a);		\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vz##NAME##_u##WSEW##m##WLMUL (vuint##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL (a);		\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vs##NAME##_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				       vint##WSEW##m##WLMUL##_t maskedoff,\
+				       vint##SEW##m##LMUL##_t a)	\
+{									\
+  return __builtin_riscv_v##OP##_vv_i##SEW##m##LMUL##_mask (mask, maskedoff, a);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vz##NAME##_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				       vuint##WSEW##m##WLMUL##_t maskedoff,\
+				       vuint##SEW##m##LMUL##_t a)	\
+{									\
+  return __builtin_riscv_v##OP##_vv_u##SEW##m##LMUL##_mask (mask, maskedoff, a);\
+}
+
+_RVV_WINT_ITERATOR_ARG (_RVV_WINT_EXTEND, extend, ext_vf2)
+_RVV_QINT_ITERATOR_ARG (_RVV_WINT_EXTEND, extend_q, ext_vf4)
+_RVV_EINT_ITERATOR_ARG (_RVV_WINT_EXTEND, extend_e, ext_vf8)
+
+#define _RVV_WFLOAT_ADD_SUB_MASK(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t maskedoff,\
+				   vfloat##SEW##m##LMUL##_t a,		\
+				   vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL##_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t maskedoff,\
+				   vfloat##SEW##m##LMUL##_t a, T b)	\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL##_scalar_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_wv_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t maskedoff,\
+				   vfloat##WSEW##m##WLMUL##_t a,	\
+				   vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##_wv_f##SEW##m##LMUL##_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_wf_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t maskedoff,\
+				   vfloat##WSEW##m##WLMUL##_t a, T b)	\
+{									\
+  return __builtin_riscv_vf##OP##_wv_f##SEW##m##LMUL##_scalar_mask (mask, maskedoff, a, b);\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WFLOAT_ADD_SUB_MASK, wadd)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WFLOAT_ADD_SUB_MASK, wsub)
+
+#define _RVV_WFLOAT_ADD_SUB(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##WSEW##m##WLMUL (vfloat##SEW##m##LMUL##_t a,		\
+			    vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##WSEW##m##WLMUL (vfloat##SEW##m##LMUL##_t a, T b)		\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_wv_f##WSEW##m##WLMUL (vfloat##WSEW##m##WLMUL##_t a,		\
+			    vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vf##OP##_wv_f##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_wf_f##WSEW##m##WLMUL (vfloat##WSEW##m##WLMUL##_t a, T b)	\
+{									\
+  return __builtin_riscv_vf##OP##_wv_f##SEW##m##LMUL##_scalar (a, b);	\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WFLOAT_ADD_SUB, wadd)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WFLOAT_ADD_SUB, wsub)
+
+
+#define _RVVINTCMP(SEW, LMUL, MLEN, T, OP, OPU)				\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OP##_vv_i##SEW##m##LMUL##_b##MLEN (vint##SEW##m##LMUL##_t a,	\
+				  vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vs##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OPU##_vv_u##SEW##m##LMUL##_b##MLEN (vuint##SEW##m##LMUL##_t a,	\
+				    vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vs##OPU##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OP##_vv_i##SEW##m##LMUL##_b##MLEN##_m (vbool##MLEN##_t mask,	\
+					 vbool##MLEN##_t maskedoff,	\
+					 vint##SEW##m##LMUL##_t a,	\
+					 vint##SEW##m##LMUL##_t b)	\
+{									\
+  return __builtin_riscv_vs##OP##int##SEW##m##LMUL##_mask (mask, maskedoff, \
+							   a, b);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OPU##_vv_u##SEW##m##LMUL##_b##MLEN##_m (vbool##MLEN##_t mask,	\
+					   vbool##MLEN##_t maskedoff,	\
+					   vuint##SEW##m##LMUL##_t a,	\
+					   vuint##SEW##m##LMUL##_t b)	\
+{									\
+  return __builtin_riscv_vs##OPU##uint##SEW##m##LMUL##_mask (mask, maskedoff, \
+							     a, b);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OP##_vx_i##SEW##m##LMUL##_b##MLEN (vint##SEW##m##LMUL##_t a,	\
+			       T b)					\
+{									\
+  return __builtin_riscv_vs##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OPU##_vx_u##SEW##m##LMUL##_b##MLEN (vuint##SEW##m##LMUL##_t a,	\
+			       u##T b)					\
+{									\
+  return __builtin_riscv_vs##OPU##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OP##_vx_i##SEW##m##LMUL##_b##MLEN##_m (vbool##MLEN##_t mask,	\
+				      vbool##MLEN##_t maskedoff,	\
+				      vint##SEW##m##LMUL##_t a,		\
+				      T b)				\
+{									\
+  return __builtin_riscv_vs##OP##int##SEW##m##LMUL##_scalar_mask (mask, maskedoff, \
+								  a, b);\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vms##OPU##_vx_u##SEW##m##LMUL##_b##MLEN##_m (vbool##MLEN##_t mask,	\
+				      vbool##MLEN##_t maskedoff,	\
+				      vuint##SEW##m##LMUL##_t a,	\
+				      u##T b)				\
+{									\
+  return __builtin_riscv_vs##OPU##uint##SEW##m##LMUL##_scalar_mask (mask, maskedoff, \
+								    a, b);\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVVINTCMP, eq, eq)
+_RVV_INT_ITERATOR_ARG (_RVVINTCMP, ne, ne)
+_RVV_INT_ITERATOR_ARG (_RVVINTCMP, lt, ltu)
+_RVV_INT_ITERATOR_ARG (_RVVINTCMP, le, leu)
+_RVV_INT_ITERATOR_ARG (_RVVINTCMP, gt, gtu)
+_RVV_INT_ITERATOR_ARG (_RVVINTCMP, ge, geu)
+
+#define _RVV_FCMP(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmf##OP##_vv_f##SEW##m##LMUL##_b##MLEN (vfloat##SEW##m##LMUL##_t a,	\
+			       vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_f##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmf##OP##_vv_f##SEW##m##LMUL##_b##MLEN##_m (vbool##MLEN##_t mask,	\
+				      vbool##MLEN##_t maskedoff,	\
+				      vfloat##SEW##m##LMUL##_t a,	\
+				      vfloat##SEW##m##LMUL##_t b)	\
+{									\
+  return __builtin_riscv_f##OP##int##SEW##m##LMUL##_mask (mask, maskedoff, \
+							   a, b);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmf##OP##_vf_f##SEW##m##LMUL##_b##MLEN (vfloat##SEW##m##LMUL##_t a,	\
+			       _RVV_F##SEW##_TYPE b)			\
+{									\
+  return __builtin_riscv_f##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmf##OP##_vf_f##SEW##m##LMUL##_b##MLEN##_m (vbool##MLEN##_t mask,	\
+				      vbool##MLEN##_t maskedoff,	\
+				      vfloat##SEW##m##LMUL##_t a,	\
+				      _RVV_F##SEW##_TYPE b)		\
+{									\
+  return __builtin_riscv_f##OP##int##SEW##m##LMUL##_scalar_mask (mask, maskedoff, \
+								  a, b);\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FCMP, eq)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FCMP, ne)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FCMP, lt)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FCMP, le)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FCMP, gt)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FCMP, ge)
+
+#define _RVVINT_MIN_MAX(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vvs##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vvu##OP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vvs##OP##int##SEW##m##LMUL##_mask (mask, maskedoff, \
+							    a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vvu##OP##uint##SEW##m##LMUL##_mask (mask, maskedoff, \
+							      a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			    T b)					\
+{									\
+  return __builtin_riscv_vss##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			    u##T b)					\
+{									\
+  return __builtin_riscv_vsu##OP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   T b)					\
+{									\
+  return __builtin_riscv_vss##OP##int##SEW##m##LMUL##_scalar_mask (mask, maskedoff, \
+								   a, b);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   u##T b)				\
+{									\
+  return __builtin_riscv_vsu##OP##uint##SEW##m##LMUL##_scalar_mask (mask, maskedoff, \
+								    a, b);\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVVINT_MIN_MAX, min)
+_RVV_INT_ITERATOR_ARG (_RVVINT_MIN_MAX, max)
+
+#define _RVVINT_DIV_REM(SEW, LMUL, MLEN, T, OP, NAME)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			      vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+			      vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vvu##OP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##SEW##m##LMUL##_t a,		\
+				     vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL##_mask (mask, maskedoff, \
+							    a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vuint##SEW##m##LMUL##_t maskedoff,	\
+				     vuint##SEW##m##LMUL##_t a,		\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vvu##OP##uint##SEW##m##LMUL##_mask (mask, maskedoff, \
+							     a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			      T b)					\
+{									\
+  return __builtin_riscv_vs##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,		\
+			      u##T b)					\
+{									\
+  return __builtin_riscv_vsu##OP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   T b)					\
+{									\
+  return __builtin_riscv_vs##OP##int##SEW##m##LMUL##_scalar_mask (mask, maskedoff, \
+								   a, b);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##u_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   u##T b)				\
+{									\
+  return __builtin_riscv_vsu##OP##uint##SEW##m##LMUL##_scalar_mask (mask, maskedoff, \
+								    a, b);\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVVINT_DIV_REM, div, div)
+_RVV_INT_ITERATOR_ARG (_RVVINT_DIV_REM, mod, rem)
+
+#define _RVVINT_MULH(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			       vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			    T b)					\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			    u##T b)					\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			      u##T b)					\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL##_scalar (a, b);	\
+}
+
+#define _RVVINT_MULH_MASK(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL##_mask (mask, maskedoff,\
+							   a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL##_mask (mask, maskedoff,\
+							    a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##SEW##m##LMUL##_t a,		\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL##_mask (mask, maskedoff,\
+							      a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   T b)					\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL##_scalar_mask (mask, maskedoff,\
+								  a, b);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   u##T b)				\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL##_scalar_mask (mask, maskedoff,\
+								   a, b);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##SEW##m##LMUL##_t maskedoff,	\
+				     vint##SEW##m##LMUL##_t a,		\
+				     u##T b)				\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL##_scalar_mask (mask, maskedoff,\
+								  a, b);\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVVINT_MULH, mulh)
+_RVV_INT_ITERATOR_ARG (_RVVINT_MULH_MASK, mulh)
+
+#define _RVVINT_WMUL(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)		\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a,			\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##WSEW##m##WLMUL (vuint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vv_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a,			\
+			       vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a,			\
+			    T b)					\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##WSEW##m##WLMUL (vuint##SEW##m##LMUL##_t a,			\
+			    u##T b)					\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vx_i##WSEW##m##WLMUL (vint##SEW##m##LMUL##_t a,			\
+			      u##T b)					\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL##_scalar (a, b);	\
+}
+
+#define _RVVINT_WMUL_MASK(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL##_mask (mask, maskedoff,\
+							   a, b);	\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL##_mask (mask, maskedoff,\
+							    a, b);	\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vv_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##WSEW##m##WLMUL##_t maskedoff,\
+				     vint##SEW##m##LMUL##_t a,		\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL##_mask (mask, maskedoff,\
+							      a, b);	\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   T b)					\
+{									\
+  return __builtin_riscv_vv##OP##int##SEW##m##LMUL##_scalar_mask (mask, maskedoff,\
+								  a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   u##T b)				\
+{									\
+  return __builtin_riscv_vv##OP##uint##SEW##m##LMUL##_scalar_mask (mask, maskedoff,\
+								   a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vx_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##WSEW##m##WLMUL##_t maskedoff,\
+				     vint##SEW##m##LMUL##_t a,		\
+				     u##T b)				\
+{									\
+  return __builtin_riscv_vv##OP##su_int##SEW##m##LMUL##_scalar_mask (mask, maskedoff,\
+								     a, b);\
+}
+
+_RVV_WINT_ITERATOR_ARG (_RVVINT_WMUL, wmul)
+_RVV_WINT_ITERATOR_ARG (_RVVINT_WMUL_MASK, wmul)
+
+#define _RVV_FLOAT_WMUL(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t maskedoff,\
+				   vfloat##SEW##m##LMUL##_t a,		\
+				   vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL##_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t maskedoff,\
+				   vfloat##SEW##m##LMUL##_t a, T b)	\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL##_scalar_mask (mask, maskedoff, a, b);\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##WSEW##m##WLMUL (vfloat##SEW##m##LMUL##_t a,		\
+			    vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##WSEW##m##WLMUL (vfloat##SEW##m##LMUL##_t a, T b)		\
+{									\
+  return __builtin_riscv_vf##OP##_vv_f##SEW##m##LMUL##_scalar (a, b);	\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WMUL, wmul)
+
+#define _RVV_MAC_INT(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t acc,			\
+			    vint##SEW##m##LMUL##_t a,			\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL (acc, a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t acc,		\
+			    vuint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL (acc, a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t acc,			\
+			    T a,					\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t acc,		\
+			    u##T a,					\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t acc,		\
+				   vint##SEW##m##LMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL##_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t acc,		\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL##_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t acc,		\
+				   T a,					\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t acc,		\
+				   u##T a,				\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_MAC_INT, macc)
+_RVV_INT_ITERATOR_ARG (_RVV_MAC_INT, madd)
+_RVV_INT_ITERATOR_ARG (_RVV_MAC_INT, nmsac)
+_RVV_INT_ITERATOR_ARG (_RVV_MAC_INT, nmsub)
+
+#define _RVV_MAC_WINT_MASK(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t acc,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL##_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t acc,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL##_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vv_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##WSEW##m##WLMUL##_t acc,	\
+				     vint##SEW##m##LMUL##_t a,		\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##su_sv_i##SEW##m##LMUL##_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##WSEW##m##WLMUL##_t acc,	\
+				   T a,					\
+				   vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##WSEW##m##WLMUL##_t acc,	\
+				   u##T a,				\
+				   vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vx_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##WSEW##m##WLMUL##_t acc,	\
+				     T a,				\
+				     vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##su_sv_i##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##us_vx_i##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				     vint##WSEW##m##WLMUL##_t acc,	\
+				     u##T a,				\
+				     vint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##us_sv_i##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}
+
+#define _RVV_MAC_WINT(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)		\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vv_i##WSEW##m##WLMUL (vint##WSEW##m##WLMUL##_t acc,		\
+			    vint##SEW##m##LMUL##_t a,			\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL (acc, a, b);	\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vv_u##WSEW##m##WLMUL (vuint##WSEW##m##WLMUL##_t acc,		\
+			    vuint##SEW##m##LMUL##_t a,			\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL (acc, a, b);	\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vv_i##WSEW##m##WLMUL (vint##WSEW##m##WLMUL##_t acc,		\
+			      vint##SEW##m##LMUL##_t a,			\
+			      vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##su_sv_i##SEW##m##LMUL (acc, a, b);	\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##WSEW##m##WLMUL (vint##WSEW##m##WLMUL##_t acc,		\
+			    T a,					\
+			    vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_i##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##u_vx_u##WSEW##m##WLMUL (vuint##WSEW##m##WLMUL##_t acc,		\
+			    u##T a,					\
+			    vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##_sv_u##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##su_vx_i##WSEW##m##WLMUL (vint##WSEW##m##WLMUL##_t acc,		\
+			      T a,					\
+			      vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##su_sv_i##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##us_vx_i##WSEW##m##WLMUL (vint##WSEW##m##WLMUL##_t acc,		\
+			      u##T a,					\
+			      vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##us_sv_i##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+
+_RVV_WINT_ITERATOR_ARG (_RVV_MAC_WINT, wmacc)
+_RVV_WINT_ITERATOR_ARG (_RVV_MAC_WINT_MASK, wmacc)
+_RVV_QINT_ITERATOR_ARG (_RVV_MAC_WINT, qmacc)
+_RVV_QINT_ITERATOR_ARG (_RVV_MAC_WINT_MASK, qmacc)
+
+#define _RVV_INT_MERGE(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vvm_i##SEW##m##LMUL (vbool##MLEN##_t mask,			\
+			     vint##SEW##m##LMUL##_t a,			\
+			     vint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##i##SEW##m##LMUL##_mask (mask, a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vvm_u##SEW##m##LMUL (vbool##MLEN##_t mask,			\
+			     vuint##SEW##m##LMUL##_t a,			\
+			     vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##u##SEW##m##LMUL##_mask (mask, a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vxm_i##SEW##m##LMUL (vbool##MLEN##_t mask,			\
+			     vint##SEW##m##LMUL##_t a,			\
+			     T b)					\
+{									\
+  return __builtin_riscv_v##OP##i##SEW##m##LMUL##_scalar_mask (mask, a, b);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vxm_u##SEW##m##LMUL (vbool##MLEN##_t mask,			\
+			     vuint##SEW##m##LMUL##_t a,			\
+			     u##T b)					\
+{									\
+  return __builtin_riscv_v##OP##u##SEW##m##LMUL##_scalar_mask (mask, a, b);\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_MERGE, merge)
+
+#define _RVV_FLOAT_MERGE(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vvm_f##SEW##m##LMUL (vbool##MLEN##_t mask,			\
+			     vfloat##SEW##m##LMUL##_t a,		\
+			     vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL##_mask (mask, a, b);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vfm_f##SEW##m##LMUL (vbool##MLEN##_t mask,			\
+			      vfloat##SEW##m##LMUL##_t a,		\
+			      _RVV_F##SEW##_TYPE b)			\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL##_scalar_mask (mask, a, b);\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_MERGE, merge)
+
+#define _RVV_FLOAT_BIN_OP_SCALAR(SEW, LMUL, MLEN, T, OP, NAME)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##NAME##_vf_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a,	\
+				   T b)					\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL##_scalar (a, b);	\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##NAME##_vf_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,	\
+					  vfloat##SEW##m##LMUL##_t maskedoff, \
+					  vfloat##SEW##m##LMUL##_t a, \
+					  T b)			\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL##_scalar_mask (	\
+      mask, maskedoff, a, b);						\
+}
+
+#define _RVV_FLOAT_BIN_OP(SEW, LMUL, MLEN, T, OP, NAME)			\
+_RVV_FLOAT_BIN_OP_SCALAR(SEW, LMUL, MLEN, T, OP, NAME)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##NAME##_vv_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a,	\
+			      vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##NAME##_vv_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,	\
+				       vfloat##SEW##m##LMUL##_t maskedoff, \
+				       vfloat##SEW##m##LMUL##_t a,	\
+				       vfloat##SEW##m##LMUL##_t b)	\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a, b);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, add, add)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, sub, sub)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP_SCALAR, rsub, rsub)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, mul, mul)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, div, div)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP_SCALAR, rdiv, rdiv)
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, max, max)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, min, min)
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, copysign, sgnj)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, ncopysign, sgnjn)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_BIN_OP, xorsign, sgnjx)
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP, and, and)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP_SCALAR, and, and)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP, or, ior)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP_SCALAR, or, ior)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP, xor, xor)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_BIN_OP_SCALAR, xor, xor)
+
+#define _RVV_INT_SLIDE(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t dst,			\
+			    vint##SEW##m##LMUL##_t a,			\
+			    size_t b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_si (dst, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_di (dst, a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t dst,		\
+			    vuint##SEW##m##LMUL##_t a,			\
+			    size_t b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_si (dst, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_di (dst, a, b);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   size_t b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_si_mask (mask,	\
+						maskedoff, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_di_mask (mask,	\
+						maskedoff, a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   size_t b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_si_mask (mask,	\
+						maskedoff, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_di_mask (mask,	\
+						maskedoff, a, b);	\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SLIDE, slideup)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SLIDE, slidedown)
+
+#define _RVV_FLOAT_SLIDE(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t dst,		\
+			    vfloat##SEW##m##LMUL##_t a,			\
+			    size_t b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_si (dst, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_di (dst, a, b);	\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				vfloat##SEW##m##LMUL##_t maskedoff,	\
+				vfloat##SEW##m##LMUL##_t a,		\
+				size_t b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_si_mask (mask,	\
+						maskedoff, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_di_mask (mask,	\
+						maskedoff, a, b);	\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_SLIDE, slideup)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_SLIDE, slidedown)
+
+#define _RVV_INT_SLIDE1(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			    long b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_si (a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_di (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			    long b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_si (a, b);		\
+  else									\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_di (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vint##SEW##m##LMUL##_t maskedoff,	\
+				   vint##SEW##m##LMUL##_t a,		\
+				   long b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_si_mask (mask,	\
+						maskedoff, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_di_mask (mask,	\
+						maskedoff, a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vuint##SEW##m##LMUL##_t maskedoff,	\
+				   vuint##SEW##m##LMUL##_t a,		\
+				   long b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_si_mask (mask,	\
+						maskedoff, a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##u##SEW##m##LMUL##_di_mask (mask,	\
+						maskedoff, a, b);	\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SLIDE1, slide1up)
+_RVV_INT_ITERATOR_ARG (_RVV_INT_SLIDE1, slide1down)
+
+#define _RVV_FLOAT_SLIDE1(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a,		\
+			    _RVV_F##SEW##_TYPE b)			\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				   vfloat##SEW##m##LMUL##_t maskedoff,	\
+				   vfloat##SEW##m##LMUL##_t a,		\
+				   _RVV_F##SEW##_TYPE b)		\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (mask,		\
+							maskedoff, a, b);\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_SLIDE1, slide1up)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_SLIDE1, slide1down)
+
+#define _RVV_FLOAT_UNARY_OP(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_v_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a)	\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL (a);		\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_v_f##SEW##m##LMUL##_m (				\
+  vbool##MLEN##_t mask,						\
+  vfloat##SEW##m##LMUL##_t maskedoff, 				\
+  vfloat##SEW##m##LMUL##_t a)					\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, sqrt)
+
+#define _RVV_FLOAT_VFCLASS(SEW, LMUL, MLEN, T, OP, NAME)		\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_v_u##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a)		\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL (a);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_v_u##SEW##m##LMUL##_m (					\
+  vbool##MLEN##_t mask,							\
+  vuint##SEW##m##LMUL##_t maskedoff, 					\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##float##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_VFCLASS, vfclass, fclass)
+
+#define _RVV_FLOAT_CVT_XF(SEW, LMUL, MLEN, T, OP, NAME)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_x_f_v_i##SEW##m##LMUL (				\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_x_f_v_i##SEW##m##LMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vint##SEW##m##LMUL##_t maskedoff, 					\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_CVT_XF, fcvt_xf, fcvt)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_CVT_XF, fcvt_rtz_xf, fcvt_rtz)
+
+#define _RVV_FLOAT_CVT_XUF(SEW, LMUL, MLEN, T, OP, NAME)		\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_xu_f_v_u##SEW##m##LMUL (				\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_xu_f_v_u##SEW##m##LMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vuint##SEW##m##LMUL##_t maskedoff, 					\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_CVT_XUF, fcvt_xuf, fcvt)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_CVT_XUF, fcvt_rtz_xuf, fcvt_rtz)
+
+#define _RVV_FLOAT_CVT_FX(SEW, LMUL, MLEN, T, OP, NAME)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_x_v_f##SEW##m##LMUL (				\
+  vint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_x_v_f##SEW##m##LMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##_t maskedoff, 					\
+  vint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_CVT_FX, fcvt_fx, fcvt)
+
+#define _RVV_FLOAT_CVT_FXU(SEW, LMUL, MLEN, T, OP, NAME)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_xu_v_f##SEW##m##LMUL (				\
+  vuint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_xu_v_f##SEW##m##LMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##_t maskedoff, 					\
+  vuint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_CVT_FXU, fcvt_fxu, fcvt)
+
+#define _RVV_FLOAT_WCVT_XF(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_x_f_v_i##WSEW##m##WLMUL (				\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_x_f_v_i##WSEW##m##WLMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vint##WSEW##m##WLMUL##_t maskedoff, 					\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WCVT_XF, wfcvt_xf, fwcvt)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WCVT_XF, wfcvt_rtz_xf, fwcvt_rtz)
+
+#define _RVV_FLOAT_WCVT_XUF(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_xu_f_v_u##WSEW##m##WLMUL (				\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vuint##WSEW##m##WLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_xu_f_v_u##WSEW##m##WLMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vuint##WSEW##m##WLMUL##_t maskedoff, 					\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WCVT_XUF, wfcvt_xuf, fwcvt)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WCVT_XUF, wfcvt_rtz_xuf, fwcvt_rtz)
+
+#define _RVV_FLOAT_WCVT_FX(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_x_v_f##WSEW##m##WLMUL (				\
+  vint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_x_v_f##WSEW##m##WLMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vfloat##WSEW##m##WLMUL##_t maskedoff, 				\
+  vint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WCVT_FX, wfcvt_fx, fwcvt)
+
+#define _RVV_FLOAT_WCVT_FXU(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_xu_v_f##WSEW##m##WLMUL (				\
+  vuint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_xu_v_f##WSEW##m##WLMUL##_m (			\
+  vbool##MLEN##_t mask,							\
+  vfloat##WSEW##m##WLMUL##_t maskedoff, 				\
+  vuint##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WCVT_FXU, wfcvt_fxu, fwcvt)
+
+#define _RVV_FLOAT_WCVT_FF(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_f_v_f##WSEW##m##WLMUL (					\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_f_v_f##WSEW##m##WLMUL##_m (					\
+  vbool##MLEN##_t mask,							\
+  vfloat##WSEW##m##WLMUL##_t maskedoff, 				\
+  vfloat##SEW##m##LMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_WCVT_FF, wfcvt_ff, fwcvt)
+
+#define _RVV_FLOAT_NCVT_XF(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_x_f_w_i##SEW##m##LMUL (					\
+  vfloat##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_x_f_w_i##SEW##m##LMUL##_m (					\
+  vbool##MLEN##_t mask,							\
+  vint##SEW##m##LMUL##_t maskedoff, 					\
+  vfloat##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_XF, nfcvt_xf, fncvt)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_XF, nfcvt_rtz_xf, fncvt_rtz)
+
+#define _RVV_FLOAT_NCVT_XUF(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_xu_f_w_u##SEW##m##LMUL (					\
+  vfloat##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_xu_f_w_u##SEW##m##LMUL##_m (					\
+  vbool##MLEN##_t mask,							\
+  vuint##SEW##m##LMUL##_t maskedoff, 					\
+  vfloat##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_XUF, nfcvt_xuf, fncvt)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_XUF, nfcvt_rtz_xuf, fncvt_rtz)
+
+#define _RVV_FLOAT_NCVT_FX(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_x_w_f##SEW##m##LMUL (			\
+  vint##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_x_w_f##SEW##m##LMUL##_m (		\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##_t maskedoff, 					\
+  vint##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_FX, nfcvt_fx, fncvt)
+
+#define _RVV_FLOAT_NCVT_FXU(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_xu_w_f##SEW##m##LMUL (			\
+  vuint##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_xu_w_f##SEW##m##LMUL##_m (		\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##_t maskedoff, 					\
+  vuint##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_FXU, nfcvt_fxu, fncvt)
+
+#define _RVV_FLOAT_NCVT_FF(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, NAME)\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_f_w_f##SEW##m##LMUL (					\
+  vfloat##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##NAME##_f_f_w_f##SEW##m##LMUL##_m (					\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##_t maskedoff, 					\
+  vfloat##WSEW##m##WLMUL##_t a)						\
+{									\
+  return __builtin_riscv_vf##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_FF, nfcvt_ff, fncvt)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_FLOAT_NCVT_FF, nfcvt_rod_ff, fncvt_rod)
+
+#define _RVV_MASK_NULLARY_OP(MLEN, OP)					\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_m_b##MLEN ()						\
+{									\
+  return __builtin_riscv_v##OP##bool##MLEN ();				\
+}
+
+#define _RVV_MASK_BIN_OP(MLEN, OP)					\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_mm_b##MLEN (vbool##MLEN##_t a,				\
+			  vbool##MLEN##_t b)				\
+{									\
+  return __builtin_riscv_v##OP##bool##MLEN (a, b);			\
+}
+
+#define _RVV_MASK_UNARY_OP_SCALAR(MLEN, OP)				\
+__extension__ extern __inline word_type					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_m_b##MLEN (vbool##MLEN##_t a)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##bool##MLEN##_si (a);			\
+  else									\
+    return __builtin_riscv_v##OP##bool##MLEN##_di (a);			\
+}									\
+__extension__ extern __inline word_type					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+v##OP##_m_b##MLEN##_m (vbool##MLEN##_t mask,		\
+				vbool##MLEN##_t a)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##bool##MLEN##_si_mask (mask, a);	\
+  else									\
+    return __builtin_riscv_v##OP##bool##MLEN##_di_mask (mask, a);	\
+}
+
+#define _RVV_MASK_UNARY_OP_NOMASK(MLEN, OP)				\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_m_b##MLEN (vbool##MLEN##_t a)				\
+{									\
+ return __builtin_riscv_v##OP##bool##MLEN (a);				\
+}									\
+
+#define _RVV_MASK_UNARY_OP(MLEN, OP)					\
+  _RVV_MASK_UNARY_OP_NOMASK(MLEN, OP)					\
+__extension__ extern __inline vbool##MLEN##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vm##OP##_m_b##MLEN##_m (vbool##MLEN##_t mask,		\
+				vbool##MLEN##_t maskedoff,		\
+				vbool##MLEN##_t a)			\
+{									\
+  return __builtin_riscv_v##OP##bool##MLEN##_mask (mask, maskedoff, a);	\
+}
+
+_RVV_MASK_ITERATOR (_RVV_MASK_NULLARY_OP, clr)
+_RVV_MASK_ITERATOR (_RVV_MASK_NULLARY_OP, set)
+_RVV_MASK_ITERATOR (_RVV_MASK_UNARY_OP_SCALAR, popc)
+_RVV_MASK_ITERATOR (_RVV_MASK_UNARY_OP_SCALAR, first)
+_RVV_MASK_ITERATOR (_RVV_MASK_UNARY_OP, sbf)
+_RVV_MASK_ITERATOR (_RVV_MASK_UNARY_OP, sof)
+_RVV_MASK_ITERATOR (_RVV_MASK_UNARY_OP, sif)
+_RVV_MASK_ITERATOR (_RVV_MASK_UNARY_OP_NOMASK, not)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, and)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, or)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, xor)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, nand)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, nor)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, xnor)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, andnot)
+_RVV_MASK_ITERATOR (_RVV_MASK_BIN_OP, ornot)
+
+#define _RVV_IOTA(SEW, LMUL, MLEN, T)					\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+viota_m_u##SEW##m##LMUL (vbool##MLEN##_t a)			\
+{									\
+  return __builtin_riscv_viotauint##SEW##m##LMUL (a);			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+viota_m_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			       vuint##SEW##m##LMUL##_t maskedoff,	\
+				      vbool##MLEN##_t a)		\
+{									\
+  return __builtin_riscv_viotauint##SEW##m##LMUL##_mask (mask,		\
+							 maskedoff, a);	\
+}
+
+_RVV_INT_ITERATOR (_RVV_IOTA)
+
+#define _RVV_VID(SEW, LMUL, MLEN, T)					\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vid_v_u##SEW##m##LMUL ()						\
+{									\
+  return __builtin_riscv_viduint##SEW##m##LMUL ();			\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vid_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+			      vuint##SEW##m##LMUL##_t maskedoff)	\
+{									\
+  return __builtin_riscv_viduint##SEW##m##LMUL##_mask (mask, maskedoff);\
+}
+
+
+_RVV_INT_ITERATOR (_RVV_VID)
+
+#define _RVV_MASK_MOVE(MLEN, OP)                                       \
+__extension__ extern __inline vbool##MLEN##_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vmmv_m_b##MLEN (vbool##MLEN##_t a)					\
+{                                                                      \
+  return a;                                                            \
+}
+
+_RVV_MASK_ITERATOR (_RVV_MASK_MOVE,)
+
+/* Helpers for FP widening multiply.  */
+#if 0
+__extension__ extern __inline vfloat32m8_t
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))
+vwmul_vv_float16m4 (vfloat16m4_t a, vfloat16m4_t b)
+{
+  return __builtin_riscv_vfwmulfloat16m4 (a, b);
+}
+
+__extension__ extern __inline vfloat32m8_t
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))
+vwmul_vf_float16m4 (vfloat16m4_t a, const float16_t b)
+{
+  return __builtin_riscv_vfwmulfloat16m4_scalar (a, b);
+}
+#endif
+
+/* Reductions.  */
+
+#define _RVV_REDUC_OP(SEW, LMUL, MLEN, T, OP, OPU)			\
+__extension__ extern __inline vint##SEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vred##OP##_vs_i##SEW##m##LMUL##_i##SEW##m1 (vint##SEW##m1_t dst,	\
+					    vint##SEW##m##LMUL##_t a,	\
+					    vint##SEW##m1_t b)		\
+{									\
+  return __builtin_riscv_reduc_##OP##int##SEW##m##LMUL (dst, b, a);	\
+}									\
+__extension__ extern __inline vuint##SEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vred##OPU##_vs_u##SEW##m##LMUL##_u##SEW##m1 (vuint##SEW##m1_t dst,	\
+					     vuint##SEW##m##LMUL##_t a,	\
+					     vuint##SEW##m1_t b)	\
+{									\
+  return __builtin_riscv_reduc_##OPU##uint##SEW##m##LMUL (dst, b, a);	\
+}									\
+__extension__ extern __inline vint##SEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vred##OP##_vs_i##SEW##m##LMUL##_i##SEW##m1_m (vbool##MLEN##_t mask,	\
+					      vint##SEW##m1_t dst,	\
+					      vint##SEW##m##LMUL##_t a,	\
+					      vint##SEW##m1_t b)	\
+{									\
+  return __builtin_riscv_reduc_##OP##int##SEW##m##LMUL##_mask (mask, dst,\
+							       b, a);	\
+}									\
+__extension__ extern __inline vuint##SEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vred##OPU##_vs_u##SEW##m##LMUL##_u##SEW##m1_m (vbool##MLEN##_t mask,	\
+					       vuint##SEW##m1_t dst,\
+					       vuint##SEW##m##LMUL##_t a,\
+					       vuint##SEW##m1_t b)	\
+{									\
+  return __builtin_riscv_reduc_##OPU##uint##SEW##m##LMUL##_mask (mask, dst,\
+								 b, a);	\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_REDUC_OP, sum, sum)
+_RVV_INT_ITERATOR_ARG (_RVV_REDUC_OP, max, maxu)
+_RVV_INT_ITERATOR_ARG (_RVV_REDUC_OP, min, minu)
+_RVV_INT_ITERATOR_ARG (_RVV_REDUC_OP, and, and)
+_RVV_INT_ITERATOR_ARG (_RVV_REDUC_OP, or, or)
+_RVV_INT_ITERATOR_ARG (_RVV_REDUC_OP, xor, xor)
+
+#define _RVV_WREDUC_OP(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP, OPU)	\
+__extension__ extern __inline vint##WSEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwred##OP##_vs_i##SEW##m##LMUL##_i##WSEW##m1 (vint##WSEW##m1_t dst,	\
+					      vint##SEW##m##LMUL##_t a,	\
+					      vint##WSEW##m1_t b)	\
+{									\
+  return __builtin_riscv_wreduc_##OP##int##SEW##m##LMUL (dst, b, a);	\
+}									\
+__extension__ extern __inline vuint##WSEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwred##OPU##_vs_u##SEW##m##LMUL##_u##WSEW##m1 (vuint##WSEW##m1_t dst,	\
+					       vuint##SEW##m##LMUL##_t a,\
+					       vuint##WSEW##m1_t b)	\
+{									\
+  return __builtin_riscv_wreduc_##OPU##uint##SEW##m##LMUL (dst, b, a);	\
+}									\
+__extension__ extern __inline vint##WSEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwred##OP##_vs_i##SEW##m##LMUL##_i##WSEW##m1_m (vbool##MLEN##_t mask,	\
+						vint##WSEW##m1_t dst,	\
+						vint##SEW##m##LMUL##_t a,\
+						vint##WSEW##m1_t b)	\
+{									\
+  return __builtin_riscv_wreduc_##OP##int##SEW##m##LMUL##_mask (mask, dst,\
+							     b, a);	\
+}									\
+__extension__ extern __inline vuint##WSEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vwred##OPU##_vs_u##SEW##m##LMUL##_u##WSEW##m1_m (vbool##MLEN##_t mask,	\
+						 vuint##WSEW##m1_t dst, \
+						 vuint##SEW##m##LMUL##_t a,\
+						 vuint##WSEW##m1_t b)	\
+{									\
+  return __builtin_riscv_wreduc_##OPU##uint##SEW##m##LMUL##_mask (mask,	dst, \
+								 b, a);	\
+}
+
+_RVV_WRED_INT_ITERATOR_ARG (_RVV_WREDUC_OP, sum, sumu)
+
+#define _RVV_FREDUC_OP(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vfloat##SEW##m1_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vfred##OP##_vs_f##SEW##m##LMUL##_f##SEW##m1 (vfloat##SEW##m1_t dst,	\
+					     vfloat##SEW##m##LMUL##_t a,\
+					     vfloat##SEW##m1_t b)	\
+{									\
+  return __builtin_riscv_freduc_##OP##float##SEW##m##LMUL (dst, b, a);	\
+}									\
+__extension__ extern __inline vfloat##SEW##m1_t				\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vfred##OP##_vs_f##SEW##m##LMUL##_f##SEW##m1_m (vbool##MLEN##_t mask,	\
+					       vfloat##SEW##m1_t dst,	\
+					       vfloat##SEW##m##LMUL##_t a,\
+					       vfloat##SEW##m1_t b)	\
+{									\
+  return __builtin_riscv_freduc_##OP##float##SEW##m##LMUL##_mask (mask, dst, \
+								  b, a);\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FREDUC_OP, sum)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FREDUC_OP, osum)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FREDUC_OP, max)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FREDUC_OP, min)
+
+#define _RVV_FWREDUC_OP(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vfloat##WSEW##m1_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vfwred##OP##_vs_f##SEW##m##LMUL##_f##WSEW##m1 (vfloat##WSEW##m1_t dst,	\
+					       vfloat##SEW##m##LMUL##_t a,\
+					       vfloat##WSEW##m1_t b)	\
+{									\
+  return __builtin_riscv_fwreduc_##OP##float##SEW##m##LMUL (dst, b, a);	\
+}									\
+__extension__ extern __inline vfloat##WSEW##m1_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vfwred##OP##_vs_f##SEW##m##LMUL##_f##WSEW##m1_m (vbool##MLEN##_t mask,	\
+						 vfloat##WSEW##m1_t dst, \
+						 vfloat##SEW##m##LMUL##_t a,\
+						 vfloat##WSEW##m1_t b)	\
+{									\
+  return __builtin_riscv_fwreduc_##OP##float##SEW##m##LMUL##_mask (mask, dst, \
+								   b, a);\
+}									\
+
+_RVV_WRED_FLOAT_ITERATOR_ARG (_RVV_FWREDUC_OP, sum)
+_RVV_WRED_FLOAT_ITERATOR_ARG (_RVV_FWREDUC_OP, osum)
+
+#define _RVV_VREINTERPRET(SEW, LMUL, MLEN, T)				\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_i##SEW##m##LMUL##_f##SEW##m##LMUL (vint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_i##SEW##m##LMUL##_f##SEW##m##LMUL (a);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_u##SEW##m##LMUL##_f##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_u##SEW##m##LMUL##_f##SEW##m##LMUL (a);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_f##SEW##m##LMUL##_i##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_f##SEW##m##LMUL##_i##SEW##m##LMUL (a);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_f##SEW##m##LMUL##_u##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_f##SEW##m##LMUL##_u##SEW##m##LMUL (a);\
+}
+
+#define _RVV_VREINTERPRET_INT(SEW, LMUL, MLEN, T)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_u##SEW##m##LMUL##_i##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_u##SEW##m##LMUL##_i##SEW##m##LMUL (a);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_i##SEW##m##LMUL##_u##SEW##m##LMUL (vint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_i##SEW##m##LMUL##_u##SEW##m##LMUL (a);\
+}
+
+_RVV_FLOAT_ITERATOR (_RVV_VREINTERPRET)
+
+_RVV_INT_ITERATOR (_RVV_VREINTERPRET_INT)
+
+#define _RVV_VREINTERPRET_XSEW_INT(SEW, LMUL, MLEN, T, XSEW, XLMUL)	\
+__extension__ extern __inline vint##XSEW##m##XLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_u##SEW##m##LMUL##_i##XSEW##m##XLMUL (vuint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_u##SEW##m##LMUL##_i##XSEW##m##XLMUL (a);\
+}									\
+__extension__ extern __inline vuint##XSEW##m##XLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_i##SEW##m##LMUL##_u##XSEW##m##XLMUL (vint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_i##SEW##m##LMUL##_u##XSEW##m##XLMUL (a);\
+}									\
+__extension__ extern __inline vint##XSEW##m##XLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_i##SEW##m##LMUL##_i##XSEW##m##XLMUL (vint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_i##SEW##m##LMUL##_i##XSEW##m##XLMUL (a);\
+}									\
+__extension__ extern __inline vuint##XSEW##m##XLMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_u##SEW##m##LMUL##_u##XSEW##m##XLMUL (vuint##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_u##SEW##m##LMUL##_u##XSEW##m##XLMUL (a);\
+}
+
+_RVV_INT_REINT_ITERATOR (_RVV_VREINTERPRET_XSEW_INT)
+
+#define _RVV_VREINTERPRET_XSEW_FLOAT(SEW, LMUL, MLEN, T, XSEW, XLMUL)	\
+__extension__ extern __inline vfloat##XSEW##m##XLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vreinterpret_v_f##SEW##m##LMUL##_f##XSEW##m##XLMUL (vfloat##SEW##m##LMUL##_t a)\
+{									\
+  return __builtin_riscv_vreinterpret_v_f##SEW##m##LMUL##_f##XSEW##m##XLMUL (a);\
+}									\
+
+_RVV_FLOAT_REINT_ITERATOR (_RVV_VREINTERPRET_XSEW_FLOAT)
+
+#define _RVV_MAC_FLOAT(SEW, LMUL, MLEN, T, OP)				\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t acc,		\
+			  vfloat##SEW##m##LMUL##_t a,			\
+			  vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL (acc, a, b);	\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t acc,		\
+			  _RVV_F##SEW##_TYPE a,				\
+			  vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##SEW##m##LMUL##_t acc,	\
+				   vfloat##SEW##m##LMUL##_t a,		\
+				   vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL##_mask (mask, acc, a, b);	\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##SEW##m##LMUL##_t acc,	\
+				   _RVV_F##SEW##_TYPE a,		\
+				   vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, macc)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, nmacc)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, msac)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, nmsac)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, madd)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, nmadd)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, msub)
+_RVV_FLOAT_ITERATOR_ARG (_RVV_MAC_FLOAT, nmsub)
+
+#define _RVV_WMAC_FLOAT(SEW, LMUL, MLEN, T, WSEW, WLMUL, WT, OP)	\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##WSEW##m##WLMUL (vfloat##WSEW##m##WLMUL##_t acc,		\
+			  vfloat##SEW##m##LMUL##_t a,			\
+			  vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL (acc, a, b);	\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##WSEW##m##WLMUL (vfloat##WSEW##m##WLMUL##_t acc,		\
+			  _RVV_F##SEW##_TYPE a,				\
+			  vfloat##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL##_scalar (acc, a, b);\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vv_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t acc,	\
+				   vfloat##SEW##m##LMUL##_t a,		\
+				   vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL##_mask (mask, acc, a, b);	\
+}									\
+__extension__ extern __inline vfloat##WSEW##m##WLMUL##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vf##OP##_vf_f##WSEW##m##WLMUL##_m (vbool##MLEN##_t mask,		\
+				   vfloat##WSEW##m##WLMUL##_t acc,	\
+				   _RVV_F##SEW##_TYPE a,		\
+				   vfloat##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_vf##OP##_sv_f##SEW##m##LMUL##_scalar_mask (mask, acc, a, b);\
+}
+
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WMAC_FLOAT, wmacc)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WMAC_FLOAT, wnmacc)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WMAC_FLOAT, wmsac)
+_RVV_WFLOAT_ITERATOR_ARG (_RVV_WMAC_FLOAT, wnmsac)
+
+#define _RVV_INT_VRGATHER(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vv_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			 vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vv_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			 vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL (a, b);		\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vv_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				vint##SEW##m##LMUL##_t maskedoff,	\
+				vint##SEW##m##LMUL##_t a,		\
+				vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vv_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				vuint##SEW##m##LMUL##_t maskedoff, 	\
+				vuint##SEW##m##LMUL##_t a,		\
+				vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a, b);					\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vx_i##SEW##m##LMUL (vint##SEW##m##LMUL##_t a,			\
+			 size_t b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_si_scalar (a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_di_scalar (a, b);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vx_u##SEW##m##LMUL (vuint##SEW##m##LMUL##_t a,			\
+			 size_t b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_si_scalar (a, b);\
+  else									\
+    return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_di_scalar (a, b);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vx_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				vint##SEW##m##LMUL##_t maskedoff,	\
+				vint##SEW##m##LMUL##_t a,		\
+				size_t b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_si_scalar_mask (	\
+		mask, maskedoff, a, b);					\
+  else									\
+    return __builtin_riscv_v##OP##int##SEW##m##LMUL##_di_scalar_mask (	\
+		mask, maskedoff, a, b);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vx_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				vuint##SEW##m##LMUL##_t maskedoff,	\
+				vuint##SEW##m##LMUL##_t a,		\
+				size_t b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_si_scalar_mask (	\
+		mask, maskedoff, a, b);					\
+  else									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_di_scalar_mask (	\
+		mask, maskedoff, a, b);					\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_VRGATHER, vrgather)
+
+#define _RVV_FLOAT_VRGATHER(SEW, LMUL, MLEN, T, INT_T, OP)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vv_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a,			\
+			 vuint##SEW##m##LMUL##_t b)			\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL (a, b);			\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vv_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				vfloat##SEW##m##LMUL##_t maskedoff, 	\
+				vfloat##SEW##m##LMUL##_t a,		\
+				vuint##SEW##m##LMUL##_t b)		\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a, b);						\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vx_f##SEW##m##LMUL (vfloat##SEW##m##LMUL##_t a,			\
+			 size_t b)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_si_scalar (a, b);	\
+  else									\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_di_scalar (a, b);	\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vx_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,			\
+				vfloat##SEW##m##LMUL##_t maskedoff,	\
+				vfloat##SEW##m##LMUL##_t a,		\
+				size_t b)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_si_scalar_mask (	\
+		mask, maskedoff, a, b);					\
+  else									\
+    return __builtin_riscv_v##OP##f##SEW##m##LMUL##_di_scalar_mask (	\
+		mask, maskedoff, a, b);					\
+}
+
+_RVV_FLOAT_INT_ITERATOR_ARG (_RVV_FLOAT_VRGATHER, vrgather)
+
+#define _RVV_INT_VCOMPRESS(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vm_i##SEW##m##LMUL (vbool##MLEN##_t mask,				\
+			 vint##SEW##m##LMUL##_t maskedoff,		\
+			 vint##SEW##m##LMUL##_t a)			\
+{									\
+  return __builtin_riscv_v##OP##int##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a);						\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vm_u##SEW##m##LMUL (vbool##MLEN##_t mask,				\
+			 vuint##SEW##m##LMUL##_t maskedoff, 		\
+			 vuint##SEW##m##LMUL##_t a)			\
+{									\
+  return __builtin_riscv_v##OP##uint##SEW##m##LMUL##_mask (		\
+	  mask, maskedoff, a);						\
+}
+
+_RVV_INT_ITERATOR_ARG (_RVV_INT_VCOMPRESS, vcompress)
+
+#define _RVV_FLOAT_VCOMPRESS(SEW, LMUL, MLEN, T, OP)			\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+OP##_vm_f##SEW##m##LMUL (vbool##MLEN##_t mask,				\
+			 vfloat##SEW##m##LMUL##_t maskedoff, 		\
+			 vfloat##SEW##m##LMUL##_t a)			\
+{									\
+  return __builtin_riscv_v##OP##f##SEW##m##LMUL##_mask (		\
+      mask, maskedoff, a);						\
+}
+
+_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_VCOMPRESS, vcompress)
+
+#define _RVVINT_AMO(SEW, LMUL, MLEN, T, ISEW, ILMUL, NAME)		\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##i##ISEW##_v_i##SEW##m##LMUL (const T *a,				\
+				vuint##ISEW##m##ILMUL##_t indexed,	\
+				vint##SEW##m##LMUL##_t value)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##ii##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed, value);\
+  else									\
+    return __builtin_riscv_##NAME##ii##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed, value);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##i##ISEW##_v_u##SEW##m##LMUL (const u##T *a,				\
+				vuint##ISEW##m##ILMUL##_t indexed,	\
+				vuint##SEW##m##LMUL##_t value)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##iu##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed, value);\
+  else									\
+    return __builtin_riscv_##NAME##iu##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed, value);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##i##ISEW##_v_i##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				       const T *a,			\
+				       vuint##ISEW##m##ILMUL##_t indexed,\
+				       vint##SEW##m##LMUL##_t value)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##ii##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, a, indexed, value);					\
+  else									\
+    return __builtin_riscv_##NAME##ii##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, a, indexed, value);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##i##ISEW##_v_u##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				       const u##T *a,			\
+				       vuint##ISEW##m##ILMUL##_t indexed,\
+				       vuint##SEW##m##LMUL##_t value)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##iu##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, a, indexed, value);					\
+  else									\
+    return __builtin_riscv_##NAME##iu##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, a, indexed, value);					\
+}
+
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamoswape)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamoadde)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamoxore)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamoande)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamoore)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamomine)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamomaxe)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamominue)
+_RVV_INT_INDEX_ITERATOR_ARG (_RVVINT_AMO, vamomaxue)
+
+#define _RVVFLOAT_AMO(SEW, LMUL, MLEN, T, ISEW, ILMUL, NAME)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##i##ISEW##_v_f##SEW##m##LMUL (const _RVV_F##SEW##_TYPE *a,		\
+				vuint##ISEW##m##ILMUL##_t indexed,	\
+				vfloat##SEW##m##LMUL##_t value)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_##NAME##if##SEW##m##LMUL##_##ISEW##m##ILMUL##_si (a, indexed, value);\
+  else									\
+    return __builtin_riscv_##NAME##if##SEW##m##LMUL##_##ISEW##m##ILMUL##_di (a, indexed, value);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+NAME##i##ISEW##_v_f##SEW##m##LMUL##_m (vbool##MLEN##_t mask,		\
+				       const _RVV_F##SEW##_TYPE *a,	\
+				       vuint##ISEW##m##ILMUL##_t indexed,\
+				       vfloat##SEW##m##LMUL##_t value)	\
+{									\
+  if (__riscv_xlen == 32)						\
+  return __builtin_riscv_##NAME##if##SEW##m##LMUL##_##ISEW##m##ILMUL##_si_mask (\
+	   mask, a, indexed, value);					\
+  else									\
+  return __builtin_riscv_##NAME##if##SEW##m##LMUL##_##ISEW##m##ILMUL##_di_mask (\
+	   mask, a, indexed, value);					\
+}
+
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamoswape)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamoadde)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamoxore)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamoande)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamoore)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamomine)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamomaxe)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamominue)
+_RVV_FLOAT_INDEX_ITERATOR_ARG (_RVVFLOAT_AMO, vamomaxue)
+
+__extension__ extern __inline size_t
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))
+vreadvl ()
+{
+  if (__riscv_xlen == 32)
+    return __builtin_riscv_vreadvlsi ();
+  else
+    return __builtin_riscv_vreadvldi ();
+}
+
+
+#define _RVVINT_TUPLE_LDST(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF (const int##SEW##_t *a)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_loadint##SEW##m##LMUL##x##NF##_si (a);	\
+  else									\
+    return __builtin_riscv_vseg_loadint##SEW##m##LMUL##x##NF##_di (a);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const int##SEW##_t *a)						\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_loadint##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vseg_loadint##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##ff_v_i##SEW##m##LMUL##x##NF (const int##SEW##_t *a)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_ff_loadint##SEW##m##LMUL##x##NF##_si (a);\
+  else									\
+    return __builtin_riscv_vseg_ff_loadint##SEW##m##LMUL##x##NF##_di (a);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##ff_v_i##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const int##SEW##_t *a)						\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_ff_loadint##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vseg_ff_loadint##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlsseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF (const int##SEW##_t *a, ptrdiff_t s)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_strided_loadint##SEW##m##LMUL##x##NF##_si (a, s);\
+  else									\
+    return __builtin_riscv_vseg_strided_loadint##SEW##m##LMUL##x##NF##_di (a, s);\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlsseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const int##SEW##_t *a, ptrdiff_t s)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_strided_loadint##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a, s);					\
+  else									\
+    return __builtin_riscv_vseg_strided_loadint##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a, s);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF (const uint##SEW##_t *a)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_loaduint##SEW##m##LMUL##x##NF##_si (a);	\
+  else									\
+    return __builtin_riscv_vseg_loaduint##SEW##m##LMUL##x##NF##_di (a);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vuint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const uint##SEW##_t *a)						\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_loaduint##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vseg_loaduint##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##ff_v_u##SEW##m##LMUL##x##NF (const uint##SEW##_t *a)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_ff_loaduint##SEW##m##LMUL##x##NF##_si (a);\
+  else									\
+    return __builtin_riscv_vseg_ff_loaduint##SEW##m##LMUL##x##NF##_di (a);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##ff_v_u##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vuint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const uint##SEW##_t *a)						\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_ff_loaduint##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vseg_ff_loaduint##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlsseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF (const uint##SEW##_t *a, ptrdiff_t s)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_strided_loaduint##SEW##m##LMUL##x##NF##_si (a, s);	\
+  else									\
+    return __builtin_riscv_vseg_strided_loaduint##SEW##m##LMUL##x##NF##_di (a, s);	\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlsseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vuint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const uint##SEW##_t *a, ptrdiff_t s)					\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_strided_loaduint##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a, s);					\
+  else									\
+    return __builtin_riscv_vseg_strided_loaduint##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a,s );					\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF (uint##SEW##_t *a,		\
+				   vuint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_storeuint##SEW##m##LMUL##x##NF##_si (b, a);	\
+  else									\
+    __builtin_riscv_vseg_storeuint##SEW##m##LMUL##x##NF##_di (b, a);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					   uint##SEW##_t *a,		\
+				   vuint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_storeuint##SEW##m##LMUL##x##NF##_si_mask (mask, b, a);	\
+  else									\
+    __builtin_riscv_vseg_storeuint##SEW##m##LMUL##x##NF##_di_mask (mask, b, a);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF (int##SEW##_t *a,			\
+				   vint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_storeint##SEW##m##LMUL##x##NF##_si (b, a);	\
+  else									\
+    __builtin_riscv_vseg_storeint##SEW##m##LMUL##x##NF##_di (b, a);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					   int##SEW##_t *a,		\
+				   vint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_storeint##SEW##m##LMUL##x##NF##_si_mask (mask, b, a);	\
+  else									\
+    __builtin_riscv_vseg_storeint##SEW##m##LMUL##x##NF##_di_mask (mask, b, a);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vssseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF (uint##SEW##_t *a, ptrdiff_t s,	\
+				   vuint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_strided_storeuint##SEW##m##LMUL##x##NF##_si (b, a, s);\
+  else									\
+    __builtin_riscv_vseg_strided_storeuint##SEW##m##LMUL##x##NF##_di (b, a, s);\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vssseg##NF##e##SEW##_v_u##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					   uint##SEW##_t *a, ptrdiff_t s,\
+				   vuint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_strided_storeuint##SEW##m##LMUL##x##NF##_si_mask (mask, b, a, s);	\
+  else									\
+    __builtin_riscv_vseg_strided_storeuint##SEW##m##LMUL##x##NF##_di_mask (mask, b, a, s);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vssseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF (int##SEW##_t *a, ptrdiff_t s,	\
+				   vint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_strided_storeint##SEW##m##LMUL##x##NF##_si (b, a, s);	\
+  else									\
+    __builtin_riscv_vseg_strided_storeint##SEW##m##LMUL##x##NF##_di (b, a, s);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vssseg##NF##e##SEW##_v_i##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					   int##SEW##_t *a,		\
+					   ptrdiff_t s,			\
+				   vint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_strided_storeint##SEW##m##LMUL##x##NF##_si_mask (mask, b, a, s);	\
+  else									\
+    __builtin_riscv_vseg_strided_storeint##SEW##m##LMUL##x##NF##_di_mask (mask, b, a, s);	\
+}									\
+
+_RVV_INT_TUPLE_ITERATOR_ARG (_RVVINT_TUPLE_LDST, )
+
+#define _RVVINT_TUPLE_IDX_LDST(SEW, LMUL, NF, MLEN, T, ISEW, ILMUL)	\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxseg##NF##ei##ISEW##_v_i##SEW##m##LMUL##x##NF (const int##SEW##_t *a,	\
+					       vuint##ISEW##m##ILMUL##_t idx)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_idx_loadi##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si (a, idx);	\
+  else									\
+    return __builtin_riscv_vseg_idx_loadi##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di (a, idx);	\
+}									\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxseg##NF##ei##ISEW##_v_i##SEW##m##LMUL##x##NF##_m (			\
+  vbool##MLEN##_t mask,							\
+  vint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const int##SEW##_t *a,						\
+  vuint##ISEW##m##ILMUL##_t idx)						\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_idx_loadi##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, maskedoff, a, idx);					\
+  else									\
+    return __builtin_riscv_vseg_idx_loadi##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, maskedoff, a, idx);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxseg##NF##ei##ISEW##_v_u##SEW##m##LMUL##x##NF (const uint##SEW##_t *a, vuint##ISEW##m##ILMUL##_t idx)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_idx_loadu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si (a, idx);\
+  else									\
+    return __builtin_riscv_vseg_idx_loadu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di (a, idx);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxseg##NF##ei##ISEW##_v_u##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vuint##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const uint##SEW##_t *a, vuint##ISEW##m##ILMUL##_t idx)			\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_idx_loadu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, maskedoff, a, idx);					\
+  else									\
+    return __builtin_riscv_vseg_idx_loadu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, maskedoff, a, idx);					\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsxseg##NF##ei##ISEW##_v_u##SEW##m##LMUL##x##NF (uint##SEW##_t *a,		\
+					vuint##ISEW##m##ILMUL##_t idx, \
+				   vuint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_idx_storeu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si (b, a, idx);	\
+  else									\
+    __builtin_riscv_vseg_idx_storeu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di (b, a, idx);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsxseg##NF##ei##ISEW##_v_i##SEW##m##LMUL##x##NF (int##SEW##_t *a,		\
+					vuint##ISEW##m##ILMUL##_t idx, \
+				   vint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_idx_storei##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si (b, a, idx);	\
+  else									\
+    __builtin_riscv_vseg_idx_storei##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di (b, a, idx);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsxseg##NF##ei##ISEW##_v_u##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					   uint##SEW##_t *a,		\
+					   vuint##ISEW##m##ILMUL##_t idx, \
+				   vuint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_idx_storeu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si_mask (mask, b, a, idx);	\
+  else									\
+    __builtin_riscv_vseg_idx_storeu##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di_mask (mask, b, a, idx);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsxseg##NF##ei##ISEW##_v_i##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					   int##SEW##_t *a,		\
+					   vuint##ISEW##m##ILMUL##_t idx, \
+				   vint##SEW##m##LMUL##x##NF##_t b)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_idx_storei##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si_mask (mask, b, a, idx);	\
+  else									\
+    __builtin_riscv_vseg_idx_storei##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di_mask (mask, b, a, idx);	\
+}									\
+
+_RVV_INT_TUPLE_INDEX_ITERATOR (_RVVINT_TUPLE_IDX_LDST)
+
+
+#define _RVVFLOAT_TUPLE_LDST(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF (const _RVV_F##SEW##_TYPE *a)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_loadfloat##SEW##m##LMUL##x##NF##_si (a);\
+  else									\
+    return __builtin_riscv_vseg_loadfloat##SEW##m##LMUL##x##NF##_di (a);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const _RVV_F##SEW##_TYPE *a)						\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_loadfloat##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vseg_loadfloat##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##ff_v_f##SEW##m##LMUL##x##NF (const _RVV_F##SEW##_TYPE *a)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_ff_loadfloat##SEW##m##LMUL##x##NF##_si (a);\
+  else									\
+    return __builtin_riscv_vseg_ff_loadfloat##SEW##m##LMUL##x##NF##_di (a);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlseg##NF##e##SEW##ff_v_f##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const _RVV_F##SEW##_TYPE *a)						\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_ff_loadfloat##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a);					\
+  else									\
+    return __builtin_riscv_vseg_ff_loadfloat##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a);					\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlsseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF (const _RVV_F##SEW##_TYPE *a, ptrdiff_t s) \
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_strided_loadfloat##SEW##m##LMUL##x##NF##_si (a, s);\
+  else									\
+    return __builtin_riscv_vseg_strided_loadfloat##SEW##m##LMUL##x##NF##_di (a, s);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlsseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const _RVV_F##SEW##_TYPE *a, ptrdiff_t s)				\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_strided_loadfloat##SEW##m##LMUL##x##NF##_si_mask (\
+	     mask, maskedoff, a, s);					\
+  else									\
+    return __builtin_riscv_vseg_strided_loadfloat##SEW##m##LMUL##x##NF##_di_mask (\
+	     mask, maskedoff, a, s);					\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF (_RVV_F##SEW##_TYPE *a,		\
+				       vfloat##SEW##m##LMUL##x##NF##_t b)\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_storefloat##SEW##m##LMUL##x##NF##_si (b, a);	\
+  else									\
+    __builtin_riscv_vseg_storefloat##SEW##m##LMUL##x##NF##_di (b, a);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					  _RVV_F##SEW##_TYPE *a,	\
+				       vfloat##SEW##m##LMUL##x##NF##_t b)\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_storefloat##SEW##m##LMUL##x##NF##_si_mask (mask, b, a);	\
+  else									\
+    __builtin_riscv_vseg_storefloat##SEW##m##LMUL##x##NF##_di_mask (mask, b, a);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vssseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF (_RVV_F##SEW##_TYPE *a, ptrdiff_t s,\
+				       vfloat##SEW##m##LMUL##x##NF##_t b)\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_strided_storefloat##SEW##m##LMUL##x##NF##_si (b, a, s);	\
+  else									\
+    __builtin_riscv_vseg_strided_storefloat##SEW##m##LMUL##x##NF##_di (b, a, s);	\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vssseg##NF##e##SEW##_v_f##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					  _RVV_F##SEW##_TYPE *a, ptrdiff_t s,	\
+				       vfloat##SEW##m##LMUL##x##NF##_t b)\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_strided_storefloat##SEW##m##LMUL##x##NF##_si_mask (mask, b, a, s);	\
+  else									\
+    __builtin_riscv_vseg_strided_storefloat##SEW##m##LMUL##x##NF##_di_mask (mask, b, a, s);	\
+}
+
+
+_RVV_FLOAT_TUPLE_ITERATOR_ARG (_RVVFLOAT_TUPLE_LDST, )
+
+#define _RVVFLOAT_TUPLE_IDX_LDST(SEW, LMUL, NF, MLEN, T, ISEW, ILMUL)	\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxseg##NF##ei##ISEW##_v_f##SEW##m##LMUL##x##NF (const _RVV_F##SEW##_TYPE *a, vuint##ISEW##m##ILMUL##_t idx)	\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_idx_loadf##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si (a, idx);\
+  else									\
+    return __builtin_riscv_vseg_idx_loadf##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di (a, idx);\
+}									\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vlxseg##NF##ei##ISEW##_v_f##SEW##m##LMUL##x##NF##_m (				\
+  vbool##MLEN##_t mask,							\
+  vfloat##SEW##m##LMUL##x##NF##_t maskedoff,				\
+  const _RVV_F##SEW##_TYPE *a, vuint##ISEW##m##ILMUL##_t idx)		\
+{									\
+  if (__riscv_xlen == 32)						\
+    return __builtin_riscv_vseg_idx_loadf##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si_mask (\
+	     mask, maskedoff, a, idx);					\
+  else									\
+    return __builtin_riscv_vseg_idx_loadf##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di_mask (\
+	     mask, maskedoff, a, idx);					\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsxseg##NF##ei##ISEW##_v_f##SEW##m##LMUL##x##NF (_RVV_F##SEW##_TYPE *a,		\
+					vuint##ISEW##m##ILMUL##_t idx,	\
+				       vfloat##SEW##m##LMUL##x##NF##_t b)\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_idx_storef##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si (b, a, idx);\
+  else									\
+    __builtin_riscv_vseg_idx_storef##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di (b, a, idx);\
+}									\
+__extension__ extern __inline void					\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vsxseg##NF##ei##ISEW##_v_f##SEW##m##LMUL##x##NF##_m (vbool##MLEN##_t mask,	\
+					  _RVV_F##SEW##_TYPE *a,	\
+					vuint##ISEW##m##ILMUL##_t idx,	\
+				       vfloat##SEW##m##LMUL##x##NF##_t b)\
+{									\
+  if (__riscv_xlen == 32)						\
+    __builtin_riscv_vseg_idx_storef##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_si_mask (mask, b, a, idx);	\
+  else									\
+    __builtin_riscv_vseg_idx_storef##SEW##m##LMUL##x##NF##_##ISEW##m##ILMUL##_di_mask (mask, b, a, idx);	\
+}									\
+
+_RVV_FLOAT_TUPLE_INDEX_ITERATOR (_RVVFLOAT_TUPLE_IDX_LDST)
+
+
+#define _RVVINT_TUPLE_INSERT(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vset_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##x##NF##_t vt,		\
+			     unsigned idx,				\
+			     vint##SEW##m##LMUL##_t v)			\
+{									\
+  return __builtin_riscv_vtuple_insertint##SEW##m##LMUL##x##NF (vt, v, idx);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vset_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##x##NF##_t vt,		\
+			     unsigned idx,				\
+			     vuint##SEW##m##LMUL##_t v)			\
+{									\
+  return __builtin_riscv_vtuple_insertuint##SEW##m##LMUL##x##NF (vt, v, idx);\
+}
+
+_RVV_INT_TUPLE_ITERATOR_ARG (_RVVINT_TUPLE_INSERT, )
+
+#define _RVVFLOAT_TUPLE_INSERT(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vset_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##x##NF##_t vt,	\
+			     unsigned idx,				\
+			     vfloat##SEW##m##LMUL##_t v)		\
+{									\
+  return __builtin_riscv_vtuple_insertfloat##SEW##m##LMUL##x##NF (vt, v, idx);\
+}
+_RVV_FLOAT_TUPLE_ITERATOR_ARG (_RVVFLOAT_TUPLE_INSERT, )
+
+#define _RVVINT_TUPLE_EXTRACT(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vget_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##x##NF##_t vt,		\
+			     unsigned idx)				\
+{									\
+  return __builtin_riscv_vtuple_extractint##SEW##m##LMUL##x##NF (vt, idx);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vget_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##x##NF##_t vt,		\
+			     unsigned idx)				\
+{									\
+  return __builtin_riscv_vtuple_extractuint##SEW##m##LMUL##x##NF (vt, idx);\
+}
+
+_RVV_INT_TUPLE_ITERATOR_ARG (_RVVINT_TUPLE_EXTRACT, )
+
+#define _RVVFLOAT_TUPLE_EXTRACT(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##_t			\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vget_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##x##NF##_t vt,	\
+			     unsigned idx)				\
+{									\
+  return __builtin_riscv_vtuple_extractfloat##SEW##m##LMUL##x##NF (vt, idx);\
+}
+_RVV_FLOAT_TUPLE_ITERATOR_ARG (_RVVFLOAT_TUPLE_EXTRACT, )
+
+#define _RVVINT_TUPLE_CREATE2(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##_t v1,		\
+				vint##SEW##m##LMUL##_t v2)		\
+{									\
+  return __builtin_riscv_vtuple_createint##SEW##m##LMUL##x##NF (v1, v2);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##_t v1,		\
+				vuint##SEW##m##LMUL##_t v2)		\
+{									\
+  return __builtin_riscv_vtuple_createuint##SEW##m##LMUL##x##NF (	\
+	   v1, v2);							\
+}
+
+_RVV_INT_TUPLE_NF2_ITERATOR_ARG (_RVVINT_TUPLE_CREATE2, )
+
+#define _RVVINT_TUPLE_CREATE3(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##_t v1,		\
+				vint##SEW##m##LMUL##_t v2,		\
+				vint##SEW##m##LMUL##_t v3)		\
+{									\
+  return __builtin_riscv_vtuple_createint##SEW##m##LMUL##x##NF (v1, v2, v3);\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##_t v1,		\
+				vuint##SEW##m##LMUL##_t v2,		\
+				vuint##SEW##m##LMUL##_t v3)		\
+{									\
+  return __builtin_riscv_vtuple_createuint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3);							\
+}
+
+_RVV_INT_TUPLE_NF3_ITERATOR_ARG (_RVVINT_TUPLE_CREATE3, )
+
+#define _RVVINT_TUPLE_CREATE4(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##_t v1,		\
+				vint##SEW##m##LMUL##_t v2,		\
+				vint##SEW##m##LMUL##_t v3,		\
+				vint##SEW##m##LMUL##_t v4)		\
+{									\
+  return __builtin_riscv_vtuple_createint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4);						\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##_t v1,		\
+				vuint##SEW##m##LMUL##_t v2,		\
+				vuint##SEW##m##LMUL##_t v3,		\
+				vuint##SEW##m##LMUL##_t v4)		\
+{									\
+  return __builtin_riscv_vtuple_createuint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4);						\
+}
+
+_RVV_INT_TUPLE_NF4_ITERATOR_ARG (_RVVINT_TUPLE_CREATE4, )
+
+#define _RVVINT_TUPLE_CREATE5(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##_t v1,		\
+				vint##SEW##m##LMUL##_t v2,		\
+				vint##SEW##m##LMUL##_t v3,		\
+				vint##SEW##m##LMUL##_t v4,		\
+				vint##SEW##m##LMUL##_t v5)		\
+{									\
+  return __builtin_riscv_vtuple_createint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5);						\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##_t v1,		\
+				vuint##SEW##m##LMUL##_t v2,		\
+				vuint##SEW##m##LMUL##_t v3,		\
+				vuint##SEW##m##LMUL##_t v4,		\
+				vuint##SEW##m##LMUL##_t v5)		\
+{									\
+  return __builtin_riscv_vtuple_createuint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5);						\
+}
+
+_RVV_INT_TUPLE_NF5_ITERATOR_ARG (_RVVINT_TUPLE_CREATE5, )
+
+#define _RVVINT_TUPLE_CREATE6(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##_t v1,		\
+				vint##SEW##m##LMUL##_t v2,		\
+				vint##SEW##m##LMUL##_t v3,		\
+				vint##SEW##m##LMUL##_t v4,		\
+				vint##SEW##m##LMUL##_t v5,		\
+				vint##SEW##m##LMUL##_t v6)		\
+{									\
+  return __builtin_riscv_vtuple_createint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##_t v1,		\
+				vuint##SEW##m##LMUL##_t v2,		\
+				vuint##SEW##m##LMUL##_t v3,		\
+				vuint##SEW##m##LMUL##_t v4,		\
+				vuint##SEW##m##LMUL##_t v5,		\
+				vuint##SEW##m##LMUL##_t v6)		\
+{									\
+  return __builtin_riscv_vtuple_createuint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6);					\
+}
+
+_RVV_INT_TUPLE_NF6_ITERATOR_ARG (_RVVINT_TUPLE_CREATE6, )
+
+#define _RVVINT_TUPLE_CREATE7(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##_t v1,		\
+				vint##SEW##m##LMUL##_t v2,		\
+				vint##SEW##m##LMUL##_t v3,		\
+				vint##SEW##m##LMUL##_t v4,		\
+				vint##SEW##m##LMUL##_t v5,		\
+				vint##SEW##m##LMUL##_t v6,		\
+				vint##SEW##m##LMUL##_t v7)		\
+{									\
+  return __builtin_riscv_vtuple_createint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6, v7);					\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##_t v1,		\
+				vuint##SEW##m##LMUL##_t v2,		\
+				vuint##SEW##m##LMUL##_t v3,		\
+				vuint##SEW##m##LMUL##_t v4,		\
+				vuint##SEW##m##LMUL##_t v5,		\
+				vuint##SEW##m##LMUL##_t v6,		\
+				vuint##SEW##m##LMUL##_t v7)		\
+{									\
+  return __builtin_riscv_vtuple_createuint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6, v7);					\
+}
+
+_RVV_INT_TUPLE_NF7_ITERATOR_ARG (_RVVINT_TUPLE_CREATE7, )
+
+#define _RVVINT_TUPLE_CREATE8(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_i##SEW##m##LMUL##x##NF (vint##SEW##m##LMUL##_t v1,		\
+				vint##SEW##m##LMUL##_t v2,		\
+				vint##SEW##m##LMUL##_t v3,		\
+				vint##SEW##m##LMUL##_t v4,		\
+				vint##SEW##m##LMUL##_t v5,		\
+				vint##SEW##m##LMUL##_t v6,		\
+				vint##SEW##m##LMUL##_t v7,		\
+				vint##SEW##m##LMUL##_t v8)		\
+{									\
+  return __builtin_riscv_vtuple_createint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6, v7, v8);				\
+}									\
+__extension__ extern __inline vuint##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_u##SEW##m##LMUL##x##NF (vuint##SEW##m##LMUL##_t v1,		\
+				vuint##SEW##m##LMUL##_t v2,		\
+				vuint##SEW##m##LMUL##_t v3,		\
+				vuint##SEW##m##LMUL##_t v4,		\
+				vuint##SEW##m##LMUL##_t v5,		\
+				vuint##SEW##m##LMUL##_t v6,		\
+				vuint##SEW##m##LMUL##_t v7,		\
+				vuint##SEW##m##LMUL##_t v8)		\
+{									\
+  return __builtin_riscv_vtuple_createuint##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6, v7, v8);				\
+}
+
+_RVV_INT_TUPLE_NF8_ITERATOR_ARG (_RVVINT_TUPLE_CREATE8, )
+
+
+#define _RVVFLOAT_TUPLE_CREATE2(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##_t v1,		\
+				vfloat##SEW##m##LMUL##_t v2)		\
+{									\
+  return __builtin_riscv_vtuple_createfloat##SEW##m##LMUL##x##NF (v1, v2);\
+}
+
+_RVV_FLOAT_TUPLE_NF2_ITERATOR_ARG (_RVVFLOAT_TUPLE_CREATE2, )
+
+#define _RVVFLOAT_TUPLE_CREATE3(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##_t v1,		\
+				vfloat##SEW##m##LMUL##_t v2,		\
+				vfloat##SEW##m##LMUL##_t v3)		\
+{									\
+  return __builtin_riscv_vtuple_createfloat##SEW##m##LMUL##x##NF (v1, v2, v3);\
+}
+
+_RVV_FLOAT_TUPLE_NF3_ITERATOR_ARG (_RVVFLOAT_TUPLE_CREATE3, )
+
+#define _RVVFLOAT_TUPLE_CREATE4(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##_t v1,		\
+				vfloat##SEW##m##LMUL##_t v2,		\
+				vfloat##SEW##m##LMUL##_t v3,		\
+				vfloat##SEW##m##LMUL##_t v4)		\
+{									\
+  return __builtin_riscv_vtuple_createfloat##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4);						\
+}
+
+_RVV_FLOAT_TUPLE_NF4_ITERATOR_ARG (_RVVFLOAT_TUPLE_CREATE4, )
+
+#define _RVVFLOAT_TUPLE_CREATE5(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##_t v1,		\
+				vfloat##SEW##m##LMUL##_t v2,		\
+				vfloat##SEW##m##LMUL##_t v3,		\
+				vfloat##SEW##m##LMUL##_t v4,		\
+				vfloat##SEW##m##LMUL##_t v5)		\
+{									\
+  return __builtin_riscv_vtuple_createfloat##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5);						\
+}
+
+_RVV_FLOAT_TUPLE_NF5_ITERATOR_ARG (_RVVFLOAT_TUPLE_CREATE5, )
+
+#define _RVVFLOAT_TUPLE_CREATE6(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##_t v1,		\
+				vfloat##SEW##m##LMUL##_t v2,		\
+				vfloat##SEW##m##LMUL##_t v3,		\
+				vfloat##SEW##m##LMUL##_t v4,		\
+				vfloat##SEW##m##LMUL##_t v5,		\
+				vfloat##SEW##m##LMUL##_t v6)		\
+{									\
+  return __builtin_riscv_vtuple_createfloat##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6);					\
+}
+
+_RVV_FLOAT_TUPLE_NF6_ITERATOR_ARG (_RVVFLOAT_TUPLE_CREATE6, )
+
+#define _RVVFLOAT_TUPLE_CREATE7(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##_t v1,		\
+				vfloat##SEW##m##LMUL##_t v2,		\
+				vfloat##SEW##m##LMUL##_t v3,		\
+				vfloat##SEW##m##LMUL##_t v4,		\
+				vfloat##SEW##m##LMUL##_t v5,		\
+				vfloat##SEW##m##LMUL##_t v6,		\
+				vfloat##SEW##m##LMUL##_t v7)		\
+{									\
+  return __builtin_riscv_vtuple_createfloat##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6, v7);					\
+}
+
+_RVV_FLOAT_TUPLE_NF7_ITERATOR_ARG (_RVVFLOAT_TUPLE_CREATE7, )
+
+#define _RVVFLOAT_TUPLE_CREATE8(SEW, LMUL, NF, MLEN, T, XARG)		\
+__extension__ extern __inline vfloat##SEW##m##LMUL##x##NF##_t		\
+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))	\
+vcreate_f##SEW##m##LMUL##x##NF (vfloat##SEW##m##LMUL##_t v1,		\
+				vfloat##SEW##m##LMUL##_t v2,		\
+				vfloat##SEW##m##LMUL##_t v3,		\
+				vfloat##SEW##m##LMUL##_t v4,		\
+				vfloat##SEW##m##LMUL##_t v5,		\
+				vfloat##SEW##m##LMUL##_t v6,		\
+				vfloat##SEW##m##LMUL##_t v7,		\
+				vfloat##SEW##m##LMUL##_t v8)		\
+{									\
+  return __builtin_riscv_vtuple_createfloat##SEW##m##LMUL##x##NF (	\
+	   v1, v2, v3, v4, v5, v6, v7, v8);				\
+}
+
+_RVV_FLOAT_TUPLE_NF8_ITERATOR_ARG (_RVVFLOAT_TUPLE_CREATE8, )
+
+#endif
+#endif
diff --git a/gcc/config/riscv/riscv_vector_itr.h b/gcc/config/riscv/riscv_vector_itr.h
new file mode 100644
index 00000000000..383f374b558
--- /dev/null
+++ b/gcc/config/riscv/riscv_vector_itr.h
@@ -0,0 +1,1537 @@
+/* DO NOT EDIT, please edit generator instead.
+   This file was generated by gen-vector-iterator with the command:
+   $ ./gen-vector-iterator -usr-c > riscv_vector_itr.h  */
+#ifndef _GCC_RISCV_VECTOR_H
+
+#error "Never included riscv_vector_itr.h, plz include riscv_vector.h"
+
+#endif
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector and integer types.  */
+#define _RVV_INT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, int8_t) \
+  MACRO (8, 2, 4, int8_t) \
+  MACRO (8, 4, 2, int8_t) \
+  MACRO (8, 8, 1, int8_t) \
+  MACRO (16, 1, 16, int16_t) \
+  MACRO (16, 2, 8, int16_t) \
+  MACRO (16, 4, 4, int16_t) \
+  MACRO (16, 8, 2, int16_t) \
+  MACRO (32, 1, 32, int32_t) \
+  MACRO (32, 2, 16, int32_t) \
+  MACRO (32, 4, 8, int32_t) \
+  MACRO (32, 8, 4, int32_t) \
+  MACRO (64, 1, 64, int64_t) \
+  MACRO (64, 2, 32, int64_t) \
+  MACRO (64, 4, 16, int64_t) \
+  MACRO (64, 8, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding widening vector type.  */
+#define _RVV_WINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, int8_t, 16, 2, int16_t) \
+  MACRO (8, 2, 4, int8_t, 16, 4, int16_t) \
+  MACRO (8, 4, 2, int8_t, 16, 8, int16_t) \
+  MACRO (16, 1, 16, int16_t, 32, 2, int32_t) \
+  MACRO (16, 2, 8, int16_t, 32, 4, int32_t) \
+  MACRO (16, 4, 4, int16_t, 32, 8, int32_t) \
+  MACRO (32, 1, 32, int32_t, 64, 2, int64_t) \
+  MACRO (32, 2, 16, int32_t, 64, 4, int64_t) \
+  MACRO (32, 4, 8, int32_t, 64, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, int8_t, 16, 2, int16_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 4, int16_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 2, int32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 4, int32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 8, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 2, int64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 4, int64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding widening vector type but with LMUL 1.  */
+#define _RVV_WRED_INT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, int8_t, 16, 1, int16_t) \
+  MACRO (8, 2, 4, int8_t, 16, 1, int16_t) \
+  MACRO (8, 4, 2, int8_t, 16, 1, int16_t) \
+  MACRO (8, 8, 1, int8_t, 16, 1, int16_t) \
+  MACRO (16, 1, 16, int16_t, 32, 1, int32_t) \
+  MACRO (16, 2, 8, int16_t, 32, 1, int32_t) \
+  MACRO (16, 4, 4, int16_t, 32, 1, int32_t) \
+  MACRO (16, 8, 2, int16_t, 32, 1, int32_t) \
+  MACRO (32, 1, 32, int32_t, 64, 1, int64_t) \
+  MACRO (32, 2, 16, int32_t, 64, 1, int64_t) \
+  MACRO (32, 4, 8, int32_t, 64, 1, int64_t) \
+  MACRO (32, 8, 4, int32_t, 64, 1, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WRED_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding quad widening vector type.  */
+#define _RVV_QINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, int8_t, 32, 4, int32_t) \
+  MACRO (8, 2, 4, int8_t, 32, 8, int32_t) \
+  MACRO (16, 1, 16, int16_t, 64, 4, int64_t) \
+  MACRO (16, 2, 8, int16_t, 64, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_QINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, int8_t, 32, 4, int32_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 32, 8, int32_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 64, 4, int64_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 64, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer modes, and info for
+   corresponding eightfold widening vector type.  */
+#define _RVV_EINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, int8_t, 64, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_EINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, int8_t, 64, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector and floating point modes.  */
+#define _RVV_FLOAT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, __float16_t) \
+  MACRO (16, 2, 8, __float16_t) \
+  MACRO (16, 4, 4, __float16_t) \
+  MACRO (16, 8, 2, __float16_t) \
+  MACRO (32, 1, 32, __float32_t) \
+  MACRO (32, 2, 16, __float32_t) \
+  MACRO (32, 4, 8, __float32_t) \
+  MACRO (32, 8, 4, __float32_t) \
+  MACRO (64, 1, 64, __float64_t) \
+  MACRO (64, 2, 32, __float64_t) \
+  MACRO (64, 4, 16, __float64_t) \
+  MACRO (64, 8, 8, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating modes, and info for
+   corresponding widening vector type.  */
+#define _RVV_WFLOAT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, __float16_t, 32, 2, __float32_t) \
+  MACRO (16, 2, 8, __float16_t, 32, 4, __float32_t) \
+  MACRO (16, 4, 4, __float16_t, 32, 8, __float32_t) \
+  MACRO (32, 1, 32, __float32_t, 64, 2, __float64_t) \
+  MACRO (32, 2, 16, __float32_t, 64, 4, __float64_t) \
+  MACRO (32, 4, 8, __float32_t, 64, 8, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WFLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, __float16_t, 32, 2, __float32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 32, 4, __float32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 32, 8, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 64, 2, __float64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 64, 4, __float64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 64, 8, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating types, and info for
+   corresponding widening vector type but with LMUL 1.  */
+#define _RVV_WRED_FLOAT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 2, 8, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 4, 4, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 8, 2, __float16_t, 32, 1, __float32_t) \
+  MACRO (32, 1, 32, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 2, 16, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 4, 8, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 8, 4, __float32_t, 64, 1, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WRED_FLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_FLOAT_INT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, __float16_t, int16_t) \
+  MACRO (16, 2, 8, __float16_t, int16_t) \
+  MACRO (16, 4, 4, __float16_t, int16_t) \
+  MACRO (16, 8, 2, __float16_t, int16_t) \
+  MACRO (32, 1, 32, __float32_t, int32_t) \
+  MACRO (32, 2, 16, __float32_t, int32_t) \
+  MACRO (32, 4, 8, __float32_t, int32_t) \
+  MACRO (32, 8, 4, __float32_t, int32_t) \
+  MACRO (64, 1, 64, __float64_t, int64_t) \
+  MACRO (64, 2, 32, __float64_t, int64_t) \
+  MACRO (64, 4, 16, __float64_t, int64_t) \
+  MACRO (64, 8, 8, __float64_t, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, int64_t, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding integer and vector type.  */
+#define _RVV_INT_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 8, int8_t, 64, 8) \
+  MACRO (8, 2, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 4, int8_t, 32, 8) \
+  MACRO (8, 4, 2, int8_t, 8, 4) \
+  MACRO (8, 4, 2, int8_t, 16, 8) \
+  MACRO (8, 8, 1, int8_t, 8, 8) \
+  MACRO (16, 1, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 16, int16_t, 64, 4) \
+  MACRO (16, 2, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 8, int16_t, 64, 8) \
+  MACRO (16, 4, 4, int16_t, 8, 2) \
+  MACRO (16, 4, 4, int16_t, 16, 4) \
+  MACRO (16, 4, 4, int16_t, 32, 8) \
+  MACRO (16, 8, 2, int16_t, 8, 4) \
+  MACRO (16, 8, 2, int16_t, 16, 8) \
+  MACRO (32, 1, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 32, int32_t, 64, 2) \
+  MACRO (32, 2, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 16, int32_t, 64, 4) \
+  MACRO (32, 4, 8, int32_t, 8, 1) \
+  MACRO (32, 4, 8, int32_t, 16, 2) \
+  MACRO (32, 4, 8, int32_t, 32, 4) \
+  MACRO (32, 4, 8, int32_t, 64, 8) \
+  MACRO (32, 8, 4, int32_t, 8, 2) \
+  MACRO (32, 8, 4, int32_t, 16, 4) \
+  MACRO (32, 8, 4, int32_t, 32, 8) \
+  MACRO (64, 1, 64, int64_t, 64, 1) \
+  MACRO (64, 2, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 32, int64_t, 64, 2) \
+  MACRO (64, 4, 16, int64_t, 16, 1) \
+  MACRO (64, 4, 16, int64_t, 32, 2) \
+  MACRO (64, 4, 16, int64_t, 64, 4) \
+  MACRO (64, 8, 8, int64_t, 8, 1) \
+  MACRO (64, 8, 8, int64_t, 16, 2) \
+  MACRO (64, 8, 8, int64_t, 32, 4) \
+  MACRO (64, 8, 8, int64_t, 64, 8) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 8, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 8, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 16, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 8, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point types, and info for
+   corresponding floating point and vector type.  */
+#define _RVV_FLOAT_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 16, __float16_t, 64, 4) \
+  MACRO (16, 2, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 8, __float16_t, 64, 8) \
+  MACRO (16, 4, 4, __float16_t, 8, 2) \
+  MACRO (16, 4, 4, __float16_t, 16, 4) \
+  MACRO (16, 4, 4, __float16_t, 32, 8) \
+  MACRO (16, 8, 2, __float16_t, 8, 4) \
+  MACRO (16, 8, 2, __float16_t, 16, 8) \
+  MACRO (32, 1, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 32, __float32_t, 64, 2) \
+  MACRO (32, 2, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 16, __float32_t, 64, 4) \
+  MACRO (32, 4, 8, __float32_t, 8, 1) \
+  MACRO (32, 4, 8, __float32_t, 16, 2) \
+  MACRO (32, 4, 8, __float32_t, 32, 4) \
+  MACRO (32, 4, 8, __float32_t, 64, 8) \
+  MACRO (32, 8, 4, __float32_t, 8, 2) \
+  MACRO (32, 8, 4, __float32_t, 16, 4) \
+  MACRO (32, 8, 4, __float32_t, 32, 8) \
+  MACRO (64, 1, 64, __float64_t, 64, 1) \
+  MACRO (64, 2, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 32, __float64_t, 64, 2) \
+  MACRO (64, 4, 16, __float64_t, 16, 1) \
+  MACRO (64, 4, 16, __float64_t, 32, 2) \
+  MACRO (64, 4, 16, __float64_t, 64, 4) \
+  MACRO (64, 8, 8, __float64_t, 8, 1) \
+  MACRO (64, 8, 8, __float64_t, 16, 2) \
+  MACRO (64, 8, 8, __float64_t, 32, 4) \
+  MACRO (64, 8, 8, __float64_t, 64, 8) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 8, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 8, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 16, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 8, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding integer and vector type.  */
+#define _RVV_INT_REINT_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, int8_t, 16, 1) \
+  MACRO (8, 1, 8, int8_t, 32, 1) \
+  MACRO (8, 1, 8, int8_t, 64, 1) \
+  MACRO (8, 1, 8, int8_t, 8, 2) \
+  MACRO (8, 1, 8, int8_t, 8, 4) \
+  MACRO (8, 1, 8, int8_t, 8, 8) \
+  MACRO (8, 2, 4, int8_t, 16, 2) \
+  MACRO (8, 2, 4, int8_t, 32, 2) \
+  MACRO (8, 2, 4, int8_t, 64, 2) \
+  MACRO (8, 2, 4, int8_t, 8, 1) \
+  MACRO (8, 2, 4, int8_t, 8, 4) \
+  MACRO (8, 2, 4, int8_t, 8, 8) \
+  MACRO (8, 4, 2, int8_t, 16, 4) \
+  MACRO (8, 4, 2, int8_t, 32, 4) \
+  MACRO (8, 4, 2, int8_t, 64, 4) \
+  MACRO (8, 4, 2, int8_t, 8, 1) \
+  MACRO (8, 4, 2, int8_t, 8, 2) \
+  MACRO (8, 4, 2, int8_t, 8, 8) \
+  MACRO (8, 8, 1, int8_t, 16, 8) \
+  MACRO (8, 8, 1, int8_t, 32, 8) \
+  MACRO (8, 8, 1, int8_t, 64, 8) \
+  MACRO (8, 8, 1, int8_t, 8, 1) \
+  MACRO (8, 8, 1, int8_t, 8, 2) \
+  MACRO (8, 8, 1, int8_t, 8, 4) \
+  MACRO (16, 1, 16, int16_t, 8, 1) \
+  MACRO (16, 1, 16, int16_t, 32, 1) \
+  MACRO (16, 1, 16, int16_t, 64, 1) \
+  MACRO (16, 1, 16, int16_t, 16, 2) \
+  MACRO (16, 1, 16, int16_t, 16, 4) \
+  MACRO (16, 1, 16, int16_t, 16, 8) \
+  MACRO (16, 2, 8, int16_t, 8, 2) \
+  MACRO (16, 2, 8, int16_t, 32, 2) \
+  MACRO (16, 2, 8, int16_t, 64, 2) \
+  MACRO (16, 2, 8, int16_t, 16, 1) \
+  MACRO (16, 2, 8, int16_t, 16, 4) \
+  MACRO (16, 2, 8, int16_t, 16, 8) \
+  MACRO (16, 4, 4, int16_t, 8, 4) \
+  MACRO (16, 4, 4, int16_t, 32, 4) \
+  MACRO (16, 4, 4, int16_t, 64, 4) \
+  MACRO (16, 4, 4, int16_t, 16, 1) \
+  MACRO (16, 4, 4, int16_t, 16, 2) \
+  MACRO (16, 4, 4, int16_t, 16, 8) \
+  MACRO (16, 8, 2, int16_t, 8, 8) \
+  MACRO (16, 8, 2, int16_t, 32, 8) \
+  MACRO (16, 8, 2, int16_t, 64, 8) \
+  MACRO (16, 8, 2, int16_t, 16, 1) \
+  MACRO (16, 8, 2, int16_t, 16, 2) \
+  MACRO (16, 8, 2, int16_t, 16, 4) \
+  MACRO (32, 1, 32, int32_t, 8, 1) \
+  MACRO (32, 1, 32, int32_t, 16, 1) \
+  MACRO (32, 1, 32, int32_t, 64, 1) \
+  MACRO (32, 1, 32, int32_t, 32, 2) \
+  MACRO (32, 1, 32, int32_t, 32, 4) \
+  MACRO (32, 1, 32, int32_t, 32, 8) \
+  MACRO (32, 2, 16, int32_t, 8, 2) \
+  MACRO (32, 2, 16, int32_t, 16, 2) \
+  MACRO (32, 2, 16, int32_t, 64, 2) \
+  MACRO (32, 2, 16, int32_t, 32, 1) \
+  MACRO (32, 2, 16, int32_t, 32, 4) \
+  MACRO (32, 2, 16, int32_t, 32, 8) \
+  MACRO (32, 4, 8, int32_t, 8, 4) \
+  MACRO (32, 4, 8, int32_t, 16, 4) \
+  MACRO (32, 4, 8, int32_t, 64, 4) \
+  MACRO (32, 4, 8, int32_t, 32, 1) \
+  MACRO (32, 4, 8, int32_t, 32, 2) \
+  MACRO (32, 4, 8, int32_t, 32, 8) \
+  MACRO (32, 8, 4, int32_t, 8, 8) \
+  MACRO (32, 8, 4, int32_t, 16, 8) \
+  MACRO (32, 8, 4, int32_t, 64, 8) \
+  MACRO (32, 8, 4, int32_t, 32, 1) \
+  MACRO (32, 8, 4, int32_t, 32, 2) \
+  MACRO (32, 8, 4, int32_t, 32, 4) \
+  MACRO (64, 1, 64, int64_t, 8, 1) \
+  MACRO (64, 1, 64, int64_t, 16, 1) \
+  MACRO (64, 1, 64, int64_t, 32, 1) \
+  MACRO (64, 1, 64, int64_t, 64, 2) \
+  MACRO (64, 1, 64, int64_t, 64, 4) \
+  MACRO (64, 1, 64, int64_t, 64, 8) \
+  MACRO (64, 2, 32, int64_t, 8, 2) \
+  MACRO (64, 2, 32, int64_t, 16, 2) \
+  MACRO (64, 2, 32, int64_t, 32, 2) \
+  MACRO (64, 2, 32, int64_t, 64, 1) \
+  MACRO (64, 2, 32, int64_t, 64, 4) \
+  MACRO (64, 2, 32, int64_t, 64, 8) \
+  MACRO (64, 4, 16, int64_t, 8, 4) \
+  MACRO (64, 4, 16, int64_t, 16, 4) \
+  MACRO (64, 4, 16, int64_t, 32, 4) \
+  MACRO (64, 4, 16, int64_t, 64, 1) \
+  MACRO (64, 4, 16, int64_t, 64, 2) \
+  MACRO (64, 4, 16, int64_t, 64, 8) \
+  MACRO (64, 8, 8, int64_t, 8, 8) \
+  MACRO (64, 8, 8, int64_t, 16, 8) \
+  MACRO (64, 8, 8, int64_t, 32, 8) \
+  MACRO (64, 8, 8, int64_t, 64, 1) \
+  MACRO (64, 8, 8, int64_t, 64, 2) \
+  MACRO (64, 8, 8, int64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_REINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, int8_t, 16, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 32, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 64, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 32, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 64, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 64, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 16, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 64, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 64, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 8, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 8, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 8, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 8, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 16, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 8, 1, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 8, 2, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 16, 2, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 8, 4, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 16, 4, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 8, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 16, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 32, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point types, and info for
+   corresponding floating and vector type.  */
+#define _RVV_FLOAT_REINT_ITERATOR(MACRO) \
+  MACRO (16, 1, 16, __float16_t, 16, 2) \
+  MACRO (16, 1, 16, __float16_t, 16, 4) \
+  MACRO (16, 1, 16, __float16_t, 16, 8) \
+  MACRO (16, 2, 8, __float16_t, 16, 1) \
+  MACRO (16, 2, 8, __float16_t, 16, 4) \
+  MACRO (16, 2, 8, __float16_t, 16, 8) \
+  MACRO (16, 4, 4, __float16_t, 16, 1) \
+  MACRO (16, 4, 4, __float16_t, 16, 2) \
+  MACRO (16, 4, 4, __float16_t, 16, 8) \
+  MACRO (16, 8, 2, __float16_t, 16, 1) \
+  MACRO (16, 8, 2, __float16_t, 16, 2) \
+  MACRO (16, 8, 2, __float16_t, 16, 4) \
+  MACRO (32, 1, 32, __float32_t, 32, 2) \
+  MACRO (32, 1, 32, __float32_t, 32, 4) \
+  MACRO (32, 1, 32, __float32_t, 32, 8) \
+  MACRO (32, 2, 16, __float32_t, 32, 1) \
+  MACRO (32, 2, 16, __float32_t, 32, 4) \
+  MACRO (32, 2, 16, __float32_t, 32, 8) \
+  MACRO (32, 4, 8, __float32_t, 32, 1) \
+  MACRO (32, 4, 8, __float32_t, 32, 2) \
+  MACRO (32, 4, 8, __float32_t, 32, 8) \
+  MACRO (32, 8, 4, __float32_t, 32, 1) \
+  MACRO (32, 8, 4, __float32_t, 32, 2) \
+  MACRO (32, 8, 4, __float32_t, 32, 4) \
+  MACRO (64, 1, 64, __float64_t, 64, 2) \
+  MACRO (64, 1, 64, __float64_t, 64, 4) \
+  MACRO (64, 1, 64, __float64_t, 64, 8) \
+  MACRO (64, 2, 32, __float64_t, 64, 1) \
+  MACRO (64, 2, 32, __float64_t, 64, 4) \
+  MACRO (64, 2, 32, __float64_t, 64, 8) \
+  MACRO (64, 4, 16, __float64_t, 64, 1) \
+  MACRO (64, 4, 16, __float64_t, 64, 2) \
+  MACRO (64, 4, 16, __float64_t, 64, 8) \
+  MACRO (64, 8, 8, __float64_t, 64, 1) \
+  MACRO (64, 8, 8, __float64_t, 64, 2) \
+  MACRO (64, 8, 8, __float64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_REINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 16, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, int8_t) \
+  MACRO (8, 1, 3, 8, int8_t) \
+  MACRO (8, 1, 4, 8, int8_t) \
+  MACRO (8, 1, 5, 8, int8_t) \
+  MACRO (8, 1, 6, 8, int8_t) \
+  MACRO (8, 1, 7, 8, int8_t) \
+  MACRO (8, 1, 8, 8, int8_t) \
+  MACRO (8, 2, 2, 4, int8_t) \
+  MACRO (8, 2, 3, 4, int8_t) \
+  MACRO (8, 2, 4, 4, int8_t) \
+  MACRO (8, 4, 2, 2, int8_t) \
+  MACRO (16, 1, 2, 16, int16_t) \
+  MACRO (16, 1, 3, 16, int16_t) \
+  MACRO (16, 1, 4, 16, int16_t) \
+  MACRO (16, 1, 5, 16, int16_t) \
+  MACRO (16, 1, 6, 16, int16_t) \
+  MACRO (16, 1, 7, 16, int16_t) \
+  MACRO (16, 1, 8, 16, int16_t) \
+  MACRO (16, 2, 2, 8, int16_t) \
+  MACRO (16, 2, 3, 8, int16_t) \
+  MACRO (16, 2, 4, 8, int16_t) \
+  MACRO (16, 4, 2, 4, int16_t) \
+  MACRO (32, 1, 2, 32, int32_t) \
+  MACRO (32, 1, 3, 32, int32_t) \
+  MACRO (32, 1, 4, 32, int32_t) \
+  MACRO (32, 1, 5, 32, int32_t) \
+  MACRO (32, 1, 6, 32, int32_t) \
+  MACRO (32, 1, 7, 32, int32_t) \
+  MACRO (32, 1, 8, 32, int32_t) \
+  MACRO (32, 2, 2, 16, int32_t) \
+  MACRO (32, 2, 3, 16, int32_t) \
+  MACRO (32, 2, 4, 16, int32_t) \
+  MACRO (32, 4, 2, 8, int32_t) \
+  MACRO (64, 1, 2, 64, int64_t) \
+  MACRO (64, 1, 3, 64, int64_t) \
+  MACRO (64, 1, 4, 64, int64_t) \
+  MACRO (64, 1, 5, 64, int64_t) \
+  MACRO (64, 1, 6, 64, int64_t) \
+  MACRO (64, 1, 7, 64, int64_t) \
+  MACRO (64, 1, 8, 64, int64_t) \
+  MACRO (64, 2, 2, 32, int64_t) \
+  MACRO (64, 2, 3, 32, int64_t) \
+  MACRO (64, 2, 4, 32, int64_t) \
+  MACRO (64, 4, 2, 16, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 2, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 2, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 2, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 3, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 3, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 3, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 3, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 4, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 4, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 4, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 4, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 5, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 5, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 5, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 5, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 6, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 6, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 6, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 6, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 7, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 7, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 7, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 7, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 8, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 8, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 8, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 8, 8, int8_t, 64, 8) \
+  MACRO (8, 2, 2, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 2, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 2, 4, int8_t, 32, 8) \
+  MACRO (8, 2, 3, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 3, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 3, 4, int8_t, 32, 8) \
+  MACRO (8, 2, 4, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 4, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 4, 4, int8_t, 32, 8) \
+  MACRO (8, 4, 2, 2, int8_t, 8, 4) \
+  MACRO (8, 4, 2, 2, int8_t, 16, 8) \
+  MACRO (16, 1, 2, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 2, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 2, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 3, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 3, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 3, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 4, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 4, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 4, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 5, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 5, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 5, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 6, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 6, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 6, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 7, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 7, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 7, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 8, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 8, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 8, 16, int16_t, 64, 4) \
+  MACRO (16, 2, 2, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 2, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 2, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 2, 8, int16_t, 64, 8) \
+  MACRO (16, 2, 3, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 3, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 3, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 3, 8, int16_t, 64, 8) \
+  MACRO (16, 2, 4, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 4, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 4, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 4, 8, int16_t, 64, 8) \
+  MACRO (16, 4, 2, 4, int16_t, 8, 2) \
+  MACRO (16, 4, 2, 4, int16_t, 16, 4) \
+  MACRO (16, 4, 2, 4, int16_t, 32, 8) \
+  MACRO (32, 1, 2, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 2, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 3, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 3, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 4, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 4, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 5, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 5, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 6, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 6, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 7, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 7, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 8, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 8, 32, int32_t, 64, 2) \
+  MACRO (32, 2, 2, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 2, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 2, 16, int32_t, 64, 4) \
+  MACRO (32, 2, 3, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 3, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 3, 16, int32_t, 64, 4) \
+  MACRO (32, 2, 4, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 4, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 4, 16, int32_t, 64, 4) \
+  MACRO (32, 4, 2, 8, int32_t, 8, 1) \
+  MACRO (32, 4, 2, 8, int32_t, 16, 2) \
+  MACRO (32, 4, 2, 8, int32_t, 32, 4) \
+  MACRO (32, 4, 2, 8, int32_t, 64, 8) \
+  MACRO (64, 1, 2, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 3, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 4, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 5, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 6, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 7, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 8, 64, int64_t, 64, 1) \
+  MACRO (64, 2, 2, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 2, 32, int64_t, 64, 2) \
+  MACRO (64, 2, 3, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 3, 32, int64_t, 64, 2) \
+  MACRO (64, 2, 4, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 4, 32, int64_t, 64, 2) \
+  MACRO (64, 4, 2, 16, int64_t, 16, 1) \
+  MACRO (64, 4, 2, 16, int64_t, 32, 2) \
+  MACRO (64, 4, 2, 16, int64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF2_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, int8_t) \
+  MACRO (8, 2, 2, 4, int8_t) \
+  MACRO (8, 4, 2, 2, int8_t) \
+  MACRO (16, 1, 2, 16, int16_t) \
+  MACRO (16, 2, 2, 8, int16_t) \
+  MACRO (16, 4, 2, 4, int16_t) \
+  MACRO (32, 1, 2, 32, int32_t) \
+  MACRO (32, 2, 2, 16, int32_t) \
+  MACRO (32, 4, 2, 8, int32_t) \
+  MACRO (64, 1, 2, 64, int64_t) \
+  MACRO (64, 2, 2, 32, int64_t) \
+  MACRO (64, 4, 2, 16, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF2_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF3_ITERATOR(MACRO) \
+  MACRO (8, 1, 3, 8, int8_t) \
+  MACRO (8, 2, 3, 4, int8_t) \
+  MACRO (16, 1, 3, 16, int16_t) \
+  MACRO (16, 2, 3, 8, int16_t) \
+  MACRO (32, 1, 3, 32, int32_t) \
+  MACRO (32, 2, 3, 16, int32_t) \
+  MACRO (64, 1, 3, 64, int64_t) \
+  MACRO (64, 2, 3, 32, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF3_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 3, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF4_ITERATOR(MACRO) \
+  MACRO (8, 1, 4, 8, int8_t) \
+  MACRO (8, 2, 4, 4, int8_t) \
+  MACRO (16, 1, 4, 16, int16_t) \
+  MACRO (16, 2, 4, 8, int16_t) \
+  MACRO (32, 1, 4, 32, int32_t) \
+  MACRO (32, 2, 4, 16, int32_t) \
+  MACRO (64, 1, 4, 64, int64_t) \
+  MACRO (64, 2, 4, 32, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF4_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 4, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF5_ITERATOR(MACRO) \
+  MACRO (8, 1, 5, 8, int8_t) \
+  MACRO (16, 1, 5, 16, int16_t) \
+  MACRO (32, 1, 5, 32, int32_t) \
+  MACRO (64, 1, 5, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF5_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 5, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF6_ITERATOR(MACRO) \
+  MACRO (8, 1, 6, 8, int8_t) \
+  MACRO (16, 1, 6, 16, int16_t) \
+  MACRO (32, 1, 6, 32, int32_t) \
+  MACRO (64, 1, 6, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF6_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 6, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF7_ITERATOR(MACRO) \
+  MACRO (8, 1, 7, 8, int8_t) \
+  MACRO (16, 1, 7, 16, int16_t) \
+  MACRO (32, 1, 7, 32, int32_t) \
+  MACRO (64, 1, 7, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF7_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 7, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF8_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, 8, int8_t) \
+  MACRO (16, 1, 8, 16, int16_t) \
+  MACRO (32, 1, 8, 32, int32_t) \
+  MACRO (64, 1, 8, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF8_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, __float16_t) \
+  MACRO (16, 1, 3, 16, __float16_t) \
+  MACRO (16, 1, 4, 16, __float16_t) \
+  MACRO (16, 1, 5, 16, __float16_t) \
+  MACRO (16, 1, 6, 16, __float16_t) \
+  MACRO (16, 1, 7, 16, __float16_t) \
+  MACRO (16, 1, 8, 16, __float16_t) \
+  MACRO (16, 2, 2, 8, __float16_t) \
+  MACRO (16, 2, 3, 8, __float16_t) \
+  MACRO (16, 2, 4, 8, __float16_t) \
+  MACRO (16, 4, 2, 4, __float16_t) \
+  MACRO (32, 1, 2, 32, __float32_t) \
+  MACRO (32, 1, 3, 32, __float32_t) \
+  MACRO (32, 1, 4, 32, __float32_t) \
+  MACRO (32, 1, 5, 32, __float32_t) \
+  MACRO (32, 1, 6, 32, __float32_t) \
+  MACRO (32, 1, 7, 32, __float32_t) \
+  MACRO (32, 1, 8, 32, __float32_t) \
+  MACRO (32, 2, 2, 16, __float32_t) \
+  MACRO (32, 2, 3, 16, __float32_t) \
+  MACRO (32, 2, 4, 16, __float32_t) \
+  MACRO (32, 4, 2, 8, __float32_t) \
+  MACRO (64, 1, 2, 64, __float64_t) \
+  MACRO (64, 1, 3, 64, __float64_t) \
+  MACRO (64, 1, 4, 64, __float64_t) \
+  MACRO (64, 1, 5, 64, __float64_t) \
+  MACRO (64, 1, 6, 64, __float64_t) \
+  MACRO (64, 1, 7, 64, __float64_t) \
+  MACRO (64, 1, 8, 64, __float64_t) \
+  MACRO (64, 2, 2, 32, __float64_t) \
+  MACRO (64, 2, 3, 32, __float64_t) \
+  MACRO (64, 2, 4, 32, __float64_t) \
+  MACRO (64, 4, 2, 16, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 2, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 2, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 3, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 3, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 3, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 4, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 4, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 4, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 5, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 5, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 5, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 6, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 6, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 6, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 7, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 7, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 7, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 8, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 8, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 8, 16, __float16_t, 64, 4) \
+  MACRO (16, 2, 2, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 2, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 2, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 2, 8, __float16_t, 64, 8) \
+  MACRO (16, 2, 3, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 3, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 3, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 3, 8, __float16_t, 64, 8) \
+  MACRO (16, 2, 4, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 4, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 4, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 4, 8, __float16_t, 64, 8) \
+  MACRO (16, 4, 2, 4, __float16_t, 8, 2) \
+  MACRO (16, 4, 2, 4, __float16_t, 16, 4) \
+  MACRO (16, 4, 2, 4, __float16_t, 32, 8) \
+  MACRO (32, 1, 2, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 2, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 3, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 3, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 4, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 4, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 5, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 5, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 6, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 6, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 7, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 7, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 8, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 8, 32, __float32_t, 64, 2) \
+  MACRO (32, 2, 2, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 2, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 2, 16, __float32_t, 64, 4) \
+  MACRO (32, 2, 3, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 3, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 3, 16, __float32_t, 64, 4) \
+  MACRO (32, 2, 4, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 4, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 4, 16, __float32_t, 64, 4) \
+  MACRO (32, 4, 2, 8, __float32_t, 8, 1) \
+  MACRO (32, 4, 2, 8, __float32_t, 16, 2) \
+  MACRO (32, 4, 2, 8, __float32_t, 32, 4) \
+  MACRO (32, 4, 2, 8, __float32_t, 64, 8) \
+  MACRO (64, 1, 2, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 3, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 4, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 5, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 6, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 7, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 8, 64, __float64_t, 64, 1) \
+  MACRO (64, 2, 2, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 2, 32, __float64_t, 64, 2) \
+  MACRO (64, 2, 3, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 3, 32, __float64_t, 64, 2) \
+  MACRO (64, 2, 4, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 4, 32, __float64_t, 64, 2) \
+  MACRO (64, 4, 2, 16, __float64_t, 16, 1) \
+  MACRO (64, 4, 2, 16, __float64_t, 32, 2) \
+  MACRO (64, 4, 2, 16, __float64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF2_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, __float16_t) \
+  MACRO (16, 2, 2, 8, __float16_t) \
+  MACRO (16, 4, 2, 4, __float16_t) \
+  MACRO (32, 1, 2, 32, __float32_t) \
+  MACRO (32, 2, 2, 16, __float32_t) \
+  MACRO (32, 4, 2, 8, __float32_t) \
+  MACRO (64, 1, 2, 64, __float64_t) \
+  MACRO (64, 2, 2, 32, __float64_t) \
+  MACRO (64, 4, 2, 16, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF2_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF3_ITERATOR(MACRO) \
+  MACRO (16, 1, 3, 16, __float16_t) \
+  MACRO (16, 2, 3, 8, __float16_t) \
+  MACRO (32, 1, 3, 32, __float32_t) \
+  MACRO (32, 2, 3, 16, __float32_t) \
+  MACRO (64, 1, 3, 64, __float64_t) \
+  MACRO (64, 2, 3, 32, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF3_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 3, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF4_ITERATOR(MACRO) \
+  MACRO (16, 1, 4, 16, __float16_t) \
+  MACRO (16, 2, 4, 8, __float16_t) \
+  MACRO (32, 1, 4, 32, __float32_t) \
+  MACRO (32, 2, 4, 16, __float32_t) \
+  MACRO (64, 1, 4, 64, __float64_t) \
+  MACRO (64, 2, 4, 32, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF4_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 4, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF5_ITERATOR(MACRO) \
+  MACRO (16, 1, 5, 16, __float16_t) \
+  MACRO (32, 1, 5, 32, __float32_t) \
+  MACRO (64, 1, 5, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF5_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 5, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF6_ITERATOR(MACRO) \
+  MACRO (16, 1, 6, 16, __float16_t) \
+  MACRO (32, 1, 6, 32, __float32_t) \
+  MACRO (64, 1, 6, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF6_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 6, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF7_ITERATOR(MACRO) \
+  MACRO (16, 1, 7, 16, __float16_t) \
+  MACRO (32, 1, 7, 32, __float32_t) \
+  MACRO (64, 1, 7, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF7_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 7, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF8_ITERATOR(MACRO) \
+  MACRO (16, 1, 8, 16, __float16_t) \
+  MACRO (32, 1, 8, 32, __float32_t) \
+  MACRO (64, 1, 8, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF8_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 8, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, __float64_t, __VA_ARGS__) \
+
diff --git a/gcc/config/riscv/t-riscv b/gcc/config/riscv/t-riscv
index 702767c1736..8fc9cc10c2f 100644
--- a/gcc/config/riscv/t-riscv
+++ b/gcc/config/riscv/t-riscv
@@ -1,7 +1,8 @@
 riscv-builtins.o: $(srcdir)/config/riscv/riscv-builtins.c $(CONFIG_H) \
   $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) $(TREE_H) $(RECOG_H) langhooks.h \
   $(DIAGNOSTIC_CORE_H) $(OPTABS_H) $(srcdir)/config/riscv/riscv-ftypes.def \
-  $(srcdir)/config/riscv/riscv-modes.def
+  $(srcdir)/config/riscv/riscv-modes.def \
+  $(srcdir)/config/riscv/riscv-vector-iterator.h
 	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
 		$(srcdir)/config/riscv/riscv-builtins.c
 
@@ -23,6 +24,8 @@ riscv-shorten-memrefs.o: $(srcdir)/config/riscv/riscv-shorten-memrefs.c
 	$(COMPILE) $<
 	$(POSTCOMPILE)
 
+riscv.o: $(srcdir)/config/riscv/riscv-vector-iterator.h
+
 PASSES_EXTRA += $(srcdir)/config/riscv/riscv-passes.def
 
 $(common_out_file): $(srcdir)/config/riscv/riscv-cores.def
diff --git a/gcc/config/riscv/vector-iterator.md b/gcc/config/riscv/vector-iterator.md
new file mode 100644
index 00000000000..827abb3807a
--- /dev/null
+++ b/gcc/config/riscv/vector-iterator.md
@@ -0,0 +1,700 @@
+;; DO NOT EDIT, please edit generator instead.
+;; This file was generated by gen-vector-iterator with the command:
+;; $ ./gen-vector-iterator -md > vector-iterator.md
+
+;; All vector modes supported.
+(define_mode_iterator VMODES [
+  VNx16QI VNx32QI VNx64QI VNx128QI
+  VNx8HI VNx16HI VNx32HI VNx64HI
+  VNx4SI VNx8SI VNx16SI VNx32SI
+  VNx2DI VNx4DI VNx8DI VNx16DI
+  VNx8HF VNx16HF VNx32HF VNx64HF
+  VNx4SF VNx8SF VNx16SF VNx32SF
+  VNx2DF VNx4DF VNx8DF VNx16DF
+])
+
+;; All vector modes supported for integer load/store/alu.
+(define_mode_iterator VIMODES [
+  VNx16QI VNx32QI VNx64QI VNx128QI
+  VNx8HI VNx16HI VNx32HI VNx64HI
+  VNx4SI VNx8SI VNx16SI VNx32SI
+  VNx2DI VNx4DI VNx8DI VNx16DI
+])
+
+;; Same as VIMODES, used for combination.
+(define_mode_iterator VIMODES2 [
+  VNx16QI VNx32QI VNx64QI VNx128QI
+  VNx8HI VNx16HI VNx32HI VNx64HI
+  VNx4SI VNx8SI VNx16SI VNx32SI
+  VNx2DI VNx4DI VNx8DI VNx16DI
+])
+
+;; All vector modes supported for widening integer alu.
+(define_mode_iterator VWIMODES [
+  VNx16QI VNx32QI VNx64QI VNx8HI
+  VNx16HI VNx32HI VNx4SI VNx8SI
+  VNx16SI])
+
+;; All vector modes supported for widening integer point reduction operation.
+(define_mode_iterator VWRED_IMODES [
+  VNx16QI VNx32QI VNx64QI VNx128QI
+  VNx8HI VNx16HI VNx32HI VNx64HI
+  VNx4SI VNx8SI VNx16SI VNx32SI
+])
+
+;; All vector modes supported for FP type-convert.
+(define_mode_iterator FCVT_VWIMODES [
+  VNx8HI VNx16HI VNx32HI VNx4SI
+  VNx8SI VNx16SI])
+
+;; All vector modes supported for quad-widening integer alu.
+(define_mode_iterator VQWIMODES [
+  VNx16QI VNx32QI VNx8HI VNx16HI
+])
+
+;; All vector modes supported for FP load/store/alu.
+(define_mode_iterator VFMODES [
+  VNx8HF VNx16HF VNx32HF VNx64HF
+  VNx4SF VNx8SF VNx16SF VNx32SF
+  VNx2DF VNx4DF VNx8DF VNx16DF
+])
+
+;; Same as VFMODES, used for combination.
+(define_mode_iterator VFMODES2 [
+  VNx8HF VNx16HF VNx32HF VNx64HF
+  VNx4SF VNx8SF VNx16SF VNx32SF
+  VNx2DF VNx4DF VNx8DF VNx16DF
+])
+
+;; All vector modes supported for widening floating point alu.
+(define_mode_iterator VWFMODES [
+  VNx8HF VNx16HF VNx32HF VNx4SF
+  VNx8SF VNx16SF])
+
+;; All vector modes supported for widening floating point reduction operation.
+(define_mode_iterator VWRED_FMODES [
+  VNx8HF VNx16HF VNx32HF VNx64HF
+  VNx4SF VNx8SF VNx16SF VNx32SF
+])
+
+;; All vector tuple modes supported.
+(define_mode_iterator VTMODES [
+  VNx2x16QI VNx3x16QI VNx4x16QI VNx5x16QI
+  VNx6x16QI VNx7x16QI VNx8x16QI VNx2x32QI
+  VNx3x32QI VNx4x32QI VNx2x64QI VNx2x8HI
+  VNx3x8HI VNx4x8HI VNx5x8HI VNx6x8HI
+  VNx7x8HI VNx8x8HI VNx2x16HI VNx3x16HI
+  VNx4x16HI VNx2x32HI VNx2x4SI VNx3x4SI
+  VNx4x4SI VNx5x4SI VNx6x4SI VNx7x4SI
+  VNx8x4SI VNx2x8SI VNx3x8SI VNx4x8SI
+  VNx2x16SI VNx2x2DI VNx3x2DI VNx4x2DI
+  VNx5x2DI VNx6x2DI VNx7x2DI VNx8x2DI
+  VNx2x4DI VNx3x4DI VNx4x4DI VNx2x8DI
+  VNx2x8HF VNx3x8HF VNx4x8HF VNx5x8HF
+  VNx6x8HF VNx7x8HF VNx8x8HF VNx2x16HF
+  VNx3x16HF VNx4x16HF VNx2x32HF VNx2x4SF
+  VNx3x4SF VNx4x4SF VNx5x4SF VNx6x4SF
+  VNx7x4SF VNx8x4SF VNx2x8SF VNx3x8SF
+  VNx4x8SF VNx2x16SF VNx2x2DF VNx3x2DF
+  VNx4x2DF VNx5x2DF VNx6x2DF VNx7x2DF
+  VNx8x2DF VNx2x4DF VNx3x4DF VNx4x4DF
+  VNx2x8DF])
+
+;; All vector masking modes.
+(define_mode_iterator VMASKMODES [
+  VNx2BI VNx4BI VNx8BI VNx16BI
+  VNx32BI VNx64BI VNx128BI])
+
+;; All vector tuple modes with NF=2.
+(define_mode_iterator VTNF2MODES [
+  VNx2x16QI VNx2x32QI VNx2x64QI VNx2x8HI
+  VNx2x16HI VNx2x32HI VNx2x4SI VNx2x8SI
+  VNx2x16SI VNx2x2DI VNx2x4DI VNx2x8DI
+  VNx2x8HF VNx2x16HF VNx2x32HF VNx2x4SF
+  VNx2x8SF VNx2x16SF VNx2x2DF VNx2x4DF
+  VNx2x8DF])
+
+;; All vector tuple modes with NF=3.
+(define_mode_iterator VTNF3MODES [
+  VNx3x16QI VNx3x32QI VNx3x8HI VNx3x16HI
+  VNx3x4SI VNx3x8SI VNx3x2DI VNx3x4DI
+  VNx3x8HF VNx3x16HF VNx3x4SF VNx3x8SF
+  VNx3x2DF VNx3x4DF])
+
+;; All vector tuple modes with NF=4.
+(define_mode_iterator VTNF4MODES [
+  VNx4x16QI VNx4x32QI VNx4x8HI VNx4x16HI
+  VNx4x4SI VNx4x8SI VNx4x2DI VNx4x4DI
+  VNx4x8HF VNx4x16HF VNx4x4SF VNx4x8SF
+  VNx4x2DF VNx4x4DF])
+
+;; All vector tuple modes with NF=5.
+(define_mode_iterator VTNF5MODES [
+  VNx5x16QI VNx5x8HI VNx5x4SI VNx5x2DI
+  VNx5x8HF VNx5x4SF VNx5x2DF])
+
+;; All vector tuple modes with NF=6.
+(define_mode_iterator VTNF6MODES [
+  VNx6x16QI VNx6x8HI VNx6x4SI VNx6x2DI
+  VNx6x8HF VNx6x4SF VNx6x2DF])
+
+;; All vector tuple modes with NF=7.
+(define_mode_iterator VTNF7MODES [
+  VNx7x16QI VNx7x8HI VNx7x4SI VNx7x2DI
+  VNx7x8HF VNx7x4SF VNx7x2DF])
+
+;; All vector tuple modes with NF=8.
+(define_mode_iterator VTNF8MODES [
+  VNx8x16QI VNx8x8HI VNx8x4SI VNx8x2DI
+  VNx8x8HF VNx8x4SF VNx8x2DF])
+
+;; Map a vector float mode to a vector int mode of the same size.
+(define_mode_attr VINTEQUIV [
+  (VNx8HF "VNx8HI") (VNx16HF "VNx16HI") (VNx32HF "VNx32HI") (VNx64HF "VNx64HI")
+  (VNx4SF "VNx4SI") (VNx8SF "VNx8SI") (VNx16SF "VNx16SI") (VNx32SF "VNx32SI")
+  (VNx2DF "VNx2DI") (VNx4DF "VNx4DI") (VNx8DF "VNx8DI") (VNx16DF "VNx16DI")
+])
+
+;; Map a vector float mode to a vector int mode of the same size.
+(define_mode_attr vintequiv [
+  (VNx8HF "vnx8hi") (VNx16HF "vnx16hi") (VNx32HF "vnx32hi") (VNx64HF "vnx64hi")
+  (VNx4SF "vnx4si") (VNx8SF "vnx8si") (VNx16SF "vnx16si") (VNx32SF "vnx32si")
+  (VNx2DF "vnx2di") (VNx4DF "vnx4di") (VNx8DF "vnx8di") (VNx16DF "vnx16di")
+])
+
+;; Map a vector int or float mode to widening vector mode.
+(define_mode_attr VWMODE [
+  (VNx16QI "VNx16HI") (VNx32QI "VNx32HI") (VNx64QI "VNx64HI") (VNx8HI "VNx8SI")
+  (VNx16HI "VNx16SI") (VNx32HI "VNx32SI") (VNx4SI "VNx4DI") (VNx8SI "VNx8DI")
+  (VNx16SI "VNx16DI") (VNx8HF "VNx8SF") (VNx16HF "VNx16SF") (VNx32HF "VNx32SF")
+  (VNx4SF "VNx4DF") (VNx8SF "VNx8DF") (VNx16SF "VNx16DF")])
+
+;; Map a vector int or float mode to widening vector mode.
+(define_mode_attr vwmode [
+  (VNx16QI "vnx16hi") (VNx32QI "vnx32hi") (VNx64QI "vnx64hi") (VNx8HI "vnx8si")
+  (VNx16HI "vnx16si") (VNx32HI "vnx32si") (VNx4SI "vnx4di") (VNx8SI "vnx8di")
+  (VNx16SI "vnx16di") (VNx8HF "vnx8sf") (VNx16HF "vnx16sf") (VNx32HF "vnx32sf")
+  (VNx4SF "vnx4df") (VNx8SF "vnx8df") (VNx16SF "vnx16df")])
+
+;; Map a vector int or float mode to quad-widening vector mode.
+(define_mode_attr VQWMODE [
+  (VNx16QI "VNx16SI") (VNx32QI "VNx32SI") (VNx8HI "VNx8DI") (VNx16HI "VNx16DI")
+  (VNx8HF "VNx8DF") (VNx16HF "VNx16DF")])
+
+;; Map a vector int or float mode to quad-widening vector mode.
+(define_mode_attr vqwmode [
+  (VNx16QI "vnx16si") (VNx32QI "vnx32si") (VNx8HI "vnx8di") (VNx16HI "vnx16di")
+  (VNx8HF "vnx8df") (VNx16HF "vnx16df")])
+
+;; Map a vector int or float mode to widening vector mode.
+(define_mode_attr EXT_VIMODES [
+  (VNx16QI "VNx16HI") (VNx32QI "VNx32HI") (VNx64QI "VNx64HI") (VNx128QI "VNx128HI")
+  (VNx8HI "VNx8SI") (VNx16HI "VNx16SI") (VNx32HI "VNx32SI") (VNx64HI "VNx64SI")
+  (VNx4SI "VNx4DI") (VNx8SI "VNx8DI") (VNx16SI "VNx16DI") (VNx32SI "VNx32DI")
+  (VNx2DI "VNx2TI") (VNx4DI "VNx4TI") (VNx8DI "VNx8TI") (VNx16DI "VNx16TI")
+])
+
+;; Map a vector int or float mode to a vector compare mode.
+(define_mode_attr VCMPEQUIV [
+  (VNx16QI "VNx16BI") (VNx32QI "VNx32BI") (VNx64QI "VNx64BI") (VNx128QI "VNx128BI")
+  (VNx8HI "VNx8BI") (VNx16HI "VNx16BI") (VNx32HI "VNx32BI") (VNx64HI "VNx64BI")
+  (VNx4SI "VNx4BI") (VNx8SI "VNx8BI") (VNx16SI "VNx16BI") (VNx32SI "VNx32BI")
+  (VNx2DI "VNx2BI") (VNx4DI "VNx4BI") (VNx8DI "VNx8BI") (VNx16DI "VNx16BI")
+  (VNx8HF "VNx8BI") (VNx16HF "VNx16BI") (VNx32HF "VNx32BI") (VNx64HF "VNx64BI")
+  (VNx4SF "VNx4BI") (VNx8SF "VNx8BI") (VNx16SF "VNx16BI") (VNx32SF "VNx32BI")
+  (VNx2DF "VNx2BI") (VNx4DF "VNx4BI") (VNx8DF "VNx8BI") (VNx16DF "VNx16BI")
+  (VNx2x16QI "VNx16BI") (VNx3x16QI "VNx16BI") (VNx4x16QI "VNx16BI") (VNx5x16QI "VNx16BI")
+  (VNx6x16QI "VNx16BI") (VNx7x16QI "VNx16BI") (VNx8x16QI "VNx16BI") (VNx2x32QI "VNx32BI")
+  (VNx3x32QI "VNx32BI") (VNx4x32QI "VNx32BI") (VNx2x64QI "VNx64BI") (VNx2x8HI "VNx8BI")
+  (VNx3x8HI "VNx8BI") (VNx4x8HI "VNx8BI") (VNx5x8HI "VNx8BI") (VNx6x8HI "VNx8BI")
+  (VNx7x8HI "VNx8BI") (VNx8x8HI "VNx8BI") (VNx2x16HI "VNx16BI") (VNx3x16HI "VNx16BI")
+  (VNx4x16HI "VNx16BI") (VNx2x32HI "VNx32BI") (VNx2x4SI "VNx4BI") (VNx3x4SI "VNx4BI")
+  (VNx4x4SI "VNx4BI") (VNx5x4SI "VNx4BI") (VNx6x4SI "VNx4BI") (VNx7x4SI "VNx4BI")
+  (VNx8x4SI "VNx4BI") (VNx2x8SI "VNx8BI") (VNx3x8SI "VNx8BI") (VNx4x8SI "VNx8BI")
+  (VNx2x16SI "VNx16BI") (VNx2x2DI "VNx2BI") (VNx3x2DI "VNx2BI") (VNx4x2DI "VNx2BI")
+  (VNx5x2DI "VNx2BI") (VNx6x2DI "VNx2BI") (VNx7x2DI "VNx2BI") (VNx8x2DI "VNx2BI")
+  (VNx2x4DI "VNx4BI") (VNx3x4DI "VNx4BI") (VNx4x4DI "VNx4BI") (VNx2x8DI "VNx8BI")
+  (VNx2x8HF "VNx8BI") (VNx3x8HF "VNx8BI") (VNx4x8HF "VNx8BI") (VNx5x8HF "VNx8BI")
+  (VNx6x8HF "VNx8BI") (VNx7x8HF "VNx8BI") (VNx8x8HF "VNx8BI") (VNx2x16HF "VNx16BI")
+  (VNx3x16HF "VNx16BI") (VNx4x16HF "VNx16BI") (VNx2x32HF "VNx32BI") (VNx2x4SF "VNx4BI")
+  (VNx3x4SF "VNx4BI") (VNx4x4SF "VNx4BI") (VNx5x4SF "VNx4BI") (VNx6x4SF "VNx4BI")
+  (VNx7x4SF "VNx4BI") (VNx8x4SF "VNx4BI") (VNx2x8SF "VNx8BI") (VNx3x8SF "VNx8BI")
+  (VNx4x8SF "VNx8BI") (VNx2x16SF "VNx16BI") (VNx2x2DF "VNx2BI") (VNx3x2DF "VNx2BI")
+  (VNx4x2DF "VNx2BI") (VNx5x2DF "VNx2BI") (VNx6x2DF "VNx2BI") (VNx7x2DF "VNx2BI")
+  (VNx8x2DF "VNx2BI") (VNx2x4DF "VNx4BI") (VNx3x4DF "VNx4BI") (VNx4x4DF "VNx4BI")
+  (VNx2x8DF "VNx8BI")])
+
+;; Map a vector int or float mode to a vector compare mode.
+(define_mode_attr vmaskmode [
+  (VNx16QI "vnx16bi") (VNx32QI "vnx32bi") (VNx64QI "vnx64bi") (VNx128QI "vnx128bi")
+  (VNx8HI "vnx8bi") (VNx16HI "vnx16bi") (VNx32HI "vnx32bi") (VNx64HI "vnx64bi")
+  (VNx4SI "vnx4bi") (VNx8SI "vnx8bi") (VNx16SI "vnx16bi") (VNx32SI "vnx32bi")
+  (VNx2DI "vnx2bi") (VNx4DI "vnx4bi") (VNx8DI "vnx8bi") (VNx16DI "vnx16bi")
+  (VNx8HF "vnx8bi") (VNx16HF "vnx16bi") (VNx32HF "vnx32bi") (VNx64HF "vnx64bi")
+  (VNx4SF "vnx4bi") (VNx8SF "vnx8bi") (VNx16SF "vnx16bi") (VNx32SF "vnx32bi")
+  (VNx2DF "vnx2bi") (VNx4DF "vnx4bi") (VNx8DF "vnx8bi") (VNx16DF "vnx16bi")
+  (VNx2x16QI "vnx16bi") (VNx3x16QI "vnx16bi") (VNx4x16QI "vnx16bi") (VNx5x16QI "vnx16bi")
+  (VNx6x16QI "vnx16bi") (VNx7x16QI "vnx16bi") (VNx8x16QI "vnx16bi") (VNx2x32QI "vnx32bi")
+  (VNx3x32QI "vnx32bi") (VNx4x32QI "vnx32bi") (VNx2x64QI "vnx64bi") (VNx2x8HI "vnx8bi")
+  (VNx3x8HI "vnx8bi") (VNx4x8HI "vnx8bi") (VNx5x8HI "vnx8bi") (VNx6x8HI "vnx8bi")
+  (VNx7x8HI "vnx8bi") (VNx8x8HI "vnx8bi") (VNx2x16HI "vnx16bi") (VNx3x16HI "vnx16bi")
+  (VNx4x16HI "vnx16bi") (VNx2x32HI "vnx32bi") (VNx2x4SI "vnx4bi") (VNx3x4SI "vnx4bi")
+  (VNx4x4SI "vnx4bi") (VNx5x4SI "vnx4bi") (VNx6x4SI "vnx4bi") (VNx7x4SI "vnx4bi")
+  (VNx8x4SI "vnx4bi") (VNx2x8SI "vnx8bi") (VNx3x8SI "vnx8bi") (VNx4x8SI "vnx8bi")
+  (VNx2x16SI "vnx16bi") (VNx2x2DI "vnx2bi") (VNx3x2DI "vnx2bi") (VNx4x2DI "vnx2bi")
+  (VNx5x2DI "vnx2bi") (VNx6x2DI "vnx2bi") (VNx7x2DI "vnx2bi") (VNx8x2DI "vnx2bi")
+  (VNx2x4DI "vnx4bi") (VNx3x4DI "vnx4bi") (VNx4x4DI "vnx4bi") (VNx2x8DI "vnx8bi")
+  (VNx2x8HF "vnx8bi") (VNx3x8HF "vnx8bi") (VNx4x8HF "vnx8bi") (VNx5x8HF "vnx8bi")
+  (VNx6x8HF "vnx8bi") (VNx7x8HF "vnx8bi") (VNx8x8HF "vnx8bi") (VNx2x16HF "vnx16bi")
+  (VNx3x16HF "vnx16bi") (VNx4x16HF "vnx16bi") (VNx2x32HF "vnx32bi") (VNx2x4SF "vnx4bi")
+  (VNx3x4SF "vnx4bi") (VNx4x4SF "vnx4bi") (VNx5x4SF "vnx4bi") (VNx6x4SF "vnx4bi")
+  (VNx7x4SF "vnx4bi") (VNx8x4SF "vnx4bi") (VNx2x8SF "vnx8bi") (VNx3x8SF "vnx8bi")
+  (VNx4x8SF "vnx8bi") (VNx2x16SF "vnx16bi") (VNx2x2DF "vnx2bi") (VNx3x2DF "vnx2bi")
+  (VNx4x2DF "vnx2bi") (VNx5x2DF "vnx2bi") (VNx6x2DF "vnx2bi") (VNx7x2DF "vnx2bi")
+  (VNx8x2DF "vnx2bi") (VNx2x4DF "vnx4bi") (VNx3x4DF "vnx4bi") (VNx4x4DF "vnx4bi")
+  (VNx2x8DF "vnx8bi")])
+
+;; Map a vector mode to its element mode.
+(define_mode_attr VSUBMODE [
+  (VNx16QI "QI") (VNx32QI "QI") (VNx64QI "QI") (VNx128QI "QI")
+  (VNx8HI "HI") (VNx16HI "HI") (VNx32HI "HI") (VNx64HI "HI")
+  (VNx4SI "SI") (VNx8SI "SI") (VNx16SI "SI") (VNx32SI "SI")
+  (VNx2DI "DI") (VNx4DI "DI") (VNx8DI "DI") (VNx16DI "DI")
+  (VNx8HF "HF") (VNx16HF "HF") (VNx32HF "HF") (VNx64HF "HF")
+  (VNx4SF "SF") (VNx8SF "SF") (VNx16SF "SF") (VNx32SF "SF")
+  (VNx2DF "DF") (VNx4DF "DF") (VNx8DF "DF") (VNx16DF "DF")
+  (VNx2x16QI "QI") (VNx3x16QI "QI") (VNx4x16QI "QI") (VNx5x16QI "QI")
+  (VNx6x16QI "QI") (VNx7x16QI "QI") (VNx8x16QI "QI") (VNx2x32QI "QI")
+  (VNx3x32QI "QI") (VNx4x32QI "QI") (VNx2x64QI "QI") (VNx2x8HI "HI")
+  (VNx3x8HI "HI") (VNx4x8HI "HI") (VNx5x8HI "HI") (VNx6x8HI "HI")
+  (VNx7x8HI "HI") (VNx8x8HI "HI") (VNx2x16HI "HI") (VNx3x16HI "HI")
+  (VNx4x16HI "HI") (VNx2x32HI "HI") (VNx2x4SI "SI") (VNx3x4SI "SI")
+  (VNx4x4SI "SI") (VNx5x4SI "SI") (VNx6x4SI "SI") (VNx7x4SI "SI")
+  (VNx8x4SI "SI") (VNx2x8SI "SI") (VNx3x8SI "SI") (VNx4x8SI "SI")
+  (VNx2x16SI "SI") (VNx2x2DI "DI") (VNx3x2DI "DI") (VNx4x2DI "DI")
+  (VNx5x2DI "DI") (VNx6x2DI "DI") (VNx7x2DI "DI") (VNx8x2DI "DI")
+  (VNx2x4DI "DI") (VNx3x4DI "DI") (VNx4x4DI "DI") (VNx2x8DI "DI")
+  (VNx2x8HF "HF") (VNx3x8HF "HF") (VNx4x8HF "HF") (VNx5x8HF "HF")
+  (VNx6x8HF "HF") (VNx7x8HF "HF") (VNx8x8HF "HF") (VNx2x16HF "HF")
+  (VNx3x16HF "HF") (VNx4x16HF "HF") (VNx2x32HF "HF") (VNx2x4SF "SF")
+  (VNx3x4SF "SF") (VNx4x4SF "SF") (VNx5x4SF "SF") (VNx6x4SF "SF")
+  (VNx7x4SF "SF") (VNx8x4SF "SF") (VNx2x8SF "SF") (VNx3x8SF "SF")
+  (VNx4x8SF "SF") (VNx2x16SF "SF") (VNx2x2DF "DF") (VNx3x2DF "DF")
+  (VNx4x2DF "DF") (VNx5x2DF "DF") (VNx6x2DF "DF") (VNx7x2DF "DF")
+  (VNx8x2DF "DF") (VNx2x4DF "DF") (VNx3x4DF "DF") (VNx4x4DF "DF")
+  (VNx2x8DF "DF")])
+
+;; Map a vector tuple mode to its vector mode.
+(define_mode_attr VTSUBMODE [
+  (VNx2x16QI "VNx16QI") (VNx3x16QI "VNx16QI") (VNx4x16QI "VNx16QI") (VNx5x16QI "VNx16QI")
+  (VNx6x16QI "VNx16QI") (VNx7x16QI "VNx16QI") (VNx8x16QI "VNx16QI") (VNx2x32QI "VNx32QI")
+  (VNx3x32QI "VNx32QI") (VNx4x32QI "VNx32QI") (VNx2x64QI "VNx64QI") (VNx2x8HI "VNx8HI")
+  (VNx3x8HI "VNx8HI") (VNx4x8HI "VNx8HI") (VNx5x8HI "VNx8HI") (VNx6x8HI "VNx8HI")
+  (VNx7x8HI "VNx8HI") (VNx8x8HI "VNx8HI") (VNx2x16HI "VNx16HI") (VNx3x16HI "VNx16HI")
+  (VNx4x16HI "VNx16HI") (VNx2x32HI "VNx32HI") (VNx2x4SI "VNx4SI") (VNx3x4SI "VNx4SI")
+  (VNx4x4SI "VNx4SI") (VNx5x4SI "VNx4SI") (VNx6x4SI "VNx4SI") (VNx7x4SI "VNx4SI")
+  (VNx8x4SI "VNx4SI") (VNx2x8SI "VNx8SI") (VNx3x8SI "VNx8SI") (VNx4x8SI "VNx8SI")
+  (VNx2x16SI "VNx16SI") (VNx2x2DI "VNx2DI") (VNx3x2DI "VNx2DI") (VNx4x2DI "VNx2DI")
+  (VNx5x2DI "VNx2DI") (VNx6x2DI "VNx2DI") (VNx7x2DI "VNx2DI") (VNx8x2DI "VNx2DI")
+  (VNx2x4DI "VNx4DI") (VNx3x4DI "VNx4DI") (VNx4x4DI "VNx4DI") (VNx2x8DI "VNx8DI")
+  (VNx2x8HF "VNx8HF") (VNx3x8HF "VNx8HF") (VNx4x8HF "VNx8HF") (VNx5x8HF "VNx8HF")
+  (VNx6x8HF "VNx8HF") (VNx7x8HF "VNx8HF") (VNx8x8HF "VNx8HF") (VNx2x16HF "VNx16HF")
+  (VNx3x16HF "VNx16HF") (VNx4x16HF "VNx16HF") (VNx2x32HF "VNx32HF") (VNx2x4SF "VNx4SF")
+  (VNx3x4SF "VNx4SF") (VNx4x4SF "VNx4SF") (VNx5x4SF "VNx4SF") (VNx6x4SF "VNx4SF")
+  (VNx7x4SF "VNx4SF") (VNx8x4SF "VNx4SF") (VNx2x8SF "VNx8SF") (VNx3x8SF "VNx8SF")
+  (VNx4x8SF "VNx8SF") (VNx2x16SF "VNx16SF") (VNx2x2DF "VNx2DF") (VNx3x2DF "VNx2DF")
+  (VNx4x2DF "VNx2DF") (VNx5x2DF "VNx2DF") (VNx6x2DF "VNx2DF") (VNx7x2DF "VNx2DF")
+  (VNx8x2DF "VNx2DF") (VNx2x4DF "VNx4DF") (VNx3x4DF "VNx4DF") (VNx4x4DF "VNx4DF")
+  (VNx2x8DF "VNx8DF")])
+
+;; Map a vector tuple mode to its vector mode.
+(define_mode_attr vtsubmode [
+  (VNx2x16QI "vnx16qi") (VNx3x16QI "vnx16qi") (VNx4x16QI "vnx16qi") (VNx5x16QI "vnx16qi")
+  (VNx6x16QI "vnx16qi") (VNx7x16QI "vnx16qi") (VNx8x16QI "vnx16qi") (VNx2x32QI "vnx32qi")
+  (VNx3x32QI "vnx32qi") (VNx4x32QI "vnx32qi") (VNx2x64QI "vnx64qi") (VNx2x8HI "vnx8hi")
+  (VNx3x8HI "vnx8hi") (VNx4x8HI "vnx8hi") (VNx5x8HI "vnx8hi") (VNx6x8HI "vnx8hi")
+  (VNx7x8HI "vnx8hi") (VNx8x8HI "vnx8hi") (VNx2x16HI "vnx16hi") (VNx3x16HI "vnx16hi")
+  (VNx4x16HI "vnx16hi") (VNx2x32HI "vnx32hi") (VNx2x4SI "vnx4si") (VNx3x4SI "vnx4si")
+  (VNx4x4SI "vnx4si") (VNx5x4SI "vnx4si") (VNx6x4SI "vnx4si") (VNx7x4SI "vnx4si")
+  (VNx8x4SI "vnx4si") (VNx2x8SI "vnx8si") (VNx3x8SI "vnx8si") (VNx4x8SI "vnx8si")
+  (VNx2x16SI "vnx16si") (VNx2x2DI "vnx2di") (VNx3x2DI "vnx2di") (VNx4x2DI "vnx2di")
+  (VNx5x2DI "vnx2di") (VNx6x2DI "vnx2di") (VNx7x2DI "vnx2di") (VNx8x2DI "vnx2di")
+  (VNx2x4DI "vnx4di") (VNx3x4DI "vnx4di") (VNx4x4DI "vnx4di") (VNx2x8DI "vnx8di")
+  (VNx2x8HF "vnx8hf") (VNx3x8HF "vnx8hf") (VNx4x8HF "vnx8hf") (VNx5x8HF "vnx8hf")
+  (VNx6x8HF "vnx8hf") (VNx7x8HF "vnx8hf") (VNx8x8HF "vnx8hf") (VNx2x16HF "vnx16hf")
+  (VNx3x16HF "vnx16hf") (VNx4x16HF "vnx16hf") (VNx2x32HF "vnx32hf") (VNx2x4SF "vnx4sf")
+  (VNx3x4SF "vnx4sf") (VNx4x4SF "vnx4sf") (VNx5x4SF "vnx4sf") (VNx6x4SF "vnx4sf")
+  (VNx7x4SF "vnx4sf") (VNx8x4SF "vnx4sf") (VNx2x8SF "vnx8sf") (VNx3x8SF "vnx8sf")
+  (VNx4x8SF "vnx8sf") (VNx2x16SF "vnx16sf") (VNx2x2DF "vnx2df") (VNx3x2DF "vnx2df")
+  (VNx4x2DF "vnx2df") (VNx5x2DF "vnx2df") (VNx6x2DF "vnx2df") (VNx7x2DF "vnx2df")
+  (VNx8x2DF "vnx2df") (VNx2x4DF "vnx4df") (VNx3x4DF "vnx4df") (VNx4x4DF "vnx4df")
+  (VNx2x8DF "vnx8df")])
+
+;; Map a vector mode to its LMUL==1 equivalent.
+;; This is for reductions which use scalars in vector registers.
+(define_mode_attr V1MODES [
+  (VNx16QI "VNx16QI") (VNx32QI "VNx16QI") (VNx64QI "VNx16QI") (VNx128QI "VNx16QI")
+  (VNx8HI "VNx8HI") (VNx16HI "VNx8HI") (VNx32HI "VNx8HI") (VNx64HI "VNx8HI")
+  (VNx4SI "VNx4SI") (VNx8SI "VNx4SI") (VNx16SI "VNx4SI") (VNx32SI "VNx4SI")
+  (VNx2DI "VNx2DI") (VNx4DI "VNx2DI") (VNx8DI "VNx2DI") (VNx16DI "VNx2DI")
+  (VNx8HF "VNx8HF") (VNx16HF "VNx8HF") (VNx32HF "VNx8HF") (VNx64HF "VNx8HF")
+  (VNx4SF "VNx4SF") (VNx8SF "VNx4SF") (VNx16SF "VNx4SF") (VNx32SF "VNx4SF")
+  (VNx2DF "VNx2DF") (VNx4DF "VNx2DF") (VNx8DF "VNx2DF") (VNx16DF "VNx2DF")
+])
+
+;; Map a vector mode to its LMUL==1 widen vector type.
+;; This is for widening reductions which use scalars in vector registers.
+(define_mode_attr VW1MODES [
+  (VNx16QI "VNx8HI") (VNx32QI "VNx8HI") (VNx64QI "VNx8HI") (VNx128QI "VNx8HI")
+  (VNx8HI "VNx4SI") (VNx16HI "VNx4SI") (VNx32HI "VNx4SI") (VNx64HI "VNx4SI")
+  (VNx4SI "VNx2DI") (VNx8SI "VNx2DI") (VNx16SI "VNx2DI") (VNx32SI "VNx2DI")
+  (VNx8HF "VNx4SF") (VNx16HF "VNx4SF") (VNx32HF "VNx4SF") (VNx64HF "VNx4SF")
+  (VNx4SF "VNx2DF") (VNx8SF "VNx2DF") (VNx16SF "VNx2DF") (VNx32SF "VNx2DF")
+])
+
+;; Map a vector tuple mode to its NF value.
+(define_mode_attr NF [
+  (VNx2x16QI "2") (VNx3x16QI "3") (VNx4x16QI "4") (VNx5x16QI "5")
+  (VNx6x16QI "6") (VNx7x16QI "7") (VNx8x16QI "8") (VNx2x32QI "2")
+  (VNx3x32QI "3") (VNx4x32QI "4") (VNx2x64QI "2") (VNx2x8HI "2")
+  (VNx3x8HI "3") (VNx4x8HI "4") (VNx5x8HI "5") (VNx6x8HI "6")
+  (VNx7x8HI "7") (VNx8x8HI "8") (VNx2x16HI "2") (VNx3x16HI "3")
+  (VNx4x16HI "4") (VNx2x32HI "2") (VNx2x4SI "2") (VNx3x4SI "3")
+  (VNx4x4SI "4") (VNx5x4SI "5") (VNx6x4SI "6") (VNx7x4SI "7")
+  (VNx8x4SI "8") (VNx2x8SI "2") (VNx3x8SI "3") (VNx4x8SI "4")
+  (VNx2x16SI "2") (VNx2x2DI "2") (VNx3x2DI "3") (VNx4x2DI "4")
+  (VNx5x2DI "5") (VNx6x2DI "6") (VNx7x2DI "7") (VNx8x2DI "8")
+  (VNx2x4DI "2") (VNx3x4DI "3") (VNx4x4DI "4") (VNx2x8DI "2")
+  (VNx2x8HF "2") (VNx3x8HF "3") (VNx4x8HF "4") (VNx5x8HF "5")
+  (VNx6x8HF "6") (VNx7x8HF "7") (VNx8x8HF "8") (VNx2x16HF "2")
+  (VNx3x16HF "3") (VNx4x16HF "4") (VNx2x32HF "2") (VNx2x4SF "2")
+  (VNx3x4SF "3") (VNx4x4SF "4") (VNx5x4SF "5") (VNx6x4SF "6")
+  (VNx7x4SF "7") (VNx8x4SF "8") (VNx2x8SF "2") (VNx3x8SF "3")
+  (VNx4x8SF "4") (VNx2x16SF "2") (VNx2x2DF "2") (VNx3x2DF "3")
+  (VNx4x2DF "4") (VNx5x2DF "5") (VNx6x2DF "6") (VNx7x2DF "7")
+  (VNx8x2DF "8") (VNx2x4DF "2") (VNx3x4DF "3") (VNx4x4DF "4")
+  (VNx2x8DF "2")])
+
+;; Map a vector mode to its VSETVLI mode, which for now is always the integer
+;; vector mode, as the integer vemode/vmmode is a superset of the float ones.
+(define_mode_attr VLMODE [
+  (VNx16QI "VNx16QI") (VNx32QI "VNx32QI") (VNx64QI "VNx64QI") (VNx128QI "VNx128QI")
+  (VNx8HI "VNx8HI") (VNx16HI "VNx16HI") (VNx32HI "VNx32HI") (VNx64HI "VNx64HI")
+  (VNx4SI "VNx4SI") (VNx8SI "VNx8SI") (VNx16SI "VNx16SI") (VNx32SI "VNx32SI")
+  (VNx2DI "VNx2DI") (VNx4DI "VNx4DI") (VNx8DI "VNx8DI") (VNx16DI "VNx16DI")
+  (VNx8HF "VNx8HI") (VNx16HF "VNx16HI") (VNx32HF "VNx32HI") (VNx64HF "VNx64HI")
+  (VNx4SF "VNx4SI") (VNx8SF "VNx8SI") (VNx16SF "VNx16SI") (VNx32SF "VNx32SI")
+  (VNx2DF "VNx2DI") (VNx4DF "VNx4DI") (VNx8DF "VNx8DI") (VNx16DF "VNx16DI")
+  (VNx2x16QI "VNx16QI") (VNx3x16QI "VNx16QI") (VNx4x16QI "VNx16QI") (VNx5x16QI "VNx16QI")
+  (VNx6x16QI "VNx16QI") (VNx7x16QI "VNx16QI") (VNx8x16QI "VNx16QI") (VNx2x32QI "VNx32QI")
+  (VNx3x32QI "VNx32QI") (VNx4x32QI "VNx32QI") (VNx2x64QI "VNx64QI") (VNx2x8HI "VNx8HI")
+  (VNx3x8HI "VNx8HI") (VNx4x8HI "VNx8HI") (VNx5x8HI "VNx8HI") (VNx6x8HI "VNx8HI")
+  (VNx7x8HI "VNx8HI") (VNx8x8HI "VNx8HI") (VNx2x16HI "VNx16HI") (VNx3x16HI "VNx16HI")
+  (VNx4x16HI "VNx16HI") (VNx2x32HI "VNx32HI") (VNx2x4SI "VNx4SI") (VNx3x4SI "VNx4SI")
+  (VNx4x4SI "VNx4SI") (VNx5x4SI "VNx4SI") (VNx6x4SI "VNx4SI") (VNx7x4SI "VNx4SI")
+  (VNx8x4SI "VNx4SI") (VNx2x8SI "VNx8SI") (VNx3x8SI "VNx8SI") (VNx4x8SI "VNx8SI")
+  (VNx2x16SI "VNx16SI") (VNx2x2DI "VNx2DI") (VNx3x2DI "VNx2DI") (VNx4x2DI "VNx2DI")
+  (VNx5x2DI "VNx2DI") (VNx6x2DI "VNx2DI") (VNx7x2DI "VNx2DI") (VNx8x2DI "VNx2DI")
+  (VNx2x4DI "VNx4DI") (VNx3x4DI "VNx4DI") (VNx4x4DI "VNx4DI") (VNx2x8DI "VNx8DI")
+  (VNx2x8HF "VNx8HI") (VNx3x8HF "VNx8HI") (VNx4x8HF "VNx8HI") (VNx5x8HF "VNx8HI")
+  (VNx6x8HF "VNx8HI") (VNx7x8HF "VNx8HI") (VNx8x8HF "VNx8HI") (VNx2x16HF "VNx16HI")
+  (VNx3x16HF "VNx16HI") (VNx4x16HF "VNx16HI") (VNx2x32HF "VNx32HI") (VNx2x4SF "VNx4SI")
+  (VNx3x4SF "VNx4SI") (VNx4x4SF "VNx4SI") (VNx5x4SF "VNx4SI") (VNx6x4SF "VNx4SI")
+  (VNx7x4SF "VNx4SI") (VNx8x4SF "VNx4SI") (VNx2x8SF "VNx8SI") (VNx3x8SF "VNx8SI")
+  (VNx4x8SF "VNx8SI") (VNx2x16SF "VNx16SI") (VNx2x2DF "VNx2DI") (VNx3x2DF "VNx2DI")
+  (VNx4x2DF "VNx2DI") (VNx5x2DF "VNx2DI") (VNx6x2DF "VNx2DI") (VNx7x2DF "VNx2DI")
+  (VNx8x2DF "VNx2DI") (VNx2x4DF "VNx4DI") (VNx3x4DF "VNx4DI") (VNx4x4DF "VNx4DI")
+  (VNx2x8DF "VNx8DI")])
+
+;; Map a vector mode to its VSETVLI mode, which for now is always the integer
+;; vector mode, as the integer vemode/vmmode is a superset of the float ones.
+(define_mode_attr vlmode [
+  (VNx16QI "vnx16qi") (VNx32QI "vnx32qi") (VNx64QI "vnx64qi") (VNx128QI "vnx128qi")
+  (VNx8HI "vnx8hi") (VNx16HI "vnx16hi") (VNx32HI "vnx32hi") (VNx64HI "vnx64hi")
+  (VNx4SI "vnx4si") (VNx8SI "vnx8si") (VNx16SI "vnx16si") (VNx32SI "vnx32si")
+  (VNx2DI "vnx2di") (VNx4DI "vnx4di") (VNx8DI "vnx8di") (VNx16DI "vnx16di")
+  (VNx8HF "vnx8hi") (VNx16HF "vnx16hi") (VNx32HF "vnx32hi") (VNx64HF "vnx64hi")
+  (VNx4SF "vnx4si") (VNx8SF "vnx8si") (VNx16SF "vnx16si") (VNx32SF "vnx32si")
+  (VNx2DF "vnx2di") (VNx4DF "vnx4di") (VNx8DF "vnx8di") (VNx16DF "vnx16di")
+  (VNx2x16QI "vnx16qi") (VNx3x16QI "vnx16qi") (VNx4x16QI "vnx16qi") (VNx5x16QI "vnx16qi")
+  (VNx6x16QI "vnx16qi") (VNx7x16QI "vnx16qi") (VNx8x16QI "vnx16qi") (VNx2x32QI "vnx32qi")
+  (VNx3x32QI "vnx32qi") (VNx4x32QI "vnx32qi") (VNx2x64QI "vnx64qi") (VNx2x8HI "vnx8hi")
+  (VNx3x8HI "vnx8hi") (VNx4x8HI "vnx8hi") (VNx5x8HI "vnx8hi") (VNx6x8HI "vnx8hi")
+  (VNx7x8HI "vnx8hi") (VNx8x8HI "vnx8hi") (VNx2x16HI "vnx16hi") (VNx3x16HI "vnx16hi")
+  (VNx4x16HI "vnx16hi") (VNx2x32HI "vnx32hi") (VNx2x4SI "vnx4si") (VNx3x4SI "vnx4si")
+  (VNx4x4SI "vnx4si") (VNx5x4SI "vnx4si") (VNx6x4SI "vnx4si") (VNx7x4SI "vnx4si")
+  (VNx8x4SI "vnx4si") (VNx2x8SI "vnx8si") (VNx3x8SI "vnx8si") (VNx4x8SI "vnx8si")
+  (VNx2x16SI "vnx16si") (VNx2x2DI "vnx2di") (VNx3x2DI "vnx2di") (VNx4x2DI "vnx2di")
+  (VNx5x2DI "vnx2di") (VNx6x2DI "vnx2di") (VNx7x2DI "vnx2di") (VNx8x2DI "vnx2di")
+  (VNx2x4DI "vnx4di") (VNx3x4DI "vnx4di") (VNx4x4DI "vnx4di") (VNx2x8DI "vnx8di")
+  (VNx2x8HF "vnx8hi") (VNx3x8HF "vnx8hi") (VNx4x8HF "vnx8hi") (VNx5x8HF "vnx8hi")
+  (VNx6x8HF "vnx8hi") (VNx7x8HF "vnx8hi") (VNx8x8HF "vnx8hi") (VNx2x16HF "vnx16hi")
+  (VNx3x16HF "vnx16hi") (VNx4x16HF "vnx16hi") (VNx2x32HF "vnx32hi") (VNx2x4SF "vnx4si")
+  (VNx3x4SF "vnx4si") (VNx4x4SF "vnx4si") (VNx5x4SF "vnx4si") (VNx6x4SF "vnx4si")
+  (VNx7x4SF "vnx4si") (VNx8x4SF "vnx4si") (VNx2x8SF "vnx8si") (VNx3x8SF "vnx8si")
+  (VNx4x8SF "vnx8si") (VNx2x16SF "vnx16si") (VNx2x2DF "vnx2di") (VNx3x2DF "vnx2di")
+  (VNx4x2DF "vnx2di") (VNx5x2DF "vnx2di") (VNx6x2DF "vnx2di") (VNx7x2DF "vnx2di")
+  (VNx8x2DF "vnx2di") (VNx2x4DF "vnx4di") (VNx3x4DF "vnx4di") (VNx4x4DF "vnx4di")
+  (VNx2x8DF "vnx8di")])
+
+;; Map a vector mode to its VSETVLI mode of widening vector mode
+;; , which for now is always the integer
+;; vector mode, as the integer vemode/vmmode is a superset of the float ones.
+(define_mode_attr VWVLMODE [
+  (VNx16QI "VNx16HI") (VNx32QI "VNx32HI") (VNx64QI "VNx64HI") (VNx8HI "VNx8SI")
+  (VNx16HI "VNx16SI") (VNx32HI "VNx32SI") (VNx4SI "VNx4DI") (VNx8SI "VNx8DI")
+  (VNx16SI "VNx16DI") (VNx8HF "VNx8SI") (VNx16HF "VNx16SI") (VNx32HF "VNx32SI")
+  (VNx4SF "VNx4DI") (VNx8SF "VNx8DI") (VNx16SF "VNx16DI")])
+
+;; Map a vector mode to its VSETVLI mode of widening vector mode
+;; , which for now is always the integer
+;; vector mode, as the integer vemode/vmmode is a superset of the float ones.
+(define_mode_attr vwvlmode [
+  (VNx16QI "vnx16hi") (VNx32QI "vnx32hi") (VNx64QI "vnx64hi") (VNx8HI "vnx8si")
+  (VNx16HI "vnx16si") (VNx32HI "vnx32si") (VNx4SI "vnx4di") (VNx8SI "vnx8di")
+  (VNx16SI "vnx16di") (VNx8HF "vnx8si") (VNx16HF "vnx16si") (VNx32HF "vnx32si")
+  (VNx4SF "vnx4di") (VNx8SF "vnx8di") (VNx16SF "vnx16di")])
+
+;; Map a vector mode to its VSETVLI mode of quad-widening vector mode
+;; , which for now is always the integer
+;; vector mode, as the integer vemode/vmmode is a superset of the float ones.
+(define_mode_attr VQWVLMODE [
+  (VNx16QI "VNx16SI") (VNx32QI "VNx32SI") (VNx8HI "VNx8DI") (VNx16HI "VNx16DI")
+  (VNx8HF "VNx8DI") (VNx16HF "VNx16DI")])
+
+;; Map a vector mode to its VSETVLI mode of quad-widening vector mode
+;; , which for now is always the integer
+;; vector mode, as the integer vemode/vmmode is a superset of the float ones.
+(define_mode_attr vqwvlmode [
+  (VNx16QI "vnx16si") (VNx32QI "vnx32si") (VNx8HI "vnx8di") (VNx16HI "vnx16di")
+  (VNx8HF "vnx8di") (VNx16HF "vnx16di")])
+
+;; Map a vector mode to its index load mode
+(define_mode_attr VMEMINXMODE [
+  (VNx16QI "VNx16QI") (VNx32QI "VNx32QI") (VNx64QI "VNx64QI") (VNx128QI "VNx128QI")
+  (VNx8HI "VNx8HI") (VNx16HI "VNx16HI") (VNx32HI "VNx32HI") (VNx64HI "VNx64HI")
+  (VNx4SI "VNx4SI") (VNx8SI "VNx8SI") (VNx16SI "VNx16SI") (VNx32SI "VNx32SI")
+  (VNx2DI "VNx2DI") (VNx4DI "VNx4DI") (VNx8DI "VNx8DI") (VNx16DI "VNx16DI")
+  (VNx8HF "VNx8HI") (VNx16HF "VNx16HI") (VNx32HF "VNx32HI") (VNx64HF "VNx64HI")
+  (VNx4SF "VNx4SI") (VNx8SF "VNx8SI") (VNx16SF "VNx16SI") (VNx32SF "VNx32SI")
+  (VNx2DF "VNx2DI") (VNx4DF "VNx4DI") (VNx8DF "VNx8DI") (VNx16DF "VNx16DI")
+  (VNx2x16QI "VNx16QI") (VNx3x16QI "VNx16QI") (VNx4x16QI "VNx16QI") (VNx5x16QI "VNx16QI")
+  (VNx6x16QI "VNx16QI") (VNx7x16QI "VNx16QI") (VNx8x16QI "VNx16QI") (VNx2x32QI "VNx32QI")
+  (VNx3x32QI "VNx32QI") (VNx4x32QI "VNx32QI") (VNx2x64QI "VNx64QI") (VNx2x8HI "VNx8HI")
+  (VNx3x8HI "VNx8HI") (VNx4x8HI "VNx8HI") (VNx5x8HI "VNx8HI") (VNx6x8HI "VNx8HI")
+  (VNx7x8HI "VNx8HI") (VNx8x8HI "VNx8HI") (VNx2x16HI "VNx16HI") (VNx3x16HI "VNx16HI")
+  (VNx4x16HI "VNx16HI") (VNx2x32HI "VNx32HI") (VNx2x4SI "VNx4SI") (VNx3x4SI "VNx4SI")
+  (VNx4x4SI "VNx4SI") (VNx5x4SI "VNx4SI") (VNx6x4SI "VNx4SI") (VNx7x4SI "VNx4SI")
+  (VNx8x4SI "VNx4SI") (VNx2x8SI "VNx8SI") (VNx3x8SI "VNx8SI") (VNx4x8SI "VNx8SI")
+  (VNx2x16SI "VNx16SI") (VNx2x2DI "VNx2DI") (VNx3x2DI "VNx2DI") (VNx4x2DI "VNx2DI")
+  (VNx5x2DI "VNx2DI") (VNx6x2DI "VNx2DI") (VNx7x2DI "VNx2DI") (VNx8x2DI "VNx2DI")
+  (VNx2x4DI "VNx4DI") (VNx3x4DI "VNx4DI") (VNx4x4DI "VNx4DI") (VNx2x8DI "VNx8DI")
+  (VNx2x8HF "VNx8HI") (VNx3x8HF "VNx8HI") (VNx4x8HF "VNx8HI") (VNx5x8HF "VNx8HI")
+  (VNx6x8HF "VNx8HI") (VNx7x8HF "VNx8HI") (VNx8x8HF "VNx8HI") (VNx2x16HF "VNx16HI")
+  (VNx3x16HF "VNx16HI") (VNx4x16HF "VNx16HI") (VNx2x32HF "VNx32HI") (VNx2x4SF "VNx4SI")
+  (VNx3x4SF "VNx4SI") (VNx4x4SF "VNx4SI") (VNx5x4SF "VNx4SI") (VNx6x4SF "VNx4SI")
+  (VNx7x4SF "VNx4SI") (VNx8x4SF "VNx4SI") (VNx2x8SF "VNx8SI") (VNx3x8SF "VNx8SI")
+  (VNx4x8SF "VNx8SI") (VNx2x16SF "VNx16SI") (VNx2x2DF "VNx2DI") (VNx3x2DF "VNx2DI")
+  (VNx4x2DF "VNx2DI") (VNx5x2DF "VNx2DI") (VNx6x2DF "VNx2DI") (VNx7x2DF "VNx2DI")
+  (VNx8x2DF "VNx2DI") (VNx2x4DF "VNx4DI") (VNx3x4DF "VNx4DI") (VNx4x4DF "VNx4DI")
+  (VNx2x8DF "VNx8DI")])
+
+;; Map a vector mode to its index load mode
+(define_mode_attr vmeminxmode [
+  (VNx16QI "vnx16qi") (VNx32QI "vnx32qi") (VNx64QI "vnx64qi") (VNx128QI "vnx128qi")
+  (VNx8HI "vnx8hi") (VNx16HI "vnx16hi") (VNx32HI "vnx32hi") (VNx64HI "vnx64hi")
+  (VNx4SI "vnx4si") (VNx8SI "vnx8si") (VNx16SI "vnx16si") (VNx32SI "vnx32si")
+  (VNx2DI "vnx2di") (VNx4DI "vnx4di") (VNx8DI "vnx8di") (VNx16DI "vnx16di")
+  (VNx8HF "vnx8hi") (VNx16HF "vnx16hi") (VNx32HF "vnx32hi") (VNx64HF "vnx64hi")
+  (VNx4SF "vnx4si") (VNx8SF "vnx8si") (VNx16SF "vnx16si") (VNx32SF "vnx32si")
+  (VNx2DF "vnx2di") (VNx4DF "vnx4di") (VNx8DF "vnx8di") (VNx16DF "vnx16di")
+  (VNx2x16QI "vnx16qi") (VNx3x16QI "vnx16qi") (VNx4x16QI "vnx16qi") (VNx5x16QI "vnx16qi")
+  (VNx6x16QI "vnx16qi") (VNx7x16QI "vnx16qi") (VNx8x16QI "vnx16qi") (VNx2x32QI "vnx32qi")
+  (VNx3x32QI "vnx32qi") (VNx4x32QI "vnx32qi") (VNx2x64QI "vnx64qi") (VNx2x8HI "vnx8hi")
+  (VNx3x8HI "vnx8hi") (VNx4x8HI "vnx8hi") (VNx5x8HI "vnx8hi") (VNx6x8HI "vnx8hi")
+  (VNx7x8HI "vnx8hi") (VNx8x8HI "vnx8hi") (VNx2x16HI "vnx16hi") (VNx3x16HI "vnx16hi")
+  (VNx4x16HI "vnx16hi") (VNx2x32HI "vnx32hi") (VNx2x4SI "vnx4si") (VNx3x4SI "vnx4si")
+  (VNx4x4SI "vnx4si") (VNx5x4SI "vnx4si") (VNx6x4SI "vnx4si") (VNx7x4SI "vnx4si")
+  (VNx8x4SI "vnx4si") (VNx2x8SI "vnx8si") (VNx3x8SI "vnx8si") (VNx4x8SI "vnx8si")
+  (VNx2x16SI "vnx16si") (VNx2x2DI "vnx2di") (VNx3x2DI "vnx2di") (VNx4x2DI "vnx2di")
+  (VNx5x2DI "vnx2di") (VNx6x2DI "vnx2di") (VNx7x2DI "vnx2di") (VNx8x2DI "vnx2di")
+  (VNx2x4DI "vnx4di") (VNx3x4DI "vnx4di") (VNx4x4DI "vnx4di") (VNx2x8DI "vnx8di")
+  (VNx2x8HF "vnx8hi") (VNx3x8HF "vnx8hi") (VNx4x8HF "vnx8hi") (VNx5x8HF "vnx8hi")
+  (VNx6x8HF "vnx8hi") (VNx7x8HF "vnx8hi") (VNx8x8HF "vnx8hi") (VNx2x16HF "vnx16hi")
+  (VNx3x16HF "vnx16hi") (VNx4x16HF "vnx16hi") (VNx2x32HF "vnx32hi") (VNx2x4SF "vnx4si")
+  (VNx3x4SF "vnx4si") (VNx4x4SF "vnx4si") (VNx5x4SF "vnx4si") (VNx6x4SF "vnx4si")
+  (VNx7x4SF "vnx4si") (VNx8x4SF "vnx4si") (VNx2x8SF "vnx8si") (VNx3x8SF "vnx8si")
+  (VNx4x8SF "vnx8si") (VNx2x16SF "vnx16si") (VNx2x2DF "vnx2di") (VNx3x2DF "vnx2di")
+  (VNx4x2DF "vnx2di") (VNx5x2DF "vnx2di") (VNx6x2DF "vnx2di") (VNx7x2DF "vnx2di")
+  (VNx8x2DF "vnx2di") (VNx2x4DF "vnx4di") (VNx3x4DF "vnx4di") (VNx4x4DF "vnx4di")
+  (VNx2x8DF "vnx8di")])
+
+;; Map a vector float mode to vector widening int mode.
+(define_mode_attr VFWIMODE [
+  (VNx8HF "VNx8SI") (VNx16HF "VNx16SI") (VNx32HF "VNx32SI") (VNx4SF "VNx4DI")
+  (VNx8SF "VNx8DI") (VNx16SF "VNx16DI")])
+
+;; Map a vector float mode to vector widening int mode.
+(define_mode_attr vfwimode [
+  (VNx8HF "vnx8si") (VNx16HF "vnx16si") (VNx32HF "vnx32si") (VNx4SF "vnx4di")
+  (VNx8SF "vnx8di") (VNx16SF "vnx16di")])
+
+;; Map a vector int mode to vector widening float mode.
+(define_mode_attr VIWFMODE [
+  (VNx8HI "VNx8SF") (VNx16HI "VNx16SF") (VNx32HI "VNx32SF") (VNx4SI "VNx4DF")
+  (VNx8SI "VNx8DF") (VNx16SI "VNx16DF")])
+
+;; Map a vector int mode to vector widening float mode.
+(define_mode_attr viwfmode [
+  (VNx8HI "vnx8sf") (VNx16HI "vnx16sf") (VNx32HI "vnx32sf") (VNx4SI "vnx4df")
+  (VNx8SI "vnx8df") (VNx16SI "vnx16df")])
+
+;; Map a vector mode to SEW
+(define_mode_attr sew [
+  (VNx16QI "8") (VNx32QI "8") (VNx64QI "8") (VNx128QI "8")
+  (VNx8HI "16") (VNx16HI "16") (VNx32HI "16") (VNx64HI "16")
+  (VNx4SI "32") (VNx8SI "32") (VNx16SI "32") (VNx32SI "32")
+  (VNx2DI "64") (VNx4DI "64") (VNx8DI "64") (VNx16DI "64")
+  (VNx8HF "16") (VNx16HF "16") (VNx32HF "16") (VNx64HF "16")
+  (VNx4SF "32") (VNx8SF "32") (VNx16SF "32") (VNx32SF "32")
+  (VNx2DF "64") (VNx4DF "64") (VNx8DF "64") (VNx16DF "64")
+  (VNx2x16QI "8") (VNx3x16QI "8") (VNx4x16QI "8") (VNx5x16QI "8")
+  (VNx6x16QI "8") (VNx7x16QI "8") (VNx8x16QI "8") (VNx2x32QI "8")
+  (VNx3x32QI "8") (VNx4x32QI "8") (VNx2x64QI "8") (VNx2x8HI "16")
+  (VNx3x8HI "16") (VNx4x8HI "16") (VNx5x8HI "16") (VNx6x8HI "16")
+  (VNx7x8HI "16") (VNx8x8HI "16") (VNx2x16HI "16") (VNx3x16HI "16")
+  (VNx4x16HI "16") (VNx2x32HI "16") (VNx2x4SI "32") (VNx3x4SI "32")
+  (VNx4x4SI "32") (VNx5x4SI "32") (VNx6x4SI "32") (VNx7x4SI "32")
+  (VNx8x4SI "32") (VNx2x8SI "32") (VNx3x8SI "32") (VNx4x8SI "32")
+  (VNx2x16SI "32") (VNx2x2DI "64") (VNx3x2DI "64") (VNx4x2DI "64")
+  (VNx5x2DI "64") (VNx6x2DI "64") (VNx7x2DI "64") (VNx8x2DI "64")
+  (VNx2x4DI "64") (VNx3x4DI "64") (VNx4x4DI "64") (VNx2x8DI "64")
+  (VNx2x8HF "16") (VNx3x8HF "16") (VNx4x8HF "16") (VNx5x8HF "16")
+  (VNx6x8HF "16") (VNx7x8HF "16") (VNx8x8HF "16") (VNx2x16HF "16")
+  (VNx3x16HF "16") (VNx4x16HF "16") (VNx2x32HF "16") (VNx2x4SF "32")
+  (VNx3x4SF "32") (VNx4x4SF "32") (VNx5x4SF "32") (VNx6x4SF "32")
+  (VNx7x4SF "32") (VNx8x4SF "32") (VNx2x8SF "32") (VNx3x8SF "32")
+  (VNx4x8SF "32") (VNx2x16SF "32") (VNx2x2DF "64") (VNx3x2DF "64")
+  (VNx4x2DF "64") (VNx5x2DF "64") (VNx6x2DF "64") (VNx7x2DF "64")
+  (VNx8x2DF "64") (VNx2x4DF "64") (VNx3x4DF "64") (VNx4x4DF "64")
+  (VNx2x8DF "64")])
+
+;; Map a vector mode to its LMUL.
+(define_mode_attr lmul [
+  (VNx16QI "1") (VNx32QI "2") (VNx64QI "4") (VNx128QI "8")
+  (VNx8HI "1") (VNx16HI "2") (VNx32HI "4") (VNx64HI "8")
+  (VNx4SI "1") (VNx8SI "2") (VNx16SI "4") (VNx32SI "8")
+  (VNx2DI "1") (VNx4DI "2") (VNx8DI "4") (VNx16DI "8")
+  (VNx8HF "1") (VNx16HF "2") (VNx32HF "4") (VNx64HF "8")
+  (VNx4SF "1") (VNx8SF "2") (VNx16SF "4") (VNx32SF "8")
+  (VNx2DF "1") (VNx4DF "2") (VNx8DF "4") (VNx16DF "8")
+  (VNx2x16QI "1") (VNx3x16QI "1") (VNx4x16QI "1") (VNx5x16QI "1")
+  (VNx6x16QI "1") (VNx7x16QI "1") (VNx8x16QI "1") (VNx2x32QI "2")
+  (VNx3x32QI "2") (VNx4x32QI "2") (VNx2x64QI "4") (VNx2x8HI "1")
+  (VNx3x8HI "1") (VNx4x8HI "1") (VNx5x8HI "1") (VNx6x8HI "1")
+  (VNx7x8HI "1") (VNx8x8HI "1") (VNx2x16HI "2") (VNx3x16HI "2")
+  (VNx4x16HI "2") (VNx2x32HI "4") (VNx2x4SI "1") (VNx3x4SI "1")
+  (VNx4x4SI "1") (VNx5x4SI "1") (VNx6x4SI "1") (VNx7x4SI "1")
+  (VNx8x4SI "1") (VNx2x8SI "2") (VNx3x8SI "2") (VNx4x8SI "2")
+  (VNx2x16SI "4") (VNx2x2DI "1") (VNx3x2DI "1") (VNx4x2DI "1")
+  (VNx5x2DI "1") (VNx6x2DI "1") (VNx7x2DI "1") (VNx8x2DI "1")
+  (VNx2x4DI "2") (VNx3x4DI "2") (VNx4x4DI "2") (VNx2x8DI "4")
+  (VNx2x8HF "1") (VNx3x8HF "1") (VNx4x8HF "1") (VNx5x8HF "1")
+  (VNx6x8HF "1") (VNx7x8HF "1") (VNx8x8HF "1") (VNx2x16HF "2")
+  (VNx3x16HF "2") (VNx4x16HF "2") (VNx2x32HF "4") (VNx2x4SF "1")
+  (VNx3x4SF "1") (VNx4x4SF "1") (VNx5x4SF "1") (VNx6x4SF "1")
+  (VNx7x4SF "1") (VNx8x4SF "1") (VNx2x8SF "2") (VNx3x8SF "2")
+  (VNx4x8SF "2") (VNx2x16SF "4") (VNx2x2DF "1") (VNx3x2DF "1")
+  (VNx4x2DF "1") (VNx5x2DF "1") (VNx6x2DF "1") (VNx7x2DF "1")
+  (VNx8x2DF "1") (VNx2x4DF "2") (VNx3x4DF "2") (VNx4x4DF "2")
+  (VNx2x8DF "4")])
+
+;; Equivalent of "size" for a vector element.
+(define_mode_attr vmsize [
+  (VNx16QI "b") (VNx32QI "b") (VNx64QI "b") (VNx128QI "b")
+  (VNx8HI "h") (VNx16HI "h") (VNx32HI "h") (VNx64HI "h")
+  (VNx4SI "w") (VNx8SI "w") (VNx16SI "w") (VNx32SI "w")
+  (VNx2DI "d") (VNx4DI "d") (VNx8DI "d") (VNx16DI "d")
+  (VNx8HF "h") (VNx16HF "h") (VNx32HF "h") (VNx64HF "h")
+  (VNx4SF "w") (VNx8SF "w") (VNx16SF "w") (VNx32SF "w")
+  (VNx2DF "d") (VNx4DF "d") (VNx8DF "d") (VNx16DF "d")
+  (VNx2x16QI "b") (VNx3x16QI "b") (VNx4x16QI "b") (VNx5x16QI "b")
+  (VNx6x16QI "b") (VNx7x16QI "b") (VNx8x16QI "b") (VNx2x32QI "b")
+  (VNx3x32QI "b") (VNx4x32QI "b") (VNx2x64QI "b") (VNx2x8HI "h")
+  (VNx3x8HI "h") (VNx4x8HI "h") (VNx5x8HI "h") (VNx6x8HI "h")
+  (VNx7x8HI "h") (VNx8x8HI "h") (VNx2x16HI "h") (VNx3x16HI "h")
+  (VNx4x16HI "h") (VNx2x32HI "h") (VNx2x4SI "w") (VNx3x4SI "w")
+  (VNx4x4SI "w") (VNx5x4SI "w") (VNx6x4SI "w") (VNx7x4SI "w")
+  (VNx8x4SI "w") (VNx2x8SI "w") (VNx3x8SI "w") (VNx4x8SI "w")
+  (VNx2x16SI "w") (VNx2x2DI "d") (VNx3x2DI "d") (VNx4x2DI "d")
+  (VNx5x2DI "d") (VNx6x2DI "d") (VNx7x2DI "d") (VNx8x2DI "d")
+  (VNx2x4DI "d") (VNx3x4DI "d") (VNx4x4DI "d") (VNx2x8DI "d")
+  (VNx2x8HF "h") (VNx3x8HF "h") (VNx4x8HF "h") (VNx5x8HF "h")
+  (VNx6x8HF "h") (VNx7x8HF "h") (VNx8x8HF "h") (VNx2x16HF "h")
+  (VNx3x16HF "h") (VNx4x16HF "h") (VNx2x32HF "h") (VNx2x4SF "w")
+  (VNx3x4SF "w") (VNx4x4SF "w") (VNx5x4SF "w") (VNx6x4SF "w")
+  (VNx7x4SF "w") (VNx8x4SF "w") (VNx2x8SF "w") (VNx3x8SF "w")
+  (VNx4x8SF "w") (VNx2x16SF "w") (VNx2x2DF "d") (VNx3x2DF "d")
+  (VNx4x2DF "d") (VNx5x2DF "d") (VNx6x2DF "d") (VNx7x2DF "d")
+  (VNx8x2DF "d") (VNx2x4DF "d") (VNx3x4DF "d") (VNx4x4DF "d")
+  (VNx2x8DF "d")])
+
+;; Map a vector mode to LMUL=1 vector mode.
+(define_mode_attr VSINGLE [
+  (VNx16QI "VNx16QI") (VNx32QI "VNx16QI") (VNx64QI "VNx16QI") (VNx128QI "VNx16QI")
+  (VNx8HI "VNx8HI") (VNx16HI "VNx8HI") (VNx32HI "VNx8HI") (VNx64HI "VNx8HI")
+  (VNx4SI "VNx4SI") (VNx8SI "VNx4SI") (VNx16SI "VNx4SI") (VNx32SI "VNx4SI")
+  (VNx2DI "VNx2DI") (VNx4DI "VNx2DI") (VNx8DI "VNx2DI") (VNx16DI "VNx2DI")
+  (VNx8HF "VNx8HF") (VNx16HF "VNx8HF") (VNx32HF "VNx8HF") (VNx64HF "VNx8HF")
+  (VNx4SF "VNx4SF") (VNx8SF "VNx4SF") (VNx16SF "VNx4SF") (VNx32SF "VNx4SF")
+  (VNx2DF "VNx2DF") (VNx4DF "VNx2DF") (VNx8DF "VNx2DF") (VNx16DF "VNx2DF")
+  (VNx2x16QI "VNx16QI") (VNx3x16QI "VNx16QI") (VNx4x16QI "VNx16QI") (VNx5x16QI "VNx16QI")
+  (VNx6x16QI "VNx16QI") (VNx7x16QI "VNx16QI") (VNx8x16QI "VNx16QI") (VNx2x32QI "VNx16QI")
+  (VNx3x32QI "VNx16QI") (VNx4x32QI "VNx16QI") (VNx2x64QI "VNx16QI") (VNx2x8HI "VNx8HI")
+  (VNx3x8HI "VNx8HI") (VNx4x8HI "VNx8HI") (VNx5x8HI "VNx8HI") (VNx6x8HI "VNx8HI")
+  (VNx7x8HI "VNx8HI") (VNx8x8HI "VNx8HI") (VNx2x16HI "VNx8HI") (VNx3x16HI "VNx8HI")
+  (VNx4x16HI "VNx8HI") (VNx2x32HI "VNx8HI") (VNx2x4SI "VNx4SI") (VNx3x4SI "VNx4SI")
+  (VNx4x4SI "VNx4SI") (VNx5x4SI "VNx4SI") (VNx6x4SI "VNx4SI") (VNx7x4SI "VNx4SI")
+  (VNx8x4SI "VNx4SI") (VNx2x8SI "VNx4SI") (VNx3x8SI "VNx4SI") (VNx4x8SI "VNx4SI")
+  (VNx2x16SI "VNx4SI") (VNx2x2DI "VNx2DI") (VNx3x2DI "VNx2DI") (VNx4x2DI "VNx2DI")
+  (VNx5x2DI "VNx2DI") (VNx6x2DI "VNx2DI") (VNx7x2DI "VNx2DI") (VNx8x2DI "VNx2DI")
+  (VNx2x4DI "VNx2DI") (VNx3x4DI "VNx2DI") (VNx4x4DI "VNx2DI") (VNx2x8DI "VNx2DI")
+  (VNx2x8HF "VNx8HF") (VNx3x8HF "VNx8HF") (VNx4x8HF "VNx8HF") (VNx5x8HF "VNx8HF")
+  (VNx6x8HF "VNx8HF") (VNx7x8HF "VNx8HF") (VNx8x8HF "VNx8HF") (VNx2x16HF "VNx8HF")
+  (VNx3x16HF "VNx8HF") (VNx4x16HF "VNx8HF") (VNx2x32HF "VNx8HF") (VNx2x4SF "VNx4SF")
+  (VNx3x4SF "VNx4SF") (VNx4x4SF "VNx4SF") (VNx5x4SF "VNx4SF") (VNx6x4SF "VNx4SF")
+  (VNx7x4SF "VNx4SF") (VNx8x4SF "VNx4SF") (VNx2x8SF "VNx4SF") (VNx3x8SF "VNx4SF")
+  (VNx4x8SF "VNx4SF") (VNx2x16SF "VNx4SF") (VNx2x2DF "VNx2DF") (VNx3x2DF "VNx2DF")
+  (VNx4x2DF "VNx2DF") (VNx5x2DF "VNx2DF") (VNx6x2DF "VNx2DF") (VNx7x2DF "VNx2DF")
+  (VNx8x2DF "VNx2DF") (VNx2x4DF "VNx2DF") (VNx3x4DF "VNx2DF") (VNx4x4DF "VNx2DF")
+  (VNx2x8DF "VNx2DF")])
+
+;; Map a vector mode to LMUL=1 vector mode.
+(define_mode_attr vsingle [
+  (VNx16QI "vnx16qi") (VNx32QI "vnx16qi") (VNx64QI "vnx16qi") (VNx128QI "vnx16qi")
+  (VNx8HI "vnx8hi") (VNx16HI "vnx8hi") (VNx32HI "vnx8hi") (VNx64HI "vnx8hi")
+  (VNx4SI "vnx4si") (VNx8SI "vnx4si") (VNx16SI "vnx4si") (VNx32SI "vnx4si")
+  (VNx2DI "vnx2di") (VNx4DI "vnx2di") (VNx8DI "vnx2di") (VNx16DI "vnx2di")
+  (VNx8HF "vnx8hf") (VNx16HF "vnx8hf") (VNx32HF "vnx8hf") (VNx64HF "vnx8hf")
+  (VNx4SF "vnx4sf") (VNx8SF "vnx4sf") (VNx16SF "vnx4sf") (VNx32SF "vnx4sf")
+  (VNx2DF "vnx2df") (VNx4DF "vnx2df") (VNx8DF "vnx2df") (VNx16DF "vnx2df")
+  (VNx2x16QI "vnx16qi") (VNx3x16QI "vnx16qi") (VNx4x16QI "vnx16qi") (VNx5x16QI "vnx16qi")
+  (VNx6x16QI "vnx16qi") (VNx7x16QI "vnx16qi") (VNx8x16QI "vnx16qi") (VNx2x32QI "vnx16qi")
+  (VNx3x32QI "vnx16qi") (VNx4x32QI "vnx16qi") (VNx2x64QI "vnx16qi") (VNx2x8HI "vnx8hi")
+  (VNx3x8HI "vnx8hi") (VNx4x8HI "vnx8hi") (VNx5x8HI "vnx8hi") (VNx6x8HI "vnx8hi")
+  (VNx7x8HI "vnx8hi") (VNx8x8HI "vnx8hi") (VNx2x16HI "vnx8hi") (VNx3x16HI "vnx8hi")
+  (VNx4x16HI "vnx8hi") (VNx2x32HI "vnx8hi") (VNx2x4SI "vnx4si") (VNx3x4SI "vnx4si")
+  (VNx4x4SI "vnx4si") (VNx5x4SI "vnx4si") (VNx6x4SI "vnx4si") (VNx7x4SI "vnx4si")
+  (VNx8x4SI "vnx4si") (VNx2x8SI "vnx4si") (VNx3x8SI "vnx4si") (VNx4x8SI "vnx4si")
+  (VNx2x16SI "vnx4si") (VNx2x2DI "vnx2di") (VNx3x2DI "vnx2di") (VNx4x2DI "vnx2di")
+  (VNx5x2DI "vnx2di") (VNx6x2DI "vnx2di") (VNx7x2DI "vnx2di") (VNx8x2DI "vnx2di")
+  (VNx2x4DI "vnx2di") (VNx3x4DI "vnx2di") (VNx4x4DI "vnx2di") (VNx2x8DI "vnx2di")
+  (VNx2x8HF "vnx8hf") (VNx3x8HF "vnx8hf") (VNx4x8HF "vnx8hf") (VNx5x8HF "vnx8hf")
+  (VNx6x8HF "vnx8hf") (VNx7x8HF "vnx8hf") (VNx8x8HF "vnx8hf") (VNx2x16HF "vnx8hf")
+  (VNx3x16HF "vnx8hf") (VNx4x16HF "vnx8hf") (VNx2x32HF "vnx8hf") (VNx2x4SF "vnx4sf")
+  (VNx3x4SF "vnx4sf") (VNx4x4SF "vnx4sf") (VNx5x4SF "vnx4sf") (VNx6x4SF "vnx4sf")
+  (VNx7x4SF "vnx4sf") (VNx8x4SF "vnx4sf") (VNx2x8SF "vnx4sf") (VNx3x8SF "vnx4sf")
+  (VNx4x8SF "vnx4sf") (VNx2x16SF "vnx4sf") (VNx2x2DF "vnx2df") (VNx3x2DF "vnx2df")
+  (VNx4x2DF "vnx2df") (VNx5x2DF "vnx2df") (VNx6x2DF "vnx2df") (VNx7x2DF "vnx2df")
+  (VNx8x2DF "vnx2df") (VNx2x4DF "vnx2df") (VNx3x4DF "vnx2df") (VNx4x4DF "vnx2df")
+  (VNx2x8DF "vnx2df")])
diff --git a/gcc/config/riscv/vector.md b/gcc/config/riscv/vector.md
new file mode 100644
index 00000000000..b558f909a84
--- /dev/null
+++ b/gcc/config/riscv/vector.md
@@ -0,0 +1,10769 @@
+;; Machine description for RISC-V vector operations.
+;; Copyright (C) 2019 Free Software Foundation, Inc.
+;; Contributed by Jim Wilson (jimw@sifive.com).
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify
+;; it under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 3, or (at your option)
+;; any later version.
+
+;; GCC is distributed in the hope that it will be useful,
+;; but WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+;; GNU General Public License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(include "vector-iterator.md")
+
+;; Operations valid for integer reductions.
+(define_code_iterator any_reduc [plus umax smax umin smin and ior xor])
+
+;; Operations valid for floating-point reductions.
+(define_code_iterator any_freduc [plus smax smin])
+
+;; Commutative operation valid for floating-point.
+(define_code_iterator any_fcomop [plus mult smax smin])
+
+;; Non-commutative operation valid for floating-point.
+(define_code_iterator any_fnoncomop [minus div])
+
+;; All operation valid for floating-point.
+(define_code_iterator any_fop [plus mult smax smin minus div])
+
+;; All operation valid for comparison.
+(define_code_iterator any_cmp [eq ne lt ltu le leu gt gtu ge geu])
+
+;; All operation valid for signed comparison.
+(define_code_iterator signed_cmp [eq ne lt le gt ge])
+
+;; All operation valid for comparison, except ge and geu.
+(define_code_iterator cmp_except_ge [eq ne lt ltu le leu gt gtu])
+
+;; All operation valid for widening add and subtract.
+(define_code_iterator add_sub [plus minus])
+
+;; All operation valid for <op>not instruction in mask-register logical.
+(define_code_iterator any_opnot [and ior])
+
+;; All operation valid for min and max.
+(define_code_iterator any_minmax [smin umin smax umax])
+
+;;All operantion valid for floating-point and integer convert.
+(define_code_iterator any_fix [fix unsigned_fix])
+(define_code_iterator any_float [float unsigned_float])
+
+;; All operation valid for all add.
+(define_code_iterator all_plus [plus ss_plus us_plus])
+
+;; All operation valid for all subtract.
+(define_code_iterator all_minus [minus ss_minus us_minus])
+
+;; All operation valid for minus and ss_minus.
+(define_code_iterator sub_and_ssub [minus ss_minus])
+
+;; <reduc> expands to the name of the reduction that implements a
+;; particular code.
+(define_code_attr reduc [(plus "sum") (umax "maxu") (smax "max") (umin "minu")
+			 (smin "min") (and "and") (ior "or") (xor "xor")])
+
+;; <invmaskop> expand to the name of the insn that inversed binary bit-wise
+;; operation for mask type.
+(define_code_attr invmaskop [(and "nand") (ior "nor") (xor "xnor")])
+
+;; <vshift> expand to the name of the vector shift that implements a
+;; particular code.
+(define_code_attr vshift [(ashift "vashl") (ashiftrt "vashr") (lshiftrt "vlshr")])
+
+;; <vshift> expand to the name of the vector narrowing shift that implements a
+;; particular code.
+(define_code_attr vnshift [(ashiftrt "vnsra") (lshiftrt "vnsrl")])
+
+;; <vmac> expand to the name of the vmacc and vnmsac that implements a
+;; particular code.
+(define_code_attr vmac [(plus "macc") (minus "nmsac")])
+
+;; <vmadd_sub> expand to the name of the vmadd and vnmsub that implements a
+;; particular code.
+(define_code_attr vmadd_sub [(plus "madd") (minus "nmsub")])
+
+;; <vfmac> expand to the name of the vfmacc and vfmsac that implements a
+;; particular code.
+(define_code_attr vfmac [(plus "macc") (minus "msac")])
+
+;; <vfnmac> expand to the name of the vfnmacc and vfnmsac that implements a
+;; particular code.
+(define_code_attr vfnmac [(plus "nmsac") (minus "nmacc")])
+
+;; <vfmadd_sub> expand to the name of the vfmadd and vfmsub that implements a
+;; particular code.
+(define_code_attr vfmadd_sub [(plus "madd") (minus "msub")])
+
+;; <vfnmadd_sub> expand to the name of the vfnmadd and vfnmsub that implements a
+;; particular code.
+(define_code_attr vfnmadd_sub [(plus "nmsub") (minus "nmadd")])
+
+;; <as> expand to the name of the fma and fms that implements a
+;; particular code.
+(define_code_attr as [(plus "a") (minus "s")])
+
+;; <sa> expand to the name of the fnma and fnms that implements a
+;; particular code.
+(define_code_attr sa [(plus "s") (minus "a")])
+
+;; <neg_add> expand to the name of saturat instruction that implements a
+;; particular code.
+(define_code_attr neg_add [(minus "add") (ss_minus "sadd") (us_minus "saddu")])
+
+;; <icmp> expand to the name of the vector integer comparison that implements a
+;; particular code.
+(define_code_attr icmp [(eq "seq") (ne "sne") (lt "slt") (ltu "sltu") (le "sle")
+			(leu "sleu") (gt "sgt") (gtu "sgtu") (ge "sge") (geu "sgeu")])
+
+(define_code_attr ltge [(lt "ltge_") (ltu "ltge_") (ge "ltge_") (geu "ltge_")
+			(eq "") (ne "") (le "") (leu "") (gt "") (gtu "") ])
+
+;; <fcmp> expand to the name of the vector floating-point comparison that
+;; implements a particular code.
+(define_code_attr fcmp [(eq "feq") (ne "fne") (lt "flt") (le "fle")
+			(gt "fgt") (ge "fge")])
+
+;; <vshift> expand to the name of the min and max that implements a
+;; particular code.
+(define_code_attr minmax [(umax "umax") (smax "smax") (umin "umin") (smin "smin")])
+
+(define_code_attr fix_cvt [(fix "fix_trunc") (unsigned_fix "fixuns_trunc")])
+(define_code_attr float_cvt [(float "float") (unsigned_float "floatuns")])
+
+;; <sz_op> expand to the name of the wcvt and wcvtu that implements a
+;; particular code.
+(define_code_attr sz_op [(sign_extend "") (zero_extend "zero_")])
+(define_code_attr sz [(sign_extend "s") (zero_extend "z")])
+
+;; Iterator and attributes for widening floating-point reduction instructions.
+(define_int_iterator WFREDUC_REDUC [UNSPEC_REDUC_SUM UNSPEC_ORDERED_REDUC_SUM])
+
+;; Iterator and attributes for widening integer reduction instructions.
+(define_int_iterator WREDUC_REDUC [UNSPEC_REDUC_SUM UNSPEC_REDUC_USUM])
+
+(define_int_attr sumu [(UNSPEC_REDUC_SUM "") (UNSPEC_REDUC_USUM "u")])
+
+;; <o> expands to an empty string when doing a unordered operation and
+;; "o" when doing an ordered operation.
+(define_int_attr o [(UNSPEC_REDUC_SUM "") (UNSPEC_ORDERED_REDUC_SUM "o")])
+
+;; Iterator and attributes for misc mask instructions.
+(define_int_iterator MISC_MASK_OP [UNSPEC_SBF UNSPEC_SIF UNSPEC_SOF])
+
+(define_int_attr misc_maskop [(UNSPEC_SBF "sbf") (UNSPEC_SIF "sif")
+			      (UNSPEC_SOF "sof")])
+
+(define_int_iterator UNSPEC_VMULH [UNSPEC_VMULHS UNSPEC_VMULHU UNSPEC_VMULHSU])
+
+(define_int_attr v_su [(UNSPEC_VMULHS "s") (UNSPEC_VMULHU "u")
+		       (UNSPEC_VMULHSU "su") (UNSPEC_VNCLIP "")
+		       (UNSPEC_VNCLIPU "u")])
+
+;; Iterator and attributes for sign-injection instructions.
+(define_int_iterator UNSPEC_COPYSIGNS [UNSPEC_COPYSIGN UNSPEC_NCOPYSIGN
+				       UNSPEC_XORSIGN])
+
+(define_int_attr copysign [(UNSPEC_COPYSIGN "copysign")
+			   (UNSPEC_NCOPYSIGN "ncopysign")
+			   (UNSPEC_XORSIGN "xorsign")])
+
+(define_int_attr nx [(UNSPEC_COPYSIGN "") (UNSPEC_NCOPYSIGN "n")
+		     (UNSPEC_XORSIGN "x")])
+
+;; Iterator and attributes for convert instructions.
+(define_int_iterator UNSPEC_FCVT [UNSPEC_LRINT UNSPEC_FCVT_XUF])
+
+(define_int_attr fcvt_xf [(UNSPEC_LRINT "lrint")
+			  (UNSPEC_FCVT_XUF "fcvt_xuf")])
+
+(define_int_attr fu [(UNSPEC_LRINT "") (UNSPEC_FCVT_XUF "u")])
+
+;; Iterator and attributes for slide instructions.
+(define_int_iterator UNSPEC_VSLIDES [UNSPEC_VSLIDEUP UNSPEC_VSLIDEDOWN])
+
+(define_int_iterator UNSPEC_VSLIDES1 [UNSPEC_VSLIDE1UP UNSPEC_VSLIDE1DOWN])
+
+(define_int_iterator UNSPEC_VFSLIDES1 [UNSPEC_VFSLIDE1UP UNSPEC_VFSLIDE1DOWN])
+
+(define_int_attr ud [(UNSPEC_VSLIDEUP "up") (UNSPEC_VSLIDEDOWN "down")
+		     (UNSPEC_VSLIDE1UP "up") (UNSPEC_VSLIDE1DOWN "down")
+		     (UNSPEC_VFSLIDE1UP "up") (UNSPEC_VFSLIDE1DOWN "down")])
+
+;; Iterator and attributes for narrowing clip instructions.
+(define_int_iterator UNSPEC_VCLIP [UNSPEC_VNCLIP UNSPEC_VNCLIPU])
+
+;; Iterator and attributes for vssrl and vssra instructions.
+(define_int_iterator UNSPEC_VSSHIFT [UNSPEC_VSSRL UNSPEC_VSSRA])
+
+(define_int_attr sshift [(UNSPEC_VSSRL "vssrl") (UNSPEC_VSSRA "vssra")])
+
+;; Iterator and attributes for all fixed-point instructions.
+(define_int_iterator UNSPEC_SAT_OP [UNSPEC_VAADDU UNSPEC_VAADD
+				    UNSPEC_VASUBU UNSPEC_VASUB UNSPEC_VSMUL])
+
+(define_int_attr sat_op [(UNSPEC_VAADDU "vaaddu") (UNSPEC_VAADD "vaadd")
+			 (UNSPEC_VASUBU "vasubu") (UNSPEC_VASUB "vasub")
+			 (UNSPEC_VSMUL "vsmul")])
+
+;; Iterator and attributes for vsxei and vsuxei instructions.
+(define_int_iterator UNSPEC_INDEXED_STORE [UNSPEC_ORDERED_INDEXED_STORE
+					   UNSPEC_UNORDERED_INDEXED_STORE])
+
+(define_int_attr order [(UNSPEC_ORDERED_INDEXED_STORE "")
+			(UNSPEC_UNORDERED_INDEXED_STORE "u")])
+
+;; Iterator and attributes for integer multiply-add instructions.
+(define_int_iterator UNSPEC_MASK_VMACS [UNSPEC_MASK_VMADD UNSPEC_MASK_VMSUB
+					UNSPEC_MASK_VMACC UNSPEC_MASK_VMSAC])
+
+(define_int_attr imac [(UNSPEC_MASK_VMADD "vmadd")
+		       (UNSPEC_MASK_VMSUB "vnmsub")
+		       (UNSPEC_MASK_VMACC "vmacc")
+		       (UNSPEC_MASK_VMSAC "vnmsac")])
+
+;; Iterator and attributes for all vector atomic instructions.
+(define_int_iterator UNSPEC_VAMO [UNSPEC_VAMO_SWAP UNSPEC_VAMO_ADD
+				  UNSPEC_VAMO_XOR  UNSPEC_VAMO_AND
+				  UNSPEC_VAMO_OR   UNSPEC_VAMO_MIN
+				  UNSPEC_VAMO_MAX  UNSPEC_VAMO_MINU
+				  UNSPEC_VAMO_MAXU])
+
+(define_int_attr vamo
+  [(UNSPEC_VAMO_SWAP "vamoswapei") (UNSPEC_VAMO_ADD "vamoaddei")
+   (UNSPEC_VAMO_XOR "vamoxorei")   (UNSPEC_VAMO_AND "vamoandei")
+   (UNSPEC_VAMO_OR "vamoorei")     (UNSPEC_VAMO_MIN "vamominei")
+   (UNSPEC_VAMO_MAX "vamomaxei")   (UNSPEC_VAMO_MINU "vamominuei")
+   (UNSPEC_VAMO_MAXU "vamomaxuei")])
+
+;; Vsetvl instructions.
+
+;; These use VIMODES because only the SEW and LMUL matter.  The int/float
+;; distinction does not.  Also, the int modes are a superset of the float
+;; modes.
+
+(define_insn "riscv_vsetvl<VIMODES:sew>m<VIMODES:lmul>_<P:mode>"
+  [(set (match_operand:P 0 "register_operand" "=r")
+	(unspec_volatile:P [(match_operand:P 1 "register_operand" "r")]
+			   UNSPECV_VSETVL))
+   (set (reg:SI VL_REGNUM) (unspec_volatile:SI [(match_dup 1)] UNSPECV_VSETVL))
+   (set (reg:VIMODES VTYPE_REGNUM) (const_int UNSPECV_VSETVL))]
+  "TARGET_VECTOR"
+  "vsetvli\t%0,%1,e<VIMODES:sew>,m<VIMODES:lmul>"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_insn "vsetvli_x0_<mode>"
+  [(set (reg:VIMODES VTYPE_REGNUM) (const_int UNSPECV_VSETVL))]
+  "TARGET_VECTOR"
+  "vsetvli\tx0,x0,e<sew>,m<lmul>"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Load/store instructions.
+
+;; Vector Unit-Stride Instructions
+
+(define_expand "vle<VMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(mem:VMODES (match_operand:P 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vle<VMODES:mode>_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(match_operand:VMODES 1 "memory_operand" "m")
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vle<sew>.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vle<VMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(if_then_else:VMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mem:VMODES
+			  (match_operand:P 3 "register_operand"))
+			(match_operand:VMODES 2 "register_operand"))
+		       (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vle<VMODES:mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(if_then_else:VMODES
+	     (match_operand:<VCMPEQUIV> 2 "register_operand" "vm")
+	     (match_operand:VMODES 1 "memory_operand" "m")
+	     (match_operand:VMODES 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vle<sew>.v\t%0,%1,%2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vse<VMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:VMODES (match_operand:P 1 "register_operand"))
+		   (unspec:VMODES
+		     [(match_operand:VMODES 0 "register_operand")
+		      (mem:VMODES (match_dup 1))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vse<VMODES:mode>_nosetvl"
+  [(set (match_operand:VMODES 1 "memory_operand" "+m")
+	(unspec:VMODES
+	  [(match_operand:VMODES 0 "register_operand" "vr")
+	   (match_dup 1)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vse<sew>.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vse<VMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:VMODES (match_operand:P 2 "register_operand"))
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:<VCMPEQUIV> 0 "register_operand")
+			 (match_operand:VMODES 1 "register_operand")
+			 (mem:VMODES (match_dup 2))]
+		       UNSPEC_MASKED_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vse<VMODES:mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "memory_operand" "+m")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:VMODES 2 "register_operand" "vr")
+	      (match_dup 0)]
+	    UNSPEC_MASKED_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vse<sew>.v\t%2,%0,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Strided Instructions
+
+(define_expand "vlse<VMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:P 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_STRIDED_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vlse<VMODES:mode>_<P:mode>_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:P 2 "register_operand" "r")
+	      (mem:BLK (scratch))]
+	    UNSPEC_STRIDED_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlse<sew>.v\t%0,(%1),%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vlse<VMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(if_then_else:VMODES
+		      (match_operand:<VCMPEQUIV> 1 "register_operand")
+		      (unspec:VMODES
+			[(match_operand:P 3 "register_operand")
+			 (match_operand:P 4 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_STRIDED_LOAD)
+		      (match_operand:VMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vlse<VMODES:mode>_<P:mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(if_then_else:VMODES
+	     (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	     (unspec:VMODES
+	       [(match_operand:P 1 "register_operand" "r")
+		(match_operand:P 2 "register_operand" "r")
+		(mem:BLK (scratch))]
+	      UNSPEC_STRIDED_LOAD)
+	     (match_operand:VMODES 4 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlse<sew>.v\t%0,(%1),%2,%3.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vsse<VMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+		        [(match_operand:VMODES 0 "register_operand")
+			 (match_operand:P 1 "register_operand")
+			 (match_operand:P 2 "register_operand")]
+		       UNSPEC_STRIDED_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vsse<VMODES:mode>_<P:mode>_nosetvl"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:VMODES 0 "register_operand" "vr")
+	      (match_operand:P 1 "register_operand" "r")
+	      (match_operand:P 2 "register_operand" "r")]
+	    UNSPEC_STRIDED_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsse<sew>.v\t%0,(%1),%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vsse<VMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:<VCMPEQUIV> 0 "register_operand")
+			 (match_operand:VMODES 1 "register_operand")
+			 (match_operand:P 2 "register_operand")
+			 (match_operand:P 3 "register_operand")]
+		       UNSPEC_STRIDED_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vsse<VMODES:mode>_<P:mode>_mask_nosetvl"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	      (match_operand:VMODES 0 "register_operand" "vr")
+	      (match_operand:P 1 "register_operand" "r")
+	      (match_operand:P 2 "register_operand" "r")]
+	    UNSPEC_STRIDED_STORE)
+	    (reg:SI VL_REGNUM)]
+	  UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsse<sew>.v\t%0,(%1),%2,%3.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Indexed Load instructions
+
+;; XXX: we should support gather load for XLEN is 32.
+(define_expand "gather_load<VMODES:mode><VIMODES:mode>"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:DI 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (match_operand:DI 3 "const_0_operand")
+			 (match_operand:DI 4 "const_1_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_LOAD_GATHER)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_64BIT && TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*gather_load<VMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:VIMODES 2 "register_operand" "vr")
+	      (match_operand 3 "const_0_operand" "J")
+	      (match_operand 4 "const_1_operand" "Wsa")
+	      (mem:BLK (scratch))]
+	    UNSPEC_LOAD_GATHER)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vlxei<VIMODES:sew>.v\t%0,(%1),%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mask_gather_load<VMODES:mode><VIMODES:mode>"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:<VMODES:VCMPEQUIV> 5 "register_operand")
+			 (match_operand:DI 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (match_operand:DI 3 "const_0_operand")
+			 (match_operand:DI 4 "const_1_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_LOAD_GATHER)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_64BIT && TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*mask_gather_load<VMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:<VMODES:VCMPEQUIV> 5 "register_operand" "vm")
+	       (match_operand:P 1 "register_operand" "r")
+	       (match_operand:VIMODES 2 "register_operand" "vr")
+	       (match_operand 3 "const_0_operand" "J")
+	       (match_operand 4 "const_1_operand" "Wsa")
+	       (mem:BLK (scratch))]
+	    UNSPEC_LOAD_GATHER)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vlxei<VIMODES:sew>.v\t%0,(%1),%2,%5.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Indexed Load instructions for builtin
+
+(define_expand "vlxei<VMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_INDEXED_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*vlxei<VMODES:mode><VIMODES:mode>_<P:mode>_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:VIMODES 2 "register_operand" "vr")
+	      (mem:BLK (scratch))]
+	    UNSPEC_INDEXED_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vlxei<VIMODES:sew>.v\t%0,(%1),%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vlxei<VMODES:mode><VIMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:<VMODES:VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VMODES 2 "register_operand")
+			 (match_operand:P 3 "register_operand")
+			 (match_operand:VIMODES 4 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_INDEXED_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*vlxei<VMODES:mode><VIMODES:mode>_<P:mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:<VMODES:VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:VMODES 2 "register_operand" "0")
+	      (match_operand:P 3 "register_operand" "r")
+	      (match_operand:VIMODES 4 "register_operand" "vr")
+	      (mem:BLK (scratch))]
+	    UNSPEC_INDEXED_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vlxei<VIMODES:sew>.v\t%0,(%3),%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Indexed Store Instructions
+
+;; XXX: we should support scatter store for XLEN is 32.
+(define_expand "scatter_store<VMODES:mode><VIMODES:mode>"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:DI 0 "register_operand")
+			 (match_operand:VIMODES 1 "register_operand")
+			 (match_operand:DI 2 "const_0_operand")
+			 (match_operand:DI 3 "const_1_operand")
+			 (match_operand:VMODES 4 "register_operand")]
+		       UNSPEC_STORE_SCATTER)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_64BIT && TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*scatter_store<VMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:P 0 "register_operand" "r")
+	      (match_operand:VIMODES 1 "register_operand" "vr")
+	      (match_operand 2 "const_0_operand" "J")
+	      (match_operand 3 "const_1_operand" "Wsa")
+	      (match_operand:VMODES 4 "register_operand" "vr")]
+	    UNSPEC_STORE_SCATTER)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vsxei<VIMODES:sew>.v\t%4,(%0),%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mask_scatter_store<VMODES:mode><VIMODES:mode>"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:<VMODES:VCMPEQUIV> 5 "register_operand")
+			 (match_operand:DI 0 "register_operand")
+			 (match_operand:VIMODES 1 "register_operand")
+			 (match_operand:DI 2 "const_0_operand")
+			 (match_operand:DI 3 "const_1_operand")
+			 (match_operand:VMODES 4 "register_operand")]
+		       UNSPEC_STORE_SCATTER)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_64BIT && TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*mask_scatter_store<VMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:<VMODES:VCMPEQUIV> 5 "register_operand" "vm")
+	      (match_operand:P 0 "register_operand" "r")
+	      (match_operand:VIMODES 1 "register_operand" "vr")
+	      (match_operand 2 "const_0_operand" "J")
+	      (match_operand 3 "const_1_operand" "Wsa")
+	      (match_operand:VMODES 4 "register_operand" "vr")]
+	    UNSPEC_STORE_SCATTER)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vsxei<VIMODES:sew>.v\t%4,(%0),%1,%5.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Ordered-Indexed Store instructions for builtin
+
+(define_expand "vs<order>xei<VMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:P 0 "register_operand")
+			 (match_operand:VIMODES 1 "register_operand")
+			 (match_operand:VMODES 2 "register_operand")]
+		       UNSPEC_INDEXED_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*vs<order>xei<VMODES:mode><VIMODES:mode>_<P:mode>_nosetvl"
+   [(set (mem:BLK (scratch))
+	 (unspec:BLK
+	   [(unspec:BLK
+	      [(match_operand:P 0 "register_operand" "r")
+	       (match_operand:VIMODES 1 "register_operand" "vr")
+	       (match_operand:VMODES 2 "register_operand" "vr")]
+	     UNSPEC_INDEXED_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vs<order>xei<VIMODES:sew>.v\t%2,(%0),%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vs<order>xei<VMODES:mode><VIMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:<VMODES:VCMPEQUIV> 0 "register_operand")
+			 (match_operand:P 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (match_operand:VMODES 3 "register_operand")]
+		       UNSPEC_INDEXED_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*vs<order>xei<VMODES:mode><VIMODES:mode>_<P:mode>_mask_nosetvl"
+   [(set (mem:BLK (scratch))
+	 (unspec:BLK
+	   [(unspec:BLK
+	      [(match_operand:<VMODES:VCMPEQUIV> 0 "register_operand" "vm")
+	       (match_operand:P 1 "register_operand" "r")
+	       (match_operand:VIMODES 2 "register_operand" "vr")
+	       (match_operand:VMODES 3 "register_operand" "vr")]
+	     UNSPEC_INDEXED_STORE)
+	   (reg:SI VL_REGNUM)]
+	  UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "vs<order>xei<VIMODES:sew>.v\t%3,(%1),%2,%0.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Move instructions.
+
+;; ??? We need the m constraints here only if we want load/store to work
+;; directly without using an intrinsic.
+
+;; If operand 1 is a const_vector, then we can't split until after reload,
+;; to ensure that the scratch operand has been allocated a reg first.
+(define_expand "mov<mode>"
+  [(set (match_operand:VMODES 0 "reg_or_mem_operand")
+	(unspec:VMODES
+	  [(match_operand:VMODES 1 "vector_move_operand")
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+{
+  /* Need to force register if mem <- !reg.  */
+  if (MEM_P (operands[0]) && !REG_P (operands[1]))
+    operands[1] = force_reg (<MODE>mode, operands[1]);
+
+  rtx dup_value;
+
+  /* If we have a const vector, then we have to load it's value into the
+     scratch reg, and then create a vec_duplicate of it.  */
+  if (const_vec_duplicate_p (operands[1], &dup_value))
+    {
+      gcc_assert (can_create_pseudo_p ());
+      rtx tmp_reg = gen_reg_rtx (<VSUBMODE>mode);
+      emit_move_insn (tmp_reg, dup_value);
+      emit_insn (gen_vsetvli_x0_<vlmode>());
+      emit_insn (gen_vec_duplicate<mode>_nosetvl (operands[0], tmp_reg));
+      DONE;
+    }
+})
+
+(define_insn "*mov<mode>"
+  [(set (match_operand:VMODES 0 "reg_or_mem_operand"  "=vr,vr,m")
+	(unspec:VMODES
+	  [(match_operand:VMODES 1 "reg_or_mem_operand"  "vr,m,vr")
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+  "@
+   vmv<lmul>r.v\t%0,%1
+   vl<lmul>r.v\t%0,%1
+   vs<lmul>r.v\t%1,%0"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mov<mode>"
+  [(set (match_operand:VMASKMODES 0 "reg_or_mem_operand")
+	(unspec:VMASKMODES
+	  [(match_operand:VMASKMODES 1 "vector_move_operand")
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+{
+  rtx ele;
+  if (const_vec_duplicate_p (operands[1], &ele))
+    {
+      gcc_assert (CONST_INT_P (ele));
+      switch (INTVAL (ele))
+	{
+	case 0:
+	  emit_insn (gen_clr<mode> (operands[0]));
+	  break;
+	case 1:
+	  emit_insn (gen_set<mode> (operands[0]));
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
+      DONE;
+    }
+})
+
+;; move pattern for vector masking type.
+(define_insn "*mov<mode>"
+  [(set (match_operand:VMASKMODES 0 "reg_or_mem_operand"  "=vr,vr, m")
+	(unspec:VMASKMODES
+	  [(match_operand:VMASKMODES 1 "reg_or_mem_operand"  " vr, m, vr")
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+  "@
+   vmv1r.v\t%0, %1
+   vl1r.v\t%0, %1
+   vs1r.v\t%1, %0"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Integer Scalar Move Instructions.
+
+;; XXX: we should support scalar move for XLEN is 32.
+(define_expand "vec_extract<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VSUBMODE> 0 "register_operand")
+		   (unspec:<VSUBMODE>
+		     [(vec_select:<VSUBMODE>
+			(match_operand:VIMODES 1 "register_operand")
+			(parallel [(const_int 0)]))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "vec_extract<mode>_nosetvl"
+  [(set (match_operand:<VSUBMODE> 0 "register_operand" "=r")
+	(unspec:<VSUBMODE>
+	  [(vec_select:<VSUBMODE>
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (parallel [(const_int 0)]))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmv.x.s\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vec_set<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(vec_merge:VIMODES
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VIMODES 1 "register_operand")
+			(const_int 1))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "vec_set<mode>_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(vec_merge:VIMODES
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	     (match_operand:VIMODES 1 "register_operand" "0")
+	     (const_int 1))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmv.s.x\t%0,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; FP Scalar Move Instructions.
+
+;; XXX: we should support scalar move for FLEN is 32.
+(define_expand "vec_extract_fext<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VSUBMODE> 0 "register_operand")
+		   (unspec:<VSUBMODE>
+		     [(vec_select:<VSUBMODE>
+			(match_operand:VFMODES 1 "register_operand")
+			(parallel [(const_int 0)]))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "vec_extract_fext<mode>_nosetvl"
+  [(set (match_operand:<VSUBMODE> 0 "register_operand" "=f")
+	(unspec:<VSUBMODE>
+	  [(vec_select:<VSUBMODE>
+	     (match_operand:VFMODES 1 "register_operand" "vr")
+	     (parallel [(const_int 0)]))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfmv.f.s\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vec_set<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(vec_merge:VFMODES
+			(vec_duplicate:VFMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VFMODES 1 "register_operand")
+			(const_int 1))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "vec_set<mode>_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(vec_merge:VFMODES
+	     (vec_duplicate:VFMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "f"))
+	     (match_operand:VFMODES 1 "register_operand" "0")
+	     (const_int 1))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfmv.s.f\t%0,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Single-Width Integer Add and Saturating Add
+
+(define_expand "<optab><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(all_plus:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "vector_arith_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(all_plus:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr,vr")
+	     (match_operand:VIMODES 2 "vector_arith_operand" "vr,vi"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   v<insn>.vv\t%0,%1,%2
+   v<insn>.vi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(all_plus:VIMODES
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(all_plus:VIMODES
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(all_plus:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "vector_arith_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (all_plus:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr,vr")
+	       (match_operand:VIMODES 4 "vector_arith_operand" "vr,vi"))
+	     (match_operand:VIMODES 2 "register_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   v<insn>.vv\t%0,%3,%4,%1.t
+   v<insn>.vi\t%0,%3,%v4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(all_plus:VIMODES
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand"))
+			  (match_operand:VIMODES 3 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (all_plus:VIMODES
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r"))
+	       (match_operand:VIMODES 3 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Single-Width Integer Subtract and Saturating Subtract
+
+(define_expand "<optab><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(sub_and_ssub:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "neg_vector_arith_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(sub_and_ssub:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr,vr")
+	     (match_operand:VIMODES 2 "neg_vector_arith_operand" "vr,vj"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   v<insn>.vv\t%0,%1,%2
+   v<neg_add>.vi\t%0,%1,%V2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "ussub<mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(us_minus:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*ussub<mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(us_minus:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (match_operand:VIMODES 2 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vssubu.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; ??? No constant immediate subtract, should be converted to an add, but
+;; this is not happening.  Maybe add it to this pattern?
+
+(define_expand "<optab><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(all_minus:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(all_minus:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(all_minus:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (all_minus:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (match_operand:VIMODES 4 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(all_minus:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (all_minus:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "rsub<mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(minus:VIMODES
+			(match_operand:VIMODES 2 "const_vector_int_operand")
+			(match_operand:VIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*rsub<mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(minus:VIMODES
+	     (match_operand:VIMODES 2 "const_vector_int_operand" "vi")
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vrsub.vi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "rsub<mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(minus:VIMODES
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*rsub<mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(minus:VIMODES
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vrsub.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "rsub<mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(minus:VIMODES
+			  (match_operand:VIMODES 3 "const_vector_int_operand")
+			  (match_operand:VIMODES 4 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*rsub<mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (minus:VIMODES
+	       (match_operand:VIMODES 3 "const_vector_int_operand" "vi")
+	       (match_operand:VIMODES 4 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vrsub.vi\t%0,%4,%v3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "rsub<mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+		        (match_operand:<VCMPEQUIV> 1 "register_operand")
+			(minus:VIMODES
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand"))
+			  (match_operand:VIMODES 3 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*rsub<mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (minus:VIMODES
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r"))
+	       (match_operand:VIMODES 3 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vrsub.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Logical instructions
+
+(define_expand "<optab><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_bitwise:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "vector_arith_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(any_bitwise:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr,vr")
+	     (match_operand:VIMODES 2 "vector_arith_operand" "vr,vi"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   v<insn>.vv\t%0,%1,%2
+   v<insn>.vi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_bitwise:VIMODES
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(any_bitwise:VIMODES
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_bitwise:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "vector_arith_operand"))
+			(match_operand:VIMODES 2 "vector_arith_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (any_bitwise:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr,vr")
+	       (match_operand:VIMODES 4 "vector_arith_operand" "vr,vi"))
+	     (match_operand:VIMODES 2 "vector_arith_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   v<insn>.vv\t%0,%3,%4,%1.t
+   v<insn>.vi\t%0,%3,%v4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_bitwise:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_bitwise:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "one_cmpl<mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(not:VIMODES
+			(match_operand:VIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*one_cmpl<mode>2_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(not:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vnot.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "one_cmpl<mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(not:VIMODES
+			  (match_operand:VIMODES 3 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*one_cmpl<mode>2_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (not:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vnot.v\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Integer Add-with-Carry / Subtract-with-Borrow Instructions
+
+(define_expand "adc<mode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(plus:VIMODES
+			(plus:VIMODES
+			  (match_operand:VIMODES 1 "register_operand")
+			  (match_operand:VIMODES 2 "vector_arith_operand"))
+		      (if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 3 "register_operand")
+			(vec_duplicate:VIMODES (const_int 1))
+			(vec_duplicate:VIMODES (const_int 0))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*adc<mode>4_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=&vr,&vr")
+	(unspec:VIMODES
+	  [(plus:VIMODES
+	     (plus:VIMODES
+	       (match_operand:VIMODES 1 "register_operand" "vr,vr")
+	       (match_operand:VIMODES 2 "vector_arith_operand" "vr,vi"))
+	     (if_then_else:VIMODES
+	       (match_operand:<VCMPEQUIV> 3 "register_operand" "vm,vm")
+	       (vec_duplicate:VIMODES (const_int 1))
+	       (vec_duplicate:VIMODES (const_int 0))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vadc.vvm\t%0,%1,%2,%3
+   vadc.vim\t%0,%1,%v2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "adc<mode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(plus:VIMODES
+			(plus:VIMODES
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))
+			  (match_operand:VIMODES 1 "register_operand"))
+			(if_then_else:VIMODES
+			  (match_operand:<VCMPEQUIV> 3 "register_operand")
+			  (vec_duplicate:VIMODES (const_int 1))
+			  (vec_duplicate:VIMODES (const_int 0))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*adc<mode>4_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=&vr")
+	(unspec:VIMODES
+	  [(plus:VIMODES
+	     (plus:VIMODES
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	       (match_operand:VIMODES 1 "register_operand" "vr"))
+	     (if_then_else:VIMODES
+	       (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	       (vec_duplicate:VIMODES (const_int 1))
+	       (vec_duplicate:VIMODES (const_int 0))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vadc.vxm\t%0,%1,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "madc<mode>4m"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(plus:VIMODES
+			   (plus:VIMODES
+			     (match_operand:VIMODES 1 "register_operand")
+			     (match_operand:VIMODES 2 "vector_arith_operand"))
+			   (if_then_else:VIMODES
+			     (match_operand:<VCMPEQUIV> 3 "register_operand")
+			     (vec_duplicate:VIMODES (const_int 1))
+			     (vec_duplicate:VIMODES (const_int 0))))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*madc<mode>4m_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr,&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(plus:VIMODES
+		(plus:VIMODES
+		  (match_operand:VIMODES 1 "register_operand" "vr,vr")
+		  (match_operand:VIMODES 2 "vector_arith_operand" "vr,vi"))
+		(if_then_else:VIMODES
+		  (match_operand:<VCMPEQUIV> 3 "register_operand" "vm,vm")
+		  (vec_duplicate:VIMODES (const_int 1))
+		  (vec_duplicate:VIMODES (const_int 0))))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vmadc.vvm\t%0,%1,%2,%3
+   vmadc.vim\t%0,%1,%v2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "madc<mode>4m_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(plus:VIMODES
+			   (plus:VIMODES
+			     (vec_duplicate:VIMODES
+			       (match_operand:<VSUBMODE> 2 "register_operand"))
+			     (match_operand:VIMODES 1 "register_operand"))
+			   (if_then_else:VIMODES
+			     (match_operand:<VCMPEQUIV> 3 "register_operand")
+			     (vec_duplicate:VIMODES (const_int 1))
+			     (vec_duplicate:VIMODES (const_int 0))))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*madc<mode>4m_scalar_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(plus:VIMODES
+		(plus:VIMODES
+		  (vec_duplicate:VIMODES
+		    (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+		   (match_operand:VIMODES 1 "register_operand" "vr"))
+		(if_then_else:VIMODES
+		  (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+		  (vec_duplicate:VIMODES (const_int 1))
+		  (vec_duplicate:VIMODES (const_int 0))))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmadc.vxm\t%0,%1,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "madc<mode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(plus:VIMODES
+			   (match_operand:VIMODES 1 "register_operand")
+			   (match_operand:VIMODES 2 "vector_arith_operand"))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*madc<mode>4_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr,&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(plus:VIMODES
+		(match_operand:VIMODES 1 "register_operand" "vr,vr")
+		(match_operand:VIMODES 2 "vector_arith_operand" "vr,vi"))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vmadc.vv\t%0,%1,%2
+   vmadc.vi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "madc<mode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(plus:VIMODES
+			   (vec_duplicate:VIMODES
+			     (match_operand:<VSUBMODE> 2 "register_operand"))
+			   (match_operand:VIMODES 1 "register_operand"))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*madc<mode>4_scalar_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(plus:VIMODES
+	        (vec_duplicate:VIMODES
+		  (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+		(match_operand:VIMODES 1 "register_operand" "vr"))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmadc.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sbc<mode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(minus:VIMODES
+			(minus:VIMODES
+			  (match_operand:VIMODES 1 "register_operand")
+			  (match_operand:VIMODES 2 "vector_arith_operand"))
+			(if_then_else:VIMODES
+			  (match_operand:<VCMPEQUIV> 3 "register_operand")
+			  (vec_duplicate:VIMODES (const_int 1))
+			  (vec_duplicate:VIMODES (const_int 0))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sbc<mode>4_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=&vr")
+	(unspec:VIMODES
+	  [(minus:VIMODES
+	     (minus:VIMODES
+	       (match_operand:VIMODES 1 "register_operand" "vr")
+	       (match_operand:VIMODES 2 "register_operand" "vr"))
+	     (if_then_else:VIMODES
+	       (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	       (vec_duplicate:VIMODES (const_int 1))
+	       (vec_duplicate:VIMODES (const_int 0))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsbc.vvm\t%0,%1,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sbc<mode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(minus:VIMODES
+			(minus:VIMODES
+			  (match_operand:VIMODES 1 "register_operand")
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand")))
+			(if_then_else:VIMODES
+			  (match_operand:<VCMPEQUIV> 3 "register_operand")
+			  (vec_duplicate:VIMODES (const_int 1))
+			  (vec_duplicate:VIMODES (const_int 0))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sbc<mode>4_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=&vr")
+	(unspec:VIMODES
+	  [(minus:VIMODES
+	     (minus:VIMODES
+	       (match_operand:VIMODES 1 "register_operand" "vr")
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r")))
+	     (if_then_else:VIMODES
+	       (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	       (vec_duplicate:VIMODES (const_int 1))
+	       (vec_duplicate:VIMODES (const_int 0))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsbc.vxm\t%0,%1,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "msbc<mode>4m"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(minus:VIMODES
+			   (minus:VIMODES
+			     (match_operand:VIMODES 1 "register_operand")
+			     (match_operand:VIMODES 2 "vector_arith_operand"))
+			   (if_then_else:VIMODES
+			     (match_operand:<VCMPEQUIV> 3 "register_operand")
+			     (vec_duplicate:VIMODES (const_int 1))
+			     (vec_duplicate:VIMODES (const_int 0))))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*msbc<mode>4m_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(minus:VIMODES
+		(minus:VIMODES
+		  (match_operand:VIMODES 1 "register_operand" "vr")
+		  (match_operand:VIMODES 2 "register_operand" "vr"))
+		(if_then_else:VIMODES
+		  (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+		  (vec_duplicate:VIMODES (const_int 1))
+		  (vec_duplicate:VIMODES (const_int 0))))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmsbc.vvm\t%0,%1,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "msbc<mode>4m_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(minus:VIMODES
+			   (minus:VIMODES
+			     (match_operand:VIMODES 1 "register_operand")
+			     (vec_duplicate:VIMODES
+			       (match_operand:<VSUBMODE> 2 "register_operand")))
+			   (if_then_else:VIMODES
+			     (match_operand:<VCMPEQUIV> 3 "register_operand")
+			     (vec_duplicate:VIMODES (const_int 1))
+			     (vec_duplicate:VIMODES (const_int 0))))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*msbc<mode>4m_scalar_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(minus:VIMODES
+		(minus:VIMODES
+		  (match_operand:VIMODES 1 "register_operand" "vr")
+		  (vec_duplicate:VIMODES
+		    (match_operand:<VSUBMODE> 2 "register_operand" "r")))
+		(if_then_else:VIMODES
+		  (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+		  (vec_duplicate:VIMODES (const_int 1))
+		  (vec_duplicate:VIMODES (const_int 0))))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmsbc.vxm\t%0,%1,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "msbc<mode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(minus:VIMODES
+			   (match_operand:VIMODES 1 "register_operand")
+			   (match_operand:VIMODES 2 "register_operand"))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*madc<mode>4_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(minus:VIMODES
+		(match_operand:VIMODES 1 "register_operand" "vr")
+		(match_operand:VIMODES 2 "register_operand" "vr"))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmsbc.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "msbc<mode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(unspec:<VCMPEQUIV>
+			[(minus:VIMODES
+			   (match_operand:VIMODES 1 "register_operand")
+			   (vec_duplicate:VIMODES
+			     (match_operand:<VSUBMODE> 2 "register_operand")))]
+		       UNSPEC_OVERFLOW)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*msbc<mode>4_scalar_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(unspec:<VCMPEQUIV>
+	     [(minus:VIMODES
+		(match_operand:VIMODES 1 "register_operand" "vr")
+		(vec_duplicate:VIMODES
+		  (match_operand:<VSUBMODE> 2 "register_operand" "r")))]
+	    UNSPEC_OVERFLOW)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmsbc.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+
+;; Floating point ALU instructions (add/sub/rsub/mul/div/rdiv).
+
+(define_expand "<optab><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(any_fop:VFMODES
+			(match_operand:VFMODES 1 "register_operand")
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<optab><mode>3_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(any_fop:VFMODES
+	     (match_operand:VFMODES 1 "register_operand" "vr")
+	     (match_operand:VFMODES 2 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<optab>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_fop:VFMODES
+			  (match_operand:VFMODES 3 "register_operand")
+			  (match_operand:VFMODES 4 "register_operand"))
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<optab><mode>3_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_fop:VFMODES
+	       (match_operand:VFMODES 3 "register_operand" "vr")
+	       (match_operand:VFMODES 4 "register_operand" "vr"))
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<optab>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(any_fcomop:VFMODES
+			(vec_duplicate:VFMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(any_fcomop:VFMODES
+	     (vec_duplicate:VFMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "f"))
+	     (match_operand:VFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<optab>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_fcomop:VFMODES
+			  (vec_duplicate:VFMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand"))
+			  (match_operand:VFMODES 3 "register_operand"))
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_fcomop:VFMODES
+	       (vec_duplicate:VFMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "f"))
+	       (match_operand:VFMODES 3 "register_operand" "vr"))
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<optab>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(any_fnoncomop:VFMODES
+		        (match_operand:VFMODES 1 "register_operand")
+			(vec_duplicate:VFMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(any_fnoncomop:VFMODES
+	     (match_operand:VFMODES 1 "register_operand" "vr")
+	     (vec_duplicate:VFMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "f")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<optab>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_fnoncomop:VFMODES
+			  (match_operand:VFMODES 3 "register_operand")
+			  (vec_duplicate:VFMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_fnoncomop:VFMODES
+	       (match_operand:VFMODES 3 "register_operand" "vr")
+	       (vec_duplicate:VFMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "f")))
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<optab>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "r<optab><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(any_fnoncomop:VFMODES
+			(vec_duplicate:VFMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*r<optab><mode>3_scalar_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(any_fnoncomop:VFMODES
+	     (vec_duplicate:VFMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "f"))
+	     (match_operand:VFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfr<optab>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "r<optab><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_fnoncomop:VFMODES
+			  (vec_duplicate:VFMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand"))
+			  (match_operand:VFMODES 3 "register_operand"))
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*r<optab><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_fnoncomop:VFMODES
+	       (vec_duplicate:VFMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "f"))
+	       (match_operand:VFMODES 3 "register_operand" "vr"))
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfr<optab>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Floating-Point Square-Root Instruction
+(define_expand "sqrt<mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(sqrt:VFMODES
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*sqrt<mode>2_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(sqrt:VFMODES
+	     (match_operand:VFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfsqrt.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sqrt<mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(sqrt:VFMODES
+			  (match_operand:VFMODES 3 "register_operand"))
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*sqrt<mode>2_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (sqrt:VFMODES
+	       (match_operand:VFMODES 3 "register_operand" "vr"))
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfsqrt.v\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Widening Integer Add/Subtract
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_vv"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 1 "register_operand"))
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_vv_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 1 "register_operand" "vr"))
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_vv_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 1 "register_operand"))
+			(any_extend:<VWMODE>
+			  (vec_duplicate:VWIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_vv_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 1 "register_operand" "vr"))
+	     (any_extend:<VWMODE>
+	       (vec_duplicate:VWIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_vv_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand"))
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_vv_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 3 "register_operand" "vr"))
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 4 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_vv_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand"))
+			  (any_extend:<VWMODE>
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_vv_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 3 "register_operand" "vr"))
+	       (any_extend:<VWMODE>
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_wv"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(match_operand:<VWMODE> 1 "register_operand")
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_wv_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (match_operand:<VWMODE> 1 "register_operand" "vr")
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.wv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_wv_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(match_operand:<VWMODE> 1 "register_operand")
+			(any_extend:<VWMODE>
+			  (vec_duplicate:VWIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_wv_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (match_operand:<VWMODE> 1 "register_operand" "vr")
+	     (any_extend:<VWMODE>
+	       (vec_duplicate:VWIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.wx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_wv_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (match_operand:<VWMODE> 3 "register_operand")
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_wv_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (match_operand:<VWMODE> 3 "register_operand" "vr")
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 4 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.wv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "w<add_sub:optab><any_extend:u><mode>_wv_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (match_operand:<VWMODE> 3 "register_operand")
+			  (any_extend:<VWMODE>
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*w<add_sub:optab><any_extend:u><mode>_wv_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (match_operand:<VWMODE> 3 "register_operand" "vr")
+	       (any_extend:<VWMODE>
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vw<add_sub:insn><any_extend:u>.wx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Widening Sign-extend and Zero-extend
+
+(define_expand "wcvt<u><mode><vwmode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(any_extend:<VWMODE>
+			(match_operand:VWIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wcvt<u><mode><vwmode>2_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(any_extend:<VWMODE>
+	     (match_operand:VWIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwcvt<u>.x.x.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wcvt<u><mode><vwmode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 3 "register_operand"))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wcvt<u><mode><vwmode>2_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 3 "register_operand" "vr"))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwcvt<u>.x.x.v\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sz_op>extend<mode><vwmode>2"
+  [(set (reg:<VWVLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(any_extend:<VWMODE>
+			(match_operand:VWIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VWVLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sz_op>extend<mode><vwmode>2_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(any_extend:<VWMODE>
+	     (match_operand:VWIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VWVLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<sz>ext.vf2\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sz_op>extend<mode><vwmode>2_mask"
+  [(set (reg:<VWVLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 3 "register_operand"))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VWVLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sz_op>extend<mode><vwmode>2_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 3 "register_operand" "vr"))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VWVLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<sz>ext.vf2\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Quad-Widening Sign-extend and Zero-extend
+
+(define_expand "<sz_op>extend<mode><vqwmode>2"
+  [(set (reg:<VQWVLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(any_extend:<VQWMODE>
+			(match_operand:VQWIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VQWVLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sz_op>extend<mode><vqwmode>2_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VQWMODE>
+	  [(any_extend:<VQWMODE>
+	     (match_operand:VQWIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VQWVLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<sz>ext.vf4\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sz_op>extend<mode><vqwmode>2_mask"
+  [(set (reg:<VQWVLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(if_then_else:<VQWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_extend:<VQWMODE>
+			  (match_operand:VQWIMODES 3 "register_operand"))
+			(match_operand:<VQWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VQWVLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sz_op>extend<mode><vqwmode>2_mask_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=vr")
+	(unspec:<VQWMODE>
+	  [(if_then_else:<VQWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_extend:<VQWMODE>
+	       (match_operand:VQWIMODES 3 "register_operand" "vr"))
+	     (match_operand:<VQWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VQWVLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<sz>ext.vf4\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Eighth Sign-extend and Zero-extend
+
+(define_expand "<sz_op>extendvnx16qivnx16di2"
+  [(set (reg:VNx16DI VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VNx16DI 0 "register_operand")
+		   (unspec:VNx16DI
+		     [(any_extend:VNx16DI
+			(match_operand:VNx16QI 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:VNx16DI VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sz_op>extendvnx16qivnx16di2_nosetvl"
+  [(set (match_operand:VNx16DI 0 "register_operand" "=&vr")
+	(unspec:VNx16DI
+	  [(any_extend:VNx16DI
+	     (match_operand:VNx16QI 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:VNx16DI VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<sz>ext.vf8\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sz_op>extendvnx16qivnx16di2_mask"
+  [(set (reg:VNx16DI VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VNx16DI 0 "register_operand")
+		   (unspec:VNx16DI
+		     [(if_then_else:VNx16DI
+			(match_operand:VNx16BI 1 "register_operand")
+			(any_extend:VNx16DI
+			  (match_operand:VNx16QI 3 "register_operand"))
+			(match_operand:VNx16DI 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:VNx16DI VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sz_op>extendvnx16qivnx16di2_mask_nosetvl"
+  [(set (match_operand:VNx16DI 0 "register_operand" "=vr")
+	(unspec:VNx16DI
+	  [(if_then_else:VNx16DI
+	     (match_operand:VNx16BI 1 "register_operand" "vm")
+	     (any_extend:VNx16DI
+	       (match_operand:VNx16QI 3 "register_operand" "vr"))
+	     (match_operand:VNx16DI 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:VNx16DI VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<sz>ext.vf8\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector Widening Floating-Point Add/Subtract
+
+(define_expand "fw<optab><mode>_vv"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 1 "register_operand"))
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_vv_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 1 "register_operand" "vr"))
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fw<optab><mode>_vv_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 1 "register_operand"))
+			(float_extend:<VWMODE>
+			  (vec_duplicate:VWFMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_vv_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 1 "register_operand" "vr"))
+	     (float_extend:<VWMODE>
+	       (vec_duplicate:VWFMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "f"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fw<optab><mode>_vv_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 3 "register_operand"))
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_vv_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 3 "register_operand" "vr"))
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 4 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fw<optab><mode>_vv_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 3 "register_operand"))
+			  (float_extend:<VWMODE>
+			    (vec_duplicate:VWFMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_vv_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 3 "register_operand" "vr"))
+	       (float_extend:<VWMODE>
+		 (vec_duplicate:VWFMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "f"))))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fw<optab><mode>_wv"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(match_operand:<VWMODE> 1 "register_operand")
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_wv_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (match_operand:<VWMODE> 1 "register_operand" "vr")
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.wv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fw<optab><mode>_wv_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(match_operand:<VWMODE> 1 "register_operand")
+			(float_extend:<VWMODE>
+			  (vec_duplicate:VWFMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_wv_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (match_operand:<VWMODE> 1 "register_operand" "vr")
+	     (float_extend:<VWMODE>
+	       (vec_duplicate:VWFMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "f"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.wf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fw<optab><mode>_wv_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (match_operand:<VWMODE> 3 "register_operand")
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_wv_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (match_operand:<VWMODE> 3 "register_operand" "vr")
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 4 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.wv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fw<optab><mode>_wv_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(add_sub:<VWMODE>
+			  (match_operand:<VWMODE> 3 "register_operand")
+			  (float_extend:<VWMODE>
+			    (vec_duplicate:VWFMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fw<optab><mode>_wv_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (add_sub:<VWMODE>
+	       (match_operand:<VWMODE> 3 "register_operand" "vr")
+	       (float_extend:<VWMODE>
+		 (vec_duplicate:VWFMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "f"))))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfw<insn>.wf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Integer multiply instructions.
+
+(define_expand "mul<mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(mult:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*mul<mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(mult:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (match_operand:VIMODES 2 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmul.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mul<mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(mult:VIMODES
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*mul<mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(mult:VIMODES
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmul.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mul<mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*mul<mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (match_operand:VIMODES 4 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmul.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mul<mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:VIMODES
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand"))
+			  (match_operand:VIMODES 3 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*mul<mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:VIMODES
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r"))
+		 (match_operand:VIMODES 3 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmul.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Multiply with vector-vector and returning high bits of product
+
+(define_expand "<su>mul<mode>3_highpart"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(truncate:VIMODES
+			(ashiftrt:<EXT_VIMODES>
+			  (mult:<EXT_VIMODES>
+			    (any_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 1 "register_operand"))
+			    (any_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 2 "register_operand")))
+			  (match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<su>mul<mode>3_highpart_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(truncate:VIMODES
+	     (ashiftrt:<EXT_VIMODES>
+	       (mult:<EXT_VIMODES>
+		 (any_extend:<EXT_VIMODES>
+		   (match_operand:VIMODES 1 "register_operand" "vr"))
+		 (any_extend:<EXT_VIMODES>
+		   (match_operand:VIMODES 2 "register_operand" "vr")))
+	       (match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulh<u>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumul<mode>3_highpart"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(truncate:VIMODES
+			(ashiftrt:<EXT_VIMODES>
+			  (mult:<EXT_VIMODES>
+			    (sign_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 1 "register_operand"))
+			    (zero_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 2 "register_operand")))
+			  (match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumul<mode>3_highpart_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(truncate:VIMODES
+	     (ashiftrt:<EXT_VIMODES>
+	       (mult:<EXT_VIMODES>
+		 (sign_extend:<EXT_VIMODES>
+		   (match_operand:VIMODES 1 "register_operand" "vr"))
+		 (zero_extend:<EXT_VIMODES>
+		   (match_operand:VIMODES 2 "register_operand" "vr")))
+	       (match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulhsu.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mulh<v_su><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:VIMODES 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")]
+		       UNSPEC_VMULH)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+  rtx shift = GEN_INT (GET_MODE_UNIT_BITSIZE (<VIMODES:MODE>mode));
+  emit_insn (gen_<v_su>mul<mode>3_highpart (operands[0], operands[1],
+					    operands[2], shift));
+  DONE;
+})
+
+;;Multiply with vector-scalar returning high bits of product
+
+(define_expand "<su>mul<mode>3_highpart_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(truncate:VIMODES
+			(ashiftrt:<EXT_VIMODES>
+			  (mult:<EXT_VIMODES>
+			    (any_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 1 "register_operand"))
+			    (any_extend:<EXT_VIMODES>
+			      (vec_duplicate:VIMODES
+				(match_operand:<VSUBMODE> 2 "register_operand"))))
+			  (match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<su>mul<mode>3_highpart_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(truncate:VIMODES
+	     (ashiftrt:<EXT_VIMODES>
+	       (mult:<EXT_VIMODES>
+		 (any_extend:<EXT_VIMODES>
+		   (match_operand:VIMODES 1 "register_operand" "vr"))
+		 (any_extend:<EXT_VIMODES>
+		   (vec_duplicate:VIMODES
+		     (match_operand:<VSUBMODE> 2 "register_operand" "r"))))
+	       (match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulh<u>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumul<mode>3_highpart_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(truncate:VIMODES
+			(ashiftrt:<EXT_VIMODES>
+			  (mult:<EXT_VIMODES>
+			    (sign_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 1 "register_operand"))
+			    (zero_extend:<EXT_VIMODES>
+			      (vec_duplicate:VIMODES
+				(match_operand:<VSUBMODE> 2 "register_operand"))))
+			(match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumul<mode>3_highpart_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(truncate:VIMODES
+	     (ashiftrt:<EXT_VIMODES>
+	       (mult:<EXT_VIMODES>
+		 (sign_extend:<EXT_VIMODES>
+		   (match_operand:VIMODES 1 "register_operand" "vr"))
+		 (zero_extend:<EXT_VIMODES>
+		   (vec_duplicate:VIMODES
+		     (match_operand:<VSUBMODE> 2 "register_operand" "r"))))
+	       (match_operand:<EXT_VIMODES> 3 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulhsu.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mulh<v_su><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:VIMODES 1 "register_operand")
+			 (match_operand:<VSUBMODE> 2 "register_operand")]
+		       UNSPEC_VMULH)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+  rtx shift = GEN_INT (GET_MODE_UNIT_BITSIZE (<VIMODES:MODE>mode));
+  emit_insn (gen_<v_su>mul<mode>3_highpart_scalar (operands[0], operands[1],
+						   operands[2], shift));
+  DONE;
+})
+
+;;Multiply with vector-vector for vector masking type
+;;and returning high bits of product
+
+(define_expand "<su>mul<mode>3_highpart_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(truncate:VIMODES
+			  (ashiftrt:<EXT_VIMODES>
+			    (mult:<EXT_VIMODES>
+			      (any_extend:<EXT_VIMODES>
+				(match_operand:VIMODES 3 "register_operand"))
+			      (any_extend:<EXT_VIMODES>
+				(match_operand:VIMODES 4 "register_operand")))
+			    (match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand")))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<su>mul<mode>3_highpart_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (truncate:VIMODES
+	       (ashiftrt:<EXT_VIMODES>
+		 (mult:<EXT_VIMODES>
+		   (any_extend:<EXT_VIMODES>
+		     (match_operand:VIMODES 3 "register_operand" "vr"))
+		   (any_extend:<EXT_VIMODES>
+		     (match_operand:VIMODES 4 "register_operand" "vr")))
+		 (match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulh<u>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumul<mode>3_highpart_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(truncate:VIMODES
+			(ashiftrt:<EXT_VIMODES>
+			  (mult:<EXT_VIMODES>
+			    (sign_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 3 "register_operand"))
+			    (zero_extend:<EXT_VIMODES>
+			      (match_operand:VIMODES 4 "register_operand")))
+			  (match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand")))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumul<mode>3_highpart_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (truncate:VIMODES
+	       (ashiftrt:<EXT_VIMODES>
+		 (mult:<EXT_VIMODES>
+		   (sign_extend:<EXT_VIMODES>
+		     (match_operand:VIMODES 3 "register_operand" "vr"))
+		   (zero_extend:<EXT_VIMODES>
+		     (match_operand:VIMODES 4 "register_operand" "vr")))
+		 (match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulhsu.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mulh<v_su><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VIMODES
+			  [(match_operand:VIMODES 3 "register_operand")
+			   (match_operand:VIMODES 4 "register_operand")]
+			 UNSPEC_VMULH)
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+  rtx shift = GEN_INT (GET_MODE_UNIT_BITSIZE (<VIMODES:MODE>mode));
+  emit_insn (gen_<v_su>mul<mode>3_highpart_mask (operands[0], operands[1],
+						 operands[2], operands[3],
+						 operands[4], shift));
+  DONE;
+})
+
+;;Multiply with vector-scalar for vector masking type
+;;and returning high bits of product
+
+(define_expand "<su>mul<mode>3_highpart_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(truncate:VIMODES
+			  (ashiftrt:<EXT_VIMODES>
+			    (mult:<EXT_VIMODES>
+			      (any_extend:<EXT_VIMODES>
+				(match_operand:VIMODES 3 "register_operand"))
+			      (any_extend:<EXT_VIMODES>
+				(vec_duplicate:VIMODES
+				  (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand")))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<su>mul<mode>3_highpart_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (truncate:VIMODES
+	       (ashiftrt:<EXT_VIMODES>
+		 (mult:<EXT_VIMODES>
+		   (any_extend:<EXT_VIMODES>
+		     (match_operand:VIMODES 3 "register_operand" "vr"))
+		   (any_extend:<EXT_VIMODES>
+		     (vec_duplicate:VIMODES
+		       (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+		 (match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulh<u>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumul<mode>3_highpart_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(truncate:VIMODES
+			  (ashiftrt:<EXT_VIMODES>
+			    (mult:<EXT_VIMODES>
+			      (sign_extend:<EXT_VIMODES>
+				(match_operand:VIMODES 3 "register_operand"))
+			      (zero_extend:<EXT_VIMODES>
+				(vec_duplicate:VIMODES
+				  (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+			  (match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand")))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumul<mode>3_highpart_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (truncate:VIMODES
+	       (ashiftrt:<EXT_VIMODES>
+		 (mult:<EXT_VIMODES>
+		   (sign_extend:<EXT_VIMODES>
+		     (match_operand:VIMODES 3 "register_operand" "vr"))
+		   (zero_extend:<EXT_VIMODES>
+		     (vec_duplicate:VIMODES
+		       (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+	       (match_operand:<EXT_VIMODES> 5 "shift_<vmsize>_operand" "Ws<vmsize>")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vmulhsu.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "mulh<v_su><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VIMODES
+			  [(match_operand:VIMODES 3 "register_operand")
+			   (match_operand:<VSUBMODE> 4 "register_operand")]
+			 UNSPEC_VMULH)
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+  rtx shift = GEN_INT (GET_MODE_UNIT_BITSIZE (<VIMODES:MODE>mode));
+  emit_insn (gen_<v_su>mul<mode>3_highpart_scalar_mask (operands[0],
+							operands[1],
+							operands[2],
+							operands[3],
+							operands[4],
+							shift));
+  DONE;
+})
+
+;; FP widen multiply instructions.
+
+(define_expand "fwmul<mode>_vv"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(mult:<VWMODE>
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 1 "register_operand"))
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fwmul<mode>_vv_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(mult:<VWMODE>
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 1 "register_operand" "vr"))
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfwmul.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fwmul<mode>_vv_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(mult:<VWMODE>
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 1 "register_operand"))
+			(float_extend:<VWMODE>
+			  (vec_duplicate:VWFMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fwmul<mode>_vv_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(mult:<VWMODE>
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 1 "register_operand" "vr"))
+	     (float_extend:<VWMODE>
+	       (vec_duplicate:VWFMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "f"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfwmul.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fwmul<mode>_vv_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:<VWMODE>
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 3 "register_operand"))
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fwmul<mode>_vv_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:<VWMODE>
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 3 "register_operand" "vr"))
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 4 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfwmul.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "fwmul<mode>_vv_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:<VWMODE>
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 3 "register_operand"))
+			  (float_extend:<VWMODE>
+			    (vec_duplicate:VWFMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*fwmul<mode>_vv_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:<VWMODE>
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 3 "register_operand" "vr"))
+	       (float_extend:<VWMODE>
+		 (vec_duplicate:VWFMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "f"))))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfwmul.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; FP multiply accumulate instructions.
+
+(define_expand "vf<vfmadd_sub><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(mult:VFMODES
+			  (match_operand:VFMODES 2 "register_operand")
+			  (match_operand:VFMODES 1 "register_operand"))
+			(match_operand:VFMODES 3 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_expand "vf<vfmac><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(mult:VFMODES
+			  (match_operand:VFMODES 2 "register_operand")
+			  (match_operand:VFMODES 3 "register_operand"))
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+;; ??? Should use fma as this is a merged mult/add.  And fms below.
+(define_expand "fm<as><mode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(mult:VFMODES
+			  (match_operand:VFMODES 1 "register_operand")
+			  (match_operand:VFMODES 2 "register_operand"))
+			(match_operand:VFMODES 3 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+;; ??? The constant is pulled out of the loop before it can be used here.
+(define_insn "*fm<as><mode>4_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr,vr")
+	(unspec:VFMODES
+	  [(add_sub:VFMODES
+	     (mult:VFMODES
+	       (match_operand:VFMODES 1 "register_operand" "vr,vr")
+	       (match_operand:VFMODES 2 "register_operand" "0,vr"))
+	     (match_operand:VFMODES 3 "register_operand" "vr,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "@
+   vf<vfmadd_sub>.vv\t%0,%1,%3
+   vf<vfmac>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfmadd_sub><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(mult:VFMODES
+			  (vec_duplicate:VFMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))
+			  (match_operand:VFMODES 1 "register_operand"))
+			(match_operand:VFMODES 3 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_expand "vf<vfmac><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(mult:VFMODES
+			  (vec_duplicate:VFMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))
+			  (match_operand:VFMODES 3 "register_operand"))
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*fm<as><mode>4_scalar_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr,vr")
+	(unspec:VFMODES
+	  [(add_sub:VFMODES
+	     (mult:VFMODES
+	       (vec_duplicate:VFMODES
+		 (match_operand:<VSUBMODE> 1 "register_operand" "f,f"))
+	       (match_operand:VFMODES 2 "register_operand" "0,vr"))
+	     (match_operand:VFMODES 3 "register_operand" "vr,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "@
+   vf<vfmadd_sub>.vf\t%0,%1,%3
+   vf<vfmac>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfmadd_sub><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (mult:VFMODES
+			     (match_operand:VFMODES 3 "register_operand")
+			     (match_operand:VFMODES 2 "register_operand"))
+			   (match_operand:VFMODES 4 "register_operand"))]
+		       UNSPEC_MASK_VFMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfmadd_sub><mode>_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(mult:VFMODES
+		  (match_operand:VFMODES 3 "register_operand" "vr")
+		  (match_operand:VFMODES 2 "register_operand" "0"))
+		(match_operand:VFMODES 4 "register_operand" "vr"))]
+	   UNSPEC_MASK_VFMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfmadd_sub>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfmac><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (mult:VFMODES
+			     (match_operand:VFMODES 3 "register_operand")
+			     (match_operand:VFMODES 4 "register_operand"))
+			   (match_operand:VFMODES 2 "register_operand"))]
+		       UNSPEC_MASK_VFMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfmac><mode>_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(mult:VFMODES
+		  (match_operand:VFMODES 3 "register_operand" "vr")
+		  (match_operand:VFMODES 4 "register_operand" "vr"))
+		(match_operand:VFMODES 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfmac>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfmadd_sub><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (mult:VFMODES
+			     (vec_duplicate:VFMODES
+			       (match_operand:<VSUBMODE> 3 "register_operand"))
+			     (match_operand:VFMODES 2 "register_operand"))
+			   (match_operand:VFMODES 4 "register_operand"))]
+		       UNSPEC_MASK_VFMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfmadd_sub><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(mult:VFMODES
+		  (vec_duplicate:VFMODES
+		    (match_operand:<VSUBMODE> 3 "register_operand" "f"))
+		  (match_operand:VFMODES 2 "register_operand" "0"))
+		(match_operand:VFMODES 4 "register_operand" "vr"))]
+	    UNSPEC_MASK_VFMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfmadd_sub>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfmac><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (mult:VFMODES
+			     (vec_duplicate:VFMODES
+			       (match_operand:<VSUBMODE> 3 "register_operand"))
+			     (match_operand:VFMODES 4 "register_operand"))
+			   (match_operand:VFMODES 2 "register_operand"))]
+		       UNSPEC_MASK_VFMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfmac><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(mult:VFMODES
+		  (vec_duplicate:VFMODES
+		    (match_operand:<VSUBMODE> 3 "register_operand" "f"))
+		  (match_operand:VFMODES 4 "register_operand" "vr"))
+		(match_operand:VFMODES 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfmac>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; FP multiply negate accumulate instructions.
+
+(define_expand "vf<vfnmadd_sub><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(neg:VFMODES
+			  (mult:VFMODES
+			    (match_operand:VFMODES 2 "register_operand")
+			    (match_operand:VFMODES 1 "register_operand")))
+			(match_operand:VFMODES 3 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_expand "vf<vfnmac><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(neg:VFMODES
+			  (mult:VFMODES
+			    (match_operand:VFMODES 2 "register_operand")
+			    (match_operand:VFMODES 3 "register_operand")))
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_expand "fnm<sa><mode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(neg:VFMODES
+			  (mult:VFMODES
+			    (match_operand:VFMODES 1 "register_operand")
+			    (match_operand:VFMODES 2 "register_operand")))
+			(match_operand:VFMODES 3 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*fnm<sa><mode>4_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr,vr")
+	(unspec:VFMODES
+	  [(add_sub:VFMODES
+	     (neg:VFMODES
+	       (mult:VFMODES
+		 (match_operand:VFMODES 1 "register_operand" "vr,vr")
+		 (match_operand:VFMODES 2 "register_operand" "0,vr")))
+	     (match_operand:VFMODES 3 "register_operand" "vr,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "@
+   vf<vfnmadd_sub>.vv\t%0,%1,%3
+   vf<vfnmac>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfnmadd_sub><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(neg:VFMODES
+			  (mult:VFMODES
+			    (vec_duplicate:VFMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand"))
+			    (match_operand:VFMODES 1 "register_operand")))
+			(match_operand:VFMODES 3 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_expand "vf<vfnmac><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(add_sub:VFMODES
+			(neg:VFMODES
+			  (mult:VFMODES
+			    (vec_duplicate:VFMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand"))
+			    (match_operand:VFMODES 3 "register_operand")))
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*fnm<sa><mode>4_scalar_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr,vr")
+	(unspec:VFMODES
+	  [(add_sub:VFMODES
+	     (neg:VFMODES
+	       (mult:VFMODES
+		 (vec_duplicate:VFMODES
+		   (match_operand:<VSUBMODE> 1 "register_operand" "f,f"))
+		 (match_operand:VFMODES 2 "register_operand" "0,vr")))
+	       (match_operand:VFMODES 3 "register_operand" "vr,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "@
+   vf<vfnmadd_sub>.vf\t%0,%1,%3
+   vf<vfnmac>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfnmadd_sub><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (neg:VFMODES
+			     (mult:VFMODES
+			       (match_operand:VFMODES 3 "register_operand")
+			       (match_operand:VFMODES 2 "register_operand")))
+			   (match_operand:VFMODES 4 "register_operand"))]
+		       UNSPEC_MASK_VFNMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfnmadd_sub><mode>_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(neg:VFMODES
+		  (mult:VFMODES
+		    (match_operand:VFMODES 3 "register_operand" "vr")
+		    (match_operand:VFMODES 2 "register_operand" "0")))
+		(match_operand:VFMODES 4 "register_operand" "vr"))]
+	    UNSPEC_MASK_VFNMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfnmadd_sub>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfnmac><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (neg:VFMODES
+			     (mult:VFMODES
+			       (match_operand:VFMODES 3 "register_operand")
+			       (match_operand:VFMODES 4 "register_operand")))
+			   (match_operand:VFMODES 2 "register_operand"))]
+		       UNSPEC_MASK_VFNMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfnmac><mode>_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(neg:VFMODES
+		  (mult:VFMODES
+		    (match_operand:VFMODES 3 "register_operand" "vr")
+		    (match_operand:VFMODES 4 "register_operand" "vr")))
+		(match_operand:VFMODES 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFNMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfnmac>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfnmadd_sub><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (neg:VFMODES
+			     (mult:VFMODES
+			       (vec_duplicate:VFMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand"))
+			       (match_operand:VFMODES 2 "register_operand")))
+			     (match_operand:VFMODES 4 "register_operand"))]
+		       UNSPEC_MASK_VFNMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfnmadd_sub><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(neg:VFMODES
+		  (mult:VFMODES
+		    (vec_duplicate:VFMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "f"))
+		    (match_operand:VFMODES 2 "register_operand" "0")))
+		(match_operand:VFMODES 4 "register_operand" "vr"))]
+	    UNSPEC_MASK_VFNMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfnmadd_sub>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vf<vfnmac><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:VFMODES
+			   (neg:VFMODES
+			     (mult:VFMODES
+			       (vec_duplicate:VFMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand"))
+			       (match_operand:VFMODES 4 "register_operand")))
+			   (match_operand:VFMODES 2 "register_operand"))]
+		       UNSPEC_MASK_VFNMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vf<vfnmac><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:VFMODES
+		(neg:VFMODES
+		  (mult:VFMODES
+		    (vec_duplicate:VFMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "f"))
+		    (match_operand:VFMODES 4 "register_operand" "vr")))
+		(match_operand:VFMODES 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFNMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vf<vfnmac>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; FP widen multiply accumulate instructions.
+
+(define_expand "vfw<vfmac><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(mult:<VWMODE>
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 2 "register_operand"))
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 3 "register_operand")))
+			 (match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfmac><mode>_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (mult:<VWMODE>
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 1 "register_operand" "vr"))
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 2 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfmac>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfw<vfmac><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(mult:<VWMODE>
+			  (vec_duplicate:<VWMODE>
+			    (float_extend:<VWMODE>
+			      (match_operand:<VSUBMODE> 2 "register_operand")))
+			  (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 3 "register_operand")))
+			(match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfmac><mode>_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (mult:<VWMODE>
+	       (vec_duplicate:<VWMODE>
+		 (float_extend:<VWMODE>
+		   (match_operand:<VSUBMODE> 2 "register_operand" "f")))
+	       (float_extend:<VWMODE>
+		 (match_operand:VWFMODES 1 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfmac>.vf\t%0,%2,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfw<vfmac><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:<VWMODE>
+			   (mult:<VWMODE>
+			     (float_extend:<VWMODE>
+			       (match_operand:VWFMODES 3 "register_operand"))
+			     (float_extend:<VWMODE>
+			       (match_operand:VWFMODES 4 "register_operand")))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_MASK_VFWMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfmac><mode>_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:<VWMODE>
+		(mult:<VWMODE>
+		  (float_extend:<VWMODE>
+		    (match_operand:VWFMODES 3 "register_operand" "vr"))
+		  (float_extend:<VWMODE>
+		    (match_operand:VWFMODES 4 "register_operand" "vr")))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFWMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfmac>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfw<vfmac><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:<VWMODE>
+			   (mult:<VWMODE>
+			     (vec_duplicate:<VWMODE>
+			       (float_extend:<VWMODE>
+				 (match_operand:<VSUBMODE> 3 "register_operand")))
+			       (float_extend:<VWMODE>
+			    (match_operand:VWFMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_MASK_VFWMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfmac><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:<VWMODE>
+		(mult:<VWMODE>
+		  (vec_duplicate:<VWMODE>
+		    (float_extend:<VWMODE>
+		      (match_operand:<VSUBMODE> 4 "register_operand" "f")))
+		    (float_extend:<VWMODE>
+		      (match_operand:VWFMODES 3 "register_operand" "vr")))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFWMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfmac>.vf\t%0,%4,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; FP widen negate multiply accumulate instructions.
+
+(define_expand "vfw<vfnmac><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(neg:<VWMODE>
+			  (mult:<VWMODE>
+			    (float_extend:<VWMODE>
+			      (match_operand:VWFMODES 2 "register_operand"))
+			    (float_extend:<VWMODE>
+			      (match_operand:VWFMODES 3 "register_operand"))))
+			(match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfnmac><mode>_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (neg:<VWMODE>
+	       (mult:<VWMODE>
+		 (float_extend:<VWMODE>
+		   (match_operand:VWFMODES 1 "register_operand" "vr"))
+		 (float_extend:<VWMODE>
+		   (match_operand:VWFMODES 2 "register_operand" "vr"))))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfnmac>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfw<vfnmac><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(add_sub:<VWMODE>
+			(neg:<VWMODE>
+			  (mult:<VWMODE>
+			    (vec_duplicate:<VWMODE>
+			      (float_extend:<VWMODE>
+				(match_operand:<VSUBMODE> 2 "register_operand")))
+			      (float_extend:<VWMODE>
+				(match_operand:VWFMODES 3 "register_operand"))))
+		        (match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfnmac><mode>_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(add_sub:<VWMODE>
+	     (neg:<VWMODE>
+	       (mult:<VWMODE>
+		 (vec_duplicate:<VWMODE>
+		   (float_extend:<VWMODE>
+		     (match_operand:<VSUBMODE> 2 "register_operand" "f")))
+		   (float_extend:<VWMODE>
+		     (match_operand:VWFMODES 1 "register_operand" "vr"))))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfnmac>.vf\t%0,%2,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfw<vfnmac><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:<VWMODE>
+			   (neg:<VWMODE>
+			     (mult:<VWMODE>
+			       (float_extend:<VWMODE>
+				 (match_operand:VWFMODES 3 "register_operand"))
+			       (float_extend:<VWMODE>
+				 (match_operand:VWFMODES 4 "register_operand"))))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_MASK_VFWNMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfnmac><mode>_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:<VWMODE>
+		(neg:<VWMODE>
+		  (mult:<VWMODE>
+		    (float_extend:<VWMODE>
+		      (match_operand:VWFMODES 3 "register_operand" "vr"))
+		    (float_extend:<VWMODE>
+		      (match_operand:VWFMODES 4 "register_operand" "vr"))))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFWNMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfnmac>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfw<vfnmac><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (add_sub:<VWMODE>
+			   (neg:<VWMODE>
+			     (mult:<VWMODE>
+			       (vec_duplicate:<VWMODE>
+				 (float_extend:<VWMODE>
+				   (match_operand:<VSUBMODE> 3 "register_operand")))
+				 (float_extend:<VWMODE>
+				   (match_operand:VWFMODES 4 "register_operand"))))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_MASK_VFWNMACC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfw<vfnmac><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (add_sub:<VWMODE>
+		(neg:<VWMODE>
+		  (mult:<VWMODE>
+		    (vec_duplicate:<VWMODE>
+		      (float_extend:<VWMODE>
+			(match_operand:<VSUBMODE> 4 "register_operand" "f")))
+		      (float_extend:<VWMODE>
+			(match_operand:VWFMODES 3 "register_operand" "vr"))))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_MASK_VFWNMACC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfw<vfnmac>.vf\t%0,%4,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Integer merge instructions.
+
+(define_expand "vec_duplicate<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(vec_duplicate:VIMODES
+			(match_operand:<VSUBMODE> 1 "reg_or_simm5_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "vec_duplicate<mode>_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(vec_duplicate:VIMODES
+	     (match_operand:<VSUBMODE> 1 "reg_or_simm5_operand" "r,Ws5"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vmv.v.x\t%0,%1
+   vmv.v.i\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; FP merge instructions.
+
+(define_expand "vec_duplicate<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(vec_duplicate:VFMODES
+			(match_operand:<VSUBMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "vec_duplicate<mode>_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr,*vr")
+	(unspec:VFMODES
+	  [(vec_duplicate:VFMODES
+	     (match_operand:<VSUBMODE> 1 "register_operand" "f,r"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "@
+   vfmv.v.f\t%0,%1
+   vmv.v.x\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; FP sign-injection instructions.
+
+(define_expand "<copysign><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:VFMODES 1 "register_operand")
+			 (match_operand:VFMODES 2 "register_operand")]
+		       UNSPEC_COPYSIGNS)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<copysign><mode>3_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:VFMODES 1 "register_operand" "vr")
+	      (match_operand:VFMODES 2 "register_operand" "vr")]
+	    UNSPEC_COPYSIGNS)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfsgnj<nx>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<copysign><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:VFMODES 1 "register_operand")
+			 (vec_duplicate:VFMODES
+			   (match_operand:<VSUBMODE> 2 "register_operand"))]
+		       UNSPEC_COPYSIGNS)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<copysign><mode>3_scalar_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:VFMODES 1 "register_operand" "vr")
+	      (vec_duplicate:VFMODES
+		(match_operand:<VSUBMODE> 2 "register_operand" "f"))]
+	    UNSPEC_COPYSIGNS)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfsgnj<nx>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<copysign><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VFMODES
+			  [(match_operand:VFMODES 3 "register_operand")
+			   (match_operand:VFMODES 4 "register_operand")]
+			 UNSPEC_COPYSIGNS)
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<copysign><mode>3_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VFMODES
+	       [(match_operand:VFMODES 3 "register_operand" "vr")
+		(match_operand:VFMODES 4 "register_operand" "vr")]
+	      UNSPEC_COPYSIGNS)
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfsgnj<nx>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<copysign><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VFMODES
+			  [(match_operand:VFMODES 3 "register_operand")
+			   (vec_duplicate:VFMODES
+			     (match_operand:<VSUBMODE> 4 "register_operand"))]
+			 UNSPEC_COPYSIGNS)
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<copysign><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VFMODES
+	       [(match_operand:VFMODES 3 "register_operand" "vr")
+		(vec_duplicate:VFMODES
+		  (match_operand:<VSUBMODE> 4 "register_operand" "f"))]
+	      UNSPEC_COPYSIGNS)
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfsgnj<nx>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector comparison support.
+
+;; XXX: Must expand unsupported comparison type.
+;; Std pattern vec_cmpu and vec_cmpeq might need implement,
+;; but we don't implement auto-vec yet, so may not meaningful yet?
+(define_expand "vec_cmp<mode><vmaskmode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(match_operator:<VCMPEQUIV> 1 "ordered_comparison_operator"
+			[(match_operand:VIMODES 2 "register_operand")
+			 (match_operand:VIMODES 3 "vector_arith_operand")])
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+  if (ltge_operator (operands[1], <VCMPEQUIV>mode)
+      && !ltge_vector_arith_operand(operands[3], <MODE>mode))
+    FAIL;
+})
+
+(define_insn "*vec_cmp<mode><vmaskmode>_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr, &vr")
+	(unspec:<VCMPEQUIV>
+	  [(match_operator:<VCMPEQUIV> 1 "vector_comparison_operator"
+	     [(match_operand:VIMODES 2 "register_operand" "vr, vr")
+	      (match_operand:VIMODES 3 "vector_arith_operand" "vr, vi")])
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "@
+  vms%B1.vv\t%0,%2,%3
+  vms%B1.vi\t%0,%2,%v3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "*ltge<mode><vmaskmode>_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr, &vr")
+	(unspec:<VCMPEQUIV>
+	  [(match_operator:<VCMPEQUIV> 1 "ltge_operator"
+	     [(match_operand:VIMODES 2 "register_operand" "vr, vr")
+	      (match_operand:VIMODES 3 "ltge_vector_arith_operand" "vr, vj")])
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "@
+  vms%B1.vv\t%0,%2,%3
+  vms%B1.vi\t%0,%2,%v3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "vec_cmp<mode><vmaskmode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 5 "register_operand")
+			(match_operator:<VCMPEQUIV> 1 "ordered_comparison_operator"
+			  [(match_operand:VIMODES 2 "register_operand")
+			   (match_operand:VIMODES 3 "vector_arith_operand")])
+			(match_operand:<VCMPEQUIV> 4 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+  if (ltge_operator (operands[1], <VCMPEQUIV>mode)
+      && !ltge_vector_arith_operand(operands[3], <MODE>mode))
+    FAIL;
+})
+
+(define_insn "*vec_cmp<mode><vmaskmode>_mask_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr, &vr")
+	(unspec:<VCMPEQUIV>
+	  [(if_then_else:<VCMPEQUIV>
+	     (match_operand:<VCMPEQUIV> 5 "register_operand" "vm, vm")
+	     (match_operator:<VCMPEQUIV> 1 "vector_comparison_operator"
+	       [(match_operand:VIMODES 2 "register_operand" "vr, vr")
+		(match_operand:VIMODES 3 "vector_arith_operand" "vr, vi")])
+	     (match_operand:<VCMPEQUIV> 4 "register_operand" "0, 0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "@
+  vms%B1.vv\t%0,%2,%3,%5.t
+  vms%B1.vi\t%0,%2,%v3,%5.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "*ltge<mode><vmaskmode>_mask_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr, &vr")
+	(unspec:<VCMPEQUIV>
+	  [(if_then_else:<VCMPEQUIV>
+	     (match_operand:<VCMPEQUIV> 5 "register_operand" "vm, vm")
+	     (match_operator:<VCMPEQUIV> 1 "ltge_operator"
+	       [(match_operand:VIMODES 2 "register_operand" "vr, vr")
+		(match_operand:VIMODES 3 "ltge_vector_arith_operand" "vr, vj")])
+	     (match_operand:<VCMPEQUIV> 4 "register_operand" "0, 0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "@
+  vms%B1.vv\t%0,%2,%3,%5.t
+  vms%B1.vi\t%0,%2,%v3,%5.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "*vec_cmp<mode><vmaskmode>_scalar_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(match_operator:<VCMPEQUIV> 1 "ordered_comparison_operator"
+	     [(match_operand:VIMODES 2 "register_operand" "vr")
+	      (vec_duplicate:VIMODES
+		(match_operand:<VSUBMODE> 3 "register_operand" "r"))])
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "vms%B1.vx\t%0,%2,%3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "*vec_cmp<mode><vmaskmode>_scalar_mask_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(if_then_else:<VCMPEQUIV>
+	     (match_operand:<VCMPEQUIV> 5 "register_operand" "vm")
+	     (match_operator:<VCMPEQUIV> 1 "ordered_comparison_operator"
+	       [(match_operand:VIMODES 2 "register_operand" "vr")
+		(vec_duplicate:VIMODES
+		  (match_operand:<VSUBMODE> 3 "register_operand" "r"))])
+	     (match_operand:<VCMPEQUIV> 4 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "vms%B1.vx\t%0,%2,%3,%5.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "vec_cmp<mode><vmaskmode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(match_operator:<VCMPEQUIV> 1 "comparison_operator"
+			[(match_operand:VFMODES 2 "register_operand")
+			 (match_operand:VFMODES 3 "register_operand")])
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vec_cmp<mode><vmaskmode>_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(match_operator:<VCMPEQUIV> 1 "comparison_operator"
+	     [(match_operand:VFMODES 2 "register_operand" "vr")
+	      (match_operand:VFMODES 3 "register_operand" "vr")])
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+ "vmf%B1.vv\t%0,%2,%3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "vec_cmp<mode><vmaskmode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(match_operator:<VCMPEQUIV> 1 "comparison_operator"
+			[(match_operand:VFMODES 2 "register_operand")
+			 (vec_duplicate:VFMODES
+			   (match_operand:<VSUBMODE> 3 "register_operand"))])
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vec_cmp<mode><vmaskmode>_scalar_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(match_operator:<VCMPEQUIV> 1 "comparison_operator"
+	     [(match_operand:VFMODES 2 "register_operand" "vr")
+	      (vec_duplicate:VFMODES
+		(match_operand:<VSUBMODE> 3 "register_operand" "f"))])
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+ "vmf%B1.vf\t%0,%2,%3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "vec_cmp<mode><vmaskmode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 5 "register_operand")
+			(match_operator:<VCMPEQUIV> 1 "comparison_operator"
+			  [(match_operand:VFMODES 2 "register_operand")
+			   (match_operand:VFMODES 3 "register_operand")])
+			(match_operand:<VCMPEQUIV> 4 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vec_cmp<mode><vmaskmode>_mask_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(if_then_else:<VCMPEQUIV>
+	     (match_operand:<VCMPEQUIV> 5 "register_operand" "vm")
+	     (match_operator:<VCMPEQUIV> 1 "comparison_operator"
+	       [(match_operand:VFMODES 2 "register_operand" "vr")
+		(match_operand:VFMODES 3 "register_operand" "vr")])
+	     (match_operand:<VCMPEQUIV> 4 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+ "vmf%B1.vv\t%0,%2,%3,%5.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "vec_cmp<mode><vmaskmode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 5 "register_operand")
+			(match_operator:<VCMPEQUIV> 1 "comparison_operator"
+			  [(match_operand:VFMODES 2 "register_operand")
+			   (vec_duplicate:VFMODES
+			     (match_operand:<VSUBMODE> 3 "register_operand"))])
+			(match_operand:<VCMPEQUIV> 4 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vec_cmp<mode><vmaskmode>_scalar_mask_nosetvl"
+  [(set (match_operand:<VCMPEQUIV> 0 "register_operand" "=&vr")
+	(unspec:<VCMPEQUIV>
+	  [(if_then_else:<VCMPEQUIV>
+	     (match_operand:<VCMPEQUIV> 5 "register_operand" "vm")
+	     (match_operator:<VCMPEQUIV> 1 "comparison_operator"
+	       [(match_operand:VFMODES 2 "register_operand" "vr")
+		(vec_duplicate:VFMODES
+		  (match_operand:<VSUBMODE> 3 "register_operand" "f"))])
+	     (match_operand:<VCMPEQUIV> 4 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+ "vmf%B1.vf\t%0,%2,%3,%5.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "mov<mode>cc"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(match_operand:VIMODES 3 "vector_arith_operand")
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+})
+
+(define_insn "mov<mode>cc_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vd,vd")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 3 "register_operand" "vm,vm")
+	     (match_operand:VIMODES 2 "vector_arith_operand" "vr, vi")
+	     (match_operand:VIMODES 1 "register_operand" "vr,vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "@
+  vmerge.vvm\t%0,%1,%2,%3
+  vmerge.vim\t%0,%1,%2,%3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "mov<mode>cc_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 3 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+})
+
+(define_insn "mov<mode>cc_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vd")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "vmerge.vxm\t%0,%1,%2,%3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+;; Vector FP merge.
+
+(define_expand "mov<mode>cc"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(match_operand:VFMODES 3 "register_operand")
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "mov<mode>cc_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vd")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	     (match_operand:VFMODES 2 "register_operand" "vr")
+	     (match_operand:VFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+ "vmerge.vvm\t%0,%1,%2,%3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "mov<mode>cc_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(vec_duplicate:VFMODES
+			  (match_operand:<VSUBMODE> 3 "register_operand"))
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "mov<mode>cc_scalar_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vd")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 3 "register_operand" "vm")
+	     (vec_duplicate:VFMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "f"))
+	     (match_operand:VFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+ "vfmerge.vfm\t%0,%1,%2,%3"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+;; ??? Missing splitters.
+
+(define_expand "vcond<mode><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (set (match_operand:VIMODES 0 "register_operand")
+	(if_then_else:VIMODES
+	  (match_operator 3 "comparison_operator"
+	    [(match_operand:VIMODES 4 "register_operand")
+	     (match_operand:VIMODES 5 "register_operand")])
+	  (match_operand:VIMODES 1 "register_operand")
+	  (match_operand:VIMODES 2 "register_operand")))]
+ "TARGET_VECTOR"
+{
+  rtx tmp = gen_reg_rtx (<VCMPEQUIV>mode);
+  emit_insn (gen_vec_cmp<mode><vmaskmode> (tmp, operands[3],
+					   operands[4], operands[5]));
+  emit_insn (gen_mov<mode>cc_nosetvl (operands[0], operands[1],
+				      operands[2], tmp));
+  DONE;
+})
+
+(define_expand "vcond<mode><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (set (match_operand:VFMODES 0 "register_operand")
+	(if_then_else:VFMODES
+	  (match_operator 3 "comparison_operator"
+	    [(match_operand:VFMODES 4 "register_operand")
+	     (match_operand:VFMODES 5 "register_operand")])
+	  (match_operand:VFMODES 1 "register_operand")
+	  (match_operand:VFMODES 2 "register_operand")))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+  rtx tmp = gen_reg_rtx (<VCMPEQUIV>mode);
+  emit_insn (gen_vec_cmp<mode><vmaskmode> (tmp, operands[3],
+					   operands[4], operands[5]));
+  emit_insn (gen_mov<mode>cc_nosetvl (operands[0], operands[1],
+				      operands[2], tmp));
+  DONE;
+})
+
+(define_expand "vcond<mode><vintequiv>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (set (match_operand:VFMODES 0 "register_operand")
+	(if_then_else:VFMODES
+	  (match_operator 3 "comparison_operator"
+	    [(match_operand:<VINTEQUIV> 4 "register_operand")
+	     (match_operand:<VINTEQUIV> 5 "register_operand")])
+	  (match_operand:VFMODES 1 "register_operand")
+	  (match_operand:VFMODES 2 "register_operand")))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+  rtx tmp = gen_reg_rtx (<VCMPEQUIV>mode);
+  emit_insn (gen_vec_cmp<vintequiv><vmaskmode> (tmp, operands[3],
+					   operands[4], operands[5]));
+  emit_insn (gen_mov<mode>cc_nosetvl (operands[0], operands[1],
+				      operands[2], tmp));
+  DONE;
+})
+
+(define_expand "vcond<vintequiv><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (set (match_operand:<VINTEQUIV> 0 "register_operand")
+	(if_then_else:<VINTEQUIV>
+	  (match_operator 3 "comparison_operator"
+	    [(match_operand:VFMODES 4 "register_operand")
+	     (match_operand:VFMODES 5 "register_operand")])
+	  (match_operand:<VINTEQUIV> 1 "register_operand")
+	  (match_operand:<VINTEQUIV> 2 "register_operand")))]
+ "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+  rtx tmp = gen_reg_rtx (<VCMPEQUIV>mode);
+  emit_insn (gen_vec_cmp<mode><vmaskmode> (tmp, operands[3],
+					   operands[4], operands[5]));
+  emit_insn (gen_mov<vintequiv>cc_nosetvl (operands[0], operands[1],
+					   operands[2], tmp));
+  DONE;
+})
+
+;; Vector mask operations
+
+(define_expand "clr<mode>"
+  [(set (match_operand:VMASKMODES 0 "register_operand")
+	(unspec:VMASKMODES
+	  [(unspec:VMASKMODES [(match_dup 1)] UNSPEC_VCLR)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+{
+  /* Using gen function instead of write (const_vector [(const_int 0)]) because,
+     emit-rtl.c:gen_rtx_CONST_VECTOR will check the number of elements is same
+     as NUNIT of mode.  */
+  operands[1] = gen_const_vec_duplicate (<MODE>mode, const0_rtx);
+})
+
+(define_insn "*clr<mode>"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=vr")
+	(unspec:VMASKMODES
+	  [(unspec:VMASKMODES
+	     [(match_operand:VMASKMODES 1 "const_vector_int_0_operand" "v0")]
+	    UNSPEC_VCLR)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+  "vmclr.m\t%0"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "set<mode>"
+  [(set (match_operand:VMASKMODES 0 "register_operand")
+	(unspec:VMASKMODES
+	  [(unspec:VMASKMODES [(match_dup 1)] UNSPEC_VSET)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+{
+  /* Using gen function instead of write (const_vector [(const_int 0)]) because,
+     emit-rtl.c:gen_rtx_CONST_VECTOR will check the number of elements is same
+     as NUNIT of mode.  */
+  operands[1] = gen_const_vec_duplicate (<MODE>mode, const1_rtx);
+})
+
+(define_insn "*set<mode>"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=vr")
+	(unspec:VMASKMODES
+	  [(unspec:VMASKMODES
+	     [(match_operand:VMASKMODES 1 "const_vector_int_1_operand" "v1")]
+	    UNSPEC_VSET)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+  "vmset.m\t%0"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "not<mode>"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=vr")
+	(unspec:VMASKMODES
+	  [(not:VMASKMODES
+	     (match_operand:VMASKMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+  "vmnot.m\t%0,%1"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+;; Vector Mask-Register Logical Instructions
+
+(define_insn "<optab><mode>3"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=vr")
+	(unspec:VMASKMODES
+	  [(any_bitwise:VMASKMODES
+	     (match_operand:VMASKMODES 1 "register_operand" "vr")
+	     (match_operand:VMASKMODES 2 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vm<insn>.mm\t%0,%1,%2"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "<invmaskop><mode>3"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=vr")
+	(unspec:VMASKMODES
+	  [(not:VMASKMODES
+	     (any_bitwise:VMASKMODES
+	       (match_operand:VMASKMODES 1 "register_operand" "vr")
+	       (match_operand:VMASKMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vm<invmaskop>.mm\t%0,%1,%2"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "<insn>not<mode>3"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=vr")
+	(unspec:VMASKMODES
+	  [(any_opnot:VMASKMODES
+	     (match_operand:VMASKMODES 1 "register_operand" "vr")
+	     (not:VMASKMODES
+	       (match_operand:VMASKMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vm<insn>not.mm\t%0,%1,%2"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+;; Adapter for built-in function.
+(define_expand "or<mode>3"
+  [(set (match_operand:VMASKMODES 0 "register_operand")
+	(unspec:VMASKMODES
+	  [(ior:VMASKMODES
+	     (match_operand:VMASKMODES 1 "register_operand")
+	     (match_operand:VMASKMODES 2 "register_operand"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+{
+})
+
+;; Misc Vector Mask-Register Operations
+;; XXX: The popcount operation will be optimize from its scalar mode,
+;; instead operands[1]'s vector mode. There is problem, we get wrong
+;; upper bound value from mode of popcount operation.
+(define_insn "popc<VMASKMODES:mode>2_<P:mode>"
+  [(set (match_operand:P 0 "register_operand" "=r")
+	(unspec:P
+	  [(unspec:P
+	     [(match_operand:VMASKMODES 1 "register_operand" "vr")]
+	    UNSPEC_VPOPCOUNT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vpopc.m\t%0,%1"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "popc<VMASKMODES:mode>2_<P:mode>_mask"
+  [(set (match_operand:P 0 "register_operand" "=r")
+	(unspec:P
+	  [(unspec:P
+	     [(and:VMASKMODES
+		(match_operand:VMASKMODES 1 "register_operand" "vm")
+		(match_operand:VMASKMODES 2 "register_operand" "vr"))]
+	    UNSPEC_VPOPCOUNT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vpopc.m\t%0,%2,%1.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "first<VMASKMODES:mode>2_<P:mode>"
+  [(set (match_operand:P 0 "register_operand" "=r")
+	(unspec:P
+	  [(unspec:P
+	     [(match_operand:VMASKMODES 1 "register_operand" "vr")]
+	    UNSPEC_FIRST)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vfirst.m\t%0,%1"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "first<VMASKMODES:mode>2_<P:mode>_mask"
+  [(set (match_operand:P 0 "register_operand" "=r")
+	(unspec:P
+	  [(unspec:P
+	     [(and:VMASKMODES
+		(match_operand:VMASKMODES 1 "register_operand" "vm")
+		(match_operand:VMASKMODES 2 "register_operand" "vr"))]
+	    UNSPEC_FIRST)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vfirst.m\t%0,%2,%1.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "iota<mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")]
+		       UNSPEC_IOTA)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+})
+
+(define_insn "*iota<mode>2_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=&vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vr")]
+	    UNSPEC_IOTA)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "viota.m\t%0,%1"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "iota<mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (match_operand:<VCMPEQUIV> 3 "register_operand")]
+		       UNSPEC_IOTA)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+})
+
+(define_insn "*iota<mode>2_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:VIMODES 2 "register_operand" "0")
+	      (match_operand:<VCMPEQUIV> 3 "register_operand" "vr")]
+	    UNSPEC_IOTA)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "viota.m\t%0,%3,%1.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "vid<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES [(const_int 0)] UNSPEC_VID)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vid<mode>_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES [(const_int 0)] UNSPEC_VID)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "vid.v\t%0"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_expand "vid<mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")]
+		       UNSPEC_VID)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+ "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vid<mode>_mask"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:VIMODES 2 "register_operand" "0")]
+	    UNSPEC_VID)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+ "TARGET_VECTOR"
+ "vid.v\t%0,%1.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "<misc_maskop><mode>"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=&vr")
+	(unspec:VMASKMODES
+	  [(unspec:VMASKMODES
+	     [(match_operand:VMASKMODES 1 "register_operand" "vr")]
+	    MISC_MASK_OP)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vm<misc_maskop>.m\t%0,%1"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+(define_insn "<misc_maskop><mode>_mask"
+  [(set (match_operand:VMASKMODES 0 "register_operand" "=&vr")
+	(unspec:VMASKMODES
+	  [(if_then_else:VMASKMODES
+	     (match_operand:VMASKMODES 1 "register_operand" "vm")
+	     (unspec:VMASKMODES
+	       [(match_operand:VMASKMODES 3 "register_operand" "vr")]
+	      MISC_MASK_OP)
+	     (match_operand:VMASKMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+ "TARGET_VECTOR"
+ "vm<misc_maskop>.m\t%0,%3,%1.t"
+ [(set_attr "type" "vector")
+  (set_attr "mode" "none")])
+
+;; Adaptor for built-in functions
+
+;; Integer compare functions
+
+(define_expand "<icmp><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(any_cmp:<VCMPEQUIV>
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "<ltge>vector_arith_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_expand "<icmp><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_cmp:<VCMPEQUIV>
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "<ltge>vector_arith_operand"))
+			(match_operand:<VCMPEQUIV> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_expand "<icmp><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(cmp_except_ge:<VCMPEQUIV>
+			(match_operand:VIMODES 1 "register_operand")
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+;; We don't have compare with grater then.
+;; Using vmslt and vmnand to replace it.
+(define_expand "sge<u><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(any_lt:<VCMPEQUIV>
+			(match_operand:VIMODES 1 "register_operand")
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])
+   (set (match_dup 0)
+	(unspec:<VCMPEQUIV>
+	  [(not:<VCMPEQUIV> (and:<VCMPEQUIV> (match_dup 0) (match_dup 0)))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+{
+})
+
+;; We don't have compare with grater then.
+;; Using vmslt and vmxor to replace it.
+(define_expand "<icmp><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(cmp_except_ge:<VCMPEQUIV>
+			  (match_operand:VIMODES 3 "register_operand")
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			(match_operand:<VCMPEQUIV> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_expand "sge<u><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_lt:<VCMPEQUIV>
+			  (match_operand:VIMODES 3 "register_operand")
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			(match_operand:<VCMPEQUIV> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])
+   (set (match_dup 0)
+	(unspec:<VCMPEQUIV>
+	  [(xor:<VCMPEQUIV> (match_dup 0) (match_dup 1))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))]
+  "TARGET_VECTOR"
+{
+})
+
+;; FP compare functions
+
+(define_expand "<fcmp><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(signed_cmp:<VCMPEQUIV>
+			(match_operand:VFMODES 1 "register_operand")
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_expand "<fcmp><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(signed_cmp:<VCMPEQUIV>
+			  (match_operand:VFMODES 3 "register_operand")
+			  (match_operand:VFMODES 4 "register_operand"))
+			(match_operand:<VCMPEQUIV> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_expand "<fcmp><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(signed_cmp:<VCMPEQUIV>
+			(match_operand:VFMODES 1 "register_operand")
+			(vec_duplicate:VFMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_expand "<fcmp><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VCMPEQUIV> 0 "register_operand")
+		   (unspec:<VCMPEQUIV>
+		     [(if_then_else:<VCMPEQUIV>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(signed_cmp:<VCMPEQUIV>
+			  (match_operand:VFMODES 3 "register_operand")
+			  (vec_duplicate:VFMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			  (match_operand:<VCMPEQUIV> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+;; Integer Reduction instructions.
+
+(define_expand "reduc_<reduc><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<V1MODES> 0 "register_operand")
+		   (unspec:<V1MODES>
+		     [(unspec:<V1MODES>
+			[(match_operand:<V1MODES> 1 "register_operand")
+			 (any_reduc:VIMODES
+			   (vec_duplicate:VIMODES
+			     (match_operand:<V1MODES> 2 "register_operand"))
+			   (match_operand:VIMODES 3 "register_operand"))]
+		       UNSPEC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*reduc_<reduc><mode>_nosetvl"
+  [(set (match_operand:<V1MODES> 0 "register_operand" "=vr")
+	(unspec:<V1MODES>
+	  [(unspec:<V1MODES>
+	     [(match_operand:<V1MODES> 1 "register_operand" "0")
+	      (any_reduc:VIMODES
+		(vec_duplicate:VIMODES
+		  (match_operand:<V1MODES> 2 "register_operand" "vr"))
+		(match_operand:VIMODES 3 "register_operand" "vr"))]
+	    UNSPEC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vred<reduc>.vs\t%0,%3,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "reduc_<reduc><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<V1MODES> 0 "register_operand")
+		   (unspec:<V1MODES>
+		     [(unspec:<V1MODES>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:<V1MODES> 2 "register_operand")
+			 (any_reduc:VIMODES
+			   (vec_duplicate:VIMODES
+			     (match_operand:<V1MODES> 3 "register_operand"))
+			   (match_operand:VIMODES 4 "register_operand"))]
+		       UNSPEC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*reduc_<reduc><mode>_mask_nosetvl"
+  [(set (match_operand:<V1MODES> 0 "register_operand" "=vr")
+	(unspec:<V1MODES>
+	  [(unspec:<V1MODES>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:<V1MODES> 2 "register_operand" "0")
+	      (any_reduc:VIMODES
+		(vec_duplicate:VIMODES
+		  (match_operand:<V1MODES> 3 "register_operand" "vr"))
+		(match_operand:VIMODES 4 "register_operand" "vr"))]
+	    UNSPEC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vred<reduc>.vs\t%0,%4,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Widening Integer Reduction Instructions
+
+(define_expand "wreduc_sum<sumu><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VW1MODES> 0 "register_operand")
+		   (unspec:<VW1MODES>
+		     [(unspec:<VW1MODES>
+			[(match_operand:<VW1MODES> 1 "register_operand")
+			 (match_operand:<VW1MODES> 2 "register_operand")
+			 (match_operand:VWRED_IMODES 3 "register_operand")]
+		       WREDUC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wreduc_sum<sumu><mode>_nosetvl"
+  [(set (match_operand:<VW1MODES> 0 "register_operand" "=&vr")
+	(unspec:<VW1MODES>
+	  [(unspec:<VW1MODES>
+	     [(match_operand:<VW1MODES> 1 "register_operand" "0")
+	      (match_operand:<VW1MODES> 2 "register_operand" "vr")
+	      (match_operand:VWRED_IMODES 3 "register_operand" "vr")]
+	    WREDUC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwredsum<sumu>.vs\t%0,%3,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wreduc_sum<sumu><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VW1MODES> 0 "register_operand")
+		   (unspec:<VW1MODES>
+		     [(unspec:<VW1MODES>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:<VW1MODES> 2 "register_operand")
+			 (match_operand:<VW1MODES> 3 "register_operand")
+			 (match_operand:VWRED_IMODES 4 "register_operand")]
+		       WREDUC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wreduc_sum<sumu><mode>_mask_nosetvl"
+  [(set (match_operand:<VW1MODES> 0 "register_operand" "=vr")
+	(unspec:<VW1MODES>
+	  [(unspec:<VW1MODES>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:<VW1MODES> 2 "register_operand" "0")
+	      (match_operand:<VW1MODES> 3 "register_operand" "vr")
+	      (match_operand:VWRED_IMODES 4 "register_operand" "vr")]
+	    WREDUC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwredsum<sumu>.vs\t%0,%4,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Floating Point Reduction instructions.
+
+(define_expand "reduc_<reduc><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<V1MODES> 0 "register_operand")
+		   (unspec:<V1MODES>
+		     [(unspec:<V1MODES>
+			[(match_operand:<V1MODES> 1 "register_operand")
+			 (any_freduc:VFMODES
+			   (vec_duplicate:VFMODES
+			     (match_operand:<V1MODES> 2 "register_operand"))
+			   (match_operand:VFMODES 3 "register_operand"))]
+		       UNSPEC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*reduc_<reduc><mode>_nosetvl"
+  [(set (match_operand:<V1MODES> 0 "register_operand" "=vr")
+	(unspec:<V1MODES>
+	  [(unspec:<V1MODES>
+	     [(match_operand:<V1MODES> 1 "register_operand" "0")
+	      (any_freduc:VFMODES
+		(vec_duplicate:VFMODES
+		  (match_operand:<V1MODES> 2 "register_operand" "vr"))
+		(match_operand:VFMODES 3 "register_operand" "vr"))]
+	    UNSPEC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfred<reduc>.vs\t%0,%3,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "reduc_<reduc><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<V1MODES> 0 "register_operand")
+		   (unspec:<V1MODES>
+		     [(unspec:<V1MODES>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:<V1MODES> 2 "register_operand")
+			 (any_reduc:VFMODES
+			   (vec_duplicate:VFMODES
+			     (match_operand:<V1MODES> 3 "register_operand"))
+			   (match_operand:VFMODES 4 "register_operand"))]
+		       UNSPEC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*reduc_<reduc><mode>_mask_nosetvl"
+  [(set (match_operand:<V1MODES> 0 "register_operand" "=vr")
+	(unspec:<V1MODES>
+	  [(unspec:<V1MODES>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:<V1MODES> 2 "register_operand" "0")
+	      (any_reduc:VFMODES
+		(vec_duplicate:VFMODES
+		  (match_operand:<V1MODES> 3 "register_operand" "vr"))
+		(match_operand:VFMODES 4 "register_operand" "vr"))]
+	    UNSPEC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfred<reduc>.vs\t%0,%4,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "reduc_osum<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<V1MODES> 0 "register_operand")
+		   (unspec:<V1MODES>
+		     [(unspec:<V1MODES>
+			[(match_operand:<V1MODES> 1 "register_operand")
+			 (plus:VFMODES
+			   (vec_duplicate:VFMODES
+			     (match_operand:<V1MODES> 2 "register_operand"))
+			   (match_operand:VFMODES 3 "register_operand"))]
+		       UNSPEC_ORDERED_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*reduc_osum<mode>_nosetvl"
+  [(set (match_operand:<V1MODES> 0 "register_operand" "=vr")
+	(unspec:<V1MODES>
+	  [(unspec:<V1MODES>
+	     [(match_operand:<V1MODES> 1 "register_operand" "0")
+	      (plus:VFMODES
+		(vec_duplicate:VFMODES
+		  (match_operand:<V1MODES> 2 "register_operand" "vr"))
+		(match_operand:VFMODES 3 "register_operand" "vr"))]
+	    UNSPEC_ORDERED_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfredosum.vs\t%0,%3,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "reduc_osum<mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<V1MODES> 0 "register_operand")
+		   (unspec:<V1MODES>
+		     [(unspec:<V1MODES>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:<V1MODES> 2 "register_operand")
+			 (plus:VFMODES
+			   (vec_duplicate:VFMODES
+			     (match_operand:<V1MODES> 3 "register_operand"))
+			   (match_operand:VFMODES 4 "register_operand"))]
+		       UNSPEC_ORDERED_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*reduc_osum<mode>_mask_nosetvl"
+  [(set (match_operand:<V1MODES> 0 "register_operand" "=vr")
+	(unspec:<V1MODES>
+	  [(unspec:<V1MODES>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:<V1MODES> 2 "register_operand" "0")
+	      (plus:VFMODES
+		(vec_duplicate:VFMODES
+		  (match_operand:<V1MODES> 3 "register_operand" "vr"))
+		(match_operand:VFMODES 4 "register_operand" "vr"))]
+	    UNSPEC_ORDERED_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfredosum.vs\t%0,%4,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Widening Floating-Point Reduction Instructions
+
+(define_expand "wreduc_<o>sum<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VW1MODES> 0 "register_operand")
+		   (unspec:<VW1MODES>
+		     [(unspec:<VW1MODES>
+			[(match_operand:<VW1MODES> 1 "register_operand")
+			 (match_operand:<VW1MODES> 2 "register_operand")
+			 (match_operand:VWRED_FMODES 3 "register_operand")]
+		       WFREDUC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*wreduc_<o>sum<mode>_nosetvl"
+  [(set (match_operand:<VW1MODES> 0 "register_operand" "=&vr")
+	(unspec:<VW1MODES>
+	  [(unspec:<VW1MODES>
+	     [(match_operand:<VW1MODES> 1 "register_operand" "0")
+	      (match_operand:<VW1MODES> 2 "register_operand" "vr")
+	      (match_operand:VWRED_FMODES 3 "register_operand" "vr")]
+	    WFREDUC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfwred<o>sum.vs\t%0,%3,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wreduc_<o>sum<mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VW1MODES> 0 "register_operand")
+		   (unspec:<VW1MODES>
+		     [(unspec:<VW1MODES>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:<VW1MODES> 2 "register_operand")
+			 (match_operand:<VW1MODES> 3 "register_operand")
+			 (match_operand:VWRED_FMODES 4 "register_operand")]
+		       WFREDUC_REDUC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wreduc_<o>sum<mode>_mask_nosetvl"
+  [(set (match_operand:<VW1MODES> 0 "register_operand" "=vr")
+	(unspec:<VW1MODES>
+	  [(unspec:<VW1MODES>
+	    [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (match_operand:<VW1MODES> 2 "register_operand" "0")
+	     (match_operand:<VW1MODES> 3 "register_operand" "vr")
+	     (match_operand:VWRED_FMODES 4 "register_operand" "vr")]
+	    WFREDUC_REDUC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vfwred<o>sum.vs\t%0,%4,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "reinterpret_<mode><vintequiv>"
+  [(set (match_operand:VFMODES 0 "register_operand")
+	(subreg:VFMODES (match_operand:<VINTEQUIV> 1 "register_operand") 0))]
+  "TARGET_VECTOR"
+{
+  emit_insn (gen_mov<mode> (operands[0],
+	     simplify_gen_subreg (<MODE>mode, operands[1],
+				  <VINTEQUIV>mode, 0)));
+  DONE;
+})
+
+(define_expand "reinterpret_<vintequiv><mode>"
+  [(set (match_operand:<VINTEQUIV> 0 "register_operand")
+	(subreg:<VINTEQUIV> (match_operand:VFMODES 1 "register_operand") 0))]
+  "TARGET_VECTOR"
+{
+  emit_insn (gen_mov<vintequiv> (operands[0],
+	     simplify_gen_subreg (<VINTEQUIV>mode, operands[1],
+				  <MODE>mode, 0)));
+  DONE;
+})
+
+(define_expand "reinterpret_<VIMODES2:mode><VIMODES:mode>"
+  [(set (match_operand:VIMODES2 0 "register_operand")
+	(subreg:VIMODES2 (match_operand:VIMODES 1 "register_operand") 0))]
+  "TARGET_VECTOR"
+{
+  emit_insn (gen_mov<VIMODES2:mode> (operands[0],
+	     simplify_gen_subreg (<VIMODES2:MODE>mode, operands[1],
+				  <VIMODES:MODE>mode, 0)));
+  DONE;
+})
+
+(define_expand "reinterpret_<VFMODES2:mode><VFMODES:mode>"
+  [(set (match_operand:VFMODES2 0 "register_operand")
+	(subreg:VFMODES2 (match_operand:VFMODES 1 "register_operand") 0))]
+  "TARGET_VECTOR"
+{
+  rtx tmp_src, tmp_dst;
+  tmp_src = gen_reg_rtx(<VFMODES:VINTEQUIV>mode);
+  tmp_dst = gen_reg_rtx(<VFMODES2:VINTEQUIV>mode);
+  emit_insn (gen_mov<VFMODES:vintequiv> (tmp_src,
+	     simplify_gen_subreg (<VFMODES:VINTEQUIV>mode, operands[1],
+				  <VFMODES:MODE>mode, 0)));
+
+  emit_insn (gen_mov<VFMODES2:vintequiv> (tmp_dst,
+	     simplify_gen_subreg (<VFMODES2:VINTEQUIV>mode, tmp_src,
+				  <VFMODES:VINTEQUIV>mode, 0)));
+
+  emit_insn (gen_mov<VFMODES2:mode> (operands[0],
+	     simplify_gen_subreg (<VFMODES2:MODE>mode, tmp_dst,
+				  <VFMODES2:VINTEQUIV>mode, 0)));
+  DONE;
+})
+
+
+(define_expand "read_vlenb"
+  [(match_operand 0 "register_operand")]
+  "TARGET_VECTOR"
+{
+  rtx imm = gen_int_mode (UNITS_PER_V_REG, Pmode);
+  emit_move_insn (operands[0], imm);
+  DONE;
+})
+
+;; Vector Single-Width Bit Shift Instructions
+
+(define_expand "<vshift><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_shift:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "vector_shift_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vshift><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(any_shift:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr,vr")
+	     (match_operand:VIMODES 2 "vector_shift_operand" "vr,vk"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   v<insn>.vv\t%0,%1,%2
+   v<insn>.vi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<vshift><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_shift:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vshift><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(any_shift:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<vshift><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_shift:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "vector_shift_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vshift><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (any_shift:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr,vr")
+	       (match_operand:VIMODES 4 "vector_shift_operand" "vr,vk"))
+	     (match_operand:VIMODES 2 "register_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   v<insn>.vv\t%0,%3,%4,%1.t
+   v<insn>.vi\t%0,%3,%v4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<vshift><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_shift:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			  (match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vshift><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_shift:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Narrowing Integer Right Shift Instructions
+
+(define_expand "<vnshift><mode>3_nv"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(truncate:VWIMODES
+			(any_shiftrt:<VWMODE>
+			  (match_operand:<VWMODE> 1 "register_operand")
+			  (match_operand:VWIMODES 2 "vector_shift_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vnshift><mode>3_nv_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=&vr,&vr")
+	(unspec:VWIMODES
+	  [(truncate:VWIMODES
+	     (any_shiftrt:<VWMODE>
+	       (match_operand:<VWMODE> 1 "register_operand" "vr,vr")
+	       (match_operand:VWIMODES 2 "vector_shift_operand" "vr,vk")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vn<insn>.wv\t%0,%1,%2
+   vn<insn>.wi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<vnshift><mode>3_nv_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(truncate:VWIMODES
+			(any_shiftrt:<VWMODE>
+			  (match_operand:<VWMODE> 1 "register_operand")
+			  (vec_duplicate:VWIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vnshift><mode>3_nv_scalar_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=&vr")
+	(unspec:VWIMODES
+	  [(truncate:VWIMODES
+	     (any_shiftrt:<VWMODE>
+	       (match_operand:<VWMODE> 1 "register_operand" "vr")
+	       (vec_duplicate:VWIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vn<insn>.wx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<vnshift><mode>3_nv_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(if_then_else:VWIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(truncate:VWIMODES
+			  (any_shiftrt:<VWMODE>
+			    (match_operand:<VWMODE> 3 "register_operand")
+			    (match_operand:VWIMODES 4 "vector_shift_operand")))
+			(match_operand:VWIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vnshift><mode>3_nv_mask_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VWIMODES
+	  [(if_then_else:VWIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (truncate:VWIMODES
+	       (any_shiftrt:<VWMODE>
+		 (match_operand:<VWMODE> 3 "register_operand" "vr,vr")
+		 (match_operand:VWIMODES 4 "vector_shift_operand" "vr,vk")))
+	     (match_operand:VWIMODES 2 "register_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vn<insn>.wv\t%0,%3,%4,%1.t
+   vn<insn>.wi\t%0,%3,%v4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<vnshift><mode>3_nv_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(if_then_else:VWIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(truncate:VWIMODES
+			  (any_shiftrt:<VWMODE>
+			    (match_operand:<VWMODE> 3 "register_operand")
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:VWIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<vnshift><mode>3_scalar_nv_mask_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=vr")
+	(unspec:VWIMODES
+	  [(if_then_else:VWIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (truncate:VWIMODES
+	       (any_shiftrt:<VWMODE>
+		 (match_operand:<VWMODE> 3 "register_operand" "vr")
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+	     (match_operand:VWIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vn<insn>.wx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Min and Max instructions
+
+(define_expand "<minmax><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_minmax:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<minmax><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(any_minmax:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (match_operand:VIMODES 2 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<minmax><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_minmax:VIMODES
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand"))
+			(match_operand:VIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<minmax><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(any_minmax:VIMODES
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	     (match_operand:VIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<minmax><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_minmax:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<minmax><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_minmax:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (match_operand:VIMODES 4 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<minmax><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_minmax:VIMODES
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand"))
+			  (match_operand:VIMODES 3 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<minmax><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_minmax:VIMODES
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r"))
+	       (match_operand:VIMODES 3 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Integer divide instructions.
+
+(define_expand "<optab><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_div:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(any_div:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (match_operand:VIMODES 2 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(any_div:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(vec_duplicate:VIMODES
+			  (match_operand:<VSUBMODE> 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+;; XXX: No divide instruction with vector-scalar,
+;; all divide patterns prefer vector-vector.
+
+(define_insn "*<optab><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(any_div:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "vr")
+	     (vec_duplicate:VIMODES
+	       (match_operand:<VSUBMODE> 2 "register_operand" "r")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_div:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (match_operand:VIMODES 4 "register_operand"))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_div:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (match_operand:VIMODES 4 "register_operand" "vr"))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<optab><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_div:VIMODES
+			  (match_operand:VIMODES 3 "register_operand")
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 4 "register_operand")))
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<optab><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_div:VIMODES
+	       (match_operand:VIMODES 3 "register_operand" "vr")
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 4 "register_operand" "r")))
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<insn>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector widening signed integer multiply
+;; and unsigned integer multiply
+
+(define_expand "wmul<u><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(mult:<VWMODE>
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 1 "register_operand"))
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmul<u><mode>_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(mult:<VWMODE>
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 1 "register_operand" "vr"))
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmul<u>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wmul<u><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(mult:<VWMODE>
+			(any_extend:<VWMODE>
+			  (match_operand:VWIMODES 1 "register_operand"))
+			(any_extend:<VWMODE>
+			  (vec_duplicate:VWIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmul<u><mode>_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(mult:<VWMODE>
+	     (any_extend:<VWMODE>
+	       (match_operand:VWIMODES 1 "register_operand" "vr"))
+	     (any_extend:<VWMODE>
+	       (vec_duplicate:VWIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmul<u>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wmul<u><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:<VWMODE>
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand"))
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmul<u><mode>_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:<VWMODE>
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 3 "register_operand" "vr"))
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 4 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmul<u>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wmul<u><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:<VWMODE>
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand"))
+			  (any_extend:<VWMODE>
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			  (match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmul<u><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:<VWMODE>
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 3 "register_operand" "vr"))
+	       (any_extend:<VWMODE>
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmul<u>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector widening signed-unsigned integer multiply
+
+(define_expand "wmulsu<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(mult:<VWMODE>
+			(sign_extend:<VWMODE>
+			  (match_operand:VWIMODES 1 "register_operand"))
+			(zero_extend:<VWMODE>
+			  (match_operand:VWIMODES 2 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmulsu<mode>_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(mult:<VWMODE>
+	     (sign_extend:<VWMODE>
+	       (match_operand:VWIMODES 1 "register_operand" "vr"))
+	     (zero_extend:<VWMODE>
+	       (match_operand:VWIMODES 2 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmulsu.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wmulsu<mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(mult:<VWMODE>
+			(sign_extend:<VWMODE>
+			  (match_operand:VWIMODES 1 "register_operand"))
+			(zero_extend:<VWMODE>
+			  (vec_duplicate:VWIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmulsu<mode>_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(mult:<VWMODE>
+	     (sign_extend:<VWMODE>
+	       (match_operand:VWIMODES 1 "register_operand" "vr"))
+	     (zero_extend:<VWMODE>
+	       (vec_duplicate:VWIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmulsu.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wmulsu<mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:<VWMODE>
+			  (sign_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand"))
+			  (zero_extend:<VWMODE>
+			    (match_operand:VWIMODES 4 "register_operand")))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmulsu<mode>_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:<VWMODE>
+	       (sign_extend:<VWMODE>
+		 (match_operand:VWIMODES 3 "register_operand" "vr"))
+	       (zero_extend:<VWMODE>
+		 (match_operand:VWIMODES 4 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmulsu.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "wmulsu<mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(mult:<VWMODE>
+			  (sign_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand"))
+			  (zero_extend:<VWMODE>
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 4 "register_operand"))))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*wmulsu<mode>_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (mult:<VWMODE>
+	       (sign_extend:<VWMODE>
+		 (match_operand:VWIMODES 3 "register_operand" "vr"))
+	       (zero_extend:<VWMODE>
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 4 "register_operand" "r"))))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmulsu.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Single-Width Integer Multiply-Add Instructions
+
+(define_expand "v<vmadd_sub><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(add_sub:VIMODES
+			(match_operand:VIMODES 3 "register_operand")
+			(mult:VIMODES
+			  (match_operand:VIMODES 2 "register_operand")
+			  (match_operand:VIMODES 1 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*v<vmadd_sub><mode>_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(add_sub:VIMODES
+	     (match_operand:VIMODES 3 "register_operand" "vr")
+	     (mult:VIMODES
+	       (match_operand:VIMODES 2 "register_operand" "vr")
+	       (match_operand:VIMODES 1 "register_operand" "0")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<vmadd_sub>.vv\t%0,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "v<vmac><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(add_sub:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(mult:VIMODES
+			  (match_operand:VIMODES 2 "register_operand")
+			  (match_operand:VIMODES 3 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*v<vmac><mode>_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(add_sub:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "0")
+	     (mult:VIMODES
+	       (match_operand:VIMODES 2 "register_operand" "vr")
+	       (match_operand:VIMODES 3 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<vmac>.vv\t%0,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "v<vmadd_sub><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(add_sub:VIMODES
+			(match_operand:VIMODES 3 "register_operand")
+			(mult:VIMODES
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))
+			  (match_operand:VIMODES 1 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*v<vmadd_sub><mode>_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(add_sub:VIMODES
+	     (match_operand:VIMODES 3 "register_operand" "vr")
+	     (mult:VIMODES
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	       (match_operand:VIMODES 1 "register_operand" "0")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<vmadd_sub>.vx\t%0,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "v<vmac><mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(add_sub:VIMODES
+			(match_operand:VIMODES 1 "register_operand")
+			(mult:VIMODES
+			  (vec_duplicate:VIMODES
+			    (match_operand:<VSUBMODE> 2 "register_operand"))
+			  (match_operand:VIMODES 3 "register_operand")))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*v<vmac><mode>_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(add_sub:VIMODES
+	     (match_operand:VIMODES 1 "register_operand" "0")
+	     (mult:VIMODES
+	       (vec_duplicate:VIMODES
+		 (match_operand:<VSUBMODE> 2 "register_operand" "r"))
+	       (match_operand:VIMODES 3 "register_operand" "vr")))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "v<vmac>.vx\t%0,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<imac><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (match_operand:VIMODES 3 "register_operand")
+			 (match_operand:VIMODES 4 "register_operand")]
+		       UNSPEC_MASK_VMACS)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<imac><mode>_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:VIMODES 2 "register_operand" "0")
+	      (match_operand:VIMODES 3 "register_operand" "vr")
+	      (match_operand:VIMODES 4 "register_operand" "vr")]
+	    UNSPEC_MASK_VMACS)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<imac>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+
+(define_expand "<imac><mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (vec_duplicate:VIMODES
+			   (match_operand:<VSUBMODE> 3 "register_operand"))
+			 (match_operand:VIMODES 4 "register_operand")]
+		       UNSPEC_MASK_VMACS)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<imac><mode>_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:VIMODES 2 "register_operand" "0")
+	      (vec_duplicate:VIMODES
+		(match_operand:<VSUBMODE> 3 "register_operand" "r"))
+	      (match_operand:VIMODES 4 "register_operand" "vr")]
+	    UNSPEC_MASK_VMACS)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<imac>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Widening Integer Multiply-Add Instructions
+
+(define_expand "<u>madd<mode><vwmode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(plus:<VWMODE>
+			(mult:<VWMODE>
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 2 "register_operand"))
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand")))
+			(match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vwmode>4_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(plus:<VWMODE>
+	     (mult:<VWMODE>
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 1 "register_operand" "vr"))
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmacc<u>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vwmode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(plus:<VWMODE>
+			(mult:<VWMODE>
+			  (sign_extend:<VWMODE>
+			    (match_operand:VWIMODES 2 "register_operand"))
+			  (zero_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand")))
+			 (match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vwmode>4_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(plus:<VWMODE>
+	     (mult:<VWMODE>
+	       (sign_extend:<VWMODE>
+		 (match_operand:VWIMODES 1 "register_operand" "vr"))
+	       (zero_extend:<VWMODE>
+		 (match_operand:VWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmaccsu.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<u>madd<mode><vwmode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(plus:<VWMODE>
+			(mult:<VWMODE>
+			  (any_extend:<VWMODE>
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand")))
+			  (any_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand")))
+			(match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vwmode>4_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(plus:<VWMODE>
+	     (mult:<VWMODE>
+	       (any_extend:<VWMODE>
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 1 "register_operand" "r")))
+	       (any_extend:<VWMODE>
+		 (match_operand:VWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmacc<u>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vwmode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(plus:<VWMODE>
+			(mult:<VWMODE>
+			  (sign_extend:<VWMODE>
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand")))
+			  (zero_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand")))
+			(match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vwmode>4_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(plus:<VWMODE>
+	     (mult:<VWMODE>
+	       (sign_extend:<VWMODE>
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 1 "register_operand" "r")))
+	       (zero_extend:<VWMODE>
+		 (match_operand:VWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmaccsu.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "usmadd<mode><vwmode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(plus:<VWMODE>
+			(mult:<VWMODE>
+			  (zero_extend:<VWMODE>
+			    (vec_duplicate:VWIMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand")))
+			  (sign_extend:<VWMODE>
+			    (match_operand:VWIMODES 3 "register_operand")))
+			(match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*usmadd<mode><vwmode>4_scalar_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(plus:<VWMODE>
+	     (mult:<VWMODE>
+	       (zero_extend:<VWMODE>
+		 (vec_duplicate:VWIMODES
+		   (match_operand:<VSUBMODE> 1 "register_operand" "r")))
+	       (sign_extend:<VWMODE>
+		 (match_operand:VWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmaccus.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<u>madd<mode><vwmode>4_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VWMODE>
+			   (mult:<VWMODE>
+			     (any_extend:<VWMODE>
+			       (match_operand:VWIMODES 3 "register_operand"))
+			     (any_extend:<VWMODE>
+			       (match_operand:VWIMODES 4 "register_operand")))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_VMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vwmode>4_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VWMODE>
+		(mult:<VWMODE>
+		  (any_extend:<VWMODE>
+		    (match_operand:VWIMODES 3 "register_operand" "vr"))
+		  (any_extend:<VWMODE>
+		    (match_operand:VWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmacc<u>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vwmode>4_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VWMODE>
+			   (mult:<VWMODE>
+			     (sign_extend:<VWMODE>
+			       (match_operand:VWIMODES 3 "register_operand"))
+			     (zero_extend:<VWMODE>
+			       (match_operand:VWIMODES 4 "register_operand")))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_VMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vwmode>4_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VWMODE>
+		(mult:<VWMODE>
+		  (sign_extend:<VWMODE>
+		    (match_operand:VWIMODES 3 "register_operand" "vr"))
+		  (zero_extend:<VWMODE>
+		    (match_operand:VWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmaccsu.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<u>madd<mode><vwmode>4_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VWMODE>
+			   (mult:<VWMODE>
+			     (any_extend:<VWMODE>
+			       (vec_duplicate:VWIMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand")))
+			     (any_extend:<VWMODE>
+			       (match_operand:VWIMODES 4 "register_operand")))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_VMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vwmode>4_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VWMODE>
+		(mult:<VWMODE>
+		  (any_extend:<VWMODE>
+		    (vec_duplicate:VWIMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "r")))
+		  (any_extend:<VWMODE>
+		    (match_operand:VWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmacc<u>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vwmode>4_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VWMODE>
+			   (mult:<VWMODE>
+			     (sign_extend:<VWMODE>
+			       (vec_duplicate:VWIMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand")))
+			     (zero_extend:<VWMODE>
+			       (match_operand:VWIMODES 4 "register_operand")))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_VMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vwmode>4_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VWMODE>
+		(mult:<VWMODE>
+		  (sign_extend:<VWMODE>
+		    (vec_duplicate:VWIMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "r")))
+		  (zero_extend:<VWMODE>
+		    (match_operand:VWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmaccsu.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "usmadd<mode><vwmode>4_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(unspec:<VWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VWMODE>
+			   (mult:<VWMODE>
+			     (zero_extend:<VWMODE>
+			       (vec_duplicate:VWIMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand")))
+			     (sign_extend:<VWMODE>
+			       (match_operand:VWIMODES 4 "register_operand")))
+			   (match_operand:<VWMODE> 2 "register_operand"))]
+		       UNSPEC_VMADD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*usmadd<mode><vwmode>4_scalar_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(unspec:<VWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VWMODE>
+		(mult:<VWMODE>
+		  (zero_extend:<VWMODE>
+		    (vec_duplicate:VWIMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "r")))
+		  (sign_extend:<VWMODE>
+		    (match_operand:VWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VMADD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vwmaccus.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Quad-Widening Integer Multiply-Add Instructions
+
+(define_expand "<u>madd<mode><vqwmode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(plus:<VQWMODE>
+			(mult:<VQWMODE>
+			  (any_extend:<VQWMODE>
+			    (match_operand:VQWIMODES 2 "register_operand"))
+			  (any_extend:<VQWMODE>
+			    (match_operand:VQWIMODES 3 "register_operand")))
+			(match_operand:<VQWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vqwmode>4_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VQWMODE>
+	  [(plus:<VQWMODE>
+	     (mult:<VQWMODE>
+	       (any_extend:<VQWMODE>
+		 (match_operand:VQWIMODES 1 "register_operand" "vr"))
+	       (any_extend:<VQWMODE>
+		 (match_operand:VQWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VQWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmacc<u>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vqwmode>4"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(plus:<VQWMODE>
+			(mult:<VQWMODE>
+			  (sign_extend:<VQWMODE>
+			    (match_operand:VQWIMODES 2 "register_operand"))
+			  (zero_extend:<VQWMODE>
+			    (match_operand:VQWIMODES 3 "register_operand")))
+			(match_operand:<VQWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vqwmode>4_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VQWMODE>
+	  [(plus:<VQWMODE>
+	     (mult:<VQWMODE>
+	       (sign_extend:<VQWMODE>
+		 (match_operand:VQWIMODES 1 "register_operand" "vr"))
+	       (zero_extend:<VQWMODE>
+		 (match_operand:VQWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VQWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmaccsu.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<u>madd<mode><vqwmode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(plus:<VQWMODE>
+			(mult:<VQWMODE>
+			  (any_extend:<VQWMODE>
+			    (vec_duplicate:VQWIMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand")))
+			  (any_extend:<VQWMODE>
+			    (match_operand:VQWIMODES 3 "register_operand")))
+			(match_operand:<VQWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vqwmode>4_scalar_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VQWMODE>
+	  [(plus:<VQWMODE>
+	     (mult:<VQWMODE>
+	       (any_extend:<VQWMODE>
+		 (vec_duplicate:VQWIMODES
+		   (match_operand:<VSUBMODE> 1 "register_operand" "r")))
+	       (any_extend:<VQWMODE>
+		 (match_operand:VQWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VQWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmacc<u>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vqwmode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(plus:<VQWMODE>
+			(mult:<VQWMODE>
+			  (sign_extend:<VQWMODE>
+			    (vec_duplicate:VQWIMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand")))
+			  (zero_extend:<VQWMODE>
+			    (match_operand:VQWIMODES 3 "register_operand")))
+			(match_operand:<VQWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vqwmode>4_scalar_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VQWMODE>
+	  [(plus:<VQWMODE>
+	     (mult:<VQWMODE>
+	       (sign_extend:<VQWMODE>
+		 (vec_duplicate:VQWIMODES
+		   (match_operand:<VSUBMODE> 1 "register_operand" "r")))
+		 (zero_extend:<VQWMODE>
+		   (match_operand:VQWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VQWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmaccsu.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "usmadd<mode><vqwmode>4_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(plus:<VQWMODE>
+			(mult:<VQWMODE>
+			  (zero_extend:<VQWMODE>
+			    (vec_duplicate:VQWIMODES
+			      (match_operand:<VSUBMODE> 2 "register_operand")))
+			  (sign_extend:<VQWMODE>
+			    (match_operand:VQWIMODES 3 "register_operand")))
+			(match_operand:<VQWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*usmadd<mode><vqwmode>4_scalar_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VQWMODE>
+	  [(plus:<VQWMODE>
+	     (mult:<VQWMODE>
+	       (zero_extend:<VQWMODE>
+		 (vec_duplicate:VQWIMODES
+		   (match_operand:<VSUBMODE> 1 "register_operand" "r")))
+	       (sign_extend:<VQWMODE>
+		 (match_operand:VQWIMODES 2 "register_operand" "vr")))
+	     (match_operand:<VQWMODE> 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmaccus.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<u>madd<mode><vqwmode>4_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		    [(unspec:<VQWMODE>
+		       [(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(plus:<VQWMODE>
+			  (mult:<VQWMODE>
+			    (any_extend:<VQWMODE>
+			      (match_operand:VQWIMODES 3 "register_operand"))
+			    (any_extend:<VQWMODE>
+			      (match_operand:VQWIMODES 4 "register_operand")))
+			(match_operand:<VQWMODE> 2 "register_operand"))]
+		       UNSPEC_VQMAC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vqwmode>4_mask_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=vr")
+	(unspec:<VQWMODE>
+	  [(unspec:<VQWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VQWMODE>
+		(mult:<VQWMODE>
+		  (any_extend:<VQWMODE>
+		    (match_operand:VQWIMODES 3 "register_operand" "vr"))
+		  (any_extend:<VQWMODE>
+		    (match_operand:VQWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VQWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VQMAC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmacc<u>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vqwmode>4_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(unspec:<VQWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VQWMODE>
+			   (mult:<VQWMODE>
+			     (sign_extend:<VQWMODE>
+			       (match_operand:VQWIMODES 3 "register_operand"))
+			     (zero_extend:<VQWMODE>
+			       (match_operand:VQWIMODES 4 "register_operand")))
+			   (match_operand:<VQWMODE> 2 "register_operand"))]
+		       UNSPEC_VQMAC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vqwmode>4_mask_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=vr")
+	(unspec:<VQWMODE>
+	  [(unspec:<VQWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VQWMODE>
+		(mult:<VQWMODE>
+		  (sign_extend:<VQWMODE>
+		    (match_operand:VQWIMODES 3 "register_operand" "vr"))
+		  (zero_extend:<VQWMODE>
+		    (match_operand:VQWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VQWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VQMAC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmaccsu.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<u>madd<mode><vqwmode>4_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(unspec:<VQWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VQWMODE>
+			   (mult:<VQWMODE>
+			     (any_extend:<VQWMODE>
+			       (vec_duplicate:VQWIMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand")))
+			     (any_extend:<VQWMODE>
+			       (match_operand:VQWIMODES 4 "register_operand")))
+			   (match_operand:<VQWMODE> 2 "register_operand"))]
+		       UNSPEC_VQMAC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<u>madd<mode><vqwmode>4_scalar_mask_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=vr")
+	(unspec:<VQWMODE>
+	  [(unspec:<VQWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VQWMODE>
+		(mult:<VQWMODE>
+		  (any_extend:<VQWMODE>
+		    (vec_duplicate:VQWIMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "r")))
+		  (any_extend:<VQWMODE>
+		    (match_operand:VQWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VQWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VQMAC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmacc<u>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "sumadd<mode><vqwmode>4_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(unspec:<VQWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VQWMODE>
+			   (mult:<VQWMODE>
+			     (sign_extend:<VQWMODE>
+			       (vec_duplicate:VQWIMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand")))
+			     (zero_extend:<VQWMODE>
+			       (match_operand:VQWIMODES 4 "register_operand")))
+			   (match_operand:<VQWMODE> 2 "register_operand"))]
+		       UNSPEC_VQMAC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*sumadd<mode><vqwmode>4_scalar_mask_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=vr")
+	(unspec:<VQWMODE>
+	  [(unspec:<VQWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VQWMODE>
+		(mult:<VQWMODE>
+		  (sign_extend:<VQWMODE>
+		    (vec_duplicate:VQWIMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "r")))
+		  (zero_extend:<VQWMODE>
+		    (match_operand:VQWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VQWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VQMAC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmaccsu.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "usmadd<mode><vqwmode>4_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VQWMODE> 0 "register_operand")
+		   (unspec:<VQWMODE>
+		     [(unspec:<VQWMODE>
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (plus:<VQWMODE>
+			   (mult:<VQWMODE>
+			     (zero_extend:<VQWMODE>
+			       (vec_duplicate:VQWIMODES
+				 (match_operand:<VSUBMODE> 3 "register_operand")))
+			     (sign_extend:<VQWMODE>
+			       (match_operand:VQWIMODES 4 "register_operand")))
+			   (match_operand:<VQWMODE> 2 "register_operand"))]
+		       UNSPEC_VQMAC)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*usmadd<mode><vqwmode>4_scalar_mask_nosetvl"
+  [(set (match_operand:<VQWMODE> 0 "register_operand" "=vr")
+	(unspec:<VQWMODE>
+	  [(unspec:<VQWMODE>
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (plus:<VQWMODE>
+		(mult:<VQWMODE>
+		  (zero_extend:<VQWMODE>
+		    (vec_duplicate:VQWIMODES
+		      (match_operand:<VSUBMODE> 3 "register_operand" "r")))
+		  (sign_extend:<VQWMODE>
+		    (match_operand:VQWIMODES 4 "register_operand" "vr")))
+		(match_operand:<VQWMODE> 2 "register_operand" "0"))]
+	    UNSPEC_VQMAC)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vqmaccus.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Single-Width Floating-Point/Integer Type-Convert Instructions
+
+(define_expand "<fix_cvt><mode><vintequiv>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VINTEQUIV> 0 "register_operand")
+		   (unspec:<VINTEQUIV>
+		     [(any_fix:<VINTEQUIV>
+			(match_operand:VFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fix_cvt><mode><vintequiv>2_nosetvl"
+  [(set (match_operand:<VINTEQUIV> 0 "register_operand" "=vr")
+	(unspec:<VINTEQUIV>
+	  [(any_fix:<VINTEQUIV>
+	     (match_operand:VFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfcvt.rtz.x<u>.f.v %0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fix_cvt><mode><vintequiv>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VINTEQUIV> 0 "register_operand")
+		   (unspec:<VINTEQUIV>
+		     [(if_then_else:<VINTEQUIV>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_fix:<VINTEQUIV>
+			  (match_operand:VFMODES 3 "register_operand"))
+			(match_operand:<VINTEQUIV> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fix_cvt><mode><vintequiv>2_mask_nosetvl"
+  [(set (match_operand:<VINTEQUIV> 0 "register_operand" "=vr")
+	(unspec:<VINTEQUIV>
+	  [(if_then_else:<VINTEQUIV>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_fix:<VINTEQUIV>
+	       (match_operand:VFMODES 3 "register_operand" "vr"))
+	     (match_operand:<VINTEQUIV> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfcvt.rtz.x<u>.f.v %0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fcvt_xf><mode><vintequiv>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VINTEQUIV> 0 "register_operand")
+		   (unspec:<VINTEQUIV>
+		     [(unspec:<VINTEQUIV>
+			[(match_operand:VFMODES 1 "register_operand")]
+		       UNSPEC_FCVT)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fcvt_xf><mode><vintequiv>2_nosetvl"
+  [(set (match_operand:<VINTEQUIV> 0 "register_operand" "=vr")
+	(unspec:<VINTEQUIV>
+	  [(unspec:<VINTEQUIV>
+	     [(match_operand:VFMODES 1 "register_operand" "vr")]
+	   UNSPEC_FCVT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfcvt.x<fu>.f.v %0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fcvt_xf><mode><vintequiv>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VINTEQUIV> 0 "register_operand")
+		   (unspec:<VINTEQUIV>
+		     [(if_then_else:<VINTEQUIV>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:<VINTEQUIV>
+			  [(match_operand:VFMODES 3 "register_operand")]
+			 UNSPEC_FCVT)
+			(match_operand:<VINTEQUIV> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fcvt_xf><mode><vintequiv>2_mask_nosetvl"
+  [(set (match_operand:<VINTEQUIV> 0 "register_operand" "=vr")
+	(unspec:<VINTEQUIV>
+	  [(if_then_else:<VINTEQUIV>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:<VINTEQUIV>
+	       [(match_operand:VFMODES 3 "register_operand" "vr")]
+	      UNSPEC_FCVT)
+	     (match_operand:<VINTEQUIV> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfcvt.x<fu>.f.v %0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<float_cvt><vintequiv><mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(any_float:VFMODES
+			(match_operand:<VINTEQUIV> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<float_cvt><vintequiv><mode>2_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(any_float:VFMODES
+	     (match_operand:<VINTEQUIV> 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfcvt.f.x<u>.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<float_cvt><vintequiv><mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_float:VFMODES
+			  (match_operand:<VINTEQUIV> 3 "register_operand"))
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<float_cvt><vintequiv><mode>2_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_float:VFMODES
+	       (match_operand:<VINTEQUIV> 3 "register_operand"  "vr"))
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfcvt.f.x<u>.v\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Widening Floating-Point/Integer Type-Convert Instructions
+
+(define_expand "<fix_cvt><mode><vfwimode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VFWIMODE> 0 "register_operand")
+		   (unspec:<VFWIMODE>
+		     [(any_fix:<VFWIMODE>
+			(match_operand:VWFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fix_cvt><mode><vfwimode>2_nosetvl"
+  [(set (match_operand:<VFWIMODE> 0 "register_operand" "=&vr")
+	(unspec:<VFWIMODE>
+	  [(any_fix:<VFWIMODE>
+	     (match_operand:VWFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.rtz.x<u>.f.v %0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fix_cvt><mode><vfwimode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VFWIMODE> 0 "register_operand")
+		   (unspec:<VFWIMODE>
+		     [(if_then_else:<VFWIMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_fix:<VFWIMODE>
+			  (match_operand:VWFMODES 3 "register_operand"))
+			(match_operand:<VFWIMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fix_cvt><mode><vfwimode>2_mask_nosetvl"
+  [(set (match_operand:<VFWIMODE> 0 "register_operand" "=vr")
+	(unspec:<VFWIMODE>
+	  [(if_then_else:<VFWIMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_fix:<VFWIMODE>
+	       (match_operand:VWFMODES 3 "register_operand" "vr"))
+	     (match_operand:<VFWIMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.rtz.x<u>.f.v %0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fcvt_xf><mode><vfwimode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VFWIMODE> 0 "register_operand")
+		   (unspec:<VFWIMODE>
+		     [(unspec:<VFWIMODE>
+			[(match_operand:VWFMODES 1 "register_operand")]
+		       UNSPEC_FCVT)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fcvt_xf><mode><vfwimode>2_nosetvl"
+  [(set (match_operand:<VFWIMODE> 0 "register_operand" "=&vr")
+	(unspec:<VFWIMODE>
+	  [(unspec:<VFWIMODE>
+	     [(match_operand:VWFMODES 1 "register_operand" "vr")]
+	    UNSPEC_FCVT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.x<fu>.f.v %0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fcvt_xf><mode><vfwimode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VFWIMODE> 0 "register_operand")
+		   (unspec:<VFWIMODE>
+		     [(if_then_else:<VFWIMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:<VFWIMODE>
+			  [(match_operand:VWFMODES 3 "register_operand")]
+			 UNSPEC_FCVT)
+			(match_operand:<VFWIMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fcvt_xf><mode><vfwimode>2_mask_nosetvl"
+  [(set (match_operand:<VFWIMODE> 0 "register_operand" "=vr")
+	(unspec:<VFWIMODE>
+	  [(if_then_else:<VFWIMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:<VFWIMODE>
+	       [(match_operand:VWFMODES 3 "register_operand" "vr")]
+	      UNSPEC_FCVT)
+	     (match_operand:<VFWIMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.x<fu>.f.v %0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<float_cvt><mode><viwfmode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VIWFMODE> 0 "register_operand")
+		   (unspec:<VIWFMODE>
+		     [(any_float:<VIWFMODE>
+			(match_operand:FCVT_VWIMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<float_cvt><mode><viwfmode>2_nosetvl"
+  [(set (match_operand:<VIWFMODE> 0 "register_operand" "=&vr")
+	(unspec:<VIWFMODE>
+	  [(any_float:<VIWFMODE>
+	     (match_operand:FCVT_VWIMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.f.x<u>.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<float_cvt><mode><viwfmode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VIWFMODE> 0 "register_operand")
+		   (unspec:<VIWFMODE>
+		     [(if_then_else:<VIWFMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_float:<VIWFMODE>
+			  (match_operand:FCVT_VWIMODES 3 "register_operand"))
+			(match_operand:<VIWFMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<float_cvt><mode><viwfmode>2_mask_nosetvl"
+  [(set (match_operand:<VIWFMODE> 0 "register_operand" "=vr")
+	(unspec:<VIWFMODE>
+	  [(if_then_else:<VIWFMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_float:<VIWFMODE>
+	       (match_operand:FCVT_VWIMODES 3 "register_operand" "vr"))
+	     (match_operand:<VIWFMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.f.x<u>.v\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "extend<mode><vwmode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(float_extend:<VWMODE>
+			(match_operand:VWFMODES 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*extend<mode><vwmode>2_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=&vr")
+	(unspec:<VWMODE>
+	  [(float_extend:<VWMODE>
+	     (match_operand:VWFMODES 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.f.f.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "extend<mode><vwmode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VWMODE> 0 "register_operand")
+		   (unspec:<VWMODE>
+		     [(if_then_else:<VWMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(float_extend:<VWMODE>
+			  (match_operand:VWFMODES 3 "register_operand"))
+			(match_operand:<VWMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*extend<mode><vwmode>2_mask_nosetvl"
+  [(set (match_operand:<VWMODE> 0 "register_operand" "=vr")
+	(unspec:<VWMODE>
+	  [(if_then_else:<VWMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (float_extend:<VWMODE>
+	       (match_operand:VWFMODES 3 "register_operand" "vr"))
+	     (match_operand:<VWMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfwcvt.f.f.v\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Narrowing Floating-Point/Integer Type-Convert Instructions
+
+(define_expand "<fix_cvt><viwfmode><mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:FCVT_VWIMODES 0 "register_operand")
+		   (unspec:FCVT_VWIMODES
+		     [(any_fix:FCVT_VWIMODES
+			(match_operand:<VIWFMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fix_cvt><viwfmode><mode>2_nosetvl"
+  [(set (match_operand:FCVT_VWIMODES 0 "register_operand" "=&vr")
+	(unspec:FCVT_VWIMODES
+	  [(any_fix:FCVT_VWIMODES
+	     (match_operand:<VIWFMODE> 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.rtz.x<u>.f.w %0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fix_cvt><viwfmode><mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:FCVT_VWIMODES 0 "register_operand")
+		   (unspec:FCVT_VWIMODES
+		     [(if_then_else:FCVT_VWIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_fix:FCVT_VWIMODES
+			  (match_operand:<VIWFMODE> 3 "register_operand"))
+			(match_operand:FCVT_VWIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fix_cvt><viwfmode><mode>2_mask_nosetvl"
+  [(set (match_operand:FCVT_VWIMODES 0 "register_operand" "=vr")
+	(unspec:FCVT_VWIMODES
+	  [(if_then_else:FCVT_VWIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_fix:FCVT_VWIMODES
+	       (match_operand:<VIWFMODE> 3 "register_operand" "vr"))
+	     (match_operand:FCVT_VWIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.rtz.x<u>.f.w %0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fcvt_xf><viwfmode><mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:FCVT_VWIMODES 0 "register_operand")
+		   (unspec:FCVT_VWIMODES
+		     [(unspec:FCVT_VWIMODES
+			[(match_operand:<VIWFMODE> 1 "register_operand")]
+		       UNSPEC_FCVT)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fcvt_xf><viwfmode><mode>2_nosetvl"
+  [(set (match_operand:FCVT_VWIMODES 0 "register_operand" "=&vr")
+	(unspec:FCVT_VWIMODES
+	  [(unspec:FCVT_VWIMODES
+	     [(match_operand:<VIWFMODE> 1 "register_operand" "vr")]
+	    UNSPEC_FCVT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.x<fu>.f.w %0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<fcvt_xf><viwfmode><mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:FCVT_VWIMODES 0 "register_operand")
+		   (unspec:FCVT_VWIMODES
+		     [(if_then_else:FCVT_VWIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:FCVT_VWIMODES
+			  [(match_operand:<VIWFMODE> 3 "register_operand")]
+			 UNSPEC_FCVT)
+		       (match_operand:FCVT_VWIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<fcvt_xf><viwfmode><mode>2_mask_nosetvl"
+  [(set (match_operand:FCVT_VWIMODES 0 "register_operand" "=vr")
+	(unspec:FCVT_VWIMODES
+	  [(if_then_else:FCVT_VWIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:FCVT_VWIMODES
+	       [(match_operand:<VIWFMODE> 3 "register_operand" "vr")]
+	      UNSPEC_FCVT)
+	     (match_operand:FCVT_VWIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.x<fu>.f.w %0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<float_cvt><vfwimode><mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWFMODES 0 "register_operand")
+		   (unspec:VWFMODES
+		     [(any_float:VWFMODES
+			(match_operand:<VFWIMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<float_cvt><vfwimode><mode>2_nosetvl"
+  [(set (match_operand:VWFMODES 0 "register_operand" "=&vr")
+	(unspec:VWFMODES
+	  [(any_float:VWFMODES
+	     (match_operand:<VFWIMODE> 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.f.x<u>.w\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<float_cvt><vfwimode><mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWFMODES 0 "register_operand")
+		   (unspec:VWFMODES
+		     [(if_then_else:VWFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(any_float:VWFMODES
+			  (match_operand:<VFWIMODE> 3 "register_operand"))
+			(match_operand:VWFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*<float_cvt><vfwimode><mode>2_mask_nosetvl"
+  [(set (match_operand:VWFMODES 0 "register_operand" "=vr")
+	(unspec:VWFMODES
+	  [(if_then_else:VWFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (any_float:VWFMODES
+	       (match_operand:<VFWIMODE> 3 "register_operand" "vr"))
+	     (match_operand:VWFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.f.x<u>.w\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "trunc<vwmode><mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWFMODES 0 "register_operand")
+		   (unspec:VWFMODES
+		     [(float_truncate:VWFMODES
+			(match_operand:<VWMODE> 1 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*trunc<vwmode><mode>2_nosetvl"
+  [(set (match_operand:VWFMODES 0 "register_operand" "=&vr")
+	(unspec:VWFMODES
+	  [(float_truncate:VWFMODES
+	     (match_operand:<VWMODE> 1 "register_operand" "vr"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.f.f.w\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "trunc<vwmode><mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWFMODES 0 "register_operand")
+		   (unspec:VWFMODES
+		     [(if_then_else:VWFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(float_truncate:VWFMODES
+			  (match_operand:<VWMODE> 3 "register_operand"))
+			(match_operand:VWFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*trunc<vwmode><mode>2_mask_nosetvl"
+  [(set (match_operand:VWFMODES 0 "register_operand" "=vr")
+	(unspec:VWFMODES
+	  [(if_then_else:VWFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (float_truncate:VWFMODES
+	       (match_operand:<VWMODE> 3 "register_operand" "vr"))
+	     (match_operand:VWFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.f.f.w\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "trunc_rod<vwmode><mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWFMODES 0 "register_operand")
+		   (unspec:VWFMODES
+		     [(unspec:VWFMODES
+			[(match_operand:<VWMODE> 1 "register_operand")]
+		       UNSPEC_ROD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*trunc_rod<vwmode><mode>2_nosetvl"
+  [(set (match_operand:VWFMODES 0 "register_operand" "=&vr")
+	(unspec:VWFMODES
+	  [(unspec:VWFMODES
+	     [(match_operand:<VWMODE> 1 "register_operand" "vr")]
+	    UNSPEC_ROD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.rod.f.f.w\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "trunc_rod<vwmode><mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWFMODES 0 "register_operand")
+		   (unspec:VWFMODES
+		     [(if_then_else:VWFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VWFMODES
+			  [(match_operand:<VWMODE> 3 "register_operand")]
+			 UNSPEC_ROD)
+			(match_operand:VWFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*trunc_rod<vwmode><mode>2_mask_nosetvl"
+  [(set (match_operand:VWFMODES 0 "register_operand" "=vr")
+	(unspec:VWFMODES
+	  [(if_then_else:VWFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VWFMODES
+	       [(match_operand:<VWMODE> 3 "register_operand" "vr")]
+	      UNSPEC_ROD)
+	     (match_operand:VWFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfncvt.rod.f.f.w\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Floating-Point Classify Instruction.
+
+(define_expand "vfclass<mode>2"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VLMODE> 0 "register_operand")
+		   (unspec:<VLMODE>
+		     [(unspec:<VLMODE>
+			[(match_operand:VFMODES 1 "register_operand")]
+		       UNSPEC_VFCLASS)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfclass<mode>2_nosetvl"
+  [(set (match_operand:<VLMODE> 0 "register_operand" "=&vr")
+	(unspec:<VLMODE>
+	  [(unspec:<VLMODE>
+	     [(match_operand:VFMODES 1 "register_operand" "vr")]
+	    UNSPEC_VFCLASS)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfclass.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfclass<mode>2_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:<VLMODE> 0 "register_operand")
+		   (unspec:<VLMODE>
+		     [(if_then_else:<VLMODE>
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:<VLMODE>
+			  [(match_operand:VFMODES 3 "register_operand")]
+			 UNSPEC_VFCLASS)
+			(match_operand:<VLMODE> 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfclass<mode>2_mask_nosetvl"
+  [(set (match_operand:<VLMODE> 0 "register_operand" "=vr")
+	(unspec:<VLMODE>
+	  [(if_then_else:<VLMODE>
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:<VLMODE>
+	       [(match_operand:VFMODES 3 "register_operand" "vr")]
+	      UNSPEC_VFCLASS)
+	     (match_operand:<VLMODE> 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfclass.v\t%0,%3,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Integer Slide Instructions.
+
+(define_expand "vslide<ud><VMODES:mode><X:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:VMODES 1 "register_operand")
+			 (match_operand:VMODES 2 "register_operand")
+			 (match_operand:X 3 "reg_or_uimm5_operand")]
+			UNSPEC_VSLIDES)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vslide<ud><VMODES:mode><X:mode>_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr, &vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:VMODES 1 "register_operand" "0, 0")
+	      (match_operand:VMODES 2 "register_operand" "vr, vr")
+	      (match_operand:X 3 "reg_or_uimm5_operand" "r, K")]
+	    UNSPEC_VSLIDES)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vslide<ud>.vx\t%0,%2,%3
+   vslide<ud>.vi\t%0,%2,%3"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vslide<ud><VMODES:mode><X:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(if_then_else:VMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VMODES
+			  [(match_operand:VMODES 3 "register_operand")
+			   (match_operand:X 4 "reg_or_uimm5_operand")]
+			 UNSPEC_VSLIDES)
+			(match_operand:VMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vslide<ud><VMODES:mode><X:mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr,&vr")
+	(unspec:VMODES
+	  [(if_then_else:VMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (unspec:VMODES
+	       [(match_operand:VMODES 3 "register_operand" "vr,vr")
+		(match_operand:X 4 "reg_or_uimm5_operand" "r,K")]
+	      UNSPEC_VSLIDES)
+	     (match_operand:VMODES 2 "register_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vslide<ud>.vx\t%0,%3,%4,%1.t
+   vslide<ud>.vi\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vslide1<ud><VIMODES:mode><X:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:VIMODES 1 "register_operand")
+			 (match_operand:X 2 "register_operand")]
+		       UNSPEC_VSLIDES1)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vslide1<ud><VIMODES:mode><X:mode>_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=&vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:VIMODES 1 "register_operand" "vr")
+	      (match_operand:X 2 "register_operand" "r")]
+	    UNSPEC_VSLIDES1)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vslide1<ud>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vslide1<ud><VIMODES:mode><X:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VIMODES
+			  [(match_operand:VIMODES 3 "register_operand")
+			   (match_operand:X 4 "register_operand")]
+			 UNSPEC_VSLIDES1)
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vslide1<ud><VIMODES:mode><X:mode>_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=&vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VIMODES
+	       [(match_operand:VIMODES 3 "register_operand" "vr")
+		(match_operand:X 4 "register_operand" "r")]
+	      UNSPEC_VSLIDES1)
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vslide1<ud>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector FP Slide Instructions.
+
+(define_expand "vfslide1<ud><mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(unspec:VFMODES
+			[(match_operand:VFMODES 1 "register_operand")
+			 (match_operand:<VSUBMODE> 2 "register_operand")]
+		       UNSPEC_VFSLIDES1)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfslide1<ud><mode>_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=&vr")
+	(unspec:VFMODES
+	  [(unspec:VFMODES
+	     [(match_operand:VFMODES 1 "register_operand" "vr")
+	      (match_operand:<VSUBMODE> 2 "register_operand" "f")]
+	    UNSPEC_VFSLIDES1)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfslide1<ud>.vf\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vfslide1<ud><mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VFMODES 0 "register_operand")
+		   (unspec:VFMODES
+		     [(if_then_else:VFMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VFMODES
+			  [(match_operand:VFMODES 3 "register_operand")
+			   (match_operand:<VSUBMODE> 4 "register_operand")]
+			 UNSPEC_VFSLIDES1)
+			(match_operand:VFMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+{
+})
+
+(define_insn "*vfslide1<ud><mode>_mask_nosetvl"
+  [(set (match_operand:VFMODES 0 "register_operand" "=&vr")
+	(unspec:VFMODES
+	  [(if_then_else:VFMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VFMODES
+	       [(match_operand:VFMODES 3 "register_operand" "vr")
+		(match_operand:<VSUBMODE> 4 "register_operand" "f")]
+	      UNSPEC_VFSLIDES1)
+	     (match_operand:VFMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR && TARGET_HARD_FLOAT"
+  "vfslide1<ud>.vf\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Register Gather Instruction
+
+(define_expand "vrgather<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:VMODES 1 "register_operand")
+			 (match_operand:<VLMODE> 2 "register_operand")]
+		       UNSPEC_VRGATHER)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vrgather<mode>_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:VMODES 1 "register_operand" "vr")
+	      (match_operand:<VLMODE> 2 "register_operand" "vr")]
+	    UNSPEC_VRGATHER)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vrgather.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vrgather<mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(if_then_else:VMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VMODES
+			  [(match_operand:VMODES 3 "register_operand")
+			   (match_operand:<VLMODE> 4 "register_operand")]
+			 UNSPEC_VRGATHER)
+			(match_operand:VMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vrgather<mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr")
+	(unspec:VMODES
+	  [(if_then_else:VMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VMODES
+	       [(match_operand:VMODES 3 "register_operand" "vr")
+		(match_operand:<VLMODE> 4 "register_operand" "vr")]
+	      UNSPEC_VRGATHER)
+	     (match_operand:VMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vrgather.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vrgather<VMODES:mode><P:mode>_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:VMODES 1 "register_operand")
+			 (match_operand:P 2 "reg_or_uimm5_operand")]
+		       UNSPEC_VRGATHER)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vrgather<VMODES:mode><P:mode>_scalar_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr,&vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:VMODES 1 "register_operand" "vr,vr")
+	      (match_operand:P 2 "reg_or_uimm5_operand" "r,K")]
+	    UNSPEC_VRGATHER)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vrgather.vx\t%0,%1,%2
+   vrgather.vi\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vrgather<VMODES:mode><P:mode>_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(if_then_else:VMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VMODES
+			  [(match_operand:VMODES 3 "register_operand")
+			   (match_operand:P 4 "reg_or_uimm5_operand")]
+			 UNSPEC_VRGATHER)
+			(match_operand:VMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vrgather<VMODES:mode><P:mode>_scalar_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr,&vr")
+	(unspec:VMODES
+	  [(if_then_else:VMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (unspec:VMODES
+	       [(match_operand:VMODES 3 "register_operand" "vr,vr")
+		(match_operand:P 4 "reg_or_uimm5_operand" "r,K")]
+	      UNSPEC_VRGATHER)
+	     (match_operand:VMODES 2 "register_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vrgather.vx\t%0,%3,%4,%1.t
+   vrgather.vi\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Compress Instruction.
+
+(define_expand "vcompress<mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VMODES 2 "register_operand")
+			 (match_operand:VMODES 3 "register_operand")]
+		       UNSPEC_VCOMPRESS)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vcompress<mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=&vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:VMODES 2 "register_operand" "0")
+	      (match_operand:VMODES 3 "register_operand" "vr")]
+	    UNSPEC_VCOMPRESS)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vcompress.vm\t%0,%3,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Narrowing Fixed-Point Clip Instructions
+
+(define_expand "vnclip<v_su><mode>3_nv"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(unspec:VWIMODES
+			[(match_operand:<VWMODE> 1 "register_operand")
+			 (match_operand:VWIMODES 2 "vector_shift_operand")]
+		       UNSPEC_VCLIP)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vnclip<v_su><mode>3_nv_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=&vr,&vr")
+	(unspec:VWIMODES
+	  [(unspec:VWIMODES
+	     [(match_operand:<VWMODE> 1 "register_operand" "vr,vr")
+	      (match_operand:VWIMODES 2 "vector_shift_operand" "vr,vk")]
+	    UNSPEC_VCLIP)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vnclip<v_su>.wv\t%0,%1,%2
+   vnclip<v_su>.wi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vnclip<v_su><mode>3_nv_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(unspec:VWIMODES
+			[(match_operand:<VWMODE> 1 "register_operand")
+			 (vec_duplicate:VWIMODES
+			   (match_operand:<VSUBMODE> 2 "register_operand"))]
+		       UNSPEC_VCLIP)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vnclip<v_su><mode>3_nv_scalar_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=&vr")
+	(unspec:VWIMODES
+	  [(unspec:VWIMODES
+	     [(match_operand:<VWMODE> 1 "register_operand" "vr")
+	      (vec_duplicate:VWIMODES
+		(match_operand:<VSUBMODE> 2 "register_operand" "r"))]
+	    UNSPEC_VCLIP)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vnclip<v_su>.wx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vnclip<v_su><mode>3_nv_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(if_then_else:VWIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VWIMODES
+			  [(match_operand:<VWMODE> 3 "register_operand")
+			   (match_operand:VWIMODES 4 "vector_shift_operand")]
+			 UNSPEC_VCLIP)
+			(match_operand:VWIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vnclip<v_su><mode>3_nv_mask_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VWIMODES
+	  [(if_then_else:VWIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (unspec:VWIMODES
+	       [(match_operand:<VWMODE> 3 "register_operand" "vr,vr")
+		(match_operand:VWIMODES 4 "vector_shift_operand" "vr,vk")]
+	      UNSPEC_VCLIP)
+	     (match_operand:VWIMODES 2 "register_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   vnclip<v_su>.wv\t%0,%3,%4,%1.t
+   vnclip<v_su>.wi\t%0,%3,%v4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vnclip<v_su><mode>3_nv_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VWIMODES 0 "register_operand")
+		   (unspec:VWIMODES
+		     [(if_then_else:VWIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VWIMODES
+			  [(match_operand:<VWMODE> 3 "register_operand")
+			   (vec_duplicate:VWIMODES
+			     (match_operand:<VSUBMODE> 4 "register_operand"))]
+			 UNSPEC_VCLIP)
+			(match_operand:VWIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vnclip<v_su><mode>3_scalar_nv_mask_nosetvl"
+  [(set (match_operand:VWIMODES 0 "register_operand" "=vr")
+	(unspec:VWIMODES
+	  [(if_then_else:VWIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VWIMODES
+	       [(match_operand:<VWMODE> 3 "register_operand" "vr")
+		(vec_duplicate:VWIMODES
+		  (match_operand:<VSUBMODE> 4 "register_operand" "r"))]
+	      UNSPEC_VCLIP)
+	     (match_operand:VWIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vnclip<v_su>.wx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;;Vector Single-Width Scaling Shift Instructions
+
+(define_expand "<sshift><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:VIMODES 1 "register_operand")
+			 (match_operand:VIMODES 2 "vector_shift_operand")]
+		       UNSPEC_VSSHIFT)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sshift><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:VIMODES 1 "register_operand" "vr,vr")
+	      (match_operand:VIMODES 2 "vector_shift_operand" "vr,vk")]
+	    UNSPEC_VSSHIFT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   <sshift>.vv\t%0,%1,%2
+   <sshift>.vi\t%0,%1,%v2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sshift><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:VIMODES 1 "register_operand")
+			 (vec_duplicate:VIMODES
+			   (match_operand:<VSUBMODE> 2 "register_operand"))]
+		       UNSPEC_VSSHIFT)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sshift><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:VIMODES 1 "register_operand" "vr")
+	      (vec_duplicate:VIMODES
+		(match_operand:<VSUBMODE> 2 "register_operand" "r"))]
+	    UNSPEC_VSSHIFT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<sshift>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sshift><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VIMODES
+			  [(match_operand:VIMODES 3 "register_operand")
+			   (match_operand:VIMODES 4 "vector_shift_operand")]
+			 UNSPEC_VSSHIFT)
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sshift><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr,vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm,vm")
+	     (unspec:VIMODES
+	       [(match_operand:VIMODES 3 "register_operand" "vr,vr")
+		(match_operand:VIMODES 4 "vector_shift_operand" "vr,vk")]
+	      UNSPEC_VSSHIFT)
+	     (match_operand:VIMODES 2 "register_operand" "0,0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "@
+   <sshift>.vv\t%0,%3,%4,%1.t
+   <sshift>.vi\t%0,%3,%v4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sshift><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VIMODES
+			  [(match_operand:VIMODES 3 "register_operand")
+			   (vec_duplicate:VIMODES
+			     (match_operand:<VSUBMODE> 4 "register_operand"))]
+			 UNSPEC_VSSHIFT)
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sshift><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VIMODES
+	       [(match_operand:VIMODES 3 "register_operand" "vr")
+		(vec_duplicate:VIMODES
+		  (match_operand:<VSUBMODE> 4 "register_operand" "r"))]
+	      UNSPEC_VSSHIFT)
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<sshift>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+
+;;Vector Averaging Add and Subtract
+;;and Fractional Multiply with Rounding and Saturation
+
+(define_expand "<sat_op><mode>3"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:VIMODES 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")]
+		       UNSPEC_SAT_OP)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sat_op><mode>3_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:VIMODES 1 "register_operand" "vr")
+	      (match_operand:VIMODES 2 "register_operand" "vr")]
+	    UNSPEC_SAT_OP)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<sat_op>.vv\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sat_op><mode>3_scalar"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(unspec:VIMODES
+			[(match_operand:VIMODES 1 "register_operand")
+			 (vec_duplicate:VIMODES
+			   (match_operand:<VSUBMODE> 2 "register_operand"))]
+		       UNSPEC_SAT_OP)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sat_op><mode>3_scalar_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(unspec:VIMODES
+	     [(match_operand:VIMODES 1 "register_operand" "vr")
+	      (vec_duplicate:VIMODES
+		(match_operand:<VSUBMODE> 2 "register_operand" "r"))]
+	    UNSPEC_SAT_OP)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<sat_op>.vx\t%0,%1,%2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sat_op><mode>3_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VIMODES
+			  [(match_operand:VIMODES 3 "register_operand")
+			   (match_operand:VIMODES 4 "register_operand")]
+			 UNSPEC_SAT_OP)
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sat_op><mode>3_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VIMODES
+	       [(match_operand:VIMODES 3 "register_operand" "vr")
+		(match_operand:VIMODES 4 "register_operand" "vr")]
+	      UNSPEC_SAT_OP)
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<sat_op>.vv\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<sat_op><mode>3_scalar_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VIMODES 0 "register_operand")
+		   (unspec:VIMODES
+		     [(if_then_else:VIMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VIMODES
+			  [(match_operand:VIMODES 3 "register_operand")
+			   (vec_duplicate:VIMODES
+			     (match_operand:<VSUBMODE> 4 "register_operand"))]
+			 UNSPEC_SAT_OP)
+			(match_operand:VIMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*<sat_op><mode>3_scalar_mask_nosetvl"
+  [(set (match_operand:VIMODES 0 "register_operand" "=vr")
+	(unspec:VIMODES
+	  [(if_then_else:VIMODES
+	     (match_operand:<VCMPEQUIV> 1 "register_operand" "vm")
+	     (unspec:VIMODES
+	       [(match_operand:VIMODES 3 "register_operand" "vr")
+		(vec_duplicate:VIMODES
+		  (match_operand:<VSUBMODE> 4 "register_operand" "r"))]
+	      UNSPEC_SAT_OP)
+	     (match_operand:VIMODES 2 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "<sat_op>.vx\t%0,%3,%4,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Unit-stride Fault-Only-First Load Instructions
+
+(define_expand "vleff<VMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec:VMODES
+			[(mem:VMODES (match_operand:P 1 "register_operand"))]
+		       UNSPEC_VLEFF)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vleff<mode>_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(unspec:VMODES
+	     [(match_operand:VMODES 1 "memory_operand"  "m")]
+	    UNSPEC_VLEFF)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vle<sew>ff.v\t%0,%1"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vleff<VMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(if_then_else:VMODES
+			(match_operand:<VCMPEQUIV> 1 "register_operand")
+			(unspec:VMODES
+			  [(mem:VMODES (match_operand:P 3 "register_operand"))]
+			 UNSPEC_VLEFF)
+			(match_operand:VMODES 2 "register_operand"))
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vleff<VMODES:mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(if_then_else:VMODES
+	     (match_operand:<VCMPEQUIV> 2 "register_operand" "vm")
+	     (unspec:VMODES
+	       [(match_operand:VMODES 1 "memory_operand" "m")]
+	      UNSPEC_VLEFF)
+	     (match_operand:VMODES 3 "register_operand" "0"))
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vle<sew>ff.v\t%0,%1,%2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector atomic instructions for builtin
+
+(define_expand "<vamo><VMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec_volatile:VMODES
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (match_operand:VMODES 3 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_VAMO)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*<vamo><VMODES:mode><VIMODES:mode>_<P:mode>_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(unspec_volatile:VMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:VIMODES 2 "register_operand" "vr")
+	      (match_operand:VMODES 3 "register_operand" "0")
+	      (mem:BLK (scratch))]
+	    UNSPEC_VAMO)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "<vamo><VIMODES:sew>.v\t%0,(%1),%2,%0"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "<vamo><VMODES:mode><VIMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VMODES 0 "register_operand")
+		   (unspec:VMODES
+		     [(unspec_volatile:VMODES
+			[(match_operand:<VMODES:VCMPEQUIV> 1 "register_operand")
+			 (match_operand:P 2 "register_operand")
+			 (match_operand:VIMODES 3 "register_operand")
+			 (match_operand:VMODES 4 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_VAMO)
+		     (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+{
+})
+
+(define_insn "*<vamo><VMODES:mode><VIMODES:mode>_<P:mode>_mask_nosetvl"
+  [(set (match_operand:VMODES 0 "register_operand" "=vr")
+	(unspec:VMODES
+	  [(unspec_volatile:VMODES
+	     [(match_operand:<VMODES:VCMPEQUIV> 1 "register_operand" "vm")
+	      (match_operand:P 2 "register_operand" "r")
+	      (match_operand:VIMODES 3 "register_operand" "vr")
+	      (match_operand:VMODES 4 "register_operand" "0")
+	      (mem:BLK (scratch))]
+	    UNSPEC_VAMO)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR
+   && known_eq (GET_MODE_NUNITS (<VMODES:MODE>mode),
+		GET_MODE_NUNITS (<VIMODES:MODE>mode))"
+  "<vamo><VIMODES:sew>.v\t%0,(%2),%3,%0,%1.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Read VL
+(define_insn "riscv_vreadvl<mode>"
+  [(set (match_operand:P 0 "register_operand" "=r")
+	(unspec_volatile:P [(reg:P VL_REGNUM)] UNSPEC_READ_VL))]
+  "TARGET_VECTOR"
+  "csrr\t%0, vl"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Read Vtype
+(define_insn "read_vtype<mode>"
+  [(set (match_operand:P 0 "register_operand" "=r")
+	(unspec:P [(reg:P VTYPE_REGNUM)] UNSPEC_READ_VTYPE))]
+  "TARGET_VECTOR"
+  "csrr\t%0, vtype"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Write Vtype
+(define_insn "write_vtype<mode>"
+  [(set (reg:P VTYPE_REGNUM)
+	(unspec:P [(match_operand:P 0 "register_operand" "r")]
+	 UNSPEC_WRITE_VTYPE))]
+  "TARGET_VECTOR"
+  "vsetvl\tx0, x0, %0"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+;; Vector segment load/store
+
+(define_expand "vseg_load<VTMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 1 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_load<VTMODES:mode>_<P:mode>"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlseg<NF>e<sew>.v\t%0, (%1)"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_load<VTMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 3 "register_operand")
+			 (match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VTMODES 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_load<VTMODES:mode>_<P:mode>_mask"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:<VCMPEQUIV> 2 "register_operand" "vm")
+	      (match_operand:VTMODES 3 "register_operand" "0")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlseg<NF>e<sew>.v\t%0, (%1), %2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_ff_load<VTMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 1 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD_FIRST_FAULT)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_ff_load<VTMODES:mode>_<P:mode>"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD_FIRST_FAULT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlseg<NF>eff.v\t%0, (%1)"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_ff_load<VTMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 3 "register_operand")
+			 (match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VTMODES 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD_FIRST_FAULT)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_ff_load<VTMODES:mode>_<P:mode>_mask"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:<VCMPEQUIV> 2 "register_operand" "vm")
+	      (match_operand:VTMODES 3 "register_operand" "0")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD_FIRST_FAULT)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlseg<NF>eff.v\t%0, (%1), %2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_idx_load<VTMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (reg:<VTMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_idx_load<VTMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:VIMODES 2 "register_operand")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlxseg<NF>ei<VIMODES:sew>.v\t%0, (%1), %2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_idx_load<VTMODES:mode><VIMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VTMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 3 "register_operand")
+			 (match_operand:VIMODES 4 "register_operand")
+			 (match_operand:<VTMODES:VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VTMODES 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_idx_load<VTMODES:mode><VIMODES:mode>_<P:mode>_mask"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:VIMODES 4 "register_operand")
+	      (match_operand:<VTMODES:VCMPEQUIV> 2 "register_operand" "vm")
+	      (match_operand:VTMODES 3 "register_operand" "0")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlxseg<NF>ei<VIMODES:sew>.v\t%0, (%1), %4, %2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_strided_load<VTMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:P 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_strided_load<VTMODES:mode>_<P:mode>"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:P 2 "register_operand")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlsseg<NF>e<sew>.v\t%0, (%1), %2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_strided_load<VTMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (match_operand:VTMODES 0 "register_operand")
+		   (unspec:VTMODES
+		     [(unspec:VTMODES
+			[(match_operand:P 3 "register_operand")
+			 (match_operand:P 4 "register_operand")
+			 (match_operand:<VCMPEQUIV> 1 "register_operand")
+			 (match_operand:VTMODES 2 "register_operand")
+			 (mem:BLK (scratch))]
+		       UNSPEC_SEG_LOAD)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_strided_load<VTMODES:mode>_<P:mode>_mask"
+  [(set (match_operand:VTMODES 0 "register_operand" "=vr")
+	(unspec:VTMODES
+	  [(unspec:VTMODES
+	     [(match_operand:P 1 "register_operand" "r")
+	      (match_operand:P 4 "register_operand")
+	      (match_operand:<VCMPEQUIV> 2 "register_operand" "vm")
+	      (match_operand:VTMODES 3 "register_operand" "0")
+	      (mem:BLK (scratch))]
+	    UNSPEC_SEG_LOAD)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vlsseg<NF>e<sew>.v\t%0, (%1), %4, %2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_store<VTMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:VTMODES 0 "register_operand")]
+		       UNSPEC_SEG_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_store<VTMODES:mode>_<P:mode>"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:P 0 "register_operand" "r")
+	      (match_operand:VTMODES 1 "register_operand" "vr")]
+	    UNSPEC_SEG_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsseg<NF>e<sew>.v\t%1, (%0)"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_store<VTMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:P 2 "register_operand")
+			 (match_operand:VTMODES 1 "register_operand")
+			 (match_operand:<VCMPEQUIV> 0 "register_operand")]
+		       UNSPEC_SEG_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_store<VTMODES:mode>_<P:mode>_mask"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:P 0 "register_operand" "r")
+	      (match_operand:VTMODES 1 "register_operand" "vr")
+	      (match_operand:<VCMPEQUIV> 2 "register_operand" "vm")]
+	    UNSPEC_SEG_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsseg<NF>e<sew>.v\t%1, (%0), %2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_idx_store<VTMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (reg:<VTMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:VIMODES 2 "register_operand")
+			 (match_operand:VTMODES 0 "register_operand")]
+		       UNSPEC_SEG_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_idx_store<VTMODES:mode><VIMODES:mode>_<P:mode>"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:P 0 "register_operand" "r")
+	      (match_operand:VIMODES 2 "register_operand")
+	      (match_operand:VTMODES 1 "register_operand" "vr")]
+	    UNSPEC_SEG_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsxseg<NF>ei<VIMODES:sew>.v\t%1, (%0), %2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_idx_store<VTMODES:mode><VIMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VTMODES:VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:P 2 "register_operand")
+			 (match_operand:VIMODES 3 "register_operand")
+			 (match_operand:VTMODES 1 "register_operand")
+			 (match_operand:<VTMODES:VCMPEQUIV> 0 "register_operand")]
+		       UNSPEC_SEG_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_idx_store<VTMODES:mode><VIMODES:mode>_<P:mode>_mask"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:P 0 "register_operand" "r")
+	      (match_operand:VIMODES 3 "register_operand")
+	      (match_operand:VTMODES 1 "register_operand" "vr")
+	      (match_operand:<VTMODES:VCMPEQUIV> 2 "register_operand" "vm")]
+	    UNSPEC_SEG_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VTMODES:VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vsxseg<NF>ei<VIMODES:sew>.v\t%1, (%0), %3, %2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_strided_store<VTMODES:mode>_<P:mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:P 1 "register_operand")
+			 (match_operand:P 2 "register_operand")
+			 (match_operand:VTMODES 0 "register_operand")]
+		       UNSPEC_SEG_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_strided_store<VTMODES:mode>_<P:mode>"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:P 0 "register_operand" "r")
+	      (match_operand:P 2 "register_operand")
+	      (match_operand:VTMODES 1 "register_operand" "vr")]
+	    UNSPEC_SEG_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vssseg<NF>e<sew>.v\t%1, (%0), %2"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vseg_strided_store<VTMODES:mode>_<P:mode>_mask"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (parallel [(set (mem:BLK (scratch))
+		   (unspec:BLK
+		     [(unspec:BLK
+			[(match_operand:P 2 "register_operand")
+			 (match_operand:P 3 "register_operand")
+			 (match_operand:VTMODES 1 "register_operand")
+			 (match_operand:<VCMPEQUIV> 0 "register_operand")]
+		       UNSPEC_SEG_STORE)
+		      (reg:SI VL_REGNUM)]
+		    UNSPEC_USEVL))
+	      (use (reg:<VLMODE> VTYPE_REGNUM))])]
+  "TARGET_VECTOR"
+{
+})
+
+(define_insn "*vseg_strided_store<VTMODES:mode>_<P:mode>_mask"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK
+	  [(unspec:BLK
+	     [(match_operand:P 0 "register_operand" "r")
+	      (match_operand:P 3 "register_operand")
+	      (match_operand:VTMODES 1 "register_operand" "vr")
+	      (match_operand:<VCMPEQUIV> 2 "register_operand" "vm")]
+	    UNSPEC_SEG_STORE)
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (use (reg:<VLMODE> VTYPE_REGNUM))]
+  "TARGET_VECTOR"
+  "vssseg<NF>e<sew>.v\t%1, (%0), %3, %2.t"
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+
+(define_insn_and_split "mov<mode>"
+  [(set (reg:<VLMODE> VTYPE_REGNUM) (const_int UNSPECV_VSETVL))
+   (set (match_operand:VTMODES 0 "nonimmediate_operand" "=vr,vr, m,vr")
+	(unspec:VTMODES
+	  [(match_operand:VTMODES 1 "vector_move_operand"  " vr, m,vr,vc")
+	   (reg:SI VL_REGNUM)]
+	 UNSPEC_USEVL))
+   (clobber (match_scratch:<VSUBMODE> 2 "=X,X,X,r"))]
+  "TARGET_VECTOR"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+{
+  int i;
+  if (REG_P (operands[0]) && REG_P (operands[1]))
+    {
+      for (i = 0; i < riscv_get_nf (<MODE>mode); ++i)
+	{
+	  poly_int64 offset = i * GET_MODE_SIZE (<VTSUBMODE>mode);
+	  rtx dst_subreg = simplify_gen_subreg (<VTSUBMODE>mode, operands[0],
+						<MODE>mode, offset);
+	  rtx src_subreg = simplify_gen_subreg (<VTSUBMODE>mode, operands[1],
+						<MODE>mode, offset);
+	  emit_move_insn (dst_subreg, src_subreg);
+	}
+    }
+  else if (REG_P (operands[0]) && MEM_P (operands[1]))
+    {
+      if (TARGET_64BIT)
+	emit_insn (gen_vseg_load<mode>_di (operands[0], XEXP (operands[1], 0)));
+      else
+	emit_insn (gen_vseg_load<mode>_si (operands[0], XEXP (operands[1], 0)));
+    }
+  else if (REG_P (operands[0]) && GET_CODE (operands[1]) == CONST_VECTOR)
+    {
+      rtx val;
+      if (!const_vec_duplicate_p (operands[1], &val))
+	gcc_unreachable ();
+
+      emit_move_insn (operands[2], val);
+
+      for (i = 0; i < riscv_get_nf (<MODE>mode); ++i)
+	{
+	  poly_int64 offset = i * GET_MODE_SIZE (<VTSUBMODE>mode);
+	  rtx dst_subreg = simplify_gen_subreg (<VTSUBMODE>mode, operands[0],
+						<MODE>mode, offset);
+
+          emit_insn (gen_vec_duplicate<vtsubmode> (dst_subreg, operands[2]));
+	}
+    }
+  else if (MEM_P (operands[0]) && REG_P (operands[1]))
+    {
+      if (TARGET_64BIT)
+	emit_insn (gen_vseg_store<mode>_di (operands[1], XEXP (operands[0], 0)));
+      else
+	emit_insn (gen_vseg_store<mode>_si (operands[1], XEXP (operands[0], 0)));
+    }
+  else
+    gcc_unreachable ();
+  DONE;
+}
+  [(set_attr "type" "vector")
+   (set_attr "mode" "none")])
+
+(define_expand "vtuple_insert<mode>"
+  [(match_operand:VTMODES 0 "register_operand")
+   (match_operand:VTMODES 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")
+   (match_operand:SI 3 "const_int_operand" "")]
+  "TARGET_VECTOR"
+{
+  if (INTVAL (operands[3]) < 0
+      || (INTVAL (operands[3]) > riscv_get_nf (<MODE>mode)))
+    {
+      gcc_unreachable ();
+      FAIL;
+    }
+  poly_int64 offset = INTVAL (operands[3]) * GET_MODE_SIZE (<VTSUBMODE>mode);
+  emit_move_insn (operands[0], operands[1]);
+  rtx subreg = simplify_gen_subreg (<VTSUBMODE>mode, operands[0],
+				    <MODE>mode, offset);
+  emit_move_insn (subreg, operands[2]);
+  DONE;
+})
+
+(define_expand "vtuple_extract<mode>"
+  [(match_operand:<VTSUBMODE> 0 "register_operand")
+   (match_operand:VTMODES 1 "register_operand")
+   (match_operand:SI 2 "const_int_operand")]
+  "TARGET_VECTOR"
+{
+  if (INTVAL (operands[2]) < 0
+      || (INTVAL (operands[2]) > riscv_get_nf (<MODE>mode)))
+    {
+      gcc_unreachable ();
+      FAIL;
+    }
+  poly_int64 offset = INTVAL (operands[2]) * GET_MODE_SIZE (<VTSUBMODE>mode);
+  rtx subreg = simplify_gen_subreg (<VTSUBMODE>mode, operands[1],
+				    <MODE>mode, offset);
+  emit_move_insn (operands[0], subreg);
+  DONE;
+})
+
+;; Maybe use define_insn_and_split might get better optimization?
+(define_expand "vtuple_create<mode>"
+  [(match_operand:VTNF2MODES 0 "register_operand")
+   (match_operand:<VTSUBMODE> 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")]
+  "TARGET_VECTOR"
+{
+  riscv_expand_vtuple_create (operands);
+  DONE;
+})
+
+(define_expand "vtuple_create<mode>"
+  [(match_operand:VTNF3MODES 0 "register_operand")
+   (match_operand:<VTSUBMODE> 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")
+   (match_operand:<VTSUBMODE> 3 "register_operand")]
+  "TARGET_VECTOR"
+{
+  riscv_expand_vtuple_create (operands);
+  DONE;
+})
+
+(define_expand "vtuple_create<mode>"
+  [(match_operand:VTNF4MODES 0 "register_operand")
+   (match_operand:<VTSUBMODE> 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")
+   (match_operand:<VTSUBMODE> 3 "register_operand")
+   (match_operand:<VTSUBMODE> 4 "register_operand")]
+  "TARGET_VECTOR"
+{
+  riscv_expand_vtuple_create (operands);
+  DONE;
+})
+
+(define_expand "vtuple_create<mode>"
+  [(match_operand:VTNF5MODES 0 "register_operand")
+   (match_operand:<VTSUBMODE> 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")
+   (match_operand:<VTSUBMODE> 3 "register_operand")
+   (match_operand:<VTSUBMODE> 4 "register_operand")
+   (match_operand:<VTSUBMODE> 5 "register_operand")]
+  "TARGET_VECTOR"
+{
+  riscv_expand_vtuple_create (operands);
+  DONE;
+})
+
+(define_expand "vtuple_create<mode>"
+  [(match_operand:VTNF6MODES 0 "register_operand")
+   (match_operand:<VTSUBMODE> 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")
+   (match_operand:<VTSUBMODE> 3 "register_operand")
+   (match_operand:<VTSUBMODE> 4 "register_operand")
+   (match_operand:<VTSUBMODE> 5 "register_operand")
+   (match_operand:<VTSUBMODE> 6 "register_operand")]
+  "TARGET_VECTOR"
+{
+  riscv_expand_vtuple_create (operands);
+  DONE;
+})
+
+(define_expand "vtuple_create<mode>"
+  [(match_operand:VTNF7MODES 0 "register_operand")
+   (match_operand:<VTSUBMODE> 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")
+   (match_operand:<VTSUBMODE> 3 "register_operand")
+   (match_operand:<VTSUBMODE> 4 "register_operand")
+   (match_operand:<VTSUBMODE> 5 "register_operand")
+   (match_operand:<VTSUBMODE> 6 "register_operand")
+   (match_operand:<VTSUBMODE> 7 "register_operand")]
+  "TARGET_VECTOR"
+{
+  riscv_expand_vtuple_create (operands);
+  DONE;
+})
+
+(define_expand "vtuple_create<mode>"
+  [(match_operand:VTNF8MODES 0 "register_operand")
+   (match_operand:<VTSUBMODE> 1 "register_operand")
+   (match_operand:<VTSUBMODE> 2 "register_operand")
+   (match_operand:<VTSUBMODE> 3 "register_operand")
+   (match_operand:<VTSUBMODE> 4 "register_operand")
+   (match_operand:<VTSUBMODE> 5 "register_operand")
+   (match_operand:<VTSUBMODE> 6 "register_operand")
+   (match_operand:<VTSUBMODE> 7 "register_operand")
+   (match_operand:<VTSUBMODE> 8 "register_operand")]
+  "TARGET_VECTOR"
+{
+  riscv_expand_vtuple_create (operands);
+  DONE;
+})
+
+(define_expand "vundefined_<mode>"
+  [(clobber (match_operand:VMODES 0 "register_operand"))]
+  "TARGET_VECTOR"
+{
+})
+
+(define_expand "vundefined_<mode>"
+  [(clobber (match_operand:VTMODES 0 "register_operand"))]
+  "TARGET_VECTOR"
+{
+})
diff --git a/gcc/expr.c b/gcc/expr.c
index a302e89e799..ee71e469f76 100644
--- a/gcc/expr.c
+++ b/gcc/expr.c
@@ -7317,7 +7317,10 @@ get_inner_reference (tree exp, poly_int64_pod *pbitsize,
 
   if (size_tree != 0)
     {
-      if (! tree_fits_uhwi_p (size_tree))
+      /* ???: Accept size_tree in INTEGER_CST / POLY_INT_CST  */
+      if (POLY_INT_CST_P (size_tree))
+	*pbitsize = tree_to_poly_uint64 (size_tree);
+      else if (! tree_fits_uhwi_p (size_tree))
 	mode = BLKmode, *pbitsize = -1;
       else
 	*pbitsize = tree_to_uhwi (size_tree);
diff --git a/gcc/testsuite/g++.target/riscv/rvv_merge-1.C b/gcc/testsuite/g++.target/riscv/rvv_merge-1.C
new file mode 100644
index 00000000000..6e4853ff92e
--- /dev/null
+++ b/gcc/testsuite/g++.target/riscv/rvv_merge-1.C
@@ -0,0 +1,45 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -march=rv32gcv -mabi=ilp32d" } */
+#include <stddef.h>
+#include <riscv_vector.h>
+
+void sub2(size_t n, const float*x, float*y, float *z)
+{
+    size_t l;
+
+    vfloat32m4_t vx, vy;
+    vfloat32m4_t vz;
+
+    for ( ; (l = vsetvl_e32m4(n)) > 0; n -= l)
+    {
+        vx = * (vfloat32m4_t *) x;
+        x += l;
+        vy = * (vfloat32m4_t *) y;
+        y += l;
+        vz = (vx < vy ? vx : vy);
+        * (vfloat32m4_t *) z = vz;
+        z += l;
+    }
+}
+
+void sub3(size_t n, const int*x, int*y, int *z)
+{
+    size_t l;
+
+    vint32m4_t vx, vy;
+    vint32m4_t vz;
+
+    for ( ; (l = vsetvl_e32m4(n)) > 0; n -= l)
+    {
+        vx = * (vint32m4_t *) x;
+        x += l;
+        vy = * (vint32m4_t *) y;
+        y += l;
+        vz = (vx < vy ? vx : vy);
+        * (vint32m4_t *) z = vz;
+        z += l;
+    }
+}
+
+/* { dg-final { scan-assembler-times "vmerge.vvm" 1 } } */
+/* { dg-final { scan-assembler-times "vmin.vv" 1 } } */
diff --git a/gcc/testsuite/gcc.dg/torture/pr39074-2.c b/gcc/testsuite/gcc.dg/torture/pr39074-2.c
index 0693f2d6fce..b09bd1546aa 100644
--- a/gcc/testsuite/gcc.dg/torture/pr39074-2.c
+++ b/gcc/testsuite/gcc.dg/torture/pr39074-2.c
@@ -31,4 +31,4 @@ int main()
 }
 
 /* { dg-final { scan-tree-dump "y.._. = { i }" "alias" } } */
-/* { dg-final { scan-tree-dump "y.._., points-to NULL, points-to vars: { D..... }" "alias" } } */
+/* { dg-final { scan-tree-dump "y.._., points-to NULL, points-to vars: { D.\[0-9\]* }" "alias" } } */
diff --git a/gcc/testsuite/gcc.dg/torture/pr39074.c b/gcc/testsuite/gcc.dg/torture/pr39074.c
index 54c444e19a4..99722fa19cd 100644
--- a/gcc/testsuite/gcc.dg/torture/pr39074.c
+++ b/gcc/testsuite/gcc.dg/torture/pr39074.c
@@ -30,4 +30,4 @@ int main()
 }
 
 /* { dg-final { scan-tree-dump "y.._. = { i }" "alias" } } */
-/* { dg-final { scan-tree-dump "y.._., points-to NULL, points-to vars: { D..... }" "alias" } } */
+/* { dg-final { scan-tree-dump "y.._., points-to NULL, points-to vars: { D.\[0-9\]* }" "alias" } } */
diff --git a/gcc/testsuite/gcc.dg/torture/pta-structcopy-1.c b/gcc/testsuite/gcc.dg/torture/pta-structcopy-1.c
index f9cf8920b63..188e4ea15a8 100644
--- a/gcc/testsuite/gcc.dg/torture/pta-structcopy-1.c
+++ b/gcc/testsuite/gcc.dg/torture/pta-structcopy-1.c
@@ -32,4 +32,4 @@ int main()
 }
 
 /* { dg-final { scan-tree-dump "y.* = { i }" "ealias" } } */
-/* { dg-final { scan-tree-dump "y.*, points-to vars: { D..... }" "ealias" } } */
+/* { dg-final { scan-tree-dump "y.*, points-to vars: { D.\[0-9\]* }" "ealias" } } */
diff --git a/gcc/testsuite/gcc.target/riscv/check-stack.c b/gcc/testsuite/gcc.target/riscv/check-stack.c
new file mode 100644
index 00000000000..0736c29dbf8
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/check-stack.c
@@ -0,0 +1,12 @@
+/* { dg-do run } */
+/* { dg-options "-O0" } */
+
+/* Allocate a large stack size to check stack point can be adjusted.  */
+int main(void)
+{
+  int *p;
+  int i[2000];
+  p = __builtin_alloca(2);
+
+  return 0;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/github-pr779.c b/gcc/testsuite/gcc.target/riscv/rvv/github-pr779.c
new file mode 100644
index 00000000000..6b001483a2a
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/github-pr779.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-additional-options "-march=rv32gcv -mabi=ilp32" } */
+
+#include <riscv_vector.h>
+int32_t x_to_s(int32_t i)
+{
+   vint32m8_t v;
+   v    = vmv_s_x_i32m8(v, i);
+   return vmv_x_s_i32m8_i32(v);
+}
+
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/riscv-rvv.exp b/gcc/testsuite/gcc.target/riscv/rvv/riscv-rvv.exp
new file mode 100644
index 00000000000..3caff453fe8
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/riscv-rvv.exp
@@ -0,0 +1,43 @@
+# Copyright (C) 2017-2020 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3 of the License, or
+# (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with GCC; see the file COPYING3.  If not see
+# <http://www.gnu.org/licenses/>.
+
+# GCC testsuite that uses the `dg.exp' driver.
+
+# Exit immediately if this isn't a RISC-V target.
+if ![istarget riscv*-*-*] then {
+  return
+}
+
+# Load support procs.
+load_lib gcc-dg.exp
+
+# If a testcase doesn't have special options, use these.
+global DEFAULT_CFLAGS
+if ![info exists DEFAULT_CFLAGS] then {
+    set DEFAULT_CFLAGS " -ansi -pedantic-errors"
+}
+
+set rvv_flags "-O2 -march=rv64gcv_zfh -mabi=lp64 -std=gnu99"
+
+# Initialize `dg'.
+dg-init
+
+# Main loop.
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.\[cS\]]] \
+	$DEFAULT_CFLAGS $rvv_flags
+
+# All done.
+dg-finish
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv-common.h b/gcc/testsuite/gcc.target/riscv/rvv/rvv-common.h
new file mode 100644
index 00000000000..60961eef229
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv-common.h
@@ -0,0 +1,1064 @@
+/* MACRO take 4 argument, scalar type, vector class, vector type
+   suffix and MLEN.
+   MACRO(STYPE, VCLASST, VCLASS, EM, MLEN)  */
+
+#include <stdint.h>
+
+/* Uitl type for easier expand floating point functions.  */
+#define _RVV_F16_TYPE float16_t
+#define _RVV_F32_TYPE float
+#define _RVV_F64_TYPE double
+
+#define RVV_INT_INDEX_TEST(MACRO)			\
+  MACRO ( int8_t, int, i,  8m1,  8,  8,  8m1,  8)	\
+  MACRO ( int8_t, int, i,  8m1,  8,  8, 16m2, 16)	\
+  MACRO ( int8_t, int, i,  8m1,  8,  8, 32m4, 32)	\
+  MACRO ( int8_t, int, i,  8m1,  8,  8, 64m8, 64)	\
+  MACRO ( int8_t, int, i,  8m2,  4,  8,  8m2,  8)	\
+  MACRO ( int8_t, int, i,  8m2,  4,  8, 16m4, 16)	\
+  MACRO ( int8_t, int, i,  8m2,  4,  8, 32m8, 32)	\
+  MACRO ( int8_t, int, i,  8m4,  2,  8,  8m4,  8)	\
+  MACRO ( int8_t, int, i,  8m4,  2,  8, 16m8, 16)	\
+  MACRO ( int8_t, int, i,  8m8,  1,  8,  8m8,  8)	\
+  MACRO (int16_t, int, i, 16m1, 16, 16, 16m1, 16)	\
+  MACRO (int16_t, int, i, 16m1, 16, 16, 32m2, 32)	\
+  MACRO (int16_t, int, i, 16m1, 16, 16, 64m4, 64)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16,  8m1,  8)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16, 16m2, 16)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16, 32m4, 32)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16, 64m8, 64)	\
+  MACRO (int16_t, int, i, 16m4,  4, 16,  8m2,  8)	\
+  MACRO (int16_t, int, i, 16m4,  4, 16, 16m4, 16)	\
+  MACRO (int16_t, int, i, 16m4,  4, 16, 32m8, 32)	\
+  MACRO (int16_t, int, i, 16m8,  2, 16,  8m4,  8)	\
+  MACRO (int16_t, int, i, 16m8,  2, 16, 16m8, 16)	\
+  MACRO (int32_t, int, i, 32m1, 32, 32, 32m1, 32)	\
+  MACRO (int32_t, int, i, 32m1, 32, 32, 64m2, 64)	\
+  MACRO (int32_t, int, i, 32m2, 16, 32, 16m1, 16)	\
+  MACRO (int32_t, int, i, 32m2, 16, 32, 32m2, 32)	\
+  MACRO (int32_t, int, i, 32m2, 16, 32, 64m4, 64)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32,  8m1,  8)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32, 16m2, 16)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32, 32m4, 32)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32, 64m8, 64)	\
+  MACRO (int32_t, int, i, 32m8,  4, 32,  8m2,  8)	\
+  MACRO (int32_t, int, i, 32m8,  4, 32, 16m4, 16)	\
+  MACRO (int32_t, int, i, 32m8,  4, 32, 32m8, 32)	\
+  MACRO (int64_t, int, i, 64m1, 64, 64, 64m1, 64)	\
+  MACRO (int64_t, int, i, 64m2, 32, 64, 32m1, 32)	\
+  MACRO (int64_t, int, i, 64m2, 32, 64, 64m2, 64)	\
+  MACRO (int64_t, int, i, 64m4, 16, 64, 16m1, 16)	\
+  MACRO (int64_t, int, i, 64m4, 16, 64, 32m2, 32)	\
+  MACRO (int64_t, int, i, 64m4, 16, 64, 64m4, 64)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64,  8m1,  8)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64, 16m2, 16)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64, 32m4, 32)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64, 64m8, 64)
+
+#define RVV_UINT_INDEX_TEST(MACRO)			\
+  MACRO ( uint8_t, uint, u,  8m1,  8,  8,  8m1,  8)	\
+  MACRO ( uint8_t, uint, u,  8m1,  8,  8, 16m2, 16)	\
+  MACRO ( uint8_t, uint, u,  8m1,  8,  8, 32m4, 32)	\
+  MACRO ( uint8_t, uint, u,  8m1,  8,  8, 64m8, 64)	\
+  MACRO ( uint8_t, uint, u,  8m2,  4,  8,  8m2,  8)	\
+  MACRO ( uint8_t, uint, u,  8m2,  4,  8, 16m4, 16)	\
+  MACRO ( uint8_t, uint, u,  8m2,  4,  8, 32m8, 32)	\
+  MACRO ( uint8_t, uint, u,  8m4,  2,  8,  8m4,  8)	\
+  MACRO ( uint8_t, uint, u,  8m4,  2,  8, 16m8, 16)	\
+  MACRO ( uint8_t, uint, u,  8m8,  1,  8,  8m8,  8)	\
+  MACRO (uint16_t, uint, u, 16m1, 16, 16, 16m1, 16)	\
+  MACRO (uint16_t, uint, u, 16m1, 16, 16, 32m2, 32)	\
+  MACRO (uint16_t, uint, u, 16m1, 16, 16, 64m4, 64)	\
+  MACRO (uint16_t, uint, u, 16m2,  8, 16,  8m1,  8)	\
+  MACRO (uint16_t, uint, u, 16m2,  8, 16, 16m2, 16)	\
+  MACRO (uint16_t, uint, u, 16m2,  8, 16, 32m4, 32)	\
+  MACRO (uint16_t, uint, u, 16m2,  8, 16, 64m8, 64)	\
+  MACRO (uint16_t, uint, u, 16m4,  4, 16,  8m2,  8)	\
+  MACRO (uint16_t, uint, u, 16m4,  4, 16, 16m4, 16)	\
+  MACRO (uint16_t, uint, u, 16m4,  4, 16, 32m8, 32)	\
+  MACRO (uint16_t, uint, u, 16m8,  2, 16,  8m4,  8)	\
+  MACRO (uint16_t, uint, u, 16m8,  2, 16, 16m8, 16)	\
+  MACRO (uint32_t, uint, u, 32m1, 32, 32, 32m1, 32)	\
+  MACRO (uint32_t, uint, u, 32m1, 32, 32, 64m2, 64)	\
+  MACRO (uint32_t, uint, u, 32m2, 16, 32, 16m1, 16)	\
+  MACRO (uint32_t, uint, u, 32m2, 16, 32, 32m2, 32)	\
+  MACRO (uint32_t, uint, u, 32m2, 16, 32, 64m4, 64)	\
+  MACRO (uint32_t, uint, u, 32m4,  8, 32,  8m1,  8)	\
+  MACRO (uint32_t, uint, u, 32m4,  8, 32, 16m2, 16)	\
+  MACRO (uint32_t, uint, u, 32m4,  8, 32, 32m4, 32)	\
+  MACRO (uint32_t, uint, u, 32m4,  8, 32, 64m8, 64)	\
+  MACRO (uint32_t, uint, u, 32m8,  4, 32,  8m2,  8)	\
+  MACRO (uint32_t, uint, u, 32m8,  4, 32, 16m4, 16)	\
+  MACRO (uint32_t, uint, u, 32m8,  4, 32, 32m8, 32)	\
+  MACRO (uint64_t, uint, u, 64m1, 64, 64, 64m1, 64)	\
+  MACRO (uint64_t, uint, u, 64m2, 32, 64, 32m1, 32)	\
+  MACRO (uint64_t, uint, u, 64m2, 32, 64, 64m2, 64)	\
+  MACRO (uint64_t, uint, u, 64m4, 16, 64, 16m1, 16)	\
+  MACRO (uint64_t, uint, u, 64m4, 16, 64, 32m2, 32)	\
+  MACRO (uint64_t, uint, u, 64m4, 16, 64, 64m4, 64)	\
+  MACRO (uint64_t, uint, u, 64m8,  8, 64,  8m1,  8)	\
+  MACRO (uint64_t, uint, u, 64m8,  8, 64, 16m2, 16)	\
+  MACRO (uint64_t, uint, u, 64m8,  8, 64, 32m4, 32)	\
+  MACRO (uint64_t, uint, u, 64m8,  8, 64, 64m8, 64)
+
+#define RVV_FLOAT_INDEX_TEST(MACRO)			\
+  MACRO (float16_t, float, f, 16m1, 16, 16, 16m1, 16)	\
+  MACRO (float16_t, float, f, 16m1, 16, 16, 32m2, 32)	\
+  MACRO (float16_t, float, f, 16m1, 16, 16, 64m4, 64)	\
+  MACRO (float16_t, float, f, 16m2,  8, 16, 16m2, 16)	\
+  MACRO (float16_t, float, f, 16m2,  8, 16, 32m4, 32)	\
+  MACRO (float16_t, float, f, 16m2,  8, 16, 64m8, 64)	\
+  MACRO (float16_t, float, f, 16m4,  4, 16, 16m4, 16)	\
+  MACRO (float16_t, float, f, 16m4,  4, 16, 32m8, 32)	\
+  MACRO (float16_t, float, f, 16m8,  2, 16, 16m8, 16)	\
+  MACRO (    float, float, f, 32m1, 32, 32, 32m1, 32)	\
+  MACRO (    float, float, f, 32m1, 32, 32, 64m2, 64)	\
+  MACRO (    float, float, f, 32m2, 16, 32, 16m1, 16)	\
+  MACRO (    float, float, f, 32m2, 16, 32, 32m2, 32)	\
+  MACRO (    float, float, f, 32m2, 16, 32, 64m4, 64)	\
+  MACRO (    float, float, f, 32m4,  8, 32, 16m2, 16)	\
+  MACRO (    float, float, f, 32m4,  8, 32, 32m4, 32)	\
+  MACRO (    float, float, f, 32m4,  8, 32, 64m8, 64)	\
+  MACRO (    float, float, f, 32m8,  4, 32, 16m4, 16)	\
+  MACRO (    float, float, f, 32m8,  4, 32, 32m8, 32)	\
+  MACRO (   double, float, f, 64m1, 64, 64, 64m1, 64)	\
+  MACRO (   double, float, f, 64m2, 32, 64, 32m1, 32)	\
+  MACRO (   double, float, f, 64m2, 32, 64, 64m2, 64)	\
+  MACRO (   double, float, f, 64m4, 16, 64, 16m1, 16)	\
+  MACRO (   double, float, f, 64m4, 16, 64, 32m2, 32)	\
+  MACRO (   double, float, f, 64m4, 16, 64, 64m4, 64)	\
+  MACRO (   double, float, f, 64m8,  8, 64, 16m2, 16)	\
+  MACRO (   double, float, f, 64m8,  8, 64, 32m4, 32)	\
+  MACRO (   double, float, f, 64m8,  8, 64, 64m8, 64)
+
+#define RVV_INT_INDEX_TEST_ARG(MACRO, ...)			\
+  MACRO ( int8_t, int, i,  8m1,  8,  8,  8m1,  8, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m1,  8,  8, 16m2, 16, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m1,  8,  8, 32m4, 32, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m1,  8,  8, 64m8, 64, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m2,  4,  8,  8m2,  8, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m2,  4,  8, 16m4, 16, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m2,  4,  8, 32m8, 32, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m4,  2,  8,  8m4,  8, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m4,  2,  8, 16m8, 16, __VA_ARGS__)	\
+  MACRO ( int8_t, int, i,  8m8,  1,  8,  8m8,  8, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m1, 16, 16, 16m1, 16, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m1, 16, 16, 32m2, 32, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m1, 16, 16, 64m4, 64, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16,  8m1,  8, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16, 16m2, 16, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16, 32m4, 32, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m2,  8, 16, 64m8, 64, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m4,  4, 16,  8m2,  8, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m4,  4, 16, 16m4, 16, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m4,  4, 16, 32m8, 32, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m8,  2, 16,  8m4,  8, __VA_ARGS__)	\
+  MACRO (int16_t, int, i, 16m8,  2, 16, 16m8, 16, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m1, 32, 32, 32m1, 32, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m1, 32, 32, 64m2, 64, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m2, 16, 32, 16m1, 16, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m2, 16, 32, 32m2, 32, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m2, 16, 32, 64m4, 64, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32,  8m1,  8, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32, 16m2, 16, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32, 32m4, 32, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m4,  8, 32, 64m8, 64, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m8,  4, 32,  8m2,  8, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m8,  4, 32, 16m4, 16, __VA_ARGS__)	\
+  MACRO (int32_t, int, i, 32m8,  4, 32, 32m8, 32, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m1, 64, 64, 64m1, 64, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m2, 32, 64, 32m1, 32, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m2, 32, 64, 64m2, 64, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m4, 16, 64, 16m1, 16, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m4, 16, 64, 32m2, 32, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m4, 16, 64, 64m4, 64, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64,  8m1,  8, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64, 16m2, 16, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64, 32m4, 32, __VA_ARGS__)	\
+  MACRO (int64_t, int, i, 64m8,  8, 64, 64m8, 64, __VA_ARGS__)
+
+#define RVV_FLOAT_INDEX_TEST_ARG(MACRO, ...)			\
+  MACRO (float16_t, float, f, 16m1, 16, 16, 16m1, 16, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m1, 16, 16, 32m2, 32, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m1, 16, 16, 64m4, 64, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m2,  8, 16, 16m2, 16, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m2,  8, 16, 32m4, 32, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m2,  8, 16, 64m8, 64, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m4,  4, 16, 16m4, 16, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m4,  4, 16, 32m8, 32, __VA_ARGS__)	\
+  MACRO (float16_t, float, f, 16m8,  2, 16, 16m8, 16, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m1, 32, 32, 32m1, 32, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m1, 32, 32, 64m2, 64, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m2, 16, 32, 16m1, 16, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m2, 16, 32, 32m2, 32, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m2, 16, 32, 64m4, 64, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m4,  8, 32, 16m2, 16, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m4,  8, 32, 32m4, 32, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m4,  8, 32, 64m8, 64, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m8,  4, 32, 16m4, 16, __VA_ARGS__)	\
+  MACRO (    float, float, f, 32m8,  4, 32, 32m8, 32, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m1, 64, 64, 64m1, 64, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m2, 32, 64, 32m1, 32, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m2, 32, 64, 64m2, 64, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m4, 16, 64, 16m1, 16, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m4, 16, 64, 32m2, 32, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m4, 16, 64, 64m4, 64, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m8,  8, 64, 16m2, 16, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m8,  8, 64, 32m4, 32, __VA_ARGS__)	\
+  MACRO (   double, float, f, 64m8,  8, 64, 64m8, 64, __VA_ARGS__)
+
+#define RVV_INT_TEST(MACRO)	\
+  MACRO( int8_t, int, i,  8m1,  8, x,  8)	\
+  MACRO( int8_t, int, i,  8m2,  4, x,  8)	\
+  MACRO( int8_t, int, i,  8m4,  2, x,  8)	\
+  MACRO( int8_t, int, i,  8m8,  1, x,  8)	\
+  MACRO(int16_t, int, i, 16m1, 16, x, 16)	\
+  MACRO(int16_t, int, i, 16m2,  8, x, 16)	\
+  MACRO(int16_t, int, i, 16m4,  4, x, 16)	\
+  MACRO(int16_t, int, i, 16m8,  2, x, 16)	\
+  MACRO(int32_t, int, i, 32m1, 32, x, 32)	\
+  MACRO(int32_t, int, i, 32m2, 16, x, 32)	\
+  MACRO(int32_t, int, i, 32m4,  8, x, 32)	\
+  MACRO(int32_t, int, i, 32m8,  4, x, 32)	\
+  MACRO(int64_t, int, i, 64m1, 64, x, 64)	\
+  MACRO(int64_t, int, i, 64m2, 32, x, 64)	\
+  MACRO(int64_t, int, i, 64m4, 16, x, 64)	\
+  MACRO(int64_t, int, i, 64m8,  8, x, 64)
+
+#define RVV_INT_TEST_ARG(MACRO, ...)		\
+  MACRO( int8_t, int, i,  8m1,  8, x, 8, __VA_ARGS__)	\
+  MACRO( int8_t, int, i,  8m2,  4, x, 8, __VA_ARGS__)	\
+  MACRO( int8_t, int, i,  8m4,  2, x, 8, __VA_ARGS__)	\
+  MACRO( int8_t, int, i,  8m8,  1, x, 8, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m1, 16, x, 16, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m2,  8, x, 16, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m4,  4, x, 16, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m8,  2, x, 16, __VA_ARGS__)	\
+  MACRO(int32_t, int, i, 32m1, 32, x, 32, __VA_ARGS__)	\
+  MACRO(int32_t, int, i, 32m2, 16, x, 32, __VA_ARGS__)	\
+  MACRO(int32_t, int, i, 32m4,  8, x, 32, __VA_ARGS__)	\
+  MACRO(int32_t, int, i, 32m8,  4, x, 32, __VA_ARGS__)	\
+  MACRO(int64_t, int, i, 64m1, 64, x, 64, __VA_ARGS__)	\
+  MACRO(int64_t, int, i, 64m2, 32, x, 64, __VA_ARGS__)	\
+  MACRO(int64_t, int, i, 64m4, 16, x, 64, __VA_ARGS__)	\
+  MACRO(int64_t, int, i, 64m8,  8, x, 64, __VA_ARGS__)
+
+#define RVV_UINT_TEST_ARG(MACRO, ...)		\
+  MACRO( uint8_t, uint, u,  8m1,  8, x,  8, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m2,  4, x,  8, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m4,  2, x,  8, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m8,  1, x,  8, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m1, 16, x, 16, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m2,  8, x, 16, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m4,  4, x, 16, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m8,  2, x, 16, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m1, 32, x, 32, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m2, 16, x, 32, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m4,  8, x, 32, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m8,  4, x, 32, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m1, 64, x, 64, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m2, 32, x, 64, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m4, 16, x, 64, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m8,  8, x, 64, __VA_ARGS__)
+
+#define RVV_UINT_TEST(MACRO)	\
+  MACRO( uint8_t, uint, u,  8m1,  8, x, 8)	\
+  MACRO( uint8_t, uint, u,  8m2,  4, x, 8)	\
+  MACRO( uint8_t, uint, u,  8m4,  2, x, 8)	\
+  MACRO( uint8_t, uint, u,  8m8,  1, x, 8)	\
+  MACRO(uint16_t, uint, u, 16m1, 16, x, 16)	\
+  MACRO(uint16_t, uint, u, 16m2,  8, x, 16)	\
+  MACRO(uint16_t, uint, u, 16m4,  4, x, 16)	\
+  MACRO(uint16_t, uint, u, 16m8,  2, x, 16)	\
+  MACRO(uint32_t, uint, u, 32m1, 32, x, 32)	\
+  MACRO(uint32_t, uint, u, 32m2, 16, x, 32)	\
+  MACRO(uint32_t, uint, u, 32m4,  8, x, 32)	\
+  MACRO(uint32_t, uint, u, 32m8,  4, x, 32)	\
+  MACRO(uint64_t, uint, u, 64m1, 64, x, 64)	\
+  MACRO(uint64_t, uint, u, 64m2, 32, x, 64)	\
+  MACRO(uint64_t, uint, u, 64m4, 16, x, 64)	\
+  MACRO(uint64_t, uint, u, 64m8,  8, x, 64)
+
+#define RVV_WINT_TEST(MACRO)				\
+  MACRO( int8_t, int, i,  8m1,  8, int16_t, 16m2, x,  8, 16)	\
+  MACRO( int8_t, int, i,  8m2,  4, int16_t, 16m4, x,  8, 16)	\
+  MACRO( int8_t, int, i,  8m4,  2, int16_t, 16m8, x,  8, 16)	\
+  MACRO(int16_t, int, i, 16m1, 16, int32_t, 32m2, x, 16, 32)	\
+  MACRO(int16_t, int, i, 16m2,  8, int32_t, 32m4, x, 16, 32)	\
+  MACRO(int16_t, int, i, 16m4,  4, int32_t, 32m8, x, 16, 32)	\
+  MACRO(int32_t, int, i, 32m1, 32, int64_t, 64m2, x, 32, 64)	\
+  MACRO(int32_t, int, i, 32m2, 16, int64_t, 64m4, x, 32, 64)	\
+  MACRO(int32_t, int, i, 32m4,  8, int64_t, 64m8, x, 32, 64)
+
+/* The unsigned is added in the macro that we call.  */
+#define RVV_WUINT_TEST(MACRO)				\
+  MACRO( uint8_t, uint, u,  8m1,  8, uint16_t, 16m2, x,  8, 16)	\
+  MACRO( uint8_t, uint, u,  8m2,  4, uint16_t, 16m4, x,  8, 16)	\
+  MACRO( uint8_t, uint, u,  8m4,  2, uint16_t, 16m8, x,  8, 16)	\
+  MACRO(uint16_t, uint, u, 16m1, 16, uint32_t, 32m2, x, 16, 32)	\
+  MACRO(uint16_t, uint, u, 16m2,  8, uint32_t, 32m4, x, 16, 32)	\
+  MACRO(uint16_t, uint, u, 16m4,  4, uint32_t, 32m8, x, 16, 32)	\
+  MACRO(uint32_t, uint, u, 32m1, 32, uint64_t, 64m2, x, 32, 64)	\
+  MACRO(uint32_t, uint, u, 32m2, 16, uint64_t, 64m4, x, 32, 64)	\
+  MACRO(uint32_t, uint, u, 32m4,  8, uint64_t, 64m8, x, 32, 64)
+
+#define RVV_WINT_TEST_ARG(MACRO, ...)			\
+  MACRO( int8_t, int, i,  8m1,  8, int16_t, 16m2, x,  8, 16, __VA_ARGS__)	\
+  MACRO( int8_t, int, i,  8m2,  4, int16_t, 16m4, x,  8, 16, __VA_ARGS__)	\
+  MACRO( int8_t, int, i,  8m4,  2, int16_t, 16m8, x,  8, 16, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m1, 16, int32_t, 32m2, x, 16, 32, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m2,  8, int32_t, 32m4, x, 16, 32, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m4,  4, int32_t, 32m8, x, 16, 32, __VA_ARGS__)	\
+  MACRO(int32_t, int, i, 32m1, 32, int64_t, 64m2, x, 32, 64, __VA_ARGS__)	\
+  MACRO(int32_t, int, i, 32m2, 16, int64_t, 64m4, x, 32, 64, __VA_ARGS__)	\
+  MACRO(int32_t, int, i, 32m4,  8, int64_t, 64m8, x, 32, 64, __VA_ARGS__)
+
+/* The unsigned is added in the macro that we call.  */
+#define RVV_WUINT_TEST_ARG(MACRO, ...)				\
+  MACRO( uint8_t, uint, u,  8m1,  8, uint16_t, 16m2, x,  8, 16, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m2,  4, uint16_t, 16m4, x,  8, 16, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m4,  2, uint16_t, 16m8, x,  8, 16, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m1, 16, uint32_t, 32m2, x, 16, 32, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m2,  8, uint32_t, 32m4, x, 16, 32, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m4,  4, uint32_t, 32m8, x, 16, 32, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m1, 32, uint64_t, 64m2, x, 32, 64, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m2, 16, uint64_t, 64m4, x, 32, 64, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m4,  8, uint64_t, 64m8, x, 32, 64, __VA_ARGS__)
+
+#define RVV_QINT_TEST_ARG(MACRO, ...)				\
+  MACRO( int8_t, int, i,  8m1,  8, int32_t, 32m4, x,  8, 32, __VA_ARGS__)	\
+  MACRO( int8_t, int, i,  8m2,  4, int32_t, 32m8, x,  8, 32, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m1, 16, int64_t, 64m4, x, 16, 64, __VA_ARGS__)	\
+  MACRO(int16_t, int, i, 16m2,  8, int64_t, 64m8, x, 16, 64, __VA_ARGS__)	\
+
+#define RVV_FLOAT_TEST(MACRO)     	\
+  MACRO(float16_t, float, f, 16m1, 16, f, 16)	\
+  MACRO(float16_t, float, f, 16m2,  8, f, 16)	\
+  MACRO(float16_t, float, f, 16m4,  4, f, 16)	\
+  MACRO(float16_t, float, f, 16m8,  2, f, 16)	\
+  MACRO(    float, float, f, 32m1, 32, f, 32)	\
+  MACRO(    float, float, f, 32m2, 16, f, 32)	\
+  MACRO(    float, float, f, 32m4,  8, f, 32)	\
+  MACRO(    float, float, f, 32m8,  4, f, 32)	\
+  MACRO(   double, float, f, 64m1, 64, f, 64)	\
+  MACRO(   double, float, f, 64m2, 32, f, 64)	\
+  MACRO(   double, float, f, 64m4, 16, f, 64)	\
+  MACRO(   double, float, f, 64m8,  8, f, 64)
+
+#define RVV_FLOAT_TEST_ARG(MACRO, ...)     	\
+  MACRO(float16_t, float, f, 16m1, 16, f, 16, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m2,  8, f, 16, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m4,  4, f, 16, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m8,  2, f, 16, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m1, 32, f, 32, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m2, 16, f, 32, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m4,  8, f, 32, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m8,  4, f, 32, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m1, 64, f, 64, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m2, 32, f, 64, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m4, 16, f, 64, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m8,  8, f, 64, __VA_ARGS__)
+
+#define RVV_FLOAT_INT_TEST(MACRO)     	\
+  MACRO(float16_t, float, f, 16m1, 16, int16_t, int, 16)	\
+  MACRO(float16_t, float, f, 16m2,  8, int16_t, int, 16)	\
+  MACRO(float16_t, float, f, 16m4,  4, int16_t, int, 16)	\
+  MACRO(float16_t, float, f, 16m8,  2, int16_t, int, 16)	\
+  MACRO(    float, float, f, 32m1, 32, int32_t, int, 32)	\
+  MACRO(    float, float, f, 32m2, 16, int32_t, int, 32)	\
+  MACRO(    float, float, f, 32m4,  8, int32_t, int, 32)	\
+  MACRO(    float, float, f, 32m8,  4, int32_t, int, 32)	\
+  MACRO(   double, float, f, 64m1, 64, int64_t, int, 64)	\
+  MACRO(   double, float, f, 64m2, 32, int64_t, int, 64)	\
+  MACRO(   double, float, f, 64m4, 16, int64_t, int, 64)	\
+  MACRO(   double, float, f, 64m8,  8, int64_t, int, 64)
+
+#define RVV_FLOAT_INT_TEST_ARG(MACRO, ...)			\
+  MACRO(float16_t, float, f, 16m1, 16, int16_t, int, 16, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m2,  8, int16_t, int, 16, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m4,  4, int16_t, int, 16, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m8,  2, int16_t, int, 16, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m1, 32, int32_t, int, 32, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m2, 16, int32_t, int, 32, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m4,  8, int32_t, int, 32, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m8,  4, int32_t, int, 32, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m1, 64, int64_t, int, 64, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m2, 32, int64_t, int, 64, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m4, 16, int64_t, int, 64, __VA_ARGS__)	\
+  MACRO(   double, float, f, 64m8,  8, int64_t, int, 64, __VA_ARGS__)
+
+#define RVV_FLOAT_CVT_INT_TEST(MACRO)     	\
+  MACRO(float16_t, float, f, 16m1, 16, int16_t, int, i, 16)	\
+  MACRO(float16_t, float, f, 16m2,  8, int16_t, int, i, 16)	\
+  MACRO(float16_t, float, f, 16m4,  4, int16_t, int, i, 16)	\
+  MACRO(float16_t, float, f, 16m8,  2, int16_t, int, i, 16)	\
+  MACRO(    float, float, f, 32m1, 32, int32_t, int, i, 32)	\
+  MACRO(    float, float, f, 32m2, 16, int32_t, int, i, 32)	\
+  MACRO(    float, float, f, 32m4,  8, int32_t, int, i, 32)	\
+  MACRO(    float, float, f, 32m8,  4, int32_t, int, i, 32)	\
+  MACRO(   double, float, f, 64m1, 64, int64_t, int, i, 64)	\
+  MACRO(   double, float, f, 64m2, 32, int64_t, int, i, 64)	\
+  MACRO(   double, float, f, 64m4, 16, int64_t, int, i, 64)	\
+  MACRO(   double, float, f, 64m8,  8, int64_t, int, i, 64)
+
+#define RVV_INT_WNCVT_INT_TEST(MACRO)	\
+  MACRO( 8m1, 16m2,  8,  8, 16)		\
+  MACRO( 8m2, 16m4,  4,  8, 16)		\
+  MACRO( 8m4, 16m8,  2,  8, 16)		\
+  MACRO(16m1, 32m2, 16, 16, 32)		\
+  MACRO(16m2, 32m4,  8, 16, 32)		\
+  MACRO(16m4, 32m8,  4, 16, 32)		\
+  MACRO(32m1, 64m2, 32, 32, 64)		\
+  MACRO(32m2, 64m4, 16, 32, 64)		\
+  MACRO(32m4, 64m8,  8, 32, 64)
+
+#define RVV_WINT_EXTEND_TEST(MACRO, ...)	\
+  MACRO( 8m1, 16m2,  8,  8, 16, __VA_ARGS__)	\
+  MACRO( 8m2, 16m4,  4,  8, 16, __VA_ARGS__)	\
+  MACRO( 8m4, 16m8,  2,  8, 16, __VA_ARGS__)	\
+  MACRO(16m1, 32m2, 16, 16, 32, __VA_ARGS__)	\
+  MACRO(16m2, 32m4,  8, 16, 32, __VA_ARGS__)	\
+  MACRO(16m4, 32m8,  4, 16, 32, __VA_ARGS__)	\
+  MACRO(32m1, 64m2, 32, 32, 64, __VA_ARGS__)	\
+  MACRO(32m2, 64m4, 16, 32, 64, __VA_ARGS__)	\
+  MACRO(32m4, 64m8,  8, 32, 64, __VA_ARGS__)
+
+#define RVV_QINT_EXTEND_TEST(MACRO, ...)	\
+  MACRO( 8m1, 32m4,  8,  8, 32, __VA_ARGS__)	\
+  MACRO( 8m2, 32m8,  4,  8, 32, __VA_ARGS__)	\
+  MACRO(16m1, 64m4, 16, 16, 64, __VA_ARGS__)	\
+  MACRO(16m2, 64m8,  8, 16, 64, __VA_ARGS__)
+
+#define RVV_EINT_EXTEND_TEST(MACRO, ...)	\
+  MACRO( 8m1, 64m8,  8,  8, 64, __VA_ARGS__)
+
+#define RVV_FLOAT_WNCVT_INT_TEST(MACRO)	\
+  MACRO(16m1, 32m2, 16, 16, 32)		\
+  MACRO(16m2, 32m4,  8, 16, 32)		\
+  MACRO(16m4, 32m8,  4, 16, 32)		\
+  MACRO(32m1, 64m2, 32, 32, 64)		\
+  MACRO(32m2, 64m4, 16, 32, 64)		\
+  MACRO(32m4, 64m8,  8, 32, 64)
+
+#define RVV_WFLOAT_TEST(MACRO)				\
+  MACRO(float16_t, float, f, 16m1,  16,  float, 32m2)	\
+  MACRO(float16_t, float, f, 16m2,   8,  float, 32m4)	\
+  MACRO(float16_t, float, f, 16m4,   4,  float, 32m8)	\
+  MACRO(    float, float, f, 32m1,  32, double, 64m2)	\
+  MACRO(    float, float, f, 32m2,  16, double, 64m4)	\
+  MACRO(    float, float, f, 32m4,   8, double, 64m8)
+
+#define RVV_WFLOAT_TEST_ARG(MACRO, ...)				\
+  MACRO(float16_t, float, f, 16m1,  16,  float, 32m2, f, 16, 32, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m2,   8,  float, 32m4, f, 16, 32, __VA_ARGS__)	\
+  MACRO(float16_t, float, f, 16m4,   4,  float, 32m8, f, 16, 32, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m1,  32, double, 64m2, f, 32, 64, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m2,  16, double, 64m4, f, 32, 64, __VA_ARGS__)	\
+  MACRO(    float, float, f, 32m4,   8, double, 64m8, f, 32, 64, __VA_ARGS__)
+
+#define RVV_INT_REDUC_TEST(MACRO)	\
+  MACRO( int8_t, int, i,  8m1,  8m1,  8,  8)	\
+  MACRO( int8_t, int, i,  8m2,  8m1,  4,  8)	\
+  MACRO( int8_t, int, i,  8m4,  8m1,  2,  8)	\
+  MACRO( int8_t, int, i,  8m8,  8m1,  1,  8)	\
+  MACRO(int16_t, int, i, 16m1, 16m1, 16, 16)	\
+  MACRO(int16_t, int, i, 16m2, 16m1,  8, 16)	\
+  MACRO(int16_t, int, i, 16m4, 16m1,  4, 16)	\
+  MACRO(int16_t, int, i, 16m8, 16m1,  2, 16)	\
+  MACRO(int32_t, int, i, 32m1, 32m1, 32, 32)	\
+  MACRO(int32_t, int, i, 32m2, 32m1, 16, 32)	\
+  MACRO(int32_t, int, i, 32m4, 32m1,  8, 32)	\
+  MACRO(int32_t, int, i, 32m8, 32m1,  4, 32)	\
+  MACRO(int64_t, int, i, 64m1, 64m1, 64, 64)	\
+  MACRO(int64_t, int, i, 64m2, 64m1, 32, 64)	\
+  MACRO(int64_t, int, i, 64m4, 64m1, 16, 64)	\
+  MACRO(int64_t, int, i, 64m8, 64m1,  8, 64)
+
+#define RVV_UINT_REDUC_TEST(MACRO)	\
+  MACRO( uint8_t, uint, u,  8m1,  8m1,  8,  8)	\
+  MACRO( uint8_t, uint, u,  8m2,  8m1,  4,  8)	\
+  MACRO( uint8_t, uint, u,  8m4,  8m1,  2,  8)	\
+  MACRO( uint8_t, uint, u,  8m8,  8m1,  1,  8)	\
+  MACRO(uint16_t, uint, u, 16m1, 16m1, 16, 16)	\
+  MACRO(uint16_t, uint, u, 16m2, 16m1,  8, 16)	\
+  MACRO(uint16_t, uint, u, 16m4, 16m1,  4, 16)	\
+  MACRO(uint16_t, uint, u, 16m8, 16m1,  2, 16)	\
+  MACRO(uint32_t, uint, u, 32m1, 32m1, 32, 32)	\
+  MACRO(uint32_t, uint, u, 32m2, 32m1, 16, 32)	\
+  MACRO(uint32_t, uint, u, 32m4, 32m1,  8, 32)	\
+  MACRO(uint32_t, uint, u, 32m8, 32m1,  4, 32)	\
+  MACRO(uint64_t, uint, u, 64m1, 64m1, 64, 64)	\
+  MACRO(uint64_t, uint, u, 64m2, 64m1, 32, 64)	\
+  MACRO(uint64_t, uint, u, 64m4, 64m1, 16, 64)	\
+  MACRO(uint64_t, uint, u, 64m8, 64m1,  8, 64)
+
+#define RVV_UINT_REDUC_TEST_ARG(MACRO, ...)		\
+  MACRO( uint8_t, uint, u,  8m1,  8m1,  8, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m2,  8m1,  4, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m4,  8m1,  2, __VA_ARGS__)	\
+  MACRO( uint8_t, uint, u,  8m8,  8m1,  1, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m1, 16m1, 16, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m2, 16m1,  8, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m4, 16m1,  4, __VA_ARGS__)	\
+  MACRO(uint16_t, uint, u, 16m8, 16m1,  2, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m1, 32m1, 32, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m2, 32m1, 16, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m4, 32m1,  8, __VA_ARGS__)	\
+  MACRO(uint32_t, uint, u, 32m8, 32m1,  4, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m1, 64m1, 64, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m2, 64m1, 32, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m4, 64m1, 16, __VA_ARGS__)	\
+  MACRO(uint64_t, uint, u, 64m8, 64m1,  8, __VA_ARGS__)
+
+#define RVV_WINT_REDUC_TEST(MACRO)		\
+  MACRO( int8_t, int, i,  8m1,  8, int16_t, 16m1,  8, 16)	\
+  MACRO( int8_t, int, i,  8m2,  4, int16_t, 16m1,  8, 16)	\
+  MACRO( int8_t, int, i,  8m4,  2, int16_t, 16m1,  8, 16)	\
+  MACRO(int16_t, int, i, 16m1, 16, int32_t, 32m1, 16, 32)	\
+  MACRO(int16_t, int, i, 16m2,  8, int32_t, 32m1, 16, 32)	\
+  MACRO(int16_t, int, i, 16m4,  4, int32_t, 32m1, 16, 32)	\
+  MACRO(int32_t, int, i, 32m1, 32, int64_t, 64m1, 32, 64)	\
+  MACRO(int32_t, int, i, 32m2, 16, int64_t, 64m1, 32, 64)	\
+  MACRO(int32_t, int, i, 32m4,  8, int64_t, 64m1, 32, 64)
+
+#define RVV_WUINT_REDUC_TEST(MACRO)			\
+  MACRO( uint8_t, uint, u,  8m1,  8, uint16_t, 16m1,  8, 16)	\
+  MACRO( uint8_t, uint, u,  8m2,  4, uint16_t, 16m1,  8, 16)	\
+  MACRO( uint8_t, uint, u,  8m4,  2, uint16_t, 16m1,  8, 16)	\
+  MACRO(uint16_t, uint, u, 16m1, 16, uint32_t, 32m1, 16, 32)	\
+  MACRO(uint16_t, uint, u, 16m2,  8, uint32_t, 32m1, 16, 32)	\
+  MACRO(uint16_t, uint, u, 16m4,  4, uint32_t, 32m1, 16, 32)	\
+  MACRO(uint32_t, uint, u, 32m1, 32, uint64_t, 64m1, 32, 64)	\
+  MACRO(uint32_t, uint, u, 32m2, 16, uint64_t, 64m1, 32, 64)	\
+  MACRO(uint32_t, uint, u, 32m4,  8, uint64_t, 64m1, 32, 64)
+
+
+#define RVV_FLOAT_REDUC_TEST(MACRO)		\
+  MACRO(float16_t, float, f, 16m1, 16m1, 16, 16)	\
+  MACRO(float16_t, float, f, 16m2, 16m1,  8, 16)	\
+  MACRO(float16_t, float, f, 16m4, 16m1,  4, 16)	\
+  MACRO(float16_t, float, f, 16m8, 16m1,  2, 16)	\
+  MACRO(    float, float, f, 32m1, 32m1, 32, 32)	\
+  MACRO(    float, float, f, 32m2, 32m1, 16, 32)	\
+  MACRO(    float, float, f, 32m4, 32m1,  8, 32)	\
+  MACRO(    float, float, f, 32m8, 32m1,  4, 32)	\
+  MACRO(   double, float, f, 64m1, 64m1, 64, 64)	\
+  MACRO(   double, float, f, 64m2, 64m1, 32, 64)	\
+  MACRO(   double, float, f, 64m4, 64m1, 16, 64)	\
+  MACRO(   double, float, f, 64m8, 64m1,  8, 64)
+
+#define RVV_WFLOAT_REDUC_TEST(MACRO)			\
+  MACRO(float16_t, float, f, 16m1,  16,  float, 32m1, 16, 32)	\
+  MACRO(float16_t, float, f, 16m2,   8,  float, 32m1, 16, 32)	\
+  MACRO(float16_t, float, f, 16m4,   4,  float, 32m1, 16, 32)	\
+  MACRO(    float, float, f, 32m1,  32, double, 64m1, 32, 64)	\
+  MACRO(    float, float, f, 32m2,  16, double, 64m1, 32, 64)	\
+  MACRO(    float, float, f, 32m4,   8, double, 64m1, 32, 64)
+
+#define RVV_SEG_NO_SEW8_TEST_ARG(MACRO, ...)\
+  MACRO (16, 1, 2, 16, __VA_ARGS__)     \
+  MACRO (16, 1, 3, 16, __VA_ARGS__)     \
+  MACRO (16, 1, 4, 16, __VA_ARGS__)     \
+  MACRO (16, 1, 5, 16, __VA_ARGS__)     \
+  MACRO (16, 1, 6, 16, __VA_ARGS__)     \
+  MACRO (16, 1, 7, 16, __VA_ARGS__)     \
+  MACRO (16, 1, 8, 16, __VA_ARGS__)     \
+  MACRO (16, 2, 2,  8, __VA_ARGS__)     \
+  MACRO (16, 2, 3,  8, __VA_ARGS__)     \
+  MACRO (16, 2, 4,  8, __VA_ARGS__)     \
+  MACRO (16, 4, 2,  4, __VA_ARGS__)     \
+  MACRO (32, 1, 2, 32, __VA_ARGS__)     \
+  MACRO (32, 1, 3, 32, __VA_ARGS__)     \
+  MACRO (32, 1, 4, 32, __VA_ARGS__)     \
+  MACRO (32, 1, 5, 32, __VA_ARGS__)     \
+  MACRO (32, 1, 6, 32, __VA_ARGS__)     \
+  MACRO (32, 1, 7, 32, __VA_ARGS__)     \
+  MACRO (32, 1, 8, 32, __VA_ARGS__)     \
+  MACRO (32, 2, 2, 16, __VA_ARGS__)     \
+  MACRO (32, 2, 3, 16, __VA_ARGS__)     \
+  MACRO (32, 2, 4, 16, __VA_ARGS__)     \
+  MACRO (32, 4, 2,  8, __VA_ARGS__)     \
+  MACRO (64, 1, 2, 64, __VA_ARGS__)     \
+  MACRO (64, 1, 3, 64, __VA_ARGS__)     \
+  MACRO (64, 1, 4, 64, __VA_ARGS__)     \
+  MACRO (64, 1, 5, 64, __VA_ARGS__)     \
+  MACRO (64, 1, 6, 64, __VA_ARGS__)     \
+  MACRO (64, 1, 7, 64, __VA_ARGS__)     \
+  MACRO (64, 1, 8, 64, __VA_ARGS__)     \
+  MACRO (64, 2, 2, 32, __VA_ARGS__)     \
+  MACRO (64, 2, 3, 32, __VA_ARGS__)     \
+  MACRO (64, 2, 4, 32, __VA_ARGS__)     \
+  MACRO (64, 4, 2, 16, __VA_ARGS__)
+
+#define RVV_SEG_TEST_ARG(MACRO, ...)		\
+  RVV_SEG_NO_SEW8_TEST_ARG(MACRO, __VA_ARGS__)	\
+  MACRO ( 8, 1, 2,  8, __VA_ARGS__)     \
+  MACRO ( 8, 1, 3,  8, __VA_ARGS__)     \
+  MACRO ( 8, 1, 4,  8, __VA_ARGS__)     \
+  MACRO ( 8, 1, 5,  8, __VA_ARGS__)     \
+  MACRO ( 8, 1, 6,  8, __VA_ARGS__)     \
+  MACRO ( 8, 1, 7,  8, __VA_ARGS__)     \
+  MACRO ( 8, 1, 8,  8, __VA_ARGS__)     \
+  MACRO ( 8, 2, 2,  4, __VA_ARGS__)     \
+  MACRO ( 8, 2, 3,  4, __VA_ARGS__)     \
+  MACRO ( 8, 2, 4,  4, __VA_ARGS__)     \
+  MACRO ( 8, 4, 2,  2, __VA_ARGS__)
+
+#define VLOAD(VCLASS, SEW, EM, x) vle##SEW##_v_##VCLASS##EM (x)
+#define VILOAD(SEW, EM, x) VLOAD(i, SEW, EM, x)
+#define VULOAD(SEW, EM, x) VLOAD(u, SEW, EM, x)
+#define VFLOAD(SEW, EM, x) VLOAD(f, SEW, EM, x)
+
+#define VSTORE(VCLASS, SEW, EM, dst, x) vse##SEW##_v_##VCLASS##EM (dst, x)
+#define VISTORE(SEW, EM, dst, x) VSTORE(i, SEW, EM, dst, x)
+#define VUSTORE(SEW, EM, dst, x) VSTORE(u, SEW, EM, dst, x)
+#define VFSTORE(SEW, EM, dst, x) VSTORE(f, SEW, EM, dst, x)
+
+#define VWSTORE vse##WSEW##_v_##VCLASS##WEM
+
+#define MSET(MLEN) vmset_m_b##MLEN ()
+
+#define RVV_UNARY_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = v##OP##_v_##VCLASS##EM (vx);			\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_UNARY_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS,	\
+					  EM, MLEN, STYPEC, SEW, OP)		\
+  void rvv##OP##VCLASS##EM##_v_m_builtin_test(		\
+	 size_t n, STYPE *x, STYPE *y, STYPE z)			\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_v_##VCLASS##EM##_m (mask, vy, vx);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = v##OP##_vv_##VCLASS##EM (vx, vy);			\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_OPERATOR_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, \
+				  SEW, OP, OPERATOR)			\
+  void rvv##OP##VCLASS##EM##_v_nomask_operator_test(		\
+	 size_t n, STYPE *x, STYPE *y, STYPE z)			\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = vx OPERATOR vy;					\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS,		\
+					EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_v_m_builtin_test(		\
+	 size_t n, STYPE *x, STYPE *y, STYPE z)			\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_vv_##VCLASS##EM##_m (mask, vy, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)\
+  void rvv##OP##VCLASS##EM##_s_nomask_builtin_test(size_t n, STYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = v##OP##_v##STYPEC##_##VCLASS##EM (vx, z);			\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_OPERATOR_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, 	\
+				     SEW, OP, OPERATOR)		\
+  void rvv##OP##VCLASS##EM##_s_nomask_operator_test(size_t n, STYPE *x,\
+						STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = vx OPERATOR z;						\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS,	\
+					   EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_s_m_builtin_test(size_t n, STYPE *x,\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_v##STYPEC##_##VCLASS##EM##_m (mask, vy, vx, z);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+
+#define RVV_BIN_BUILTIN_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_i_nomask_builtin_test(size_t n, STYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = v##OP##_v##STYPEC##_##VCLASS##EM (vx, 11);			\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_OPERATOR_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,	\
+				  STYPEC, SEW, OP, OPERATOR)		\
+  void rvv##OP##VCLASS##EM##_i_nomask_operator_test(size_t n, STYPE *x,\
+					        STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = vx OPERATOR 11;					\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_BUILTIN_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS,		\
+					EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_i_m_builtin_test(size_t n, STYPE *x,\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;					\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_vx_##VCLASS##EM##_m (mask, vy, vx, 11);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_BIN_BUILTIN_VEC_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM,	\
+					    MLEN, STYPEC, SEW, OP)	\
+  RVV_BIN_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  RVV_BIN_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST(STYPE, VCLASST, VCLASS, EM,	\
+					    MLEN, STYPEC, SEW, OP)		\
+  RVV_BIN_BUILTIN_VEC_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  RVV_BIN_BUILTIN_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM,	\
+					       MLEN, STYPEC, SEW, OP)		\
+  RVV_BIN_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  RVV_BIN_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS, EM,	\
+						   MLEN, STYPEC, SEW, OP)		\
+  RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  RVV_BIN_BUILTIN_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_BIN_OPERATOR_VEC_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,	\
+					 STYPEC, SEW, OP, OPERATOR) 			\
+  RVV_BIN_OPERATOR_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP, OPERATOR)	\
+  RVV_BIN_OPERATOR_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP, OPERATOR)
+
+#define RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,	\
+					     STYPEC, SEW, OP, OPERATOR) 		\
+  RVV_BIN_OPERATOR_VEC_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP,		\
+				   OPERATOR)				\
+  RVV_BIN_OPERATOR_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP, OPERATOR)
+
+#define RVV_TER_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS,	EM, MLEN, STYPEC, SEW, OP)		\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,	\
+						   STYPE *y, STYPE *z)	\
+  {									\
+    v##VCLASST##EM##_t vx, vy, vz;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    vz = v##OP##_vv_##VCLASS##EM (vx, vy, vz);			\
+    VSTORE(VCLASS, SEW, EM, z, vz);						\
+  }
+
+#define RVV_TER_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS,			\
+					EM, MLEN, STYPEC, SEW, OP)			\
+  void rvv##OP##VCLASS##EM##_v_m_builtin_test(			\
+	 size_t n, STYPE *x, STYPE *y, STYPE z)				\
+  {									\
+    v##VCLASST##EM##_t vx, vy;					\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    mask = MSET (MLEN);					\
+    vy = v##OP##_vv_##VCLASS##EM##_m (mask, vy, vy, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);						\
+  }
+
+#define RVV_TER_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)\
+  void rvv##OP##VCLASS##EM##_s_nomask_builtin_test(size_t n, STYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = v##OP##_vx_##VCLASS##EM (vy, vx, z);		\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_TER_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS,	\
+					   EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_s_m_builtin_test(size_t n, STYPE *x,\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_vx_##VCLASS##EM##_m (mask, vy, vy, vx, z);\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_MAC_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,	\
+						   STYPE *y, STYPE *z)	\
+  {									\
+    v##VCLASST##EM##_t vx, vy, vz;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    vz = v##OP##_vv_##VCLASS##EM (vx, vy, vz);			\
+    VSTORE(VCLASS, SEW, EM, z, vz);						\
+  }
+
+#define RVV_MAC_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS,			\
+					EM, MLEN, STYPEC, SEW, OP)			\
+  void rvv##OP##VCLASS##EM##_v_m_builtin_test(			\
+	 size_t n, STYPE *x, STYPE *y, STYPE z)				\
+  {									\
+    v##VCLASST##EM##_t vx, vy;					\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    mask = MSET (MLEN);					\
+    vy = v##OP##_vv_##VCLASS##EM##_m (mask, vy, vx, vy);		\
+    VSTORE(VCLASS, SEW, EM, y, vy);						\
+  }
+
+#define RVV_MAC_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)\
+  void rvv##OP##VCLASS##EM##_s_nomask_builtin_test(size_t n, STYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = v##OP##_v##STYPEC##_##VCLASS##EM (vy, z, vx);		\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_MAC_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS,	\
+					   EM, MLEN, STYPEC, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_s_m_builtin_test(size_t n, STYPE *x,\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_v##STYPEC##_##VCLASS##EM##_m (mask, vy, z, vx);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+
+#define RVV_TER_BUILTIN_VEC_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS,		\
+					       EM, MLEN, STYPEC, SEW, OP)		\
+  RVV_TER_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  RVV_TER_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  RVV_TER_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  RVV_TER_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS,		\
+					       EM, MLEN, STYPEC, SEW, OP)		\
+  RVV_MAC_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  RVV_MAC_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  RVV_MAC_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  RVV_MAC_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_NINT_BIN_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,		\
+				      WSTYPE, WEM, STYPEC, SEW, WSEW, OP)	\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, WSTYPE *x,	\
+					       STYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##WEM##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vy = v##OP##_wv_##VCLASS##EM (vx, vy);				\
+    VSTORE(VCLASS, SEW, EM, y, vy);						\
+  }
+
+#define RVV_NINT_BIN_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,	\
+					     WSTYPE, WEM, STYPEC, SEW, WSEW, OP)\
+  void rvv##OP##VCLASS##EM##_v_m_builtin_test(		\
+	 size_t n, WSTYPE *x, STYPE *y, STYPE z)			\
+  {								\
+    v##VCLASST##WEM##_t vx;					\
+    v##VCLASST##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_wv_##VCLASS##EM##_m (mask, vy, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_NINT_BIN_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+				      WSTYPE, WEM, STYPEC, SEW, WSEW, OP)	\
+  void rvv##OP##VCLASS##EM##_s_nomask_builtin_test(size_t n, WSTYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##WEM##_t vx;					\
+    v##VCLASST##EM##_t vy;					\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = v##OP##_w##STYPEC##_##VCLASS##EM (vx, z);			\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_NINT_BIN_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+				      WSTYPE, WEM, STYPEC, SEW, WSEW, OP)		\
+  void rvv##OP##VCLASS##EM##_s_m_builtin_test(size_t n, WSTYPE *x,\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##WEM##_t vx;					\
+    v##VCLASST##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_w##STYPEC##_##VCLASS##EM##_m (mask, vy, vx, z);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_NINT_BIN_BUILTIN_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+				      WSTYPE, WEM, STYPEC, SEW, WSEW, OP)	\
+  void rvv##OP##VCLASS##EM##_i_nomask_builtin_test(size_t n, WSTYPE *x,\
+					       STYPE *y, STYPE z)\
+  {								\
+    v##VCLASST##WEM##_t vx;					\
+    v##VCLASST##EM##_t vy;					\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vy = v##OP##_w##STYPEC##_##VCLASS##EM (vx, 11);			\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_NINT_BIN_BUILTIN_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+				      WSTYPE, WEM, STYPEC, SEW, WSEW, OP)	\
+  void rvv##OP##VCLASS##EM##_i_m_builtin_test(size_t n, WSTYPE *x,\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##WEM##_t vx;					\
+    v##VCLASST##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = v##OP##_w##STYPEC##_##VCLASS##EM##_m (mask, vy, vx, 11);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+
+
+#define RVV_NINT_BIN_BUILTIN_VEC_SCALAR_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+						       WSTYPE, WEM, STYPEC, SEW, OP)	\
+  RVV_NINT_BIN_BUILTIN_VEC_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, OP)\
+  RVV_NINT_BIN_BUILTIN_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, OP)\
+  RVV_NINT_BIN_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, OP)
+
+#define RVV_NINT_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+						       WSTYPE, WEM, STYPEC, SEW, OP)		\
+  RVV_NINT_BIN_BUILTIN_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, OP)\
+  RVV_NINT_BIN_BUILTIN_VEC_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, OP)\
+  RVV_NINT_BIN_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, OP)
+
+#define RVV_SHIFT_VEC_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       uint##SEW##_t *y, STYPE z)\
+  {								\
+    v##VCLASST##EM##_t vx;					\
+    vuint##EM##_t vy;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VULOAD(SEW, EM, y);				\
+    vx = v##OP##_vv_##VCLASS##EM (vx, vy);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  RVV_BIN_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_SHIFT_VEC_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)\
+  void rvv##OP##VCLASS##EM##_v_m_builtin_test(		\
+	 size_t n, uint##SEW##_t *x, STYPE *y, STYPE z)		\
+  {								\
+    v##VCLASST##EM##_t vy;					\
+    vuint##EM##_t vx;						\
+    vbool##MLEN##_t mask;					\
+    vx = VULOAD(SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    mask = MSET (MLEN);					\
+    vy = v##OP##_vv_##VCLASS##EM##_m (mask, vy, vy, vx);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }								\
+  RVV_BIN_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+						   WSTYPE, WEM, STYPEC, SEW, WSEW, OP)\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, WSTYPE *x,	\
+					       uint##SEW##_t *z, STYPE *y)\
+  {									\
+    v##VCLASST##WEM##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vuint##EM##_t vz;							\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);					\
+    vz = VULOAD(SEW, EM, z);						\
+    vy = v##OP##_wv_##VCLASS##EM (vx, vz);				\
+    VSTORE(VCLASS, SEW, EM, y, vy);						\
+  }									\
+  RVV_NINT_BIN_BUILTIN_IMM_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)\
+  RVV_NINT_BIN_BUILTIN_SCALAR_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)
+
+#define RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN,\
+							  WSTYPE, WEM, STYPEC, SEW, WSEW, OP)	\
+  void rvv##OP##VCLASS##EM##_v_m_builtin_test(		\
+	 size_t n, WSTYPE *x, STYPE *y, uint##SEW##_t *z)	\
+  {								\
+    v##VCLASST##WEM##_t vx;					\
+    v##VCLASST##EM##_t vy;					\
+    vuint##EM##_t vz;						\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, WSEW, WEM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VULOAD(SEW, EM, z);					\
+    mask = MSET (MLEN);					\
+    vy = v##OP##_wv_##VCLASS##EM##_m (mask, vy, vx, vz);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }								\
+  RVV_NINT_BIN_BUILTIN_IMM_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)\
+  RVV_NINT_BIN_BUILTIN_SCALAR_MASKED_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub.c
new file mode 100644
index 00000000000..adbdaaa858c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, aadd)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, aaddu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, asub)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, asubu)
+
+/* { dg-final { scan-assembler-times "vaadd.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vaadd.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vaaddu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vaaddu.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vasub.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vasub.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vasubu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vasubu.vx" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub2.c
new file mode 100644
index 00000000000..5e89ab5606a
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_aaddsub2.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, aadd)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, aaddu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, asub)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, asubu)
+
+/* { dg-final { scan-assembler-times "vaadd.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vaadd.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vaaddu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vaaddu.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vasub.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vasub.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vasubu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vasubu.vx" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_adc.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_adc.c
new file mode 100644
index 00000000000..e400faf02b9
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_adc.c
@@ -0,0 +1,22 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VADC(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)             \
+  void vadc##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {               \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vbool##MLEN##_t carryin;                                                 \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    carryin = MSET (MLEN);                                          \
+    vy = vadc_vvm_##VCLASS##EM (vx, vy, carryin);                            \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                  \
+  }
+
+RVV_INT_TEST(VADC)
+
+/* { dg-final { scan-assembler-times "vadc.vv" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_add.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_add.c
new file mode 100644
index 00000000000..669cd4e83bd
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_add.c
@@ -0,0 +1,17 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, add, +)
+RVV_UINT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, add, +)
+RVV_FLOAT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_TEST, fadd, +)
+
+/* { dg-final { scan-assembler-times "vadd.vv" 64 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vadd.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vadd.vi" 32 } } */
+/* { dg-final { scan-assembler-times "vfadd.vv" 24 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vfadd.vf" 0 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_add2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_add2.c
new file mode 100644
index 00000000000..ee0c548023e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_add2.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, add)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, add)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fadd)
+
+/* { dg-final { scan-assembler-times "vadd.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vadd.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vadd.vi" 32 } } */
+/* { dg-final { scan-assembler-times "vfadd.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfadd.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_add3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_add3.c
new file mode 100644
index 00000000000..ee0c548023e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_add3.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, add)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, add)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fadd)
+
+/* { dg-final { scan-assembler-times "vadd.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vadd.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vadd.vi" 32 } } */
+/* { dg-final { scan-assembler-times "vfadd.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfadd.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_amo.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_amo.c
new file mode 100644
index 00000000000..7f48d571c17
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_amo.c
@@ -0,0 +1,98 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_AMO(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW, OP, OPU)	\
+  void test_amo##IEM##_##OP##e_##VCLASS##EM (uint##ISEW##_t *index, int##SEW##_t *y) {\
+    vint##EM##_t vy;						\
+    vuint##IEM##_t vindex;					\
+    vindex = VULOAD(ISEW, IEM, index);				\
+    vy = VILOAD(SEW, EM, y);					\
+    vy = vamo##OP##i##ISEW##_v_i##EM(y, vindex, vy);		\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }								\
+  void test_uamo##IEM##_##OP##e_##VCLASS##EM (uint##ISEW##_t *index, uint##SEW##_t *y) {\
+    vuint##EM##_t vy;						\
+    vuint##IEM##_t vindex;					\
+    vindex = VULOAD(ISEW, IEM, index);				\
+    vy = VULOAD(SEW, EM, y);					\
+    vy = vamo##OPU##i##ISEW##_v_u##EM(y, vindex, vy);		\
+    VUSTORE(SEW, EM, y, vy);					\
+  }								\
+
+#define RVV_TEST_FAMO(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW, OP)	\
+  void test_amo##IEM##_##OP##e_##VCLASS##EM (uint##ISEW##_t *index, STYPE *y) {	\
+    v##VCLASST##EM##_t vy;					\
+    vuint##IEM##_t vindex;					\
+    vindex = VULOAD(ISEW, IEM, index);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vy = vamo##OP##i##ISEW##_v_##VCLASS##EM(y, vindex, vy);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, swape, swape)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, adde, adde)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, xore, xore)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, ore, ore)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, ande, ande)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, mine, mine)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, maxe, maxe)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, minue, minue)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, maxue, maxue)
+
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, swape)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, adde)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, xore)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, ore)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, ande)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, mine)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, maxe)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, minue)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, maxue)
+
+/* { dg-final { scan-assembler-times "vamoswapei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoswapei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoswapei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoswapei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoaddei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoaddei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoaddei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoaddei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoxorei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoxorei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoxorei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoxorei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoorei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoorei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoorei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoorei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoandei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoandei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoandei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoandei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamominei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamominei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamominei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamominei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamominuei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamominuei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamominuei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamominuei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamomaxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamomaxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamomaxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamomaxei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamomaxuei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamomaxuei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamomaxuei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamomaxuei64.v" 29 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_amo2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_amo2.c
new file mode 100644
index 00000000000..196c1955bd7
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_amo2.c
@@ -0,0 +1,104 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_AMO(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW, OP, OPU)	\
+  void test_amo##IEM##_##OP##e_##VCLASS##EM (uint##ISEW##_t *index, int##SEW##_t *y) {\
+    vint##EM##_t vy;						\
+    vuint##IEM##_t vindex;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);					\
+    vindex = VULOAD(ISEW, IEM, index);				\
+    vy = VILOAD(SEW, EM, y);					\
+    vy = vamo##OP##i##ISEW##_v_i##EM##_m(mask, y, vindex, vy);	\
+    VISTORE(SEW, EM, y, vy);					\
+  }								\
+  void test_uamo##IEM##_##OP##e_##VCLASS##EM (uint##ISEW##_t *index, uint##SEW##_t *y) {\
+    vuint##EM##_t vy;						\
+    vuint##IEM##_t vindex;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);					\
+    vindex = VULOAD(ISEW, IEM, index);				\
+    vy = VULOAD(SEW, EM, y);					\
+    vy = vamo##OPU##i##ISEW##_v_u##EM##_m(mask, y, vindex, vy);	\
+    VUSTORE(SEW, EM, y, vy);					\
+  }								\
+
+#define RVV_TEST_FAMO(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW, OP)	\
+  void test_amo##IEM##_##OP##e_##VCLASS##EM (uint##ISEW##_t *index, STYPE *y) {	\
+    vbool##MLEN##_t mask;					\
+    vuint##IEM##_t vindex;					\
+    mask = MSET (MLEN);					\
+    v##VCLASST##EM##_t vy;					\
+    vindex = VULOAD(ISEW, IEM, index);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vy = vamo##OP##i##ISEW##_v_##VCLASS##EM##_m(mask, y, vindex, vy);\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, swape, swape)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, adde, adde)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, xore, xore)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, ore, ore)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, ande, ande)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, mine, mine)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, maxe, maxe)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, minue, minue)
+RVV_INT_INDEX_TEST_ARG (RVV_TEST_AMO, maxue, maxue)
+
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, swape)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, adde)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, xore)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, ore)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, ande)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, mine)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, maxe)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, minue)
+RVV_FLOAT_INDEX_TEST_ARG (RVV_TEST_FAMO, maxue)
+
+/* { dg-final { scan-assembler-times "vamoswapei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoswapei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoswapei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoswapei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoaddei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoaddei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoaddei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoaddei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoxorei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoxorei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoxorei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoxorei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoorei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoorei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoorei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoorei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamoandei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamoandei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamoandei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamoandei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamominei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamominei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamominei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamominei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamominuei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamominuei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamominuei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamominuei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamomaxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamomaxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamomaxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamomaxei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vamomaxuei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vamomaxuei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vamomaxuei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vamomaxuei64.v" 29 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_and.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_and.c
new file mode 100644
index 00000000000..7edcbb5ac43
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_and.c
@@ -0,0 +1,14 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, and, &)
+RVV_UINT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, and, &)
+
+
+/* { dg-final { scan-assembler-times "vand.vv" 64 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vand.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vand.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_and2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_and2.c
new file mode 100644
index 00000000000..d1fed761b32
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_and2.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, and)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, and)
+
+/* { dg-final { scan-assembler-times "vand.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vand.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vand.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_and3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_and3.c
new file mode 100644
index 00000000000..da2adc7f811
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_and3.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, and)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, and)
+
+
+/* { dg-final { scan-assembler-times "vand.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vand.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vand.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_biquad_df2T_stage_f32.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_biquad_df2T_stage_f32.c
new file mode 100644
index 00000000000..bd5f70aa512
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_biquad_df2T_stage_f32.c
@@ -0,0 +1,164 @@
+// https://github.com/PaulStoffregen/arm_math/blob/master/src/arm_biquad_cascade_df2T_f32.c
+
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+#define float32_t float
+typedef struct {
+  uint8_t numStages; /**< number of 2nd order stages in the filter.  Overall
+                        order is 2*numStages. */
+  float32_t *pState; /**< points to the array of state coefficients.  The array
+                        is of length 2*numStages. */
+  const float32_t *pCoeffs; /**< points to the array of coefficients.  The array
+                               is of length 5*numStages. */
+} riscv_biquad_cascade_df2T_instance_f32;
+
+void riscv_biquad_df2T_stage_f32(const float32_t *pIn, float32_t *pOut,
+                                 const float32_t *pCoeffs, float32_t *pState,
+                                 uint32_t sample);
+
+void riscv_biquad_cascade_df2T_f32(
+    const riscv_biquad_cascade_df2T_instance_f32 *S, const float32_t *pSrc,
+    float32_t *pDst, uint32_t blockSize) {
+  const float32_t *pIn = pSrc;           /*  source pointer            */
+  float32_t *pOut = pDst;                /*  destination pointer       */
+  float32_t *pState = S->pState;         /*  State pointer             */
+  const float32_t *pCoeffs = S->pCoeffs; /*  coefficient pointer       */
+  float32_t acc1;                        /*  accumulator               */
+  float32_t b0, b1, b2, a1, a2;          /*  Filter coefficients       */
+  float32_t Xn1;                         /*  temporary input           */
+  float32_t d1, d2;                      /*  state variables           */
+  uint32_t sample, stage = S->numStages; /*  loop counters             */
+
+  do {
+    sample = blockSize;
+    riscv_biquad_df2T_stage_f32(pIn, pOut, pCoeffs, pState, sample);
+    pCoeffs += 5U;
+    pState += 2U;
+    /* The current stage input is given as the output to the next stage */
+    pIn = pDst;
+
+    /*Reset the output working pointer */
+    pOut = pDst;
+
+    /* decrement the loop counter */
+    stage--;
+
+  } while (stage > 0u);
+}
+
+// scalar version
+#if 0
+void riscv_biquad_df2T_stage_f32(const float32_t *pIn, float32_t *pOut,
+                                 const float32_t *pCoeffs, float32_t *pState,
+                                 uint32_t sample) {
+  /* Reading the coefficients */
+  b0 = pCoeffs[0];
+  b1 = pCoeffs[1];
+  b2 = pCoeffs[2];
+  a1 = pCoeffs[3];
+  a2 = pCoeffs[4];
+
+  /*Reading the state values */
+  d1 = pState[0];
+  d2 = pState[1];
+
+  while (sample > 0u) {
+    /* Read the input */
+    Xn1 = *pIn++;
+
+    /* y[n] = b0 * x[n] + d1 */
+    acc1 = (b0 * Xn1) + d1;
+
+    /* Store the result in the accumulator in the destination buffer. */
+    *pOut++ = acc1;
+
+    /* Every time after the output is computed state should be updated. */
+    /* d1 = b1 * x[n] + a1 * y[n] + d2 */
+    d1 = ((b1 * Xn1) + (a1 * acc1)) + d2;
+
+    /* d2 = b2 * x[n] + a2 * y[n] */
+    d2 = (b2 * Xn1) + (a2 * acc1);
+
+    /* decrement the loop counter */
+    sample--;
+  }
+
+  /* Store the updated state variables back into the state array */
+  pState[0] = d1;
+  pState[1] = d2;
+}
+#endif
+
+// rvv intrinsic version
+// This is tail-zeroing implementation
+void riscv_biquad_df2T_stage_f32(const float32_t *pIn, float32_t *pOut,
+                                 const float32_t *pCoeffs, float32_t *pState,
+                                 uint32_t sample) {
+
+  // this kernel implementation assume VLMAX >= 3 when SEW is 32
+  // assert(rvvsetvl32m1(-1) >= 3);
+
+  /* Reading the coefficients */
+  // b0 = pCoeffs[0];
+  // b1 = pCoeffs[1];
+  // b2 = pCoeffs[2];
+  size_t vl = vsetvl_e32m1(3); // set vl = 3
+  vfloat32m1_t v_coef_b;
+  v_coef_b = vle32_v_f32m1(pCoeffs);
+  pCoeffs += vl;
+
+  // a1 = pCoeffs[3];
+  // a2 = pCoeffs[4];
+  vl = vsetvl_e32m1(2); // set vl = 2
+  vfloat32m1_t v_coef_a;
+  v_coef_a = vle32_v_f32m1(pCoeffs);
+  pCoeffs += vl;
+
+  /*Reading the state values */
+  // d1 = pState[0];
+  // d2 = pState[1];
+  vfloat32m1_t v_d;
+  v_d = *(vfloat32m1_t *)pState;
+
+  while (sample > 0u) {
+    /* Read the input */
+    float32_t xn = *pIn++;
+
+    // acc1 = (b0 * Xn1) + d1;
+    vsetvl_e32m1(3); // set vl = 3
+    v_d = vfmacc_vf_f32m1(
+        v_d, xn,
+        v_coef_b); // v_d = {b0 * x[n] + d1, b1 * x[n] + d2, b2 * x[n] + 0x0}
+    float acc1 = vfmv_f_s_f32m1_f32 (v_d); // acc1 = v_d[0] = b0 * x[n] + d1
+
+    /* Store the result in the accumulator in the destination buffer. */
+    *pOut++ = acc1;
+
+    // I'm not sure why does need to use another vector register
+    vfloat32m1_t v_slide;
+    v_slide = vcopy_v_f32m1(v_d);
+
+    v_d = vslidedown_vx_f32m1(
+	vundefined_f32m1 (),
+        v_slide,
+        0x1); // v_d = {b1 * x[n] + d2, b2 * x[n] + 0x0, nan}
+
+    // d1 = (b1 * Xn1) + (a1 * acc1) + d2;
+    // d2 = (b2 * Xn1) + (a2 * acc1);
+    vsetvl_e32m1(2); // set vl = 2
+    v_d = vfmacc_vf_f32m1(v_d, acc1,
+                                v_coef_a); // v_d = {b1 * x[n] + d2 + a1 * acc1,
+                                           // b2 * x[n] + 0x0 + a2 * acc1}
+
+    /* decrement the loop counter */
+    sample--;
+  }
+
+  // pState[0] = d1;
+  // pState[1] = d2;
+  *(vfloat32m1_t *)pState = v_d;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise.c
new file mode 100644
index 00000000000..dff06052456
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise.c
@@ -0,0 +1,22 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, xor)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, xor)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, and)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, and)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, or)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, or)
+
+/* { dg-final { scan-assembler-times "vxor.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vxor.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vxor.vi" 32 } } */
+/* { dg-final { scan-assembler-times "vand.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vand.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vand.vi" 32 } } */
+/* { dg-final { scan-assembler-times "vor.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vor.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vor.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise2.c
new file mode 100644
index 00000000000..a022e950ff5
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_bitwise2.c
@@ -0,0 +1,17 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, sll)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, sll)
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, sra)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, srl)
+
+/* { dg-final { scan-assembler-times "vsll.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vsll.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vsra.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsra.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsrl.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsrl.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_compact_non_zero.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compact_non_zero.c
new file mode 100644
index 00000000000..d3cab17da55
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compact_non_zero.c
@@ -0,0 +1,78 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+/*
+https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc
+
+----
+    # Compact non-zero elements from input memory array to output memory array
+    #
+    # size_t compact_non_zero(size_t n, const int* in, int* out)
+    # {
+    #   size_t i;
+    #   size_t count = 0;
+    #   int *p = out;
+    #
+    #   for (i=0; i<n; i++)
+    #   {
+    #       const int v = *in++;
+    #       if (v != 0)
+    #           *p++ = v;
+    #   }
+    #
+    #   return (size_t) (p - out);
+    # }
+    #
+    # a0 = n
+    # a1 = &in
+    # a2 = &out
+
+compact_non_zero:
+    li a6, 0                      # Clear count of non-zero elements
+loop:
+    vsetvli a5, a0, e32,m8  # 32-bit integers
+    vlw.v v8, (a1)                # Load input vector
+      sub a0, a0, a5              # Decrement number done
+      slli a5, a5, 2              # Multiply by four bytes
+    vmsne.vi v0, v8, 0            # Locate non-zero values
+      add a1, a1, a5              # Bump input pointer
+    vpopc.m a5, v0                # Count number of elements set in v0
+    viota.m v16, v0               # Get destination offsets of active elements
+      add a6, a6, a5              # Accumulate number of elements
+    vsll.vi v16, v16, 2, v0.t     # Multiply offsets by four bytes
+      slli a5, a5, 2              # Multiply number of non-zero elements by four bytes
+    vsuxw.v v8, (a2), v16, v0.t   # Scatter using scaled viota results under mask
+      add a2, a2, a5              # Bump output pointer
+      bnez a0, loop               # Any more?
+
+      mv a0, a6                   # Return count
+      ret
+----
+*/
+
+size_t compact_non_zero(size_t n, const int32_t* in, int32_t* out) {
+  size_t count = 0;
+  size_t vl;
+  for (; vl = vsetvl_e32m8(n); n -= vl) {
+    vint32m8_t value;
+    value = *(vint32m8_t*) in;
+    vbool4_t non_zeros_m;
+    non_zeros_m = vmsne_vx_i32m8_b4(value, 0);
+    int32_t non_zeros_count = vpopc_m_b4(non_zeros_m);
+    count += non_zeros_count;
+    vuint32m8_t offset;
+    offset = viota_m_u32m8(non_zeros_m);
+    // example:
+    // mask is           1,1,0,1
+    // offset is         2,1,1,0
+    // active offset is  ^ ^   ^
+    offset = vsll_vx_u32m8_m(non_zeros_m, offset, offset, 2); // Multiply offsets by four bytes
+    vsuxei32_v_i32m8_m(non_zeros_m, out, offset, value);
+    in+=vl;
+    out+=non_zeros_count;
+  }
+  return count;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-1.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-1.c
new file mode 100644
index 00000000000..1dc8a90d028
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-1.c
@@ -0,0 +1,83 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VCOMPARE_VV(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vv(size_t n, STYPE *x, STYPE *y, STYPE z) {         \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask = vms##OP##_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                 \
+    vx = vadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define VCOMPARE_VX(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vx(size_t n, STYPE *x, STYPE *y, STYPE z) {         \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask = vms##OP##_vx_##VCLASS##EM##_b##MLEN(vx, z);                            \
+    vx = vadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define VCOMPARE_VI(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vi(size_t n, STYPE *x, STYPE *y, STYPE z) {         \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask = vms##OP##_vx_##VCLASS##EM##_b##MLEN(vx, 12);                           \
+    vx = vadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define TEST_COMPARE_VV_VX(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VV (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  VCOMPARE_VX (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define TEST_COMPARE_VX_VI(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VX (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  VCOMPARE_VI (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define TEST_COMPARE_VV_VX_VI(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  TEST_COMPARE_VV_VX(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VI (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX_VI, eq)
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX_VI, eq)
+/* { dg-final { scan-assembler-times "vmseq.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vmseq.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vmseq.vi" 32 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX_VI, ne)
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX_VI, ne)
+/* { dg-final { scan-assembler-times "vmsne.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vmsne.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vmsne.vi" 32 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX, lt)
+/* { dg-final { scan-assembler-times "vmslt.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmslt.vx" 16 } } */
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX, ltu)
+/* { dg-final { scan-assembler-times "vmsltu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmsltu.vx" 16 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX_VI, le)
+/* { dg-final { scan-assembler-times "vmsle.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmsle.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsle.vi" 16 } } */
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX_VI, leu)
+/* { dg-final { scan-assembler-times "vmsleu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmsleu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsleu.vi" 16 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VX_VI, gt)
+/* { dg-final { scan-assembler-times "vmsgt.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsgt.vi" 16 } } */
+RVV_UINT_TEST_ARG(TEST_COMPARE_VX_VI, gtu)
+/* { dg-final { scan-assembler-times "vmsgtu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsgtu.vi" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-2.c
new file mode 100644
index 00000000000..67835d7bb8d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compare-2.c
@@ -0,0 +1,90 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VCOMPARE_VV(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vv(size_t n, STYPE *x, STYPE *y, STYPE *z) {        \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask = MSET (MLEN);                                             \
+    mask = vms##OP##_vv_##VCLASS##EM##_b##MLEN##_m(mask, mask, vx, vz);              \
+    vx = vadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define VCOMPARE_VX(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vx(size_t n, STYPE *x, STYPE *y, STYPE *z) {        \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask = MSET (MLEN);                                             \
+    mask = vms##OP##_vx_##VCLASS##EM##_b##MLEN##_m(mask, mask, vx, *z);              \
+    vx = vadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+
+#define VCOMPARE_VI(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vi(size_t n, STYPE *x, STYPE *y, STYPE *z) {        \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask = MSET (MLEN);                                             \
+    mask = vms##OP##_vx_##VCLASS##EM##_b##MLEN##_m(mask, mask, vx, 10);              \
+    vx = vadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define TEST_COMPARE_VV_VX(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VV (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  VCOMPARE_VX (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define TEST_COMPARE_VX_VI(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VX (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  VCOMPARE_VI (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+#define TEST_COMPARE_VV_VX_VI(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)	\
+  TEST_COMPARE_VV_VX(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VI (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX_VI, eq)
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX_VI, eq)
+/* { dg-final { scan-assembler-times "vmseq.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vmseq.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vmseq.vi" 32 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX_VI, ne)
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX_VI, ne)
+/* { dg-final { scan-assembler-times "vmsne.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vmsne.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vmsne.vi" 32 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX, lt)
+/* { dg-final { scan-assembler-times "vmslt.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmslt.vx" 16 } } */
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX, ltu)
+/* { dg-final { scan-assembler-times "vmsltu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmsltu.vx" 16 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VV_VX_VI, le)
+/* { dg-final { scan-assembler-times "vmsle.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmsle.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsle.vi" 16 } } */
+RVV_UINT_TEST_ARG(TEST_COMPARE_VV_VX_VI, leu)
+/* { dg-final { scan-assembler-times "vmsleu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmsleu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsleu.vi" 16 } } */
+RVV_INT_TEST_ARG(TEST_COMPARE_VX_VI, gt)
+/* { dg-final { scan-assembler-times "vmsgt.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsgt.vi" 16 } } */
+RVV_UINT_TEST_ARG(TEST_COMPARE_VX_VI, gtu)
+/* { dg-final { scan-assembler-times "vmsgtu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmsgtu.vi" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_compress.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compress.c
new file mode 100644
index 00000000000..4b14de6862e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_compress.c
@@ -0,0 +1,23 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_COMPRESS(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)	\
+  void test_compress_##VCLASS##EM (STYPE *x, STYPE *y, long s) {\
+    v##VCLASST##EM##_t vx;					\
+    v##VCLASST##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    mask = MSET (MLEN);					\
+    vx = vcompress_vm_##VCLASS##EM(mask, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+
+RVV_INT_TEST (RVV_TEST_COMPRESS)
+RVV_UINT_TEST (RVV_TEST_COMPRESS)
+RVV_FLOAT_TEST (RVV_TEST_COMPRESS)
+
+/* { dg-final { scan-assembler-times "vcompress.vm" 44 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_cond_example.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_cond_example.c
new file mode 100644
index 00000000000..af2dcb25025
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_cond_example.c
@@ -0,0 +1,52 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+// https://github.com/riscv/riscv-v-spec/blob/master/vector-examples.adoc
+// == Vector Assembly Code Examples
+
+/*
+=== Conditional example
+
+----
+# (int16) z[i] = ((int8) x[i] < 5) ? (int16) a[i] : (int16) b[i];
+#
+# Fixed 16b SEW:
+
+loop:
+    vsetvli t0, a0, e16  # Use 16b elements.
+    vlb.v v0, (a1)          # Get x[i], sign-extended to 16b
+      sub a0, a0, t0        # Decrement element count
+      add a1, a1, t0        # x[i] Bump pointer
+    vmslt.vi v0, v0, 5      # Set mask in v0
+      slli t0, t0, 1        # Multiply by 2 bytes
+    vlh.v v1, (a2), v0.t    # z[i] = a[i] case
+    vmnot.m v0, v0          # Invert v0
+      add a2, a2, t0        # a[i] bump pointer
+    vlh.v v1, (a3), v0.t    # z[i] = b[i] case
+      add a3, a3, t0        # b[i] bump pointer
+    vsh.v v1, (a4)          # Store z
+      add a4, a4, t0        # b[i] bump pointer
+      bnez a0, loop
+----
+*/
+
+void cond_example(int16_t *x, int16_t *a, int16_t *b, int16_t *z, size_t n) {
+  size_t vl;
+  vint16m1_t v0, maskedoff, result;
+  vbool16_t mask;
+  for (; vl = vsetvl_e16m1(n); n -= vl) {
+    v0 = vle16_v_i16m1(x);
+    mask = vmslt_vx_i16m1_b16(v0, 5);
+    result = vle16_v_i16m1_m(mask, maskedoff, a);
+    mask = vmnot_m_b16(mask);
+    result = vle16_v_i16m1_m(mask, result, b);
+    *(vint16m1_t *)z = result;
+    a += vl;
+    b += vl;
+    z += vl;
+    x += vl;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_div.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_div.c
new file mode 100644
index 00000000000..31b4d9db79c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_div.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, div, /)
+RVV_UINT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, div, /)
+RVV_FLOAT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_TEST, fdiv, /)
+
+/* { dg-final { scan-assembler-times "vdiv.vv" 48 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vdiv.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vdivu.vv" 48 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vdivu.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vfdiv.vv" 24 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vfdiv.vf" 0 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_div2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_div2.c
new file mode 100644
index 00000000000..653a6225d7d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_div2.c
@@ -0,0 +1,31 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, div)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, divu)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fdiv)
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VRDIV(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                    \
+  void vrsub##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {              \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vy = vfrdiv_vf_##VCLASS##EM (vy, z);;                                                               \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                  \
+  }
+
+RVV_FLOAT_TEST(VRDIV)
+
+
+/* { dg-final { scan-assembler-times "vdiv.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdiv.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vdivu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdivu.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vfdiv.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfdiv.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfrdiv.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_div3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_div3.c
new file mode 100644
index 00000000000..7809c591385
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_div3.c
@@ -0,0 +1,33 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, div)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, divu)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fdiv)
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VRDIV(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)            \
+  void vrsub##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {       \
+    v##VCLASST##EM##_t vx, vy;                                          \
+    vbool##MLEN##_t mask;                                               \
+    mask = MSET (MLEN);                                                 \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                     \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                     \
+    vy = vfrdiv_vf_##VCLASS##EM##_m (mask, vx, vy, z);			\
+    VSTORE(VCLASS, SEW, EM, y, vy);                                     \
+  }
+
+RVV_FLOAT_TEST(VRDIV)
+
+
+/* { dg-final { scan-assembler-times "vdiv.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdiv.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vdivu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdivu.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vfdiv.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfdiv.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfrdiv.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem.c
new file mode 100644
index 00000000000..c3743ee91a0
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, div)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, divu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, rem)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, remu)
+
+/* { dg-final { scan-assembler-times "vdiv.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdiv.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vrem.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vrem.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vdivu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdivu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vremu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vremu.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem2.c
new file mode 100644
index 00000000000..775cdec1d45
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_divrem2.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, div)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, divu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, rem)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, remu)
+
+/* { dg-final { scan-assembler-times "vdiv.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdiv.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vrem.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vrem.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vdivu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vdivu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vremu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vremu.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_dup_const_double.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_dup_const_double.c
new file mode 100644
index 00000000000..d6411e436e5
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_dup_const_double.c
@@ -0,0 +1,51 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdio.h>
+
+#define float64_t double
+
+int foo(double *x, double *y, int n) {
+  int count=0;
+  double t=0;
+  for(int i=0; i<n; ++i) {
+    if (*x!= 0.0) {
+      t += *x * *y;
+      count++;
+    }
+    x++;
+    y++;
+  }
+  printf("sum=%lf\n", t);
+  return count;
+}
+
+int foo_rvv(float64_t *x, float64_t *y, int n) {
+  int count = 0;
+  size_t vl;
+  /* set vlmax */
+  vsetvlmax_e64m1();
+  vfloat64m1_t vec_t, vec_zero, vec_x, vec_y;
+  vbool64_t mask;
+  vec_t = vsplat_s_f64m1(0.0);
+  vec_zero = vsplat_s_f64m1(0.0);
+  for (;vl=vsetvl_e64m1(n);n-=vl) {
+     vec_x = vle64_v_f64m1(x);
+     mask = vmfne_vf_f64m1_b64(vec_x, 0.0);
+     vec_y = vle64_v_f64m1_m(mask, vec_zero /*maskoffed*/, y);
+     vec_t = vfmacc_vv_f64m1_m(mask, vec_x, vec_y, vec_t);
+     count = count + vpopc_m_b64(mask);
+     x+=vl;
+     y+=vl;
+  }
+  /* set vlmax */
+  vsetvlmax_e64m1();
+  vfloat64m1_t vec_sum;
+  vec_sum = vfmv_s_f_f64m1(vec_sum, 0.0); /* move scalar to vec_sum[0] */
+  vec_sum = vfredsum_vs_f64m1_f64m1(vec_sum, vec_t, vec_sum);  /* vec_sum[0] = sum(vec_sum[0] , vec_t[*] ) */
+  double t = vfmv_f_s_f64m1_f64(vec_sum);  /*rd = vs2[0]*/
+  printf("sum=%lf\n", t);
+  return count;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_example1.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example1.c
new file mode 100644
index 00000000000..846421b71f0
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example1.c
@@ -0,0 +1,36 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+// sample intrinsic functions for exploration
+
+/* foo1
+#define N (1000)
+double a[N], b[N], c[N];
+Int n, I;
+
+for (i = 0; I < N; i++) {
+  a[i] = b[i] + (double)i * c[i];
+}
+*/
+
+void foo1(double *a, double *b, double *c, int n) {
+  size_t vl;
+  vfloat64m2_t vec_n_double, vec_b, vec_c;
+  // set VLMAX and init vector arrary
+  vsetvlmax_e32m1();
+  vuint32m1_t vec_i = vid_v_u32m1();
+  for (; vl = vsetvl_e64m2(n); n -= vl) {
+    vec_n_double = vfwcvt_f_xu_v_f64m2 (vec_i);
+    vec_b = vle64_v_f64m2(b);
+    vec_c = vle64_v_f64m2(c);
+    *(vfloat64m2_t *)a = vec_b + vec_n_double * vec_c;
+    vec_i = vadd_vx_u32m1(vec_i, vl);
+    a += vl;
+    b += vl;
+    c += vl;
+  }
+}
+
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_example2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example2.c
new file mode 100644
index 00000000000..82f4344e909
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example2.c
@@ -0,0 +1,52 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdio.h>
+
+// scalar code
+#if 0
+int foo(double *x, double *y, int n) {
+  int count=0;
+   t=0;
+  for(int i=0; i<n; ++i) {
+    if (*x!= 0.0) {
+      t += *x * *y;
+      count++;
+    }
+    x++;
+    y++;
+  }
+  printf("sum=%lf\n", t);
+  return count;
+}
+#endif
+
+int foo_rvv(double *x, double *y, int n) {
+  int count = 0;
+  size_t vl;
+  /* set vlmax */
+  vsetvlmax_e64m1();
+  vfloat64m1_t vec_t, vec_zero, vec_x, vec_y;
+  vbool64_t mask;
+  vec_t = vsplat_s_f64m1(0.0f);
+  vec_zero = vsplat_s_f64m1(0.0f);
+  for (; vl = vsetvl_e64m1(n); n -= vl) {
+    vec_x = vle64_v_f64m1(x);
+    mask = vmfne_vf_f64m1_b64 (vec_x, 0.0f);
+    vec_y = vle64_v_f64m1_m(mask, vec_zero /*maskoffed*/, y);
+    vec_t = vfmacc_vv_f64m1_m(mask, vec_x, vec_y, vec_t);
+    count = count + vpopc_m_b64(mask);
+    x += vl;
+    y += vl;
+  }
+  /* set vlmax */
+  vsetvlmax_e64m1();
+  vfloat64m1_t vec_sum;
+  vec_sum = vfmv_s_f_f64m1 (vec_sum, 0.0f); /* move scalar to vec_sum[0] */
+  vec_sum = vfredsum_vs_f64m1_f64m1(vec_sum, vec_t, vec_sum);  /* vec_sum[0] = sum(vec_sum[0] , vec_t[*] ) */
+  double t = vfmv_f_s_f64m1_f64 (vec_sum);  /*rd = vs2[0]*/
+  printf("sum=%lf\n", t);
+  return count;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_example3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example3.c
new file mode 100644
index 00000000000..dfdaf100646
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example3.c
@@ -0,0 +1,39 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+// sample intrinsic functions for exploration
+
+/* foo2
+#define N (1000)
+double a[N], b[N], c[N];
+Int n, i;
+
+for (i = 0; i < n; i++) {
+  if (a[i] != 0)
+    b[i] = b[i] / a[i];
+  else
+    b[i] = (double)N;
+}
+*/
+
+void foo2(double *a, double *b, double *c, int n) {
+  vfloat64m1_t vec_n, vec_a, vec_b;
+  vbool64_t mask;
+  // set VLMAX and init vector arrary
+  vsetvlmax_e64m1();
+  vec_n = vsplat_s_f64m1((double)n);
+  size_t vl;
+
+  for (; vl = vsetvl_e64m1(n); n -= vl) {
+    vec_a = vle64_v_f64m1(a);
+    vec_b = vle64_v_f64m1(b);
+    mask = vmfne_vf_f64m1_b64(vec_a, 0.0);
+    vec_b = vfdiv_vv_f64m1_m(mask, vec_n /*maskedoff*/, vec_b, vec_a);
+    *(vfloat64m1_t *)b = vec_b;
+    a += vl;
+    b += vl;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_example4.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example4.c
new file mode 100644
index 00000000000..50bdd7baac5
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example4.c
@@ -0,0 +1,45 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+#define N 1000
+
+// sample intrinsic functions for exploration
+/* foo3
+double a[N][N], b[N][N], c[N[N]];
+Int n, i;
+for I {
+  for j {
+          c[i][j] = 0; // Vectorize this loop  the j loop
+          For k c[i][j] += a[i][k] * b[k][j];
+        }
+  }
+*/
+
+// This is not an optimized algorithm
+double A[N][N], B[N][N], C[N][N];
+void foo3() {
+  int i, j, k;
+  size_t vl;
+  vfloat64m1_t vec_a, vec_b, vec_c, vec_sum;
+  for (int i = 0; i < N; ++i) {
+    for (int j = 0; j < N; ++j) {
+      k = N;
+      vsetvlmax_e64m1();
+      vec_c = vsplat_s_f64m1(0.0); /* splat scalr to vec_c */
+      for (; vl = vsetvl_e64m1(k); k -= vl) {
+        vec_a = *(vfloat64m1_t *)&A[i][N - k];
+        vec_b = *(vfloat64m1_t *)&B[N - k][j];
+        vec_c = vec_a * vec_b + vec_c;
+      }
+      vsetvlmax_e64m1();
+      vec_sum =
+          vfmv_s_f_f64m1 (vec_sum, 0.0); /* move scalar to vec_sum[0] */
+      vec_sum = vfredsum_vs_f64m1_f64m1(
+          vec_sum, vec_c, vec_sum); /* vd[0] =  sum( vec_sum[0] , vec_c[*] ) */
+      C[i][j] = vfmv_f_s_f64m1_f64 (vec_sum); /* rd = vec_sum[0] */
+    }
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_example5.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example5.c
new file mode 100644
index 00000000000..0f40f2b0fd7
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_example5.c
@@ -0,0 +1,38 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+// sample intrinsic functions for exploration
+
+/* foo4
+#define N (1000)
+double a[N], b[N], c[N], d[N], e[N];
+Int n, i;
+
+for (i = 0; i < n; i++) {
+  a[I] = a[I] + ((b[I] + c[I]) / 2) +
+         (((d[I] + e[I]) / 2) + (b[I] - e[I]) / 2); // With LMUL = 8
+}
+*/
+
+void foo4(int n, double *a, double *b, double *c, double *d, double *e) {
+  size_t vl;
+  for (; vl = vsetvl_e64m8(n); n -= vl) {
+    vfloat64m8_t vec_a;
+    // implicitly div is unsupported not.
+    // vec_a = vec_a + ((vec_b + vec_c) / 2) +
+    //        (((vec_d + vec_e) / 2) + ((vec_b - vec_e) / 2));
+    vec_a = *(vfloat64m8_t *)a +
+            vfdiv_vf_f64m8(*(vfloat64m8_t *)b + *(vfloat64m8_t *)c, 2) +
+            vfdiv_vf_f64m8(*(vfloat64m8_t *)d + *(vfloat64m8_t *)e, 2) +
+            vfdiv_vf_f64m8(*(vfloat64m8_t *)b - *(vfloat64m8_t *)e, 2);
+    *(vfloat64m8_t *)a = vec_a;
+    a += vl;
+    b += vl;
+    c += vl;
+    d += vl;
+    e += vl;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_explicit_load_store.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_explicit_load_store.c
new file mode 100644
index 00000000000..d8dfb42ca1d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_explicit_load_store.c
@@ -0,0 +1,26 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                          \
+  void vadd##VCLASS##EM(STYPE *x, STYPE *y) {                                  \
+    v##VCLASST##EM##_t vx;                                                    \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    VSTORE(VCLASS, SEW, EM, y, vx);                                                  \
+  }
+
+RVV_INT_TEST(TEST)
+RVV_FLOAT_TEST(TEST)
+
+/* { dg-final { scan-assembler-times "vle8\.v" 4 } } */
+/* { dg-final { scan-assembler-times "vle16\.v" 8 } } */
+/* { dg-final { scan-assembler-times "vle32\.v" 8 } } */
+/* { dg-final { scan-assembler-times "vle64\.v" 8 } } */
+/* { dg-final { scan-assembler-times "vse8.v\t" 4 } } */
+/* { dg-final { scan-assembler-times "vse16.v\t" 8 } } */
+/* { dg-final { scan-assembler-times "vse32.v\t" 8 } } */
+/* { dg-final { scan-assembler-times "vse64.v\t" 8 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_extend.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_extend.c
new file mode 100644
index 00000000000..19b97c67954
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_extend.c
@@ -0,0 +1,36 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_WFCVT_TEST(EM, WEM, MLEN, SEW, WSEW, NAME)					\
+  void rvv_s##NAME##_v_i##WEM##_v_nomask_builtin_test(size_t n, int##SEW##_t *x,	\
+						      int##WSEW##_t *y, int##WSEW##_t z)\
+  {											\
+    vint##EM##_t vx;									\
+    vint##WEM##_t vy;									\
+    vx = VILOAD(SEW, EM, x);								\
+    vy = vs##NAME##_i##WEM (vx);							\
+    VISTORE(WSEW, WEM, y, vy);								\
+  }											\
+  void rvv_z##NAME##_v_u##WEM##_v_nomask_builtin_test(size_t n, uint##WSEW##_t *x,	\
+						      uint##SEW##_t *y, uint##WSEW##_t z)\
+  {											\
+    vuint##WEM##_t vx;									\
+    vuint##EM##_t vy;									\
+    vy = VULOAD(SEW, EM, y);								\
+    vx = vz##NAME##_u##WEM(vy);							\
+    VUSTORE(WSEW, WEM, x, vx);								\
+  }
+
+RVV_WINT_EXTEND_TEST(RVV_WFCVT_TEST, ext_vf2)
+RVV_QINT_EXTEND_TEST(RVV_WFCVT_TEST, ext_vf4)
+RVV_EINT_EXTEND_TEST(RVV_WFCVT_TEST, ext_vf8)
+
+/* { dg-final { scan-assembler-times "vsext.vf2" 9 } } */
+/* { dg-final { scan-assembler-times "vzext.vf2" 9 } } */
+/* { dg-final { scan-assembler-times "vsext.vf4" 4 } } */
+/* { dg-final { scan-assembler-times "vzext.vf4" 4 } } */
+/* { dg-final { scan-assembler "vsext.vf8"} } */
+/* { dg-final { scan-assembler "vzext.vf8"} } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_extend2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_extend2.c
new file mode 100644
index 00000000000..56fa14065b2
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_extend2.c
@@ -0,0 +1,42 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_WFCVT_TEST(EM, WEM, MLEN, SEW, WSEW, NAME)	\
+  void rvv_s##NAME##_v_i##WEM##_v_nomask_builtin_test(size_t n, int##SEW##_t *x,	\
+						      int##WSEW##_t *y, int##WSEW##_t z)\
+  {											\
+    vint##EM##_t vx;									\
+    vint##WEM##_t vy;									\
+    vbool##MLEN##_t mask;								\
+    mask = MSET (MLEN);								\
+    vx = VILOAD(SEW, EM, x);								\
+    vy = VILOAD(WSEW, WEM, y);								\
+    vy = vs##NAME##_i##WEM##_m (mask, vy, vx);					\
+    VISTORE(WSEW, WEM, y, vy);								\
+  }											\
+  void rvv_z##NAME##_v_u##WEM##_v_nomask_builtin_test(size_t n, uint##WSEW##_t *x,	\
+						      uint##SEW##_t *y, uint##WSEW##_t z)\
+  {											\
+    vuint##WEM##_t vx;									\
+    vuint##EM##_t vy;									\
+    vbool##MLEN##_t mask;								\
+    mask = MSET (MLEN);								\
+    vx = VULOAD(WSEW, WEM, x);								\
+    vy = VULOAD(SEW, EM, y);								\
+    vx = vz##NAME##_u##WEM##_m (mask, vx, vy);					\
+    VUSTORE(WSEW, WEM, x, vx);								\
+  }
+
+RVV_WINT_EXTEND_TEST(RVV_WFCVT_TEST, ext_vf2)
+RVV_QINT_EXTEND_TEST(RVV_WFCVT_TEST, ext_vf4)
+RVV_EINT_EXTEND_TEST(RVV_WFCVT_TEST, ext_vf8)
+
+/* { dg-final { scan-assembler-times "vsext.vf2" 9 } } */
+/* { dg-final { scan-assembler-times "vzext.vf2" 9 } } */
+/* { dg-final { scan-assembler-times "vsext.vf4" 4 } } */
+/* { dg-final { scan-assembler-times "vzext.vf4" 4 } } */
+/* { dg-final { scan-assembler "vsext.vf8"} } */
+/* { dg-final { scan-assembler "vzext.vf8"} } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-1.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-1.c
new file mode 100644
index 00000000000..4fbd4cc0821
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-1.c
@@ -0,0 +1,50 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VCOMPARE_VV(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vv(size_t n, STYPE *x, STYPE *y, STYPE z) {         \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask = vmf##OP##_vv_##VCLASS##EM##_b##MLEN(vx, vy);                           \
+    vx = vfadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define VCOMPARE_VF(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vx(size_t n, STYPE *x, STYPE *y, STYPE z) {         \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask = vmf##OP##_vf_##VCLASS##EM##_b##MLEN(vx, z);                                 \
+    vx = vfadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define TEST_COMPARE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VV (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  VCOMPARE_VF (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, eq)
+/* { dg-final { scan-assembler-times "vmfeq.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmfeq.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, ne)
+/* { dg-final { scan-assembler-times "vmfne.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmfne.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, lt)
+/* { dg-final { scan-assembler-times "vmflt.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmflt.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, le)
+/* { dg-final { scan-assembler-times "vmfle.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmfle.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(VCOMPARE_VF, gt)
+/* { dg-final { scan-assembler-times "vmfgt.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(VCOMPARE_VF, ge)
+/* { dg-final { scan-assembler-times "vmfge.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-2.c
new file mode 100644
index 00000000000..d67a848a665
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcompare-2.c
@@ -0,0 +1,54 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VCOMPARE_VV(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vv(size_t n, STYPE *x, STYPE *y, STYPE *z) {        \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask = MSET (MLEN);                                             \
+    mask = vmf##OP##_vv_##VCLASS##EM##_b##MLEN##_m(mask, mask, vx, vz);             \
+    vx = vfadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define VCOMPARE_VF(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                               \
+  void v##OP##VCLASS##EM##_vx(size_t n, STYPE *x, STYPE *y, STYPE *z) {        \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask = MSET (MLEN);                                             \
+    mask = vmf##OP##_vf_##VCLASS##EM##_b##MLEN##_m(mask, mask, vx, *z);             \
+    vx = vfadd_vv_##VCLASS##EM##_m (mask, vy, vx, vy);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define TEST_COMPARE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  VCOMPARE_VV (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  VCOMPARE_VF (STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)
+
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, eq)
+/* { dg-final { scan-assembler-times "vmfeq.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmfeq.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, ne)
+/* { dg-final { scan-assembler-times "vmfne.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmfne.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, lt)
+/* { dg-final { scan-assembler-times "vmflt.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmflt.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(TEST_COMPARE, le)
+/* { dg-final { scan-assembler-times "vmfle.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vmfle.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(VCOMPARE_VF, gt)
+/* { dg-final { scan-assembler-times "vmfgt.vf" 12 } } */
+RVV_FLOAT_TEST_ARG(VCOMPARE_VF, ge)
+/* { dg-final { scan-assembler-times "vmfge.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt.c
new file mode 100644
index 00000000000..ed3e4785029
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt.c
@@ -0,0 +1,72 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_FCVT_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, ISTYPE, IVCLASST, IVCLASS, SEW)	\
+  void rvvcvtfi##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    v##IVCLASST##EM##_t vy;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = vfcvt_x_f_v_i##EM (vx);			\
+    VSTORE(IVCLASS, SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtfui##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+						u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = vfcvt_xu_f_v_u##EM (vx);			\
+    VUSTORE(SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtrtzfi##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    v##IVCLASST##EM##_t vy;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = vfcvt_rtz_x_f_v_i##EM (vx);			\
+    VSTORE(IVCLASS, SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtrtzfui##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+						u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = vfcvt_rtz_xu_f_v_u##EM (vx);			\
+    VUSTORE(SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtif##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    v##IVCLASST##EM##_t vy;						\
+    vy = VLOAD(IVCLASS, SEW, EM, y);					\
+    vx = vfcvt_f_x_v_f##EM (vy);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);						\
+  }									\
+  void rvvcvtuif##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+						u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vy = VULOAD(SEW, EM, y);					\
+    vx = vfcvt_f_xu_v_f##EM (vy);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);						\
+  }									\
+
+
+
+RVV_FLOAT_CVT_INT_TEST(RVV_FCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vfcvt.xu.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.x.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.rtz.xu.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.rtz.x.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.f.xu.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.f.x.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt2.c
new file mode 100644
index 00000000000..07961d955aa
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fcvt2.c
@@ -0,0 +1,90 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_FCVT_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, ISTYPE, IVCLASST, IVCLASS, SEW)	\
+  void rvvcvtfi##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    v##IVCLASST##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(IVCLASS, SEW, EM, y);					\
+    vy = vfcvt_x_f_v_i##EM##_m (mask, vy, vx);	\
+    VSTORE(IVCLASS, SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtfui##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+						u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vy = vfcvt_xu_f_v_u##EM##_m (mask, vy, vx);	\
+    VUSTORE(SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtrtzfi##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    v##IVCLASST##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(IVCLASS, SEW, EM, y);					\
+    vy = vfcvt_rtz_x_f_v_i##EM##_m (mask, vy, vx);	\
+    VSTORE(IVCLASS, SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtrtzfui##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+						u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vy = vfcvt_rtz_xu_f_v_u##EM##_m (mask, vy, vx);	\
+    VUSTORE(SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtif##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    v##IVCLASST##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(IVCLASS, SEW, EM, y);					\
+    vx = vfcvt_f_x_v_f##EM##_m (mask, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);						\
+  }									\
+  void rvvcvtuif##SEW##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+						u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vx = vfcvt_f_xu_v_f##EM##_m (mask, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);						\
+  }									\
+
+
+
+RVV_FLOAT_CVT_INT_TEST(RVV_FCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vfcvt.xu.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.x.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.rtz.xu.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.rtz.x.f.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.f.xu.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfcvt.f.x.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fir_filter.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fir_filter.c
new file mode 100644
index 00000000000..93fb69e4a2e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fir_filter.c
@@ -0,0 +1,121 @@
+// cmsis dsp example FIR filter
+// https://github.com/ARM-software/CMSIS_5/blob/develop/CMSIS/DSP/Source/FilteringFunctions/arm_fir_f32.c
+
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+#if 0
+// scalar version
+float *fir_kernel(float *pStateCurnt, float *pState, float *pCoeffs,
+                         const float *pSrc, float *pDst, uint32_t numTaps,
+                         uint32_t blockSize) {
+  uint32_t blkCnt = blockSize;
+
+  while (blkCnt > 0u) {
+
+    /* Copy one sample at a time into state buffer */
+    *pStateCurnt = *pSrc;
+
+    /* Set the accumulator to zero */
+    float acc0 = 0.0f;
+
+    /* Initialize state pointer */
+    float *px = pState;
+
+    /* Initialize Coefficient pointer */
+    float *pb = (pCoeffs);
+
+    uint32_t i = numTaps;
+
+    /* Perform the multiply-accumulates */
+    do {
+      acc0 += *px * *pb;
+      px++;
+      pb++;
+      i--;
+
+    } while (i > 0u);
+
+    /* The result is store in the destination buffer. */
+    *pDst = acc0;
+
+    pStateCurnt++;
+    pSrc++;
+    pDst++;
+
+    /* Advance state pointer by 1 for the next sample */
+    pState = pState + 1;
+
+    blkCnt--;
+  }
+
+  return pState;
+}
+#endif
+
+float *fir_kernel(float *pStateCurnt, float *pState, float *pCoeffs,
+                         const float *pSrc, float *pDst, uint32_t numTaps,
+                         uint32_t blockSize) {
+  uint32_t blkCnt = blockSize;
+  size_t vl;
+
+  for (; vl = vsetvl_e32m1(blkCnt); blkCnt -= vl) {
+
+    /* Copy sample vl time into state buffer */
+    *(vfloat32m1_t *)pStateCurnt = *(vfloat32m1_t *)pSrc;
+
+    // nested loop will compute pDst vl times.
+    for (int l = 0; l < vl; l++) {
+
+      /* Set the accumulator to zero */
+      float acc0 = 0.0;
+
+      /* Initialize state pointer */
+      float *px = pState;
+
+      /* Initialize Coefficient pointer */
+      float *pb = pCoeffs;
+
+      uint32_t i = numTaps;
+
+      /* Perform the multiply-accumulates */
+      // init zero vector
+      vfloat32m1_t vsum;
+      vsum = vsplat_s_f32m1(0.0);
+      size_t nested_vl;
+      for (; nested_vl = vsetvl_e32m1(i); i -= nested_vl) {
+        vfloat32m1_t *vpx = (vfloat32m1_t *)px;
+        vfloat32m1_t *vpb = (vfloat32m1_t *)pb;
+
+        // acc0 += *px * *pb;
+        vfloat32m1_t vacc;
+        vacc = vfmul_vv_f32m1(*vpx, *vpb);
+        vsum = vfredsum_vs_f32m1_f32m1(vsum, vacc, vsum); // reduction sum
+
+        // acc0 = vacc[0];
+        float tmp = vfmv_f_s_f32m1_f32(vsum);
+        acc0 += tmp;
+
+        px += nested_vl;
+        pb += nested_vl;
+
+      } while (i > 0u);
+
+      /* The result is store in the destination buffer. */
+      *pDst = acc0;
+
+      /* Advance state pointer by 1 for the next sample */
+      pState = pState + 1;
+      *pDst++;
+    }
+
+    pStateCurnt += vl;
+    pSrc += vl;
+
+  }
+
+  return pState;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection.c
new file mode 100644
index 00000000000..81b1a530812
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, fsgnj)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, fsgnjn)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, fsgnjx)
+
+/* { dg-final { scan-assembler-times "vfsgnj.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnj.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjn.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjn.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjx.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjx.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection2.c
new file mode 100644
index 00000000000..84673dbbbcf
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fsign-injection2.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fsgnj)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fsgnjn)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fsgnjx)
+
+/* { dg-final { scan-assembler-times "vfsgnj.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnj.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjn.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjn.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjx.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsgnjx.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_funary.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_funary.c
new file mode 100644
index 00000000000..2760e9ad3d7
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_funary.c
@@ -0,0 +1,23 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_FCLASS_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, ISTYPE, IVCLASS, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,	\
+					       u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = v##OP##_v_u##EM (vx);				\
+    VUSTORE(SEW, EM, y, vy);					\
+  }
+
+RVV_FLOAT_INT_TEST_ARG(RVV_FCLASS_TEST, fclass)
+
+RVV_FLOAT_TEST_ARG(RVV_UNARY_BUILTIN_VEC_TEST, fsqrt)
+
+/* { dg-final { scan-assembler-times "vfclass.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfsqrt.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_funary2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_funary2.c
new file mode 100644
index 00000000000..fe389d47f4d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_funary2.c
@@ -0,0 +1,26 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_FCLASS_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, ISTYPE, IVCLASS, SEW, OP)	\
+  void rvv##OP##VCLASS##EM##_v_nomask_builtin_test(size_t n, STYPE *x,	\
+					       u##ISTYPE *y, STYPE z)	\
+  {									\
+    v##VCLASST##EM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vy = v##OP##_v_u##EM##_m (mask, vy, vx);		\
+    VUSTORE(SEW, EM, y, vy);					\
+  }
+
+RVV_FLOAT_INT_TEST_ARG(RVV_FCLASS_TEST, fclass)
+
+RVV_FLOAT_TEST_ARG(RVV_UNARY_BUILTIN_VEC_MASKED_TEST, fsqrt)
+
+/* { dg-final { scan-assembler-times "vfclass.v" 12 } } */
+/* { dg-final { scan-assembler-times "vfsqrt.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_func_call.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_func_call.c
new file mode 100644
index 00000000000..231952fef7d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_func_call.c
@@ -0,0 +1,22 @@
+/* { dg-do compile } */
+/* { dg-additional-options "-O0 -g" } */
+
+#include <riscv_vector.h>
+
+vfloat32m8_t foo(vfloat32m8_t op0, const float a, vfloat32m8_t op1) {
+  return vfmacc_vf_f32m8(op0, a, op1);
+}
+
+void saxpy_vec(size_t n, const float a, const float *x, float *y) {
+  size_t l;
+  vfloat32m8_t vx, vy;
+  for (; (l = vsetvl_e32m8(n)) > 0; n -= l) {
+    vx = vle32_v_f32m8(x);
+    x += l;
+    vy = vle32_v_f32m8(y);
+    foo(vx, a, vy);
+    vy = vfmacc_vf_f32m8(vy, a, vx);
+    vse32_v_f32m8 (y, vy);
+    y += l;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac.c
new file mode 100644
index 00000000000..fe46df031db
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac.c
@@ -0,0 +1,49 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)		\
+  void v##OP##VCLASS##EM(size_t n, STYPE *op1, STYPE *op2, WSTYPE * op0)\
+  {									\
+    v##VCLASST##EM##_t vop1;						\
+    v##VCLASST##EM##_t vop2;						\
+    v##VCLASST##WEM##_t vop0;						\
+    vop0 = VLOAD(VCLASS, WSEW, WEM, op0);					\
+    vop1 = VLOAD(VCLASS, SEW, EM, op1);					\
+    vop2 = VLOAD(VCLASS, SEW, EM, op2);					\
+    vop0 = v##OP##_vv_##VCLASS##WEM (vop0, vop1, vop2);		\
+    VSTORE(VCLASS, WSEW, WEM, op0, vop0);					\
+  }
+
+#define VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)		\
+  void x##OP##VCLASS##EM(size_t n, STYPE  op1, STYPE *op2, WSTYPE * op0)\
+  {									\
+    v##VCLASST##EM##_t vop2;						\
+    v##VCLASST##WEM##_t vop0;						\
+    vop0 = VLOAD(VCLASS, WSEW, WEM, op0);					\
+    vop2 = VLOAD(VCLASS, SEW, EM, op2);					\
+    vop0 = v##OP##_vf_##VCLASS##WEM (vop0, op1, vop2);		\
+    VSTORE(VCLASS, WSEW, WEM, op0, vop0);					\
+  }
+
+#define VWMAC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)   \
+  VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)      \
+  VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)
+
+RVV_WFLOAT_TEST_ARG(VWMAC, fwmacc)
+RVV_WFLOAT_TEST_ARG(VWMAC, fwnmacc)
+RVV_WFLOAT_TEST_ARG(VWMAC, fwmsac)
+RVV_WFLOAT_TEST_ARG(VWMAC, fwnmsac)
+
+/* { dg-final { scan-assembler-times "vfwmacc.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmacc.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmacc.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmacc.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmsac.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmsac.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmsac.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmsac.vf" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac2.c
new file mode 100644
index 00000000000..36d3245ff56
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_fwmac2.c
@@ -0,0 +1,59 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)		\
+  void v##OP##VCLASS##EM(size_t n, STYPE *op1, STYPE *op2, WSTYPE * op0,\
+			 WSTYPE *mo)					\
+  {									\
+    v##VCLASST##EM##_t vop1;						\
+    v##VCLASST##EM##_t vop2;						\
+    v##VCLASST##WEM##_t vop0, vmo;					\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vop0 = VLOAD(VCLASS, WSEW, WEM, op0);					\
+    vmo = VLOAD(VCLASS, WSEW, WEM, mo);					\
+    vop1 = VLOAD(VCLASS, SEW, EM, op1);					\
+    vop2 = VLOAD(VCLASS, SEW, EM, op2);					\
+    vop0 = v##OP##_vv_##VCLASS##WEM##_m (mask, vop0, vop1,		\
+					      vop2);			\
+    VSTORE(VCLASS, WSEW, WEM, op0, vop0);					\
+  }
+
+#define VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)		\
+  void x##OP##VCLASS##EM(size_t n, STYPE  op1, STYPE *op2, WSTYPE * op0,\
+			 WSTYPE *mo)					\
+  {									\
+    v##VCLASST##EM##_t vop2;						\
+    v##VCLASST##WEM##_t vop0, vmo;					\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vop0 = VLOAD(VCLASS, WSEW, WEM, op0);					\
+    vmo = VLOAD(VCLASS, WSEW, WEM, mo);					\
+    vop2 = VLOAD(VCLASS, SEW, EM, op2);					\
+    vop0 = v##OP##_vf_##VCLASS##WEM##_m (mask, vop0, op1,		\
+					      vop2);			\
+    VSTORE(VCLASS, WSEW, WEM, op0, vop0);					\
+  }
+
+#define VWMAC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)   \
+  VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)      \
+  VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)
+
+RVV_WFLOAT_TEST_ARG(VWMAC, fwmacc)
+RVV_WFLOAT_TEST_ARG(VWMAC, fwnmacc)
+RVV_WFLOAT_TEST_ARG(VWMAC, fwmsac)
+RVV_WFLOAT_TEST_ARG(VWMAC, fwnmsac)
+
+/* { dg-final { scan-assembler-times "vfwmacc.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmacc.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmacc.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmacc.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmsac.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmsac.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmsac.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwnmsac.vf" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_implicit_load_store.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_implicit_load_store.c
new file mode 100644
index 00000000000..71e461acba0
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_implicit_load_store.c
@@ -0,0 +1,55 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define TEST(STYPE, VCLASS, EM)                                                \
+  void vadd##VCLASS##EM(STYPE *x, STYPE *y) {                                  \
+    *(v##VCLASS##EM##_t *)y = *(v##VCLASS##EM##_t *)x;                     \
+  }
+
+TEST(char, int, 8m1)
+TEST(char, int, 8m2)
+TEST(char, int, 8m4)
+TEST(char, int, 8m8)
+
+TEST(short, int, 16m1)
+TEST(short, int, 16m2)
+TEST(short, int, 16m4)
+TEST(short, int, 16m8)
+
+TEST(int, int, 32m1)
+TEST(int, int, 32m2)
+TEST(int, int, 32m4)
+TEST(int, int, 32m8)
+
+TEST(long long, int, 64m1)
+TEST(long long, int, 64m2)
+TEST(long long, int, 64m4)
+TEST(long long, int, 64m8)
+
+TEST (float16_t, float, 16m1)
+TEST (float16_t, float, 16m2)
+TEST (float16_t, float, 16m4)
+TEST (float16_t, float, 16m8)
+
+TEST (float, float, 32m1)
+TEST (float, float, 32m2)
+TEST (float, float, 32m4)
+TEST (float, float, 32m8)
+
+TEST (double, float, 64m1)
+TEST (double, float, 64m2)
+TEST (double, float, 64m4)
+TEST (double, float, 64m8)
+
+/* { dg-final { scan-assembler-times "vl1r\.v" 7 } } */
+/* { dg-final { scan-assembler-times "vl2r\.v" 7 } } */
+/* { dg-final { scan-assembler-times "vl4r\.v" 7 } } */
+/* { dg-final { scan-assembler-times "vl8r\.v" 7 } } */
+/* { dg-final { scan-assembler-times "vs1r\.v" 7 } } */
+/* { dg-final { scan-assembler-times "vs2r\.v" 7 } } */
+/* { dg-final { scan-assembler-times "vs4r\.v" 7 } } */
+/* { dg-final { scan-assembler-times "vs8r\.v" 7 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load-2.c
new file mode 100644
index 00000000000..8881f458f7a
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load-2.c
@@ -0,0 +1,30 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW)                   \
+  void vsloadstore##ISEW##_##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                                        STYPE *y, STYPE z, uint##ISEW##_t *index) {     \
+    v##VCLASST##EM##_t vx, vy, vz;                                                      \
+    vuint##IEM##_t vindex;                                                              \
+    vbool##MLEN##_t mask;                                                               \
+    mask = MSET (MLEN);                                                             \
+    vindex = VULOAD(ISEW, IEM, index);                                                       \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                         \
+    vx = vlxe##i##ISEW##_v_##VCLASS##EM##_m(mask, vy, x, vindex);                       \
+    vz = vx + vy;                                                                       \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                         \
+  }
+
+RVV_INT_INDEX_TEST(VSLOADSTORE)
+RVV_UINT_INDEX_TEST(VSLOADSTORE)
+RVV_FLOAT_INDEX_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vlxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vlxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vlxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vlxei64.v" 29 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load.c
new file mode 100644
index 00000000000..d673470d1ac
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-load.c
@@ -0,0 +1,27 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW)       \
+  void vsloadstore##ISEW##_##VCLASS##EM(size_t n, long stride, STYPE *x,    \
+                               STYPE *y, STYPE z, uint##ISEW##_t *index) {  \
+    v##VCLASST##EM##_t vx, vy, vz;                                          \
+    vuint##IEM##_t vindex;                                                  \
+    vbool##MLEN##_t mask;                                                   \
+    vindex = VULOAD(ISEW, IEM, index);                                           \
+    vx = vlxe##i##ISEW##_v_##VCLASS##EM(x, vindex);                            \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                             \
+    vz = vx + vy;                                                           \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                             \
+  }
+
+RVV_INT_INDEX_TEST(VSLOADSTORE)
+RVV_UINT_INDEX_TEST(VSLOADSTORE)
+RVV_FLOAT_INDEX_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vlxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vlxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vlxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vlxei64.v" 29 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store-2.c
new file mode 100644
index 00000000000..c481d6f421f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store-2.c
@@ -0,0 +1,47 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW)                              \
+  void vsloadstore##ISEW##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z, uint##ISEW##_t *index) {           \
+    v##VCLASST##EM##_t vx, vy, vz;                                            \
+    vuint##IEM##_t vindex;                                              \
+    vbool##MLEN##_t mask;                                                    \
+    mask = MSET (MLEN);                                             \
+    vindex = VULOAD(ISEW, IEM, index);                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    vsxe##i##ISEW##_v_##VCLASS##EM##_m(mask, x, vindex, vz);                                       \
+  }									\
+  void vusloadstore##ISEW##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z, uint##ISEW##_t *index) {           \
+    v##VCLASST##EM##_t vx, vy, vz;                                            \
+    vuint##IEM##_t vindex;                                              \
+    vbool##MLEN##_t mask;                                                    \
+    mask = MSET (MLEN);                                             \
+    vindex = VULOAD(ISEW, IEM, index);                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    vsuxe##i##ISEW##_v_##VCLASS##EM##_m(mask, x, vindex, vz);                                       \
+  }
+
+RVV_INT_INDEX_TEST(VSLOADSTORE)
+RVV_UINT_INDEX_TEST(VSLOADSTORE)
+RVV_FLOAT_INDEX_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vsxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vsxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vsxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vsxei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vsuxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vsuxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vsuxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vsuxei64.v" 29 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store.c
new file mode 100644
index 00000000000..c173699d35f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_indexed-store.c
@@ -0,0 +1,45 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, SEW, IEM, ISEW)             \
+  void vsloadstore##ISEW##VCLASS##EM(size_t n, long stride, STYPE *x,             \
+                                     STYPE *y, STYPE z, uint##ISEW##_t *index) {  \
+    v##VCLASST##EM##_t vx, vy, vz;                                                \
+    vuint##IEM##_t vindex;                                                        \
+    vbool##MLEN##_t mask;                                                         \
+    vindex = VULOAD(ISEW, IEM, index);                                                 \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                   \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                   \
+    vz = vx + vy;                                                                 \
+    vsxe##i##ISEW##_v_##VCLASS##EM(x, vindex, vz);                                   \
+  }									          \
+  void vsuloadstore##ISEW##VCLASS##EM(size_t n, long stride, STYPE *x,            \
+                                      STYPE *y, STYPE z, uint##ISEW##_t *index) { \
+    v##VCLASST##EM##_t vx, vy, vz;                                                \
+    vuint##IEM##_t vindex;                                                        \
+    vbool##MLEN##_t mask;                                                         \
+    vindex = VULOAD(ISEW, IEM, index);                                                 \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                   \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                   \
+    vz = vx + vy;                                                                 \
+    vsuxe##i##ISEW##_v_##VCLASS##EM(x, vindex, vz);                                  \
+  }
+
+RVV_INT_INDEX_TEST(VSLOADSTORE)
+RVV_UINT_INDEX_TEST(VSLOADSTORE)
+RVV_FLOAT_INDEX_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vsxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vsxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vsxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vsxei64.v" 29 } } */
+
+/* { dg-final { scan-assembler-times "vsuxei8.v" 20 } } */
+/* { dg-final { scan-assembler-times "vsuxei16.v" 33 } } */
+/* { dg-final { scan-assembler-times "vsuxei32.v" 34 } } */
+/* { dg-final { scan-assembler-times "vsuxei64.v" 29 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_load-global.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_load-global.c
new file mode 100644
index 00000000000..072c6f38c27
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_load-global.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+int32_t arr2[20] = {0};
+int32_t arr1[20] = {1,2, 33};
+void foo(int32_t *c){
+   vint32m1_t va, vb, vc;
+   va = vle32_v_i32m1((int32_t*)&arr1);
+   vb = vle32_v_i32m1((int32_t*)&arr2);
+   vc = va + vb;
+   vse32_v_i32m1(c, vc);
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_load-stack.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_load-stack.c
new file mode 100644
index 00000000000..d28c88f3027
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_load-stack.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+void foo(int32_t *c){
+   int32_t arr1[20] = {1,2, 33};
+   int32_t arr2[20] = {0};
+   vint32m1_t va, vb, vc;
+   va = vle32_v_i32m1((int32_t*)&arr1);
+   vb = vle32_v_i32m1((int32_t*)&arr2);
+   vc = va + vb;
+   vse32_v_i32m1(c, vc);
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_switch_elem.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_switch_elem.c
new file mode 100644
index 00000000000..2bcfa42d5da
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_switch_elem.c
@@ -0,0 +1,49 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+/*
+https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc
+
+=== Examples
+
+The SEW and LMUL settings can be changed dynamically to provide high
+throughput on mixed-width operations in a single loop.
+
+
+# Loop using only widest elements: vloop_widest_elem.c
+
+# Alternative loop that switches element widths.
+
+loop:
+    vsetvli a3, a0, e16,m4  # vtype = 16-bit integer vectors
+    vlh.v v4, (a1)          # Get 16b vector
+      slli t1, a3, 1        # Multiply length by two bytes/element
+      add a1, a1, t1        # Bump pointer
+    vwmul.vx v8, v4, x10    # 32b in <v8--v15>
+
+    vsetvli x0, a0, e32,m8  # Operate on 32b values
+    vsrl.vi v8, v8, 3
+    vsw.v v8, (a2)          # Store vector of 32b
+      slli t1, a3, 2        # Multiply length by four bytes/element
+      add a2, a2, t1        # Bump pointer
+      sub a0, a0, a3        # Decrement count
+      bnez a0, loop         # Any more?
+----
+*/
+
+void foo2(int16_t *a1, int32_t *a2, int32_t x10, int n) {
+  size_t vl;
+  for (; vl = vsetvl_e16m4(n); n -= vl) {
+    vint16m4_t v4;
+    v4 = vle16_v_i16m4(a1);
+    vint32m8_t v8;
+    v8 = vwmul_vx_i32m8(v4, x10);
+    v8 = vsra_vx_i32m8(v8, 3);
+    vse32_v_i32m8(a2, v8);
+    a1 += vl;
+    a2 += vl;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_widest_elem.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_widest_elem.c
new file mode 100644
index 00000000000..7a2d2b8d81e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_loop_widest_elem.c
@@ -0,0 +1,43 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+/*
+https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc
+
+=== Examples
+
+The SEW and LMUL settings can be changed dynamically to provide high
+throughput on mixed-width operations in a single loop.
+
+
+# Loop using only widest elements:
+
+loop:
+    vsetvli a3, a0, e32,m8  # Use only 32-bit elements
+    vlh.v v8, (a1)          # Sign-extend 16b load values to 32b elements
+      sll t1, a3, 1         # Multiply length by two bytes/element
+      add a1, a1, t1        # Bump pointer
+    vmul.vx  v8, v8, x10    # 32b multiply result
+    vsrl.vi  v8, v8, 3      # Shift elements
+    vsw.v v8, (a2)          # Store vector of 32b results
+      sll t1, a3, 2         # Multiply length by four bytes/element
+      add a2, a2, t1        # Bump pointer
+      sub a0, a0, a3        # Decrement count
+      bnez a0, loop         # Any more?
+*/
+
+void foo(int32_t *a1, int32_t *a2, int32_t x10, int n) {
+  size_t vl;
+  for (; vl = vsetvl_e32m8(n); n -= vl) {
+    vint32m8_t v8;
+    v8 = vle32_v_i32m8(a1);
+    v8 = v8 * x10;
+    v8 = vsra_vx_i32m8(v8, 3);
+    vse32_v_i32m8(a2, v8);
+    a1 += vl;
+    a2 += vl;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mac.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mac.c
new file mode 100644
index 00000000000..0b09a4397b2
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mac.c
@@ -0,0 +1,37 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, macc)
+RVV_INT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, nmsac)
+RVV_INT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, madd)
+RVV_INT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, nmsub)
+RVV_UINT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, macc)
+RVV_UINT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, nmsac)
+RVV_UINT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, madd)
+RVV_UINT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, nmsub)
+
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fmacc)
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fnmacc)
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fmsac)
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fnmsac)
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fmadd)
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fnmadd)
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fmsub)
+RVV_FLOAT_TEST_ARG(RVV_MAC_BUILTIN_VEC_SCALAR_MASKED_TEST, fnmsub)
+
+/* { dg-final { scan-assembler-times "v(?:macc|madd)\.vv" 128 } } */
+/* { dg-final { scan-assembler-times "v(?:macc|madd)\.vx" 128 } } */
+/* { dg-final { scan-assembler-times "v(?:nmsac|nmsub)\.vv" 128 } } */
+/* { dg-final { scan-assembler-times "v(?:nmsac|nmsub)\.vx" 128 } } */
+
+/* { dg-final { scan-assembler-times "vfmacc.vv|vfmadd.vv" 48 } } */
+/* { dg-final { scan-assembler-times "vfmacc.vf|vfmadd.vf" 48 } } */
+/* { dg-final { scan-assembler-times "vfnmacc.vv|vfnmadd.vv" 48 } } */
+/* { dg-final { scan-assembler-times "vfnmacc.vf|vfnmadd.vf" 48 } } */
+/* { dg-final { scan-assembler-times "vfmsac.vv|vfmsub.vv" 48 } } */
+/* { dg-final { scan-assembler-times "vfmsac.vf|vfmsub.vf" 48 } } */
+/* { dg-final { scan-assembler-times "vfnmsac.vv|vfnmsub.vv" 48 } } */
+/* { dg-final { scan-assembler-times "vfnmsac.vf|vfnmsub.vf" 48 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_macc.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_macc.c
new file mode 100644
index 00000000000..6dbedcf0944
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_macc.c
@@ -0,0 +1,73 @@
+/* { dg-do compile } */
+
+#include "rvv-common.h"
+#include <riscv_vector.h>
+#include <stddef.h>
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VMACC(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)				\
+  void vmacc##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##EM##_t vx, vy, vz;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);						\
+    vy = VLOAD(VCLASS, SEW, EM, y);						\
+    vz = VLOAD(VCLASS, SEW, EM, z);						\
+    vz = vx * vy + vz;							\
+    VSTORE(VCLASS, SEW, EM, z, vz);						\
+  }
+
+#define VMACC_IMM(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)			\
+  void vmacc##VCLASS##EM##_imm(size_t n, STYPE x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##EM##_t vx, vy, vz;					\
+    vy = VLOAD(VCLASS, SEW, EM, y);						\
+    vz = VLOAD(VCLASS, SEW, EM, z);						\
+    vz = x * vy + vz;							\
+    VSTORE(VCLASS, SEW, EM, z, vz);						\
+  }
+
+#define VINT_MAC(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)			\
+  void v##OP##_##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##EM##_t vx, vy, vz;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);						\
+    vy = VLOAD(VCLASS, SEW, EM, y);						\
+    vz = VLOAD(VCLASS, SEW, EM, z);						\
+    vz = v##OP##_vv_##VCLASS##EM (vz, vx, vy);				\
+    VSTORE(VCLASS, SEW, EM, z, vz);						\
+  }
+
+#define VINT_MAC_IMM(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)		\
+  void v##OP##_##VCLASS##EM##_imm(size_t n, STYPE x, STYPE *y, STYPE *z) {\
+    v##VCLASST##EM##_t vx, vy, vz;					\
+    vy = VLOAD(VCLASS, SEW, EM, y);						\
+    vz = VLOAD(VCLASS, SEW, EM, z);						\
+    vz = v##OP##_vx_##VCLASS##EM (vz, x, vy);				\
+    VSTORE(VCLASS, SEW, EM, z, vz);						\
+  }
+
+RVV_INT_TEST_ARG (VINT_MAC, macc)
+RVV_INT_TEST_ARG (VINT_MAC, madd)
+RVV_INT_TEST_ARG (VINT_MAC_IMM, macc)
+RVV_INT_TEST_ARG (VINT_MAC_IMM, madd)
+RVV_UINT_TEST_ARG (VINT_MAC, macc)
+RVV_UINT_TEST_ARG (VINT_MAC, madd)
+RVV_UINT_TEST_ARG (VINT_MAC_IMM, macc)
+RVV_UINT_TEST_ARG (VINT_MAC_IMM, madd)
+RVV_INT_TEST_ARG (VINT_MAC, nmsac)
+RVV_INT_TEST_ARG (VINT_MAC, nmsub)
+RVV_INT_TEST_ARG (VINT_MAC_IMM, nmsac)
+RVV_INT_TEST_ARG (VINT_MAC_IMM, nmsub)
+RVV_UINT_TEST_ARG (VINT_MAC, nmsac)
+RVV_UINT_TEST_ARG (VINT_MAC, nmsub)
+RVV_UINT_TEST_ARG (VINT_MAC_IMM, nmsac)
+RVV_UINT_TEST_ARG (VINT_MAC_IMM, nmsub)
+
+RVV_FLOAT_TEST(VMACC)
+RVV_FLOAT_TEST(VMACC_IMM)
+
+/* { dg-final { scan-assembler-times "v(?:macc|madd)\.vv" 64 } } */
+/* { dg-final { scan-assembler-times "v(?:macc|madd)\.vx" 64 } } */
+/* { dg-final { scan-assembler-times "v(?:nmsac|nmsub)\.vv" 64 } } */
+/* { dg-final { scan-assembler-times "v(?:nmsac|nmsub)\.vx" 64 } } */
+
+/* { dg-final { scan-assembler-times "vf(?:macc|madd)\.vv" 24 } } */
+/* { dg-final { scan-assembler-times "vf(?:macc|madd)\.vf" 0 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vv.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vv.c
new file mode 100644
index 00000000000..c54e56dd00e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vv.c
@@ -0,0 +1,48 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VADCSBC(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                                   \
+  void v##OP##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {              \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t carryin, carryout;                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    carryout = v##OP##_vv_##VCLASS##EM##_b##MLEN (vx, vy);                            \
+    vy = vadc_vvm_##VCLASS##EM (vx, vy, carryout);                          \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_scalar(size_t n, STYPE *x, STYPE *y, STYPE z) {     \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t carryin, carryout;                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    carryout = v##OP##_vx_##VCLASS##EM##_b##MLEN (vx,  z);                            \
+    vy = vadc_vvm_##VCLASS##EM (vx, vy, carryout);                          \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_imm(size_t n, STYPE *x, STYPE *y, STYPE z) {        \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t carryin, carryout;                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    carryout = v##OP##_vx_##VCLASS##EM##_b##MLEN (vx, 11);                            \
+    vy = vadc_vvm_##VCLASS##EM (vx, vy, carryout);                          \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }
+
+
+
+
+RVV_INT_TEST_ARG(VADCSBC, madc)
+RVV_INT_TEST_ARG(VADCSBC, msbc)
+
+/* { dg-final { scan-assembler-times "vmadc.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmadc.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmadc.vi" 16 } } */
+/* { dg-final { scan-assembler-times "vmsbc.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmsbc.vx" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vvm.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vvm.c
new file mode 100644
index 00000000000..3668a35a89f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_madc-vvm.c
@@ -0,0 +1,50 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VADCSBC(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                                   \
+  void v##OP##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {              \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t carryin, carryout;                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    carryin = MSET (MLEN);                                          \
+    carryout = v##OP##_vvm_##VCLASS##EM##_b##MLEN (vx, vy, carryin);                  \
+    vy = vadc_vvm_##VCLASS##EM (vx, vy, carryout);                           \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, STYPE *x, STYPE *y, STYPE z) {          \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t carryin, carryout;                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    carryin = MSET (MLEN);                                          \
+    carryout = v##OP##_vxm_##VCLASS##EM##_b##MLEN (vx, z, carryin);                   \
+    vy = vadc_vvm_##VCLASS##EM (vx, vy, carryout);                           \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_i(size_t n, STYPE *x, STYPE *y, STYPE z) {          \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t carryin, carryout;                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    carryin = MSET (MLEN);                                          \
+    carryout = v##OP##_vxm_##VCLASS##EM##_b##MLEN (vx, 11, carryin);                  \
+    vy = vadc_vvm_##VCLASS##EM (vx, vy, carryout);                           \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }
+
+
+
+RVV_INT_TEST_ARG(VADCSBC, madc)
+RVV_INT_TEST_ARG(VADCSBC, msbc)
+
+/* { dg-final { scan-assembler-times "vmadc.vvm" 16 } } */
+/* { dg-final { scan-assembler-times "vmadc.vxm" 16 } } */
+/* { dg-final { scan-assembler-times "vmadc.vim" 16 } } */
+/* { dg-final { scan-assembler-times "vmsbc.vvm" 16 } } */
+/* { dg-final { scan-assembler-times "vmsbc.vxm" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-clr.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-clr.c
new file mode 100644
index 00000000000..84242b770ed
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-clr.c
@@ -0,0 +1,23 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VCLR(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                          \
+  void vsbf##VCLASS##EM(size_t n, STYPE *x, STYPE *y) {                        \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vbool##MLEN##_t mask0;                                                   \
+    vbool##MLEN##_t rv;                                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    mask0 = vmclr_m_b##MLEN ();                                            \
+    vy = vadd_vv_##VCLASS##EM##_m (mask0, vy, vx, vy);                   \
+    * (v##VCLASST##EM##_t *) y = vy;                                          \
+  }
+
+RVV_INT_TEST(VCLR)
+
+/* { dg-final { scan-assembler-times "vmclr.m" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-1.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-1.c
new file mode 100644
index 00000000000..0147e5ccaa6
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-1.c
@@ -0,0 +1,86 @@
+/* { dg-do compile } */
+/* -fno-expensive-optimizations for disable bswap pass, it will cause ICE when
+   there is vector operation with bitwise-or  */
+/* { dg-additional-options "-fno-expensive-optimizations" } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VMBIN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP, OPERATOR)           \
+  void vm##OP##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE *z) {            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2, mask3;                                    \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                           \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN##_m(mask1, mask1, vx, vz);          \
+    mask3 = mask1 OPERATOR mask2;                                              \
+    vx = vadd_vv_##VCLASS##EM##_m (mask3, vy, vx, vy);                   \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }                                                                            \
+  void vm##OP##VCLASS##EM##2(size_t n, STYPE *x, STYPE *y, STYPE *z) {         \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2, mask3;                                    \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN##_m(mask1, mask1, vx, vz);              \
+    mask3 = v##OP##_mm_b##MLEN(mask1, mask2);                            \
+    vx = vadd_vv_##VCLASS##EM##_m (mask3, vy, vx, vy);                   \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+#define VMNBIN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                    \
+  void vm##OP##VCLASS##EM##2(size_t n, STYPE *x, STYPE *y, STYPE *z) {         \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2, mask3;                                    \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                           \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN##_m(mask1, mask1, vx, vz);         \
+    mask3 = v##OP##_mm_b##MLEN(mask1, mask2);                            \
+    vx = vadd_vv_##VCLASS##EM##_m (mask3, vy, vx, vy);                   \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+
+#define VMNOT(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                         \
+  void vmnot##VCLASS##EM##2(size_t n, STYPE *x, STYPE *y, STYPE *z) {          \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2, mask3;                                    \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                   \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN##_m(mask1, mask1, vx, vz);              \
+    mask3 = vmnot_m_b##MLEN(mask2);                                       \
+    vx = vadd_vv_##VCLASS##EM##_m (mask3, vy, vx, vy);                   \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+
+RVV_INT_TEST_ARG(VMBIN, mand, &)
+RVV_INT_TEST_ARG(VMBIN, mor, |)
+RVV_INT_TEST_ARG(VMBIN, mxor, ^)
+RVV_INT_TEST_ARG(VMNBIN, mnand)
+RVV_INT_TEST_ARG(VMNBIN, mnor)
+RVV_INT_TEST_ARG(VMNBIN, mxnor)
+RVV_INT_TEST_ARG(VMNBIN, mandnot)
+RVV_INT_TEST_ARG(VMNBIN, mornot)
+/* XXX: vmnot can't generate by oprator now, GCC will ICE.  */
+RVV_INT_TEST(VMNOT)
+
+/* { dg-final { scan-assembler-times "vmand.mm" 32 } } */
+/* { dg-final { scan-assembler-times "vmor.mm" 32 } } */
+/* { dg-final { scan-assembler-times "vmxor.mm" 32 } } */
+/* { dg-final { scan-assembler-times "vmnand.mm" 16 } } */
+/* { dg-final { scan-assembler-times "vmnor.mm" 16 } } */
+/* { dg-final { scan-assembler-times "vmxnor.mm" 16 } } */
+/* { dg-final { scan-assembler-times "vmandnot.mm" 16 } } */
+/* { dg-final { scan-assembler-times "vmornot.mm" 16 } } */
+/* { dg-final { scan-assembler-times "vmnot.m" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-2.c
new file mode 100644
index 00000000000..4dc908b29e2
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-2.c
@@ -0,0 +1,117 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VPOPC(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                         \
+  long vpopc##VCLASS##EM##2(size_t n, STYPE *x, STYPE *y) {                    \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vbool##MLEN##_t mask1;                                                   \
+    long rv;                                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    rv = vpopc_m_b##MLEN(mask1);                                         \
+    return rv;                                                                 \
+  }                                                                            \
+  long vpopc##VCLASS##EM##2_m(size_t n, STYPE *x, STYPE *y, STYPE *z) {     \
+    v##VCLASST##EM##_t vx, vy, vz;                                            \
+    vbool##MLEN##_t mask1, mask2;                                            \
+    long rv;                                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                                 \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vz);                                   \
+    rv = vpopc_m_b##MLEN##_m(mask1, mask2);                           \
+    return rv;                                                                 \
+  }                                                                            \
+  long vpopc##VCLASS##EM##2_m_opt(size_t n, STYPE *x, STYPE *y, STYPE *z) { \
+    v##VCLASST##EM##_t vx, vy, vz;                                            \
+    vbool##MLEN##_t mask1, mask2, mask3;                                     \
+    long rv;                                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                                 \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vz);                                   \
+    /* Combine & and popc into masked popc  */                                 \
+    mask3 = mask1 & mask2;                                                     \
+    rv = vpopc_m_b##MLEN(mask3);                                         \
+    return rv;                                                                 \
+  }
+
+#define VFIRST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                        \
+  long vfirst##VCLASS##EM##2(size_t n, STYPE *x, STYPE *y) {                   \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask1;                                                  \
+    long rv;                                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    rv = vfirst_m_b##MLEN(mask1);                                        \
+    return rv;                                                                 \
+  }                                                                            \
+  long vfirst##VCLASS##EM##2_m(size_t n, STYPE *x, STYPE *y, STYPE *z) {    \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2;                                           \
+    long rv;                                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vz);                                   \
+    rv = vfirst_m_b##MLEN##_m(mask1, mask2);                          \
+    return rv;                                                                 \
+  }                                                                            \
+
+#define VIOTA(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                         \
+  void viota##VCLASS##EM##2(size_t n, STYPE *x, STYPE *y) {                    \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask1;                                                  \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask1 = vmsltu_vv_##VCLASS##EM##_b##MLEN(vx, vy);                           \
+    vy = viota_m_u##EM (mask1);                                              \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+  void viota##VCLASS##EM##2_m(size_t n, STYPE *x, STYPE *y, STYPE *z) {    \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2;                                           \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask1 = vmsltu_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    mask2 = vmsltu_vv_##VCLASS##EM##_b##MLEN(vx, vz);                                   \
+    vy = viota_m_u##EM##_m(mask1, vx, mask2);				\
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+
+#define VID(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                           \
+  void vid##VCLASS##EM##2(size_t n, STYPE *x, STYPE *y) {                      \
+    v##VCLASST##EM##_t vy;                                                   \
+    vy = vid_v_u##EM ();                                                       \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+  void vid##VCLASS##EM##2_m(size_t n, STYPE *x, STYPE *y, STYPE *z) {       \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2;                                           \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask1 = vmsltu_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    vy = vid_v_u##EM##_m(mask1, vy);                                        \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+  }                                                                            \
+
+
+RVV_INT_TEST(VPOPC)
+RVV_INT_TEST(VFIRST)
+RVV_UINT_TEST(VIOTA)
+RVV_UINT_TEST(VID)
+
+/* { dg-final { scan-assembler-times "vpopc.m" 48 } } */
+/* { dg-final { scan-assembler-times "vfirst.m" 32 } } */
+/* { dg-final { scan-assembler-times "vid.v" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-3.c
new file mode 100644
index 00000000000..874d39edfd0
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-op-3.c
@@ -0,0 +1,42 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VMMISC(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, OP)                                    \
+  void v##OP##VCLASS##EM(size_t n, STYPE *x, STYPE *y) {                       \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask1, mask2;                                           \
+    vbool##MLEN##_t rv;                                                     \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    mask2 = v##OP##_m_b##MLEN(mask1);                                    \
+    vy = vadd_vv_##VCLASS##EM##_m (mask2, vy, vx, vy);                   \
+    * (v##VCLASST##EM##_t *) y = vy;                                         \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_m(size_t n, STYPE *x, STYPE *y, STYPE *z) {      \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask1, mask2, mask3;                                    \
+    long rv;                                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, SEW, EM, z);                                               \
+    mask1 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vy);                                   \
+    mask2 = vmslt_vv_##VCLASS##EM##_b##MLEN(vx, vz);                                   \
+    mask3 = vmslt_vv_##VCLASS##EM##_b##MLEN(vy, vz);                                   \
+    mask3 = v##OP##_m_b##MLEN##_m(mask1, mask2, mask3);               \
+    vy = vadd_vv_##VCLASS##EM##_m (mask3, vy, vx, vy);                   \
+    * (v##VCLASST##EM##_t *) y = vy;                                         \
+  }                                                                            \
+
+RVV_INT_TEST_ARG(VMMISC, msbf)
+RVV_INT_TEST_ARG(VMMISC, msif)
+RVV_INT_TEST_ARG(VMMISC, msof)
+
+/* { dg-final { scan-assembler-times "vmsbf.m" 32 } } */
+/* { dg-final { scan-assembler-times "vmsif.m" 32 } } */
+/* { dg-final { scan-assembler-times "vmsof.m" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-set.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-set.c
new file mode 100644
index 00000000000..54a4bb97979
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mask-set.c
@@ -0,0 +1,23 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSET(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                          \
+  void vsbf##VCLASS##EM(size_t n, STYPE *x, STYPE *y) {                        \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vbool##MLEN##_t mask0;                                                   \
+    vbool##MLEN##_t rv;                                                      \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    mask0 = MSET (MLEN);                                            \
+    vy = vadd_vv_##VCLASS##EM##_m (mask0, vy, vx, vy);                   \
+    * (v##VCLASST##EM##_t *) y = vy;                                          \
+  }
+
+RVV_INT_TEST(VSET)
+
+/* { dg-final { scan-assembler-times "vmset.m" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin.c
new file mode 100644
index 00000000000..964e347d7f9
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin.c
@@ -0,0 +1,27 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VMAXMIN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                    \
+  void vmaxmin##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {          \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                              \
+    vy = vfmax_vv_##VCLASS##EM (vx, vy);                                     \
+    vy = vfmax_vf_##VCLASS##EM (vy, z);                                      \
+    vy = vfmin_vv_##VCLASS##EM (vx, vy);                                     \
+    vy = vfmin_vf_##VCLASS##EM (vy, z);                                      \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                               \
+  }
+
+/* XXX: integer intrinsics have not finished */
+RVV_FLOAT_TEST(VMAXMIN)
+
+/* { dg-final { scan-assembler-times "vfmax.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfmax.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfmin.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfmin.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin2.c
new file mode 100644
index 00000000000..e8b5f99a868
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_maxmin2.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VMAXMIN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                       \
+  void vmaxmin##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {            \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vbool##MLEN##_t mask;                                                    \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    mask = MSET (MLEN);                                             \
+    vy = vfmax_vv_##VCLASS##EM##_m(mask, vy, vx, vy);                     \
+    vy = vfmax_vf_##VCLASS##EM##_m(mask, vy, vy, z);                      \
+    vy = vfmin_vv_##VCLASS##EM##_m(mask, vy, vx, vy);                     \
+    vy = vfmin_vf_##VCLASS##EM##_m(mask, vy, vy, z);                      \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                  \
+  }
+
+RVV_FLOAT_TEST(VMAXMIN)
+
+/* { dg-final { scan-assembler-times "vfmax.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfmax.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfmin.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfmin.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_memcpy.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_memcpy.c
new file mode 100644
index 00000000000..8d0028640a7
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_memcpy.c
@@ -0,0 +1,39 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+/*
+
+=== Memcpy example
+
+----
+    # void *memcpy(void* dest, const void* src, size_t n)
+    # a0=dest, a1=src, a2=n
+    #
+  memcpy:
+      mv a3, a0 # Copy destination
+  loop:
+    vsetvli t0, a2, e8,m8  # Vectors of 8b
+    vlb.v v0, (a1)                # Load bytes
+      add a1, a1, t0              # Bump pointer
+      sub a2, a2, t0              # Decrement count
+    vsb.v v0, (a3)                # Store bytes
+      add a3, a3, t0              # Bump pointer
+      bnez a2, loop               # Any more?
+      ret                         # Return
+----
+*/
+
+void *memcpy(void *vdest, const void *vsrc, size_t n) {
+  size_t vl;
+  uint8_t *dest = (uint8_t *)vdest;
+  uint8_t *src = (uint8_t *)vsrc;
+  for (; vl = vsetvl_e8m8(n); n -= vl) {
+    *(vint8m8_t *)dest = *(vint8m8_t *)src;
+    src += vl;
+    dest += vl;
+  }
+  return dest;
+}
+
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_merge.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_merge.c
new file mode 100644
index 00000000000..ae0657cda3d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_merge.c
@@ -0,0 +1,74 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_INT_MERGE_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)			\
+  void rvv##VCLASS##EM##_v_builtin_test(size_t n, STYPE *x,	\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = vmerge_vvm_##VCLASS##EM (mask, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }								\
+  void rvv##VCLASS##EM##_s_builtin_test(size_t n, STYPE *x, 	\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = vmerge_vxm_##VCLASS##EM (mask, vx, z);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }								\
+  void rvv##VCLASS##EM##_i_builtin_test(size_t n, STYPE *x, 	\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = vmerge_vxm_##VCLASS##EM (mask, vx, 11);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+#define RVV_FLOAT_MERGE_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)	\
+  void rvv##VCLASS##EM##_s_builtin_test(size_t n, STYPE *x, 	\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = vfmerge_vfm_##VCLASS##EM (mask, vx, z);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }								\
+  void rvv##VCLASS##EM##_i_builtin_test(size_t n, STYPE *x, 	\
+					     STYPE *y, STYPE z)	\
+  {								\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    mask = MSET (MLEN);				\
+    vy = vfmerge_vfm_##VCLASS##EM (mask, vx, 11);	\
+    VSTORE(VCLASS, SEW, EM, y, vy);					\
+  }
+
+
+RVV_INT_TEST(RVV_INT_MERGE_TEST)
+RVV_UINT_TEST(RVV_INT_MERGE_TEST)
+RVV_FLOAT_TEST(RVV_FLOAT_MERGE_TEST)
+
+/* { dg-final { scan-assembler-times "vmerge.vvm" 32 } } */
+/* { dg-final { scan-assembler-times "vmerge.vxm" 32 } } */
+/* { dg-final { scan-assembler-times "vmerge.vim" 32 } } */
+/* { dg-final { scan-assembler-times "vfmerge.vfm" 24 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax.c
new file mode 100644
index 00000000000..2ce69d1ff5d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, min)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, minu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, max)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, maxu)
+
+/* { dg-final { scan-assembler-times "vmin.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmin.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmax.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmax.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vminu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vminu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmaxu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmaxu.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax2.c
new file mode 100644
index 00000000000..87715e3789f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_minmax2.c
@@ -0,0 +1,19 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, min)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, minu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, max)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, maxu)
+
+/* { dg-final { scan-assembler-times "vmin.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmin.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmax.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmax.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vminu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vminu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmaxu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmaxu.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mixed_width.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mixed_width.c
new file mode 100644
index 00000000000..cf534f7d4ef
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mixed_width.c
@@ -0,0 +1,57 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+// https://github.com/riscv/riscv-v-spec/blob/master/vector-examples.adoc
+// == Vector Assembly Code Examples
+
+/*
+=== Example with mixed-width mask and compute.
+
+----
+# Code using one width for predicate and different width for masked
+# compute.
+#   int8_t a[]; int32_t b[], c[];
+#   for (i=0;  i<n; i++) { b[i] =  (a[i] < 5) ? c[i] : 1; }
+#
+# Mixed-width code that keeps SEW/LMUL=8
+  loop:
+    vsetvli a4, a0, e8,m1  # Byte vector for predicate calc
+    vlb.v v1, (a1)                # Load a[i]
+      add a1, a1, a4              # Bump pointer.
+    vmslt.vi v0, v1, 5            # a[i] < 5?
+
+    vsetvli x0, a0, e32,m4 # Vector of 32-bit values.
+      sub a0, a0, a4              # Decrement count
+    vmv.v.i v4, 1                 # Splat immediate to destination
+    vlw.v v4, (a3), v0.t          # Load requested elements of C.
+      sll t1, a4, 2
+      add a3, a3, t1              # Bump pointer.
+    vsw.v v4, (a2)                # Store b[i].
+      add a2, a2, t1              # Bump pointer.
+      bnez a0, loop               # Any more?
+----
+*/
+
+void mixedwidth_m(int8_t *a, int32_t * b, int32_t* c, size_t n) {
+  size_t vl;
+  vbool8_t mask;
+  vint32m4_t const_one, vec_c;
+  for (; vl = vsetvl_e8m1(n); n -= vl) {
+    mask = vmslt_vx_i8m1_b8(*(vint8m1_t *)a, 5);
+    const_one = vsplat_s_i32m4(1);
+    vec_c = vle32_v_i32m4_m(mask, const_one /*maskedoff*/, c);
+    *(vint32m4_t *)b = vec_c;
+    /* in Jim's test case,
+     https://github.com/sifive/riscv-gcc-internal/pull/3/commits/6afe672655fb05d7472147b617edf8ef6cccc459#diff-0c8f8f39238398a98aca2cee450facc9R57
+     it looks like we support below implicit operations
+     vint8m1_t const_five = vsplat_s_i8m1(5);
+     *(vint32m4_t *)b = (*(vint8m1 *)a < const_five) ? *(vint32m4_t*)c : const_one;
+     */
+    a += vl;
+    b += vl;
+    c += vl;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul.c
new file mode 100644
index 00000000000..2df82d75167
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_TEST, mul, *)
+RVV_UINT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_TEST, mul, *)
+RVV_FLOAT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_TEST, mul, *)
+
+/* { dg-final { scan-assembler-times "vmul.vv" 64 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vmul.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vfmul.vv" 24 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vfmul.vf" 0 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul2.c
new file mode 100644
index 00000000000..d76af31b591
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul2.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, mul)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, mul)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, fmul)
+
+
+/* { dg-final { scan-assembler-times "vmul.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vmul.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vfmul.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfmul.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul3.c
new file mode 100644
index 00000000000..3fe4caca8b4
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mul3.c
@@ -0,0 +1,14 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, mul)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, mul)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fmul)
+
+/* { dg-final { scan-assembler-times "vmul.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vmul.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vfmul.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfmul.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh.c
new file mode 100644
index 00000000000..997ea137ce6
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh.c
@@ -0,0 +1,42 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_MULHSU_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)		\
+  void rvvmulhsu##EM##_svv(size_t n, STYPE *x,		\
+				 u##STYPE *y, STYPE z)		\
+  {								\
+    v##VCLASST##EM##_t vx;					\
+    vuint##EM##_t vy;					\
+    vx = VILOAD(SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vx = vmulhsu_vv_i##EM (vx, vy);				\
+    VSTORE(VCLASS, SEW, EM, x, vx);				\
+  }								\
+  void rvvmulhsu##EM##_svx(size_t n, STYPE *x,		\
+				 u##STYPE *y, STYPE z)		\
+  {								\
+    v##VCLASST##EM##_t vx;					\
+    vuint##EM##_t vy;					\
+    vx = VILOAD(SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vx = vmulhsu_vx_i##EM (vx, z);				\
+    VSTORE(VCLASS, SEW, EM, x, vx);				\
+  }								\
+
+RVV_INT_TEST(RVV_MULHSU_TEST)
+/* { dg-final { scan-assembler-times "vmulhsu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmulhsu.vx" 16 } } */
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, mulh)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, mulhu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_TEST, smul)
+
+/* { dg-final { scan-assembler-times "vmulh.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmulh.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmulhu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmulhu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsmul.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsmul.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh2.c
new file mode 100644
index 00000000000..7b68f44d6a6
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mulh2.c
@@ -0,0 +1,46 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_MULHSU_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)		\
+  void rvvmulhsu##EM##_svv(size_t n, STYPE *x,		\
+				 u##STYPE *y, STYPE z)		\
+  {								\
+    v##VCLASST##EM##_t vx;					\
+    vuint##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VILOAD(SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vx = vmulhsu_vv_##VCLASS##EM##_m (mask, vx, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void rvvmulhsu##EM##_svx(size_t n, STYPE *x,		\
+				 u##STYPE *y, STYPE z)		\
+  {								\
+    v##VCLASST##EM##_t vx;					\
+    vuint##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VILOAD(SEW, EM, x);					\
+    vy = VULOAD(SEW, EM, y);					\
+    vx = vmulhsu_vx_##VCLASS##EM##_m (mask, vx, vx, z);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }
+
+RVV_INT_TEST(RVV_MULHSU_TEST)
+/* { dg-final { scan-assembler-times "vmulhsu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmulhsu.vx" 16 } } */
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, mulh)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, mulhu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, smul)
+
+/* { dg-final { scan-assembler-times "vmulh.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmulh.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vmulhu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vmulhu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsmul.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsmul.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_mv.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mv.c
new file mode 100644
index 00000000000..73332dbf501
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_mv.c
@@ -0,0 +1,42 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_MV_SV_VS(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)\
+  STYPE test_mv_xs##VCLASS##EM (STYPE *x) {			\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    return vmv_x_s_##VCLASS##EM##_##VCLASS##SEW (vx);			\
+  }								\
+  void test_mv_sx##VCLASS##EM (STYPE *x, STYPE s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vmv_s_x_##VCLASS##EM(vx, s);				\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }
+
+RVV_INT_TEST (RVV_TEST_MV_SV_VS)
+RVV_UINT_TEST (RVV_TEST_MV_SV_VS)
+
+#define RVV_TEST_FMV_SV_VS(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)\
+  STYPE test_mv_xs##VCLASS##EM (STYPE *x) {			\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    return vfmv_f_s_##VCLASS##EM##_f##SEW(vx);				\
+  }								\
+  void test_mv_sx##VCLASS##EM (STYPE *x, STYPE s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vfmv_s_f_##VCLASS##EM(vx, s);				\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }
+
+
+RVV_FLOAT_TEST (RVV_TEST_FMV_SV_VS)
+
+/* { dg-final { scan-assembler-times "vmv.s.x" 32 } } */
+/* { dg-final { scan-assembler-times "vmv.x.s" 32 } } */
+/* { dg-final { scan-assembler-times "vfmv.f.s" 12 } } */
+/* { dg-final { scan-assembler-times "vfmv.s.f" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip.c
new file mode 100644
index 00000000000..c4806b5c37f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_WINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_TEST, nclip)
+RVV_WUINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_TEST, nclipu)
+
+/* { dg-final { scan-assembler-times "vnclip.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnclip.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnclip.wi" 9 } } */
+/* { dg-final { scan-assembler-times "vnclipu.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnclipu.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnclipu.wi" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip2.c
new file mode 100644
index 00000000000..9bb0e5708ce
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nclip2.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_WINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, nclip)
+RVV_WUINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, nclipu)
+
+/* { dg-final { scan-assembler-times "vnclip.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnclip.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnclip.wi" 9 } } */
+/* { dg-final { scan-assembler-times "vnclipu.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnclipu.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnclipu.wi" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt.c
new file mode 100644
index 00000000000..2693b2a0c8a
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt.c
@@ -0,0 +1,91 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_NFCVT_TEST(EM, WEM, MLEN, SEW, WSEW)	\
+  void rvvcvtfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+					       int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vint##EM##_t vy;							\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = vfncvt_x_f_w_i##EM (vx);			\
+    VISTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvvcvtfui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						u##int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vx = VFLOAD(WSEW, WEM, x);					\
+    vy = vfncvt_xu_f_w_u##EM (vx);			\
+    VUSTORE(SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtrtzfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+					       int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vint##EM##_t vy;							\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = vfncvt_rtz_x_f_w_i##EM (vx);			\
+    VISTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvvcvtrtzfui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						u##int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vx = VFLOAD(WSEW, WEM, x);					\
+    vy = vfncvt_rtz_xu_f_w_u##EM (vx);			\
+    VUSTORE(SEW, EM, y, vy);					\
+  }									\
+  void rvvcvtif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+					       int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vint##WEM##_t vy;						\
+    vy = VILOAD(WSEW, WEM, y);					\
+    vx = vfncvt_f_x_w_f##EM (vy);			\
+    VFSTORE(SEW, EM, x, vx);						\
+  }									\
+  void rvvcvtuif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						u##int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vuint##WEM##_t vy;						\
+    vy = VULOAD(WSEW, WEM, y);						\
+    vx = vfncvt_f_xu_w_f##EM (vy);			\
+    VFSTORE(SEW, EM, x, vx);						\
+  }\
+  void rvvcvtff##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						_RVV_F##WSEW##_TYPE *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vfloat##WEM##_t vy;						\
+    vy = VFLOAD(WSEW, WEM, y);						\
+    vx = vfncvt_f_f_w_f##EM (vy);			\
+    VFSTORE(SEW, EM, x, vx);						\
+  }									\
+  void rvvcvtrff##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						_RVV_F##WSEW##_TYPE *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vfloat##WEM##_t vy;						\
+    vy = VFLOAD(WSEW, WEM, y);						\
+    vx = vfncvt_rod_f_f_w_f##EM (vy);			\
+    VFSTORE(SEW, EM, x, vx);						\
+  }
+
+
+RVV_FLOAT_WNCVT_INT_TEST(RVV_NFCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vfncvt.xu.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.x.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.rtz.xu.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.rtz.x.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.f.xu.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.f.x.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.f.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.rod.f.f.w" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt2.c
new file mode 100644
index 00000000000..2b5221d02b5
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nfcvt2.c
@@ -0,0 +1,115 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_NFCVT_TEST(EM, WEM, MLEN, SEW, WSEW)	\
+  void rvvcvtfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+					       int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vint##EM##_t vy;							\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = VILOAD(SEW, EM, y);						\
+    vy = vfncvt_x_f_w_i##EM##_m (mask, vy, vx);		\
+    VISTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvvcvtfui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						u##int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = VULOAD(SEW, EM, y);						\
+    vy = vfncvt_xu_f_w_u##EM##_m (mask, vy, vx);		\
+    VUSTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvvcvtrtzfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+					       int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vint##EM##_t vy;							\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = VILOAD(SEW, EM, y);						\
+    vy = vfncvt_rtz_x_f_w_i##EM##_m (mask, vy, vx);		\
+    VISTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvvcvtrtzfui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						u##int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = VULOAD(SEW, EM, y);						\
+    vy = vfncvt_rtz_xu_f_w_u##EM##_m (mask, vy, vx);		\
+    VUSTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvvcvtif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+					       int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vint##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vy = VILOAD(WSEW, WEM, y);						\
+    vx = VFLOAD(SEW, EM, x);						\
+    vx = vfncvt_f_x_w_f##EM##_m (mask, vx, vy);		\
+    VFSTORE(SEW, EM, x, vx);						\
+  }									\
+  void rvvcvtuif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						u##int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vuint##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vy = VULOAD(WSEW, WEM, y);						\
+    vx = VFLOAD(SEW, EM, x);						\
+    vx = vfncvt_f_xu_w_f##EM##_m (mask, vx, vy);		\
+    VFSTORE(SEW, EM, x, vx);						\
+  }\
+  void rvvcvtff##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						_RVV_F##WSEW##_TYPE *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vfloat##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vy = VFLOAD(WSEW, WEM, y);						\
+    vx = VFLOAD(SEW, EM, x);						\
+    vx = vfncvt_f_f_w_f##EM##_m (mask, vx, vy);		\
+    VFSTORE(SEW, EM, x, vx);						\
+  }									\
+  void rvvcvtrff##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						_RVV_F##WSEW##_TYPE *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vfloat##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vy = VFLOAD(WSEW, WEM, y);						\
+    vx = VFLOAD(SEW, EM, x);						\
+    vx = vfncvt_rod_f_f_w_f##EM##_m (mask, vx, vy);	\
+    VFSTORE(SEW, EM, x, vx);						\
+  }
+
+
+RVV_FLOAT_WNCVT_INT_TEST(RVV_NFCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vfncvt.xu.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.x.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.rtz.xu.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.rtz.x.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.f.xu.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.f.x.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.f.f.w" 6 } } */
+/* { dg-final { scan-assembler-times "vfncvt.rod.f.f.w" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift.c
new file mode 100644
index 00000000000..e4a6db65f10
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_WINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_TEST, nsra)
+RVV_WUINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_TEST, nsrl)
+
+/* { dg-final { scan-assembler-times "vnsrl.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnsrl.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnsrl.wi" 9 } } */
+/* { dg-final { scan-assembler-times "vnsra.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnsra.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnsra.wi" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift2.c
new file mode 100644
index 00000000000..df557601856
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_nshift2.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_WINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, nsra)
+RVV_WUINT_TEST_ARG(RVV_NINT_SHIFT_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, nsrl)
+
+/* { dg-final { scan-assembler-times "vnsrl.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnsrl.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnsrl.wi" 9 } } */
+/* { dg-final { scan-assembler-times "vnsra.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vnsra.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vnsra.wi" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_or.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_or.c
new file mode 100644
index 00000000000..3572397b095
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_or.c
@@ -0,0 +1,14 @@
+/* { dg-do compile } */
+/* { dg-additional-options "-fno-expensive-optimizations" } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, or, |)
+RVV_UINT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, or, |)
+
+
+/* { dg-final { scan-assembler-times "vor.vv" 64 } } */
+/* { dg-final { scan-assembler-times "vor.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vor.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_or2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_or2.c
new file mode 100644
index 00000000000..a6207f07f03
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_or2.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, or)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, or)
+
+/* { dg-final { scan-assembler-times "vor.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vor.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vor.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_or3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_or3.c
new file mode 100644
index 00000000000..a6207f07f03
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_or3.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, or)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, or)
+
+/* { dg-final { scan-assembler-times "vor.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vor.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vor.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac.c
new file mode 100644
index 00000000000..3656da7097e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac.c
@@ -0,0 +1,52 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void v##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE *op1,             \
+                                                 OP2T##STYPE *op2,             \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP1T##int##EM##_t vop1;                                           \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0;                                          \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vop1 = VLOAD(OP1U, SEW, EM, op1);                                     \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vv_##OP0U##WEM (vop0, vop1, vop2);                          \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void x##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE  op1,             \
+                                                 OP2T##STYPE *op2,             \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0;                                          \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vx_##OP0U##WEM (vop0, op1, vop2);		          \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)      \
+  VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)
+
+RVV_QINT_TEST_ARG(VWMAC,      qmacc, i, i, i,  ,  ,  )
+RVV_QINT_TEST_ARG(VWMAC,     qmaccu, u, u, u, u, u, u)
+RVV_QINT_TEST_ARG(VWMAC,    qmaccsu, i, i, u,  ,  , u)
+RVV_QINT_TEST_ARG(VWMAC_VX, qmaccus, i, u, i,  , u,  )
+
+/* { dg-final { scan-assembler-times "vqmacc.vv" 4 } } */
+/* { dg-final { scan-assembler-times "vqmacc.vx" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccu.vv" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccu.vx" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccsu.vv" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccsu.vx" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccus.vx" 4 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac2.c
new file mode 100644
index 00000000000..c216e715f63
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_qmac2.c
@@ -0,0 +1,60 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void v##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE *op1,             \
+                                                 OP2T##STYPE *op2,             \
+                                                 OP0T##WSTYPE * mo,            \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP1T##int##EM##_t vop1;                                           \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0, vmo;                                         \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vmo = VLOAD(OP0U, WSEW, WEM, mo);                                      \
+    vop1 = VLOAD(OP1U, SEW, EM, op1);                                     \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vv_##OP0U##WEM##_m (mask, vop0, vop1, vop2);   \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void x##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE  op1,             \
+                                                 OP2T##STYPE *op2,             \
+						 OP0T##WSTYPE * mo,            \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0, vmo;                                          \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vmo = VLOAD(OP0U, WSEW, WEM, mo);                                      \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vx_##OP0U##WEM##_m (mask, vop0, op1, vop2);    \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)      \
+  VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)
+
+RVV_QINT_TEST_ARG(VWMAC,      qmacc, i, i, i,  ,  ,  )
+RVV_QINT_TEST_ARG(VWMAC,     qmaccu, u, u, u, u, u, u)
+RVV_QINT_TEST_ARG(VWMAC,    qmaccsu, i, i, u,  ,  , u)
+RVV_QINT_TEST_ARG(VWMAC_VX, qmaccus, i, u, i,  , u,  )
+
+/* { dg-final { scan-assembler-times "vqmacc.vv" 4 } } */
+/* { dg-final { scan-assembler-times "vqmacc.vx" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccu.vv" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccu.vx" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccsu.vv" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccsu.vx" 4 } } */
+/* { dg-final { scan-assembler-times "vqmaccus.vx" 4 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_readvl.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_readvl.c
new file mode 100644
index 00000000000..845043d7cbb
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_readvl.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+
+size_t x()
+{
+  return vreadvl ();
+}
+
+/* { dg-final { scan-assembler-times "csrr" 1 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc.c
new file mode 100644
index 00000000000..9c10f4d71cc
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc.c
@@ -0,0 +1,66 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VREDUC(STYPE, VCLASST, VCLASS, EM, EMONE, MLEN, SEW)		\
+  void vreduc##VCLASS##EM(size_t n, STYPE *x, STYPE *y) {		\
+    v##VCLASST##EMONE##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vx = VLOAD (VCLASS, SEW, EMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = vredsum_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredmax_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredmin_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredand_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredor_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);		\
+    vx = vredxor_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    VSTORE (VCLASS, SEW, EMONE, x, vx);					\
+  }
+#define VREDUCU(STYPE, VCLASST, VCLASS, EM, EMONE, MLEN, SEW)		\
+  void vreduc##VCLASS##EM(size_t n, STYPE *x, STYPE *y) {		\
+    v##VCLASST##EMONE##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vx = VLOAD (VCLASS, SEW, EMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = vredsum_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredmaxu_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredminu_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredand_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vredor_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);		\
+    vx = vredxor_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    VSTORE (VCLASS, SEW, EMONE, x, vx);					\
+  }
+
+#define VFREDUC(STYPE, VCLASST, VCLASS, EM, EMONE, MLEN, SEW)		\
+  void vreduc##VCLASS##EM(size_t n, STYPE *x, STYPE *y) {		\
+    v##VCLASST##EMONE##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vx = VLOAD (VCLASS, SEW, EMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = vfredsum_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vfredosum_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vfredmax_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    vx = vfredmin_vs_##VCLASS##EM##_##VCLASS##EMONE (vx, vy, vx);	\
+    VSTORE (VCLASS, SEW, EMONE, x, vx);					\
+  }
+
+RVV_INT_REDUC_TEST(VREDUC)
+RVV_UINT_REDUC_TEST(VREDUCU)
+RVV_FLOAT_REDUC_TEST(VFREDUC)
+
+/* { dg-final { scan-assembler-times "vredsum.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vredmaxu.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredmax.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredminu.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredmin.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredand.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vredor.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vredxor.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vfredsum.vs" 12 } } */
+/* { dg-final { scan-assembler-times "vfredosum.vs" 12 } } */
+/* { dg-final { scan-assembler-times "vfredmax.vs" 12 } } */
+/* { dg-final { scan-assembler-times "vfredmin.vs" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc2.c
new file mode 100644
index 00000000000..f4f082bba00
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_reduc2.c
@@ -0,0 +1,74 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VREDUC(STYPE, VCLASST, VCLASS, EM, EMONE, MLEN, SEW)			\
+  void vreduc##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##EMONE##_t vx;						\
+    v##VCLASST##EM##_t vy, vz;						\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD (VCLASS, SEW, EMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    mask = vmslt_vv_##VCLASS##EM##_b##MLEN (vy, vz);				\
+    vx = vredsum_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredmax_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredmin_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredand_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredor_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredxor_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    VSTORE (VCLASS, SEW, EMONE, x, vx);					\
+  }
+#define VREDUCU(STYPE, VCLASST, VCLASS, EM, EMONE, MLEN, SEW)		\
+  void vreduc##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##EMONE##_t vx;						\
+    v##VCLASST##EM##_t vy, vz;						\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD (VCLASS, SEW, EMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    mask = vmsltu_vv_##VCLASS##EM##_b##MLEN (vy, vz);				\
+    vx = vredsum_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredmaxu_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredminu_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredand_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredor_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vredxor_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    VSTORE (VCLASS, SEW, EMONE, x, vx);					\
+  }
+#define VFREDUC(STYPE, VCLASST, VCLASS, EM, EMONE, MLEN, SEW)			\
+  void vreduc##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##EMONE##_t vx;						\
+    v##VCLASST##EM##_t vy, vz;					\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD (VCLASS, SEW, EMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    mask = MSET (MLEN);					\
+    vx = vfredsum_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vfredosum_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vfredmax_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    vx = vfredmin_vs_##VCLASS##EM##_##VCLASS##EMONE##_m (mask, vx, vy, vx);		\
+    VSTORE (VCLASS, SEW, EMONE, x, vx);					\
+  }
+
+RVV_INT_REDUC_TEST(VREDUC)
+RVV_UINT_REDUC_TEST(VREDUCU)
+RVV_FLOAT_REDUC_TEST(VFREDUC)
+
+/* { dg-final { scan-assembler-times "vredsum.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vredmaxu.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredmax.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredminu.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredmin.vs" 16 } } */
+/* { dg-final { scan-assembler-times "vredand.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vredor.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vredxor.vs" 32 } } */
+/* { dg-final { scan-assembler-times "vfredsum.vs" 12 } } */
+/* { dg-final { scan-assembler-times "vfredosum.vs" 12 } } */
+/* { dg-final { scan-assembler-times "vfredmax.vs" 12 } } */
+/* { dg-final { scan-assembler-times "vfredmin.vs" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_reint.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_reint.c
new file mode 100644
index 00000000000..f336b475d4c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_reint.c
@@ -0,0 +1,68 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_FREINT_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, ISTYPE, IVCLASST, IVCLASS, SEW)	\
+  void rvreintuf##SEW##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       u##ISTYPE *y, STYPE z)	\
+  {									\
+    vfloat##EM##_t vx;							\
+    vuint##EM##_t vy;							\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = vreinterpret_v_f##EM##_u##EM (vx);				\
+    VUSTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvreintif##SEW##EM##_v_nomask_builtin_test(size_t n, STYPE *x,	\
+					       ISTYPE *y, STYPE z)	\
+  {									\
+    vfloat##EM##_t vx;							\
+    vint##EM##_t vy;							\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = vreinterpret_v_f##EM##_i##EM (vx);				\
+    VISTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvreintfu##SEW##EM##_v_nomask_builtin_test(size_t n, u##ISTYPE *x,\
+					       STYPE *y, STYPE z)	\
+  {									\
+    vfloat##EM##_t vy;							\
+    vuint##EM##_t vx;							\
+    vx = VULOAD(SEW, EM, x);						\
+    vy = vreinterpret_v_u##EM##_f##EM (vx);				\
+    VFSTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvreintfi##SEW##EM##_v_nomask_builtin_test(size_t n, ISTYPE *x,	\
+					       STYPE *y, STYPE z)	\
+  {									\
+    vfloat##EM##_t vy;							\
+    vint##EM##_t vx;							\
+    vx = VILOAD(SEW, EM, x);						\
+    vy = vreinterpret_v_i##EM##_f##EM (vx);				\
+    VFSTORE(SEW, EM, y, vy);						\
+  }
+
+#define RVV_REINT_TEST(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)	\
+  void rvreintui##SEW##EM##_v_nomask_builtin_test(size_t n, STYPE *x,\
+					       u##STYPE *y, STYPE z)	\
+  {									\
+    vint##EM##_t vx;							\
+    vuint##EM##_t vy;							\
+    vx = VILOAD(SEW, EM, x);						\
+    vy = vreinterpret_v_i##EM##_u##EM (vx);				\
+    VUSTORE(SEW, EM, y, vy);						\
+  }									\
+  void rvreintiu##SEW##EM##_v_nomask_builtin_test(size_t n, u##STYPE *x,\
+					       STYPE *y, STYPE z)	\
+  {									\
+    vuint##EM##_t vx;							\
+    vint##EM##_t vy;							\
+    vx = VULOAD(SEW, EM, x);						\
+    vy = vreinterpret_v_u##EM##_i##EM (vx);				\
+    VISTORE(SEW, EM, y, vy);						\
+  }
+
+
+RVV_FLOAT_CVT_INT_TEST(RVV_FREINT_TEST)
+RVV_INT_TEST(RVV_REINT_TEST)
+
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather.c
new file mode 100644
index 00000000000..720cc644c69
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather.c
@@ -0,0 +1,76 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_RGATHER(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)		\
+  void test_rgather_sv##VCLASS##EM (STYPE *x, u##STYPE *y, long s) {\
+    v##VCLASST##EM##_t vx;					\
+    vuint##EM##_t vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VULOAD(SEW, EM, y);				\
+    vx = vrgather_vv_##VCLASS##EM(vx, vy);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_sx##VCLASS##EM (STYPE *x, long s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vrgather_vx_##VCLASS##EM(vx, s);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_si##VCLASS##EM (STYPE *x, long s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vrgather_vx_##VCLASS##EM(vx, 11);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_svu##EM (u##STYPE *x, u##STYPE *y, long s) {\
+    vuint##EM##_t vx;					\
+    vuint##EM##_t vy;					\
+    vx = VULOAD(SEW, EM, x);				\
+    vy = VULOAD(SEW, EM, y);				\
+    vx = vrgather_vv_u##EM(vx, vy);			\
+    VUSTORE(SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_sxu##EM (u##STYPE *x, long s) {		\
+    vuint##EM##_t vx;					\
+    vx = VULOAD(SEW, EM, x);				\
+    vx = vrgather_vx_u##EM(vx, s);			\
+    VUSTORE(SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_siu##EM (u##STYPE *x, long s) {		\
+    vuint##EM##_t vx;					\
+    vx = VULOAD(SEW, EM, x);				\
+    vx = vrgather_vx_u##EM(vx, 11);			\
+    VUSTORE(SEW, EM, x, vx);					\
+  }
+
+#define RVV_TEST_FLOAT_RGATHER(STYPE, VCLASST, VCLASS, EM, MLEN, ISTYPE, IVCLASS, SEW)\
+  void test_rgather_sv##VCLASS##EM (STYPE *x, u##ISTYPE *y, long s) {\
+    v##VCLASST##EM##_t vx;					\
+    vuint##EM##_t vy;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VULOAD(SEW, EM, y);				\
+    vx = vrgather_vv_##VCLASS##EM(vx, vy);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_sx##VCLASS##EM (STYPE *x, long s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vrgather_vx_##VCLASS##EM(vx, s);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_si##VCLASS##EM (STYPE *x, long s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vrgather_vx_##VCLASS##EM(vx, 11);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }
+
+RVV_INT_TEST (RVV_TEST_RGATHER)
+RVV_FLOAT_INT_TEST (RVV_TEST_FLOAT_RGATHER)
+
+/* { dg-final { scan-assembler-times "vrgather.vv" 44 } } */
+/* { dg-final { scan-assembler-times "vrgather.vx" 44 } } */
+/* { dg-final { scan-assembler-times "vrgather.vi" 44 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather2.c
new file mode 100644
index 00000000000..63dc056380c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_rgather2.c
@@ -0,0 +1,112 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_RGATHER(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)		\
+  void test_rgather_sv##VCLASS##EM (STYPE *x, u##STYPE *y, STYPE *z, long s) {\
+    v##VCLASST##EM##_t vx;					\
+    v##VCLASST##EM##_t vz;					\
+    vuint##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VULOAD(SEW, EM, y);				\
+    vz = VLOAD(VCLASS, SEW, EM, z);				\
+    vx = vrgather_vv_##VCLASS##EM##_m(mask, vz, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_sx##VCLASS##EM (STYPE *x, STYPE *z, long s) {\
+    v##VCLASST##EM##_t vx;					\
+    v##VCLASST##EM##_t vz;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vz = VLOAD(VCLASS, SEW, EM, z);				\
+    vx = vrgather_vx_##VCLASS##EM##_m(mask, vz, vx, s);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_si##VCLASS##EM (STYPE *x, STYPE *z, long s) {\
+    v##VCLASST##EM##_t vx;					\
+    v##VCLASST##EM##_t vz;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vz = VLOAD(VCLASS, SEW, EM, z);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vrgather_vx_##VCLASS##EM##_m(mask, vz, vx, 11);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_svu##EM (u##STYPE *x, u##STYPE *y, u##STYPE *z, long s) {\
+    vuint##EM##_t vx;					\
+    vuint##EM##_t vz;					\
+    vuint##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VULOAD(SEW, EM, x);				\
+    vy = VULOAD(SEW, EM, y);				\
+    vz = VULOAD(SEW, EM, z);				\
+    vx = vrgather_vv_u##EM##_m(mask, vz, vx, vy);	\
+    VUSTORE(SEW, EM, x, vx);				\
+  }								\
+  void test_rgather_sxu##EM (u##STYPE *x, u##STYPE *z, long s) {\
+    vuint##EM##_t vx;					\
+    vuint##EM##_t vz;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VULOAD(SEW, EM, x);				\
+    vz = VULOAD(SEW, EM, z);				\
+    vx = vrgather_vx_u##EM##_m(mask, vz, vx, s);	\
+    VUSTORE(SEW, EM, x, vx);				\
+  }								\
+  void test_rgather_siu##EM (u##STYPE *x, u##STYPE *z, long s) {\
+    vuint##EM##_t vx;					\
+    vuint##EM##_t vz;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VULOAD(SEW, EM, x);				\
+    vz = VULOAD(SEW, EM, z);				\
+    vx = vrgather_vx_u##EM##_m(mask, vz, vx, 11);	\
+    VUSTORE(SEW, EM, x, vx);				\
+  }
+
+#define RVV_TEST_FLOAT_RGATHER(STYPE, VCLASST, VCLASS, EM, MLEN, ISTYPE, IVCLASS, SEW)\
+  void test_rgather_sv##VCLASS##EM (STYPE *x, u##ISTYPE *y, STYPE *z, long s) {\
+    v##VCLASST##EM##_t vx;					\
+    v##VCLASST##EM##_t vz;					\
+    vuint##EM##_t vy;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VULOAD(SEW, EM, y);				\
+    vz = VLOAD(VCLASS, SEW, EM, z);				\
+    vx = vrgather_vv_##VCLASS##EM##_m(mask, vz, vx, vy);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_sx##VCLASS##EM (STYPE *x, STYPE *z, long s) {	\
+    v##VCLASST##EM##_t vx;					\
+    v##VCLASST##EM##_t vz;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vz = VLOAD(VCLASS, SEW, EM, z);				\
+    vx = vrgather_vx_##VCLASS##EM##_m(mask, vz, vx, s);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_rgather_si##VCLASS##EM (STYPE *x, STYPE *z, long s) {	\
+    v##VCLASST##EM##_t vx;					\
+    v##VCLASST##EM##_t vz;					\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vz = VLOAD(VCLASS, SEW, EM, z);				\
+    vx = vrgather_vx_##VCLASS##EM##_m(mask, vz, vx, 11);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }
+
+RVV_INT_TEST (RVV_TEST_RGATHER)
+RVV_FLOAT_INT_TEST (RVV_TEST_FLOAT_RGATHER)
+
+/* { dg-final { scan-assembler-times "vrgather.vv" 44 } } */
+/* { dg-final { scan-assembler-times "vrgather.vx" 44 } } */
+/* { dg-final { scan-assembler-times "vrgather.vi" 44 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub.c
new file mode 100644
index 00000000000..baf0f9601a8
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub.c
@@ -0,0 +1,21 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, sadd)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, saddu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, ssub)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_TEST, ssubu)
+
+/* { dg-final { scan-assembler-times "vsadd.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsadd.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsadd.vi" 32 } } */
+/* { dg-final { scan-assembler-times "vsaddu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsaddu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsaddu.vi" 16 } } */
+/* { dg-final { scan-assembler-times "vssub.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssub.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vssubu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssubu.vx" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub2.c
new file mode 100644
index 00000000000..db8ccbea2a0
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saddsub2.c
@@ -0,0 +1,21 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, sadd)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, saddu)
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, ssub)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, ssubu)
+
+/* { dg-final { scan-assembler-times "vsadd.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsadd.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsadd.vi" 16 } } */
+/* { dg-final { scan-assembler-times "vsaddu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsaddu.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsaddu.vi" 16 } } */
+/* { dg-final { scan-assembler-times "vssub.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssub.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vssubu.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssubu.vx" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy.c
new file mode 100644
index 00000000000..7c5fd44d820
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy.c
@@ -0,0 +1,26 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+
+void saxpy(size_t n, const float a, const float *x, float *y) {
+  size_t l;
+
+  vfloat32m8_t vx, vy;
+
+  for (; (l = vsetvl_e32m8(n)) > 0; n -= l) {
+    vx = vle32_v_f32m8(x);
+    x += l;
+    vy = vle32_v_f32m8(y);
+    // vfmacc
+    vy = a * vx + vy;
+    vse32_v_f32m8(y, vy);
+    y += l;
+  }
+}
+
+/* { dg-final { scan-assembler-times "vsetvli\ta\[0-9\],a\[0-9\],e32,m8" 2 } } */
+/* { dg-final { scan-assembler-times "vle32.v" 2 } } */
+/* { dg-final { scan-assembler "vsetvli\tx0,x0,e32,m8"} } */
+/* { dg-final { scan-assembler "vf(?:macc|madd).vv" } } */
+/* { dg-final { scan-assembler "vse32.v" } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy2.c
new file mode 100644
index 00000000000..843790b3806
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_saxpy2.c
@@ -0,0 +1,26 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+
+void saxpy_2(size_t n, const float a, const float *x, float *y) {
+  size_t l;
+
+  vfloat32m8_t vx, vy;
+
+  for (; (l = vsetvl_e32m8(n)) > 0; n -= l) {
+    vx = vle32_v_f32m8(x);
+    x += l;
+    vy = vle32_v_f32m8(y);
+    // vfmsac.vv
+    vy = a * vx - vy;
+    vse32_v_f32m8(y, vy);
+    y += l;
+  }
+}
+
+/* { dg-final { scan-assembler-times "vsetvli\ta\[0-9\],a\[0-9\],e32,m8" 2 } } */
+/* { dg-final { scan-assembler-times "vle32.v" 2 } } */
+/* { dg-final { scan-assembler "vsetvli\tx0,x0,e32,m8"} } */
+/* { dg-final { scan-assembler "vf(?:msac|msub).vv" } } */
+/* { dg-final { scan-assembler "vse32.v" } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_setvlmax.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_setvlmax.c
new file mode 100644
index 00000000000..abc3e66aa69
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_setvlmax.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSETVLMAX(STYPE, VCLASST, VCLASS, EM, EMONE, MLEN, SEW)			\
+  size_t vlmax##VCLASS##EM(size_t n, STYPE *x, STYPE *y) {		\
+    return vsetvlmax_e##EM  ();					\
+  }
+
+RVV_INT_REDUC_TEST(VSETVLMAX)
+
+/* { dg-final { scan-assembler-times "vsetvli" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_sgemm.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sgemm.c
new file mode 100644
index 00000000000..dde21977175
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sgemm.c
@@ -0,0 +1,60 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+// https://github.com/riscv/riscv-v-spec/blob/master/vector-examples.adoc
+// == Vector Assembly Code Examples
+
+/*
+=== SGEMM example
+----
+# RV64IDV system
+#
+# void
+# sgemm_nn(size_t n,
+#          size_t m,
+#          size_t k,
+#          const float*a,   // m * k matrix
+#          size_t lda,
+#          const float*b,   // k * n matrix
+#          size_t ldb,
+#          float*c,         // m * n matrix
+#          size_t ldc)
+#
+#  c += a*b (alpha=1, no transpose on input matrices)
+#  matrices stored in C row-major order
+
+*/
+void segmm_nn(size_t size_n, size_t size_m, size_t size_k,
+              const double *a, // m * k matrix
+              size_t lda,
+              const double *b, // k * n matrix
+              size_t ldb,
+              double *c, // m * n matrix
+              size_t ldc) {
+  int i, j, k;
+  size_t vl;
+  vfloat64m1_t vec_c;
+  for (int i = 0; i < size_m; ++i) {
+    j = size_n;
+    const double *bnp = b;
+    double *cnp = c;
+    for (; vl = vsetvl_e64m1(j); j -= vl) {
+      const double *akp = a;
+      const double *bkp = bnp;
+      vec_c = *(vfloat64m1_t *)cnp;
+      for (k = 0; k < size_k; ++k) {
+        vec_c = vfmacc_vf_f64m1(vec_c, *akp, *(vfloat64m1_t *)bkp);
+        bkp += ldb;
+        akp++;
+      }
+      *(vfloat64m1_t *)cnp = vec_c;
+      cnp += vl;
+      bnp += vl;
+    }
+    a += lda;
+    c += ldc;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_shift.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_shift.c
new file mode 100644
index 00000000000..f07c880f126
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_shift.c
@@ -0,0 +1,23 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_TEST, sll)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_TEST, sll)
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_TEST, sra)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_TEST, srl)
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_TEST, ssra)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_TEST, ssrl)
+
+/* { dg-final { scan-assembler-times "vsll.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vsll.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vsra.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsra.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsrl.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsrl.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vssra.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssra.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vssrl.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssrl.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_shift2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_shift2.c
new file mode 100644
index 00000000000..e6ab91d9e92
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_shift2.c
@@ -0,0 +1,23 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, sll)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, sll)
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, sra)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, srl)
+RVV_INT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, ssra)
+RVV_UINT_TEST_ARG(RVV_SHIFT_VEC_SCALAR_MASKED_TEST, ssrl)
+
+/* { dg-final { scan-assembler-times "vsll.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vsll.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vsra.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsra.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vsrl.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vsrl.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vssra.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssra.vx" 16 } } */
+/* { dg-final { scan-assembler-times "vssrl.vv" 16 } } */
+/* { dg-final { scan-assembler-times "vssrl.vx" 16 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_slide.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_slide.c
new file mode 100644
index 00000000000..df716f44321
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_slide.c
@@ -0,0 +1,62 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_SLIDEUPDOWN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)		\
+  void test_slideup_sx##VCLASS##EM (STYPE *x, long s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vslideup_vx_##VCLASS##EM(vx, vx, s);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);				\
+  }								\
+  void test_slidedown_sx##VCLASS##EM (STYPE *x, long s) {	\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vslidedown_vx_##VCLASS##EM(vx, vx, s);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);				\
+  }								\
+  void test_slideup_si##VCLASS##EM (STYPE *x, long s) {		\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vslideup_vx_##VCLASS##EM(vx, vx, 11);			\
+    VSTORE(VCLASS, SEW, EM, x, vx);				\
+  }								\
+  void test_slidedown_si##VCLASS##EM (STYPE *x, long s) {	\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = vslidedown_vx_##VCLASS##EM(vx, vx, 11);		\
+    VSTORE(VCLASS, SEW, EM, x, vx);				\
+  }								\
+
+
+#define RVV_TEST_SLIDE1UPDOWN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, PREFIX)	\
+  void test_slide1up_sx##VCLASS##EM (STYPE *x, STYPE s) {	\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = v##PREFIX##slide1up_v##STYPEC##_##VCLASS##EM(vx, s);		\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_slide1down_sx##VCLASS##EM (STYPE *x, STYPE s) {	\
+    v##VCLASST##EM##_t vx;					\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vx = v##PREFIX##slide1down_v##STYPEC##_##VCLASS##EM(vx, s);		\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }
+
+RVV_INT_TEST_ARG (RVV_TEST_SLIDE1UPDOWN, )
+RVV_UINT_TEST_ARG (RVV_TEST_SLIDE1UPDOWN, )
+RVV_FLOAT_TEST_ARG (RVV_TEST_SLIDE1UPDOWN, f)
+RVV_INT_TEST (RVV_TEST_SLIDEUPDOWN)
+RVV_UINT_TEST (RVV_TEST_SLIDEUPDOWN)
+RVV_FLOAT_TEST (RVV_TEST_SLIDEUPDOWN)
+
+/* { dg-final { scan-assembler-times "vslideup.vx" 44 } } */
+/* { dg-final { scan-assembler-times "vslideup.vi" 44 } } */
+/* { dg-final { scan-assembler-times "vslidedown.vx" 44 } } */
+/* { dg-final { scan-assembler-times "vslidedown.vi" 44 } } */
+/* { dg-final { scan-assembler-times "vslide1up.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vslide1down.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vfslide1up.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfslide1down.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_slide2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_slide2.c
new file mode 100644
index 00000000000..7dec8216203
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_slide2.c
@@ -0,0 +1,80 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_TEST_SLIDEUPDOWN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)		\
+  void test_slideup_sx##VCLASS##EM (STYPE *x, STYPE *y, long s) {\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vx = vslideup_vx_##VCLASS##EM##_m(mask, vy, vx, s);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_slidedown_sx##VCLASS##EM (STYPE *x, STYPE *y, long s) {\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vx = vslidedown_vx_##VCLASS##EM##_m(mask, vy, vx, s);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_slideup_si##VCLASS##EM (STYPE *x, STYPE *y, long s) {\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vx = vslideup_vx_##VCLASS##EM##_m(mask, vy, vx, 11);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }								\
+  void test_slidedown_si##VCLASS##EM (STYPE *x, STYPE *y, long s) {\
+    v##VCLASST##EM##_t vx, vy;				\
+    vbool##MLEN##_t mask;					\
+    mask = MSET (MLEN);				\
+    vx = VLOAD(VCLASS, SEW, EM, x);				\
+    vy = VLOAD(VCLASS, SEW, EM, y);				\
+    vx = vslidedown_vx_##VCLASS##EM##_m(mask, vy, vx, 11);\
+    VSTORE(VCLASS, SEW, EM, x, vx);					\
+  }
+
+
+#define RVV_TEST_SLIDE1UPDOWN(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW, PREFIX)	\
+  void test_slide1up_sx##VCLASS##EM (STYPE *x, STYPE *y, STYPE s) {	\
+    v##VCLASST##EM##_t vx, vy;					\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = v##PREFIX##slide1up_v##STYPEC##_##VCLASS##EM##_m(mask, vy, vx, s);		\
+    VSTORE(VCLASS, SEW, EM, x, vx);						\
+  }									\
+  void test_slide1down_sx##VCLASS##EM (STYPE *x, STYPE *y, STYPE s) {	\
+    v##VCLASST##EM##_t vx, vy;					\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VLOAD(VCLASS, SEW, EM, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = v##PREFIX##slide1down_v##STYPEC##_##VCLASS##EM##_m(mask, vy, vx, s);	\
+    VSTORE(VCLASS, SEW, EM, x, vx);						\
+  }
+
+RVV_INT_TEST_ARG (RVV_TEST_SLIDE1UPDOWN, )
+RVV_UINT_TEST_ARG (RVV_TEST_SLIDE1UPDOWN, )
+RVV_FLOAT_TEST_ARG (RVV_TEST_SLIDE1UPDOWN, f)
+RVV_INT_TEST (RVV_TEST_SLIDEUPDOWN)
+RVV_UINT_TEST (RVV_TEST_SLIDEUPDOWN)
+RVV_FLOAT_TEST (RVV_TEST_SLIDEUPDOWN)
+
+/* { dg-final { scan-assembler-times "vslideup.vx" 44 } } */
+/* { dg-final { scan-assembler-times "vslideup.vi" 44 } } */
+/* { dg-final { scan-assembler-times "vslidedown.vx" 44 } } */
+/* { dg-final { scan-assembler-times "vslidedown.vi" 44 } } */
+/* { dg-final { scan-assembler-times "vslide1up.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vslide1down.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vfslide1up.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vfslide1down.vf" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_stack-1.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_stack-1.c
new file mode 100644
index 00000000000..d52c5b21c77
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_stack-1.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+
+void bar(vint8m1_t *);
+void barx(int *);
+void foo(){
+  int x[10];
+  vint8m1_t a;
+  bar(&a);
+  barx(&x[0]);
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_strcpy.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strcpy.c
new file mode 100644
index 00000000000..a9ccd748179
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strcpy.c
@@ -0,0 +1,51 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+/*
+https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc
+
+=== Example using vector mask instructions
+
+The following is an example of vectorizing a data-dependent exit loop.
+
+----
+  # char* strcpy(char *dst, const char* src)
+strcpy:
+      mv a2, a0             # Copy dst
+loop:
+    vsetvli x0, x0, e8   # Max length vectors of bytes
+    vlbuff.v v1, (a1)       # Get src bytes
+      csrr t1, vl           # Get number of bytes fetched
+    vmseq.vi v0, v1, 0      # Flag zero bytes
+    vfirst.m a3, v0         # Zero found?
+      add a1, a1, t1        # Bump pointer
+    vmsif.m v0, v0          # Set mask up to and including zero byte.
+    vsb.v v1, (a2), v0.t    # Write out bytes
+      add a2, a2, t1        # Bump pointer
+      bltz a3, loop         # Zero byte not found, so loop
+
+      ret
+*/
+
+char *strcpy(char *dst, const char* src) {
+  char *old_dst = dst;
+  int zero_find = -1;
+  while (zero_find < 0) {
+    vsetvlmax_e8m1();
+    vuint8m1_t value;
+    value = vle8ff_v_u8m1((uint8_t *)src);
+    size_t vl =vreadvl();
+    vbool8_t cmp;
+    cmp = vmseq_vx_u8m1_b8(value, 0);
+    zero_find = vfirst_m_b8(cmp); // if no zero than return -1
+    vbool8_t mask;
+    mask = vmsif_m_b8(cmp); // set mask up to and including zero byte
+    vse8_v_u8m1_m(mask, (uint8_t *)dst, value);
+    src+=vl;
+    dst+=vl;
+  }
+  return old_dst;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load-2.c
new file mode 100644
index 00000000000..9e1d29fd4e8
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load-2.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                            \
+    vbool##MLEN##_t mask;                                                    \
+    mask = MSET (MLEN);                                             \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vx = vlse##SEW##_v_##VCLASS##EM##_m(mask, vy, x, stride);                     \
+    vz = vx + vy;                                                              \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                  \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vlse8.v" 8 } } */
+/* { dg-final { scan-assembler-times "vlse16.v" 12 } } */
+/* { dg-final { scan-assembler-times "vlse32.v" 12 } } */
+/* { dg-final { scan-assembler-times "vlse64.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load.c
new file mode 100644
index 00000000000..ea7da2bfc4e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-load.c
@@ -0,0 +1,27 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = vlse##SEW##_v_##VCLASS##EM(x, stride);                                      \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vlse8.v" 8 } } */
+/* { dg-final { scan-assembler-times "vlse16.v" 12 } } */
+/* { dg-final { scan-assembler-times "vlse32.v" 12 } } */
+/* { dg-final { scan-assembler-times "vlse64.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store-2.c
new file mode 100644
index 00000000000..4c9a5cf9280
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store-2.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                            \
+    vbool##MLEN##_t mask;                                                    \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    vsse##SEW##_v_##VCLASS##EM##_m(mask, x, stride, vz);                          \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vsse8.v" 8 } } */
+/* { dg-final { scan-assembler-times "vsse16.v" 12 } } */
+/* { dg-final { scan-assembler-times "vsse32.v" 12 } } */
+/* { dg-final { scan-assembler-times "vsse64.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store.c
new file mode 100644
index 00000000000..557abd6c69d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strided-store.c
@@ -0,0 +1,27 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                            \
+    vbool##MLEN##_t mask;                                                    \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    vsse##SEW##_v_##VCLASS##EM(x, stride, vz);                                       \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vsse8.v" 8 } } */
+/* { dg-final { scan-assembler-times "vsse16.v" 12 } } */
+/* { dg-final { scan-assembler-times "vsse32.v" 12 } } */
+/* { dg-final { scan-assembler-times "vsse64.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_strlen.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strlen.c
new file mode 100644
index 00000000000..a8732cee86f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strlen.c
@@ -0,0 +1,53 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+/*
+https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc
+
+=== Examples
+
+*/
+/*
+strlen example using unit-stride fault-only-first instruction
+
+# size_t strlen(const char *str)
+# a0 holds *str
+
+strlen:
+    mv a3, a0             # Save start
+loop:
+    vsetvli a1, x0, e8  # Vector of bytes of maximum length
+    vlbff.v v1, (a3)      # Load bytes
+    csrr a1, vl           # Get bytes read
+    vmseq.vi v0, v1, 0    # Set v0[i] where v1[i] = 0
+    vfirst.m a2, v0       # Find first set bit
+    add a3, a3, a1        # Bump pointer
+    bltz a2, loop         # Not found?
+
+    add a0, a0, a1        # Sum start + bump
+    add a3, a3, a2        # Add index
+    sub a0, a3, a0        # Subtract start address+bump
+
+    ret
+*/
+
+size_t strlen(const char *str) {
+  const char *start = str;
+  int32_t first_set = -1;
+  while (first_set < 0) {
+    vsetvlmax_e8m1(); // setvm max
+    vuint8m1_t value;
+    value = vle8ff_v_u8m1((const uint8_t *)str);
+    size_t vl = vreadvl();
+    vbool8_t cmp;
+    cmp = vmseq_vx_u8m1_b8(value, 0);
+    first_set = vfirst_m_b8(cmp); /* reutrn -1 mean not found */
+    str += vl;
+  }
+  return str - start;
+}
+
+
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_strncpy.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strncpy.c
new file mode 100644
index 00000000000..88f15d89795
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_strncpy.c
@@ -0,0 +1,73 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+/*
+https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc
+
+char* strncpy(char *dst, const char* src, size_t n)
+strncpy:
+      mv a3, a0             # Copy dst
+loop:
+    vsetvli x0, a2, e8   # Vectors of bytes.
+    vlbuff.v v1, (a1)       # Get src bytes
+    vmseq.vi v0, v1, 0      # Flag zero bytes
+    vfirst.m a4, v0         # Zero found?
+    vmsif.m v0, v0          # Set mask up to and including zero byte.
+    vsb.v v1, (a3), v0.t    # Write out bytes
+      csrr t1, vl           # Get number of bytes fetched
+      sub a2, a2, t1        # Decrement count.
+      bgez a4, zero_tail    # Zero remaining bytes.
+      add a1, a1, t1        # Bump pointer
+      add a3, a3, t1        # Bump pointer
+      bnez a2, loop         # Anymore?
+
+      ret
+
+zero_tail:
+    vsetvli x0, a2, e8,m8   # Vectors of bytes.
+    vmv.v.i v0, 0           # Splat zero.
+
+zero_loop:
+    vsetvli t1, a2, e8,m8   # Vectors of bytes.
+    vsb.v v0, (a3)          # Store zero.
+      sub a2, a2, t1        # Decrement count.
+      add a3, a3, t1        # Bump pointer
+      bnez a2, zero_loop    # Anymore?
+
+      ret
+
+----
+*/
+
+char *strncpy(char *dst, const char *src, size_t n) {
+  char *old_dst = dst;
+  int zero_find = -1;
+  size_t vl;
+  while (zero_find < 0) {
+    vsetvlmax_e8m1();
+    vuint8m1_t value;
+    value = vle8ff_v_u8m1((uint8_t *)src);
+    size_t vl = vreadvl();
+    vbool8_t cmp;
+    cmp = vmseq_vx_u8m1_b8(value, 0);
+    zero_find = vfirst_m_b8(cmp); // if no zero than return -1
+    vbool8_t mask;
+    mask = vmsif_m_b8(cmp); // set mask up to and including zero byte
+    vse8_v_u8m1_m(mask, (uint8_t *)dst, value);
+    n -= vl;
+    src += vl;
+    dst += vl;
+  }
+  // handle zero tail
+  vsetvlmax_e8m8();
+  vuint8m8_t zeros;
+  zeros = vsplat_s_u8m8(0);
+  for (; vl = vsetvl_e8m8(n); n -= vl) {
+    *(vuint8m8_t *)dst = zeros;
+    dst += vl;
+  }
+  return old_dst;
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub.c
new file mode 100644
index 00000000000..1871e089a93
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub.c
@@ -0,0 +1,17 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, sub, -)
+RVV_UINT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, sub, -)
+RVV_FLOAT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_TEST, fsub, -)
+
+/* { dg-final { scan-assembler-times "vsub.vv" 64 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vsub.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vadd.vi" 32 } } */
+/* { dg-final { scan-assembler-times "vfsub.vv" 24 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vfsub.vf" 0 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub2.c
new file mode 100644
index 00000000000..1799cd0d34c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub2.c
@@ -0,0 +1,42 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, sub)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, sub)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fsub)
+
+/* Same for reverse subtract.  */
+#define VRSUB(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                         \
+  void vrsub##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {              \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    vy = vrsub_vx_##VCLASS##EM (vy, z);                                     \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                  \
+    vx = vrsub_vx_##VCLASS##EM (vx, 1);                                     \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                  \
+  }
+/* Same as above without the immediate for reverse subtract.  */
+#define VRSUB_NO_IMM(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                  \
+  void vrsub##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {              \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vy = vfrsub_vf_##VCLASS##EM (vy, z);                                     \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                  \
+  }
+
+RVV_INT_TEST(VRSUB)
+RVV_UINT_TEST(VRSUB)
+RVV_FLOAT_TEST(VRSUB_NO_IMM)
+
+/* { dg-final { scan-assembler-times "vsub.vv" 32 } } */
+/* sub only provide scalar version.  */
+/* { dg-final { scan-assembler-times "vsub.vx" 64 } } */
+/* { dg-final { scan-assembler-times "vfsub.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsub.vf" 12 } } */
+/* { dg-final { scan-assembler-times "vrsub.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vrsub.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub3.c
new file mode 100644
index 00000000000..a7820edaaa4
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_sub3.c
@@ -0,0 +1,47 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, sub)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, sub)
+RVV_FLOAT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_MASKED_TEST, fsub)
+
+/* Reverse subtract.  */
+#define VRSUB(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                         \
+  void vrsub##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {              \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    mask = MSET (MLEN);                                             \
+    vy = vrsub_vx_##VCLASS##EM##_m (mask, vx, vy, z);                    \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                \
+    vx = vrsub_vx_##VCLASS##EM##_m (mask, vx, vx, 1);                    \
+    VSTORE(VCLASS, SEW, EM, x, vx);                                                \
+  }
+/* Same as above without the immediate for reverse subtract.  */
+#define VRSUB_NO_IMM(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                  \
+  void vrsub##VCLASS##EM(size_t n, STYPE *x, STYPE *y, STYPE z) {              \
+    vbool##MLEN##_t mask;                                                    \
+    v##VCLASST##EM##_t vx, vy;                                                \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                                 \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                                 \
+    mask = MSET (MLEN);                                             \
+    vy = vfrsub_vf_##VCLASS##EM##_m (mask, vx, vy, z);                    \
+    VSTORE(VCLASS, SEW, EM, y, vy);                                                  \
+  }
+
+
+RVV_INT_TEST(VRSUB)
+RVV_UINT_TEST(VRSUB)
+RVV_FLOAT_TEST(VRSUB_NO_IMM)
+
+/* { dg-final { scan-assembler-times "vsub.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vsub.vx" 64 } } */
+/* { dg-final { scan-assembler-times "vfsub.vv" 12 } } */
+/* { dg-final { scan-assembler-times "vfsub.vf" 12 } } */
+/* rsub only provide scalar version.  */
+/* { dg-final { scan-assembler-times "vrsub.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vrsub.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_type.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_type.c
new file mode 100644
index 00000000000..ad2bfdd375f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_type.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+
+#include "rvv-common.h"
+
+#define DECL_F_VEC_TUPLE_TYPES(SEW, LMUL, NR, MLEN, X) \
+  vfloat##SEW##m##LMUL##x##NR##_t f##SEW##m##LMUL##x##NR;
+
+#define DECL_I_VEC_TUPLE_TYPES(SEW, LMUL, NR, MLEN, X) \
+  vint##SEW##m##LMUL##x##NR##_t i##SEW##m##LMUL##x##NR; \
+  vuint##SEW##m##LMUL##x##NR##_t u##SEW##m##LMUL##x##NR;
+
+void type_test()
+{
+  RVV_SEG_NO_SEW8_TEST_ARG(DECL_F_VEC_TUPLE_TYPES, );
+  RVV_SEG_TEST_ARG(DECL_I_VEC_TUPLE_TYPES, );
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_utils.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_utils.c
new file mode 100644
index 00000000000..ad2bfdd375f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_tuple_utils.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+
+#include "rvv-common.h"
+
+#define DECL_F_VEC_TUPLE_TYPES(SEW, LMUL, NR, MLEN, X) \
+  vfloat##SEW##m##LMUL##x##NR##_t f##SEW##m##LMUL##x##NR;
+
+#define DECL_I_VEC_TUPLE_TYPES(SEW, LMUL, NR, MLEN, X) \
+  vint##SEW##m##LMUL##x##NR##_t i##SEW##m##LMUL##x##NR; \
+  vuint##SEW##m##LMUL##x##NR##_t u##SEW##m##LMUL##x##NR;
+
+void type_test()
+{
+  RVV_SEG_NO_SEW8_TEST_ARG(DECL_F_VEC_TUPLE_TYPES, );
+  RVV_SEG_TEST_ARG(DECL_I_VEC_TUPLE_TYPES, );
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unary.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unary.c
new file mode 100644
index 00000000000..df788db4b8c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unary.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_UNARY_BUILTIN_VEC_TEST, not)
+RVV_UINT_TEST_ARG(RVV_UNARY_BUILTIN_VEC_TEST, not)
+
+/* { dg-final { scan-assembler-times "vnot.v" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unary2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unary2.c
new file mode 100644
index 00000000000..eafab1e51b0
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unary2.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_UNARY_BUILTIN_VEC_MASKED_TEST, not)
+RVV_UINT_TEST_ARG(RVV_UNARY_BUILTIN_VEC_MASKED_TEST, not)
+
+/* { dg-final { scan-assembler-times "vnot.v" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_uninit_val.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_uninit_val.c
new file mode 100644
index 00000000000..6029724f77e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_uninit_val.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include <stdint.h>
+
+void uninit(int16_t *x, int16_t *z, size_t n) {
+  size_t vl;
+  vint16m1_t v0, v1, v2;
+  vbool16_t mask;
+  for (; vl = vsetvl_e16m1(n); n -= vl) {
+    v0 = vle16_v_i16m1(x);
+    v1 = vadd_vv_i16m1 (v0, v2);
+    *(vint16m1_t *)z = v1;
+    z += vl;
+    x += vl;
+  }
+}
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load-2.c
new file mode 100644
index 00000000000..30f34669530
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load-2.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = vle##SEW##ff_v_##VCLASS##EM##_m(mask, vx, y);                            \
+    vz = vx + vy;                                                              \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vle8ff.v" 8 } } */
+/* { dg-final { scan-assembler-times "vle16ff.v" 12 } } */
+/* { dg-final { scan-assembler-times "vle32ff.v" 12 } } */
+/* { dg-final { scan-assembler-times "vle64ff.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load.c
new file mode 100644
index 00000000000..d80ee616f2d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-ff-load.c
@@ -0,0 +1,27 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = vle##SEW##ff_v_##VCLASS##EM(y);                                             \
+    vz = vx + vy;                                                              \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vle8ff.v" 8 } } */
+/* { dg-final { scan-assembler-times "vle16ff.v" 12 } } */
+/* { dg-final { scan-assembler-times "vle32ff.v" 12 } } */
+/* { dg-final { scan-assembler-times "vle64ff.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load-2.c
new file mode 100644
index 00000000000..b6eade1a116
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load-2.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vx = vle##SEW##_v_##VCLASS##EM##_m (mask, vy, x);                              \
+    vz = vx + vy;                                                              \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vle8.v" 16 } } */
+/* { dg-final { scan-assembler-times "vle16.v" 24 } } */
+/* { dg-final { scan-assembler-times "vle32.v" 24 } } */
+/* { dg-final { scan-assembler-times "vle64.v" 24 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load.c
new file mode 100644
index 00000000000..d29f0bdb45d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-load.c
@@ -0,0 +1,27 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vle8.v" 16 } } */
+/* { dg-final { scan-assembler-times "vle16.v" 24 } } */
+/* { dg-final { scan-assembler-times "vle32.v" 24 } } */
+/* { dg-final { scan-assembler-times "vle64.v" 24 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store-2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store-2.c
new file mode 100644
index 00000000000..51110f66e22
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store-2.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    vse##SEW##_v_##VCLASS##EM##_m(mask, x, vz);                                   \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vse8.v" 8 } } */
+/* { dg-final { scan-assembler-times "vse16.v" 12 } } */
+/* { dg-final { scan-assembler-times "vse32.v" 12 } } */
+/* { dg-final { scan-assembler-times "vse64.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store.c
new file mode 100644
index 00000000000..627de052e8d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_unit-stride-store.c
@@ -0,0 +1,27 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VSLOADSTORE(STYPE, VCLASST, VCLASS, EM, MLEN, STYPEC, SEW)                                   \
+  void vsloadstore##VCLASS##EM(size_t n, long stride, STYPE *x,                \
+                               STYPE *y, STYPE z) {                            \
+    v##VCLASST##EM##_t vx, vy, vz;                                           \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vx + vy;                                                              \
+    VSTORE(VCLASS, SEW, EM, x, vz);                                                \
+  }
+
+RVV_INT_TEST(VSLOADSTORE)
+RVV_UINT_TEST(VSLOADSTORE)
+RVV_FLOAT_TEST(VSLOADSTORE)
+
+/* { dg-final { scan-assembler-times "vse8.v" 8 } } */
+/* { dg-final { scan-assembler-times "vse16.v" 12 } } */
+/* { dg-final { scan-assembler-times "vse32.v" 12 } } */
+/* { dg-final { scan-assembler-times "vse64.v" 12 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wadd-wv.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wadd-wv.c
new file mode 100644
index 00000000000..9c593f20c63
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wadd-wv.c
@@ -0,0 +1,34 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWADD(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW)         \
+  void vwadd##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, WSTYPE *z) {           \
+    v##VCLASST##EM##_t vy;                                                   \
+    v##VCLASST##WEM##_t vx, vz;                                              \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vwadd_wv_##VCLASS##WEM (vx, vy);                                    \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+#define VWADDU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW)        \
+  void vwaddu##EM(size_t n, WSTYPE *x, STYPE *y,                       \
+                          WSTYPE *z) {                                         \
+    v##VCLASST##EM##_t vy;                                                   \
+    v##VCLASST##WEM##_t vx, vz;                                              \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = vwaddu_wv_##VCLASS##WEM (vx, vy);                                    \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+RVV_WINT_TEST(VWADD)
+RVV_WUINT_TEST(VWADDU)
+
+/* { dg-final { scan-assembler-times "vwadd.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.wv" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv.c
new file mode 100644
index 00000000000..3ca0fc62c3e
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv.c
@@ -0,0 +1,93 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWADDSUB(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)                     \
+  void v##OP##VCLASS##EM(size_t n, STYPE *x, STYPE *y, WSTYPE *z) {            \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = v##OP##_vv_##VCLASS##WEM (vx, vy);                                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, STYPE *x, STYPE y, WSTYPE *z) {         \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vz = v##OP##_v##STYPEC##_##VCLASS##WEM (vx, y);                                  \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+#define VWADDSUBU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)    \
+  void v##OP##u##EM(size_t n, STYPE *x, STYPE *y, WSTYPE *z) {         \
+    v##VCLASST##EM##_t vx, vy;                                              \
+    v##VCLASST##WEM##_t vz;                                                 \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                              \
+    vz = v##OP##_vv_##VCLASS##WEM (vx, vy);                                \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }                                                                            \
+  void v##OP##u##EM##_s(size_t n, STYPE *x, STYPE y, WSTYPE *z) {      \
+    v##VCLASST##EM##_t vx, vy;                                              \
+    v##VCLASST##WEM##_t vz;                                                 \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                              \
+    vz = v##OP##_v##STYPEC##_##VCLASS##WEM (vx, y);                                \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }
+
+#define VWMULSU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)      \
+  void v##OP##VCLASS##EM(size_t n, STYPE *x, u##STYPE *y, WSTYPE *z) {         \
+    v##VCLASST##EM##_t vx;                                                   \
+    vuint##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VULOAD(SEW, EM, y);                                              \
+    vz = v##OP##_vv_##VCLASS##WEM (vx, vy);                                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, STYPE *x, u##STYPE y, WSTYPE *z) {      \
+    v##VCLASST##EM##_t vx;                                                   \
+    vuint##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vz = v##OP##_v##STYPEC##_##VCLASS##WEM (vx, y);                                  \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+RVV_WINT_TEST_ARG(VWADDSUB, wadd)
+RVV_WUINT_TEST_ARG(VWADDSUBU, waddu)
+RVV_WINT_TEST_ARG(VWADDSUB, wsub)
+RVV_WUINT_TEST_ARG(VWADDSUBU, wsubu)
+RVV_WINT_TEST_ARG(VWADDSUB, wmul)
+RVV_WUINT_TEST_ARG(VWADDSUBU, wmulu)
+RVV_WINT_TEST_ARG(VWMULSU, wmulsu)
+
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwadd)
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwsub)
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwmul)
+
+/* { dg-final { scan-assembler-times "vwadd.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwadd.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmul.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmul.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulsu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulsu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vfwadd.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwadd.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmul.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmul.vf" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv2.c
new file mode 100644
index 00000000000..26339b5077d
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-vv2.c
@@ -0,0 +1,111 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWADDSUB(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)                     \
+  void v##OP##VCLASS##EM(size_t n, STYPE *x, STYPE *y, WSTYPE *z) {            \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                              \
+    vz = v##OP##_vv_##VCLASS##WEM##_m (mask, vz, vx, vy);                \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, STYPE *x, STYPE y, WSTYPE *z) {         \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                              \
+    vz = v##OP##_v##STYPEC##_##VCLASS##WEM##_m (mask, vz, vx, y);                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+#define VWADDSUBU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)    \
+  void v##OP##u##EM(size_t n, STYPE *x, STYPE *y, WSTYPE *z) {         \
+    v##VCLASST##EM##_t vx, vy;                                              \
+    v##VCLASST##WEM##_t vz;                                                 \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                              \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                             \
+    vz = v##OP##_vv_##VCLASS##WEM##_m (mask, vz, vx, vy);              \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }                                                                            \
+  void v##OP##u##EM##_s(size_t n, STYPE *x, STYPE y, WSTYPE *z) {      \
+    v##VCLASST##EM##_t vx, vy;                                              \
+    v##VCLASST##WEM##_t vz;                                                 \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                              \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                             \
+    vz = v##OP##_v##STYPEC##_##VCLASS##WEM##_m (mask, vz, vx, y);               \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }
+
+#define VWMULSU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)      \
+  void v##OP##VCLASS##EM(size_t n, STYPE *x, u##STYPE *y, WSTYPE *z) {         \
+    v##VCLASST##EM##_t vx;                                                   \
+    vuint##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VULOAD(SEW, EM, y);                                              \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                              \
+    vz = v##OP##_vv_##VCLASS##WEM##_m (mask, vz, vx, vy);                \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, STYPE *x, u##STYPE y, WSTYPE *z) {      \
+    v##VCLASST##EM##_t vx;                                                   \
+    vuint##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                              \
+    vz = v##OP##_v##STYPEC##_##VCLASS##WEM##_m (mask, vz, vx, y);                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+RVV_WINT_TEST_ARG(VWADDSUB, wadd)
+RVV_WUINT_TEST_ARG(VWADDSUBU, waddu)
+RVV_WINT_TEST_ARG(VWADDSUB, wsub)
+RVV_WUINT_TEST_ARG(VWADDSUBU, wsubu)
+RVV_WINT_TEST_ARG(VWADDSUB, wmul)
+RVV_WUINT_TEST_ARG(VWADDSUBU, wmulu)
+RVV_WINT_TEST_ARG(VWMULSU, wmulsu)
+
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwadd)
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwsub)
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwmul)
+
+/* { dg-final { scan-assembler-times "vwadd.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwadd.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmul.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmul.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulsu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmulsu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vfwadd.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwadd.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.vf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmul.vv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwmul.vf" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv.c
new file mode 100644
index 00000000000..4e708da903a
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv.c
@@ -0,0 +1,63 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWADDSUB(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)                     \
+  void v##OP##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, WSTYPE *z) {           \
+    v##VCLASST##EM##_t vy;                                                   \
+    v##VCLASST##WEM##_t vx, vz;                                              \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = v##OP##_wv_##VCLASS##WEM (vx, vy);                                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, WSTYPE *x, STYPE y, WSTYPE *z) {        \
+    v##VCLASST##EM##_t vy;                                                   \
+    v##VCLASST##WEM##_t vx, vz;                                              \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                              \
+    vz = v##OP##_w##STYPEC##_##VCLASS##WEM (vx, y);                                  \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+#define VWADDSUBU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)                    \
+  void v##OP##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, WSTYPE *z) {        \
+    v##VCLASST##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vx, vz;                                             \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                             \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                              \
+    vz = v##OP##_wv_##VCLASS##WEM (vx, vy);                                \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, WSTYPE *x, STYPE y, WSTYPE *z) {     \
+    v##VCLASST##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vx, vz;                                             \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                             \
+    vz = v##OP##_w##STYPEC##_##VCLASS##WEM (vx, y);                                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }
+
+
+RVV_WINT_TEST_ARG(VWADDSUB, wadd)
+RVV_WUINT_TEST_ARG(VWADDSUBU, waddu)
+RVV_WINT_TEST_ARG(VWADDSUB, wsub)
+RVV_WUINT_TEST_ARG(VWADDSUBU, wsubu)
+
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwadd)
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwsub)
+
+/* { dg-final { scan-assembler-times "vwadd.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwadd.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vfwadd.wv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwadd.wf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.wv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.wf" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv2.c
new file mode 100644
index 00000000000..d163d5af368
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wbinop-wv2.c
@@ -0,0 +1,74 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWADDSUB(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)                     \
+  void v##OP##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, WSTYPE *z) {           \
+    v##VCLASST##EM##_t vy;                                                   \
+    v##VCLASST##WEM##_t vx, vz;                                              \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                              \
+    vz = v##OP##_wv_##VCLASS##WEM##_m (mask, vz, vx, vy);                \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, WSTYPE *x, STYPE y, WSTYPE *z) {        \
+    v##VCLASST##EM##_t vy;                                                   \
+    v##VCLASST##WEM##_t vx, vz;                                              \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                              \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                              \
+    vz = v##OP##_w##STYPEC##_##VCLASS##WEM##_m (mask, vz, vx, y);                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+#define VWADDSUBU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP)                    \
+  void v##OP##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, WSTYPE *z) {        \
+    v##VCLASST##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vx, vz;                                             \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                             \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                              \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                             \
+    vz = v##OP##_wv_##VCLASS##WEM##_m (mask, vz, vx, vy);              \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }                                                                            \
+  void v##OP##VCLASS##EM##_s(size_t n, WSTYPE *x, STYPE y, WSTYPE *z) {     \
+    v##VCLASST##EM##_t vy;                                                  \
+    v##VCLASST##WEM##_t vx, vz;                                             \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vx = VLOAD(VCLASS, WSEW, WEM, x);                                             \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                             \
+    vz = v##OP##_w##STYPEC##_##VCLASS##WEM##_m (mask, vz, vx, y);               \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }
+
+RVV_WINT_TEST_ARG(VWADDSUB, wadd)
+RVV_WUINT_TEST_ARG(VWADDSUBU, waddu)
+RVV_WINT_TEST_ARG(VWADDSUB, wsub)
+RVV_WUINT_TEST_ARG(VWADDSUBU, wsubu)
+
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwadd)
+RVV_WFLOAT_TEST_ARG(VWADDSUB, fwsub)
+
+/* { dg-final { scan-assembler-times "vwadd.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwadd.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vwaddu.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.wv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsub.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.wx" 9 } } */
+/* { dg-final { scan-assembler-times "vfwadd.wv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwadd.wf" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.wv" 6 } } */
+/* { dg-final { scan-assembler-times "vfwsub.wf" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt.c
new file mode 100644
index 00000000000..e34aef875d0
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt.c
@@ -0,0 +1,30 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_WFCVT_TEST(EM, WEM, MLEN, SEW, WSEW)	\
+  void rvvcvtfi##SEW##int##EM##_v_nomask_builtin_test(size_t n, int##SEW##_t *x,\
+					       int##WSEW##_t *y, int##WSEW##_t z)	\
+  {									\
+    vint##EM##_t vx;							\
+    vint##WEM##_t vy;						\
+    vx = VILOAD(SEW, EM, x);						\
+    vy = vwcvt_x_x_v_i##WEM (vx);			\
+    VISTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtuif##SEW##int##EM##_v_nomask_builtin_test(size_t n, uint##WSEW##_t *x,\
+						  uint##SEW##_t *y, uint##WSEW##_t z)	\
+  {									\
+    vuint##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vy = VULOAD(SEW, EM, y);						\
+    vx = vwcvtu_x_x_v_u##WEM (vy);			\
+    VUSTORE(WSEW, WEM, x, vx);						\
+  }
+
+RVV_INT_WNCVT_INT_TEST(RVV_WFCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vwcvt.x.x.v" 9 } } */
+/* { dg-final { scan-assembler-times "vwcvtu.x.x.v" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt2.c
new file mode 100644
index 00000000000..70d3286a59c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wcvt2.c
@@ -0,0 +1,36 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_WFCVT_TEST(EM, WEM, MLEN, SEW, WSEW)	\
+  void rvvcvtfi##SEW##int##EM##_v_nomask_builtin_test(size_t n, int##SEW##_t *x,\
+					       int##WSEW##_t *y, int##WSEW##_t z)	\
+  {									\
+    vint##EM##_t vx;							\
+    vint##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VILOAD(SEW, EM, x);						\
+    vy = VILOAD(WSEW, WEM, y);						\
+    vy = vwcvt_x_x_v_i##WEM##_m (mask, vy, vx);	\
+    VISTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtuif##SEW##int##EM##_v_nomask_builtin_test(size_t n, uint##WSEW##_t *x,\
+						  uint##SEW##_t *y, uint##WSEW##_t z)	\
+  {									\
+    vuint##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VULOAD(WSEW, WEM, x);						\
+    vy = VULOAD(SEW, EM, y);						\
+    vx = vwcvtu_x_x_v_u##WEM##_m (mask, vx, vy);	\
+    VUSTORE(WSEW, WEM, x, vx);						\
+  }
+
+RVV_INT_WNCVT_INT_TEST(RVV_WFCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vwcvt.x.x.v" 9 } } */
+/* { dg-final { scan-assembler-times "vwcvtu.x.x.v" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt.c
new file mode 100644
index 00000000000..c4789f13e6c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt.c
@@ -0,0 +1,82 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_WFCVT_TEST(EM, WEM, MLEN, SEW, WSEW)	\
+  void rvvcvtfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+					       int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vint##WEM##_t vy;							\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = vfwcvt_x_f_v_i##WEM (vx);			\
+    VISTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtfui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						u##int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vuint##WEM##_t vy;						\
+    vx = VFLOAD(SEW, EM, x);					\
+    vy = vfwcvt_xu_f_v_u##WEM (vx);			\
+    VUSTORE(WSEW, WEM, y, vy);					\
+  }									\
+  void rvvcvtrtzfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+					       int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vint##WEM##_t vy;							\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = vfwcvt_rtz_x_f_v_i##WEM (vx);			\
+    VISTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtfrtzui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						u##int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vuint##WEM##_t vy;						\
+    vx = VFLOAD(SEW, EM, x);					\
+    vy = vfwcvt_rtz_xu_f_v_u##WEM (vx);			\
+    VUSTORE(WSEW, WEM, y, vy);					\
+  }									\
+  void rvvcvtif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+					       int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vint##EM##_t vy;						\
+    vy = VILOAD(SEW, EM, y);					\
+    vx = vfwcvt_f_x_v_f##WEM (vy);			\
+    VFSTORE(WSEW, WEM, x, vx);						\
+  }									\
+  void rvvcvtuif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						u##int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vy = VULOAD(SEW, EM, y);						\
+    vx = vfwcvt_f_xu_v_f##WEM (vy);			\
+    VFSTORE(WSEW, WEM, x, vx);						\
+  }\
+  void rvvcvtff##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						_RVV_F##SEW##_TYPE *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vfloat##EM##_t vy;						\
+    vy = VFLOAD(SEW, EM, y);						\
+    vx = vfwcvt_f_f_v_f##WEM (vy);			\
+    VFSTORE(WSEW, WEM, x, vx);						\
+  }
+
+
+
+RVV_FLOAT_WNCVT_INT_TEST(RVV_WFCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vfwcvt.xu.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.x.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.rtz.xu.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.rtz.x.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.f.xu.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.f.x.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.f.f.v" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt2.c
new file mode 100644
index 00000000000..11c8db4b6dc
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wfcvt2.c
@@ -0,0 +1,103 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+#define RVV_WFCVT_TEST(EM, WEM, MLEN, SEW, WSEW)			\
+  void rvvcvtfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+					       int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vint##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = VILOAD(WSEW, WEM,y);						\
+    vy = vfwcvt_x_f_v_i##WEM##_m (mask, vy, vx);	\
+    VISTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtfui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						u##int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vuint##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = VULOAD(WSEW, WEM,y);						\
+    vy = vfwcvt_xu_f_v_u##WEM##_m (mask, vy, vx);	\
+    VUSTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtrtzfi##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+					       int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vint##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = VILOAD(WSEW, WEM,y);						\
+    vy = vfwcvt_rtz_x_f_v_i##WEM##_m (mask, vy, vx);	\
+    VISTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtrtzfui##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##SEW##_TYPE *x,\
+						u##int##WSEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##EM##_t vx;						\
+    vuint##WEM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(SEW, EM, x);						\
+    vy = VULOAD(WSEW, WEM,y);						\
+    vy = vfwcvt_rtz_xu_f_v_u##WEM##_m (mask, vy, vx);	\
+    VUSTORE(WSEW, WEM, y, vy);						\
+  }									\
+  void rvvcvtif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+					       int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vint##EM##_t vy;							\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = VILOAD(SEW, EM, y);						\
+    vx = vfwcvt_f_x_v_f##WEM##_m (mask, vx, vy);	\
+    VFSTORE(WSEW, WEM, x, vx);						\
+  }									\
+  void rvvcvtuif##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						u##int##SEW##_t *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vuint##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = VULOAD(SEW, EM, y);						\
+    vx = vfwcvt_f_xu_v_f##WEM##_m (mask, vx, vy);	\
+    VFSTORE(WSEW, WEM, x, vx);						\
+  }									\
+  void rvvcvtff##SEWfloat##EM##_v_nomask_builtin_test(size_t n, _RVV_F##WSEW##_TYPE *x,\
+						_RVV_F##SEW##_TYPE *y, _RVV_F##SEW##_TYPE z)	\
+  {									\
+    vfloat##WEM##_t vx;						\
+    vfloat##EM##_t vy;						\
+    vbool##MLEN##_t mask;						\
+    mask = MSET (MLEN);					\
+    vx = VFLOAD(WSEW, WEM, x);						\
+    vy = VFLOAD(SEW, EM, y);						\
+    vx = vfwcvt_f_f_v_f##WEM##_m (mask, vx, vy);	\
+    VFSTORE(WSEW, WEM, x, vx);						\
+  }
+
+
+
+RVV_FLOAT_WNCVT_INT_TEST(RVV_WFCVT_TEST)
+
+/* { dg-final { scan-assembler-times "vfwcvt.xu.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.x.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.rtz.xu.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.rtz.x.f.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.f.xu.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.f.x.v" 6 } } */
+/* { dg-final { scan-assembler-times "vfwcvt.f.f.v" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac.c
new file mode 100644
index 00000000000..36e57de302c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac.c
@@ -0,0 +1,52 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void v##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE *op1,             \
+                                                 OP2T##STYPE *op2,             \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP1T##int##EM##_t vop1;                                           \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0;                                          \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vop1 = VLOAD(OP1U, SEW, EM, op1);                                     \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vv_##OP0U##WEM (vop0, vop1, vop2);                   \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void x##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE  op1,             \
+                                                 OP2T##STYPE *op2,             \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0;                                          \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vx_##OP0U##WEM (vop0, op1, vop2);                           \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)      \
+  VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)
+
+RVV_WINT_TEST_ARG(VWMAC,      wmacc, i, i, i,  ,  ,  )
+RVV_WINT_TEST_ARG(VWMAC,     wmaccu, u, u, u, u, u, u)
+RVV_WINT_TEST_ARG(VWMAC,    wmaccsu, i, i, u,  ,  , u)
+RVV_WINT_TEST_ARG(VWMAC_VX, wmaccus, i, u, i,  , u,  )
+
+/* { dg-final { scan-assembler-times "vwmacc.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmacc.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccsu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccsu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccus.vx" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac2.c
new file mode 100644
index 00000000000..0bb6d1a3363
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmac2.c
@@ -0,0 +1,60 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void v##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE *op1,             \
+                                                 OP2T##STYPE *op2,             \
+                                                 OP0T##WSTYPE * mo,            \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP1T##int##EM##_t vop1;                                           \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0, vmo;                                         \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vmo = VLOAD(OP0U, WSEW, WEM, mo);                                      \
+    vop1 = VLOAD(OP1U, SEW, EM, op1);                                     \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vv_##OP0U##WEM##_m (mask, vop0, vop1, vop2); \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  void x##OP##VCLASS##EM##OP0U##_##OP1U##_##OP2U(size_t n,                     \
+                                                 OP1T##STYPE  op1,             \
+                                                 OP2T##STYPE *op2,             \
+						 OP0T##WSTYPE * mo,            \
+                                                 OP0T##WSTYPE * op0) {         \
+    v##OP2T##int##EM##_t vop2;                                           \
+    v##OP0T##int##WEM##_t vop0, vmo;                                          \
+    vbool##MLEN##_t mask;                                                   \
+    mask = MSET (MLEN);                                             \
+    vop0 = VLOAD(OP0U, WSEW, WEM, op0);                                    \
+    vmo = VLOAD(OP0U, WSEW, WEM, mo);                                      \
+    vop2 = VLOAD(OP2U, SEW, EM, op2);                                     \
+    vop0 = v##OP##_vx_##OP0U##WEM##_m (mask, vop0, op1, vop2); \
+    VSTORE(OP0U, WSEW, WEM, op0, vop0);                                     \
+  }
+
+#define VWMAC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)   \
+  VWMAC_VV(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)      \
+  VWMAC_VX(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW, OP, OP0U, OP1U, OP2U, OP0T, OP1T, OP2T)
+
+RVV_WINT_TEST_ARG(VWMAC,      wmacc, i, i, i,  ,  ,  )
+RVV_WINT_TEST_ARG(VWMAC,     wmaccu, u, u, u, u, u, u)
+RVV_WINT_TEST_ARG(VWMAC,    wmaccsu, i, i, u,  ,  , u)
+RVV_WINT_TEST_ARG(VWMAC_VX, wmaccus, i, u, i,  , u,  )
+
+/* { dg-final { scan-assembler-times "vwmacc.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmacc.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccsu.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccsu.vx" 9 } } */
+/* { dg-final { scan-assembler-times "vwmaccus.vx" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmadd_scalar.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmadd_scalar.c
new file mode 100644
index 00000000000..fbe5b328908
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wmadd_scalar.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+
+void wsaxpy(size_t n, const float16_t a, const float16_t *x, float *y) {
+  size_t l;
+
+  vfloat16m4_t vx;
+  vfloat32m8_t vy;
+
+  for (; (l = vsetvl_e16m4(n)) > 0; n -= l) {
+    vx = vle16_v_f16m4(x); /* setvl x0, x0, e16, m4; vlde.v vx, (xx) */
+    x += l;
+    vy = vle32_v_f32m8(y); /* setvl x0, x0, e32, m8; vlde.v vy, (xy) */
+    vy = vfwmacc_vf_f32m8(vy, a, vx);
+    vse32_v_f32m8(y, vy); /* setvl x0, x0, e32, m8; vste.v vy, (xy) */
+    y += l;
+  }
+}
+
+/* { dg-final { scan-assembler-times "vsetvli\tx0,x0,e16,m4" 2} } */
+/* { dg-final { scan-assembler "vle16.v" } } */
+/* { dg-final { scan-assembler "vle32.v" } } */
+/* { dg-final { scan-assembler "vsetvli\tx0,x0,e32,m8"} } */
+/* { dg-final { scan-assembler "vfwmacc.vf" } } */
+/* { dg-final { scan-assembler "vse32.v" } } */
+/* { dg-final { scan-assembler "vsetvli\ta\[0-9\],a\[0-9\],e16,m4"} } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc.c
new file mode 100644
index 00000000000..47ff2682d4f
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc.c
@@ -0,0 +1,47 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), an
+   the e and m value.  */
+#define VWREDUC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEMONE, SEW, WSEW)		\
+  void vreduc##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y) {		\
+    v##VCLASST##WEMONE##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vx = VLOAD(VCLASS, WSEW, WEMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = vwredsum_vs_##VCLASS##EM##_##VCLASS##WEMONE (vx, vy, vx);		\
+    VSTORE (VCLASS, WSEW, WEMONE, x, vx);					\
+  }
+
+#define VWREDUCU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEMONE, SEW, WSEW)		\
+  void vreduc##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y) {		\
+    v##VCLASST##WEMONE##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vx = VLOAD(VCLASS, WSEW, WEMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = vwredsumu_vs_##VCLASS##EM##_##VCLASS##WEMONE (vx, vy, vx);		\
+    VSTORE (VCLASS, WSEW, WEMONE, x, vx);					\
+  }
+
+#define VFWREDUC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEMONE, SEW, WSEW)		\
+  void vreduc##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y) {		\
+    v##VCLASST##WEMONE##_t vx;						\
+    v##VCLASST##EM##_t vy;						\
+    vx = VLOAD(VCLASS, WSEW, WEMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vx = vfwredsum_vs_##VCLASS##EM##_##VCLASS##WEMONE (vx, vy, vx);		\
+    vx = vfwredosum_vs_##VCLASS##EM##_##VCLASS##WEMONE (vx, vy, vx);	\
+    VSTORE (VCLASS, WSEW, WEMONE, x, vx);					\
+  }
+
+RVV_WINT_REDUC_TEST(VWREDUC)
+RVV_WUINT_REDUC_TEST(VWREDUCU)
+RVV_WFLOAT_REDUC_TEST(VFWREDUC)
+
+/* { dg-final { scan-assembler-times "vwredsum.vs" 9 } } */
+/* { dg-final { scan-assembler-times "vwredsumu.vs" 9 } } */
+/* { dg-final { scan-assembler-times "vfwredsum.vs" 6 } } */
+/* { dg-final { scan-assembler-times "vfwredosum.vs" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc2.c
new file mode 100644
index 00000000000..090ac92db26
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wreduc2.c
@@ -0,0 +1,54 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWREDUC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEMONE, SEW, WSEW)		\
+  void vreduc##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##WEMONE##_t vx;						\
+    v##VCLASST##EM##_t vy, vz;						\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD(VCLASS, WSEW, WEMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    mask = vmslt_vv_##VCLASS##EM##_b##MLEN (vy, vz);		\
+    vx = vwredsum_vs_##VCLASS##EM##_##VCLASS##WEMONE##_m (mask, vx, vy, vx);\
+    VSTORE (VCLASS, WSEW, WEMONE, x, vx);					\
+  }
+#define VWREDUCU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEMONE, SEW, WSEW)		\
+  void vreduc##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##WEMONE##_t vx;						\
+    v##VCLASST##EM##_t vy, vz;						\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD(VCLASS, WSEW, WEMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    mask = vmsltu_vv_##VCLASS##EM##_b##MLEN (vy, vz);		\
+    vx = vwredsumu_vs_##VCLASS##EM##_##VCLASS##WEMONE##_m (mask, vx, vy, vx);\
+    VSTORE (VCLASS, WSEW, WEMONE, x, vx);					\
+  }
+#define VFWREDUC(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEMONE, SEW, WSEW)		\
+  void vreduc##VCLASS##EM(size_t n, WSTYPE *x, STYPE *y, STYPE *z) {	\
+    v##VCLASST##WEMONE##_t vx;						\
+    v##VCLASST##EM##_t vy, vz;						\
+    vbool##MLEN##_t mask;						\
+    vx = VLOAD(VCLASS, WSEW, WEMONE, x);					\
+    vy = VLOAD(VCLASS, SEW, EM, y);					\
+    vz = VLOAD(VCLASS, SEW, EM, z);					\
+    mask = MSET (MLEN);					\
+    vx = vfwredsum_vs_##VCLASS##EM##_##VCLASS##WEMONE##_m (mask, vx, vy, vx);	\
+    vx = vfwredosum_vs_##VCLASS##EM##_##VCLASS##WEMONE##_m (mask, vx, vy, vx);	\
+    VSTORE (VCLASS, WSEW, WEMONE, x, vx);					\
+  }
+
+RVV_WINT_REDUC_TEST(VWREDUC)
+RVV_WUINT_REDUC_TEST(VWREDUCU)
+RVV_WFLOAT_REDUC_TEST(VFWREDUC)
+
+/* { dg-final { scan-assembler-times "vwredsum.vs" 9 } } */
+/* { dg-final { scan-assembler-times "vwredsumu.vs" 9 } } */
+/* { dg-final { scan-assembler-times "vfwredsum.vs" 6 } } */
+/* { dg-final { scan-assembler-times "vfwredosum.vs" 6 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_wsub-vv2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wsub-vv2.c
new file mode 100644
index 00000000000..e9f655a3f5a
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_wsub-vv2.c
@@ -0,0 +1,40 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+/* Takes the scalar type STYPE, vector class VCLASS (int or float), and
+   the e and m value.  */
+#define VWSUB(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW)            \
+  void vwadd##VCLASS##EM(size_t n, STYPE *x, STYPE *y, WSTYPE *z) {            \
+    v##VCLASST##EM##_t vx, vy;                                               \
+    v##VCLASST##WEM##_t vz;                                                  \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                               \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                               \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                              \
+    mask = MSET (MLEN);                                             \
+    vz = vwsub_vv_##VCLASS##WEM##_m (mask, vz, vx, vy);                   \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                               \
+  }
+
+#define VWSUBU(STYPE, VCLASST, VCLASS, EM, MLEN, WSTYPE, WEM, STYPEC, SEW, WSEW)           \
+  void vwadd##VCLASS##EM(size_t n, STYPE *x, STYPE *y,                        \
+                          WSTYPE *z) {                                         \
+    v##VCLASST##EM##_t vx, vy;                                              \
+    v##VCLASST##WEM##_t vz;                                                 \
+    vbool##MLEN##_t mask;                                                   \
+    vx = VLOAD(VCLASS, SEW, EM, x);                                              \
+    vy = VLOAD(VCLASS, SEW, EM, y);                                              \
+    vz = VLOAD(VCLASS, WSEW, WEM, z);                                             \
+    mask = MSET (MLEN);                                             \
+    vz = vwsubu_vv_##VCLASS##WEM##_m (mask, vz, vx, vy);                 \
+    VSTORE(VCLASS, WSEW, WEM, z, vz);                                              \
+  }
+
+RVV_WINT_TEST(VWSUB)
+RVV_WUINT_TEST(VWSUBU)
+
+/* { dg-final { scan-assembler-times "vwsub.vv" 9 } } */
+/* { dg-final { scan-assembler-times "vwsubu.vv" 9 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor.c
new file mode 100644
index 00000000000..efc787131c1
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor.c
@@ -0,0 +1,14 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, xor, ^)
+RVV_UINT_TEST_ARG(RVV_BIN_OPERATOR_VEC_SCALAR_IMM_TEST, xor, ^)
+
+
+/* { dg-final { scan-assembler-times "vxor.vv" 64 } } */
+/* combine not work with vec_dup currently.  */
+/* { dg-final { scan-assembler-times "vxor.vx" 0 } } */
+/* { dg-final { scan-assembler-times "vxor.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor2.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor2.c
new file mode 100644
index 00000000000..0fd04953fe3
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor2.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, xor)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, xor)
+
+/* { dg-final { scan-assembler-times "vxor.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vxor.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vxor.vi" 32 } } */
diff --git a/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor3.c b/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor3.c
new file mode 100644
index 00000000000..7fe53c55ca1
--- /dev/null
+++ b/gcc/testsuite/gcc.target/riscv/rvv/rvv_xor3.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+
+#include <riscv_vector.h>
+#include <stddef.h>
+#include "rvv-common.h"
+
+RVV_INT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, xor)
+RVV_UINT_TEST_ARG(RVV_BIN_BUILTIN_VEC_SCALAR_IMM_MASKED_TEST, xor)
+
+
+/* { dg-final { scan-assembler-times "vxor.vv" 32 } } */
+/* { dg-final { scan-assembler-times "vxor.vx" 32 } } */
+/* { dg-final { scan-assembler-times "vxor.vi" 32 } } */
diff --git a/gcc/tree-vect-generic.c b/gcc/tree-vect-generic.c
index 8b00f325054..d0595587a05 100644
--- a/gcc/tree-vect-generic.c
+++ b/gcc/tree-vect-generic.c
@@ -52,6 +52,9 @@ static void expand_vector_operations_1 (gimple_stmt_iterator *);
 static unsigned int
 nunits_for_known_piecewise_op (const_tree type)
 {
+  /* ??? */
+  if (!TYPE_VECTOR_SUBPARTS (type).is_constant ())
+    return 0;
   return TYPE_VECTOR_SUBPARTS (type).to_constant ();
 }
 
@@ -281,6 +284,9 @@ expand_vector_piecewise (gimple_stmt_iterator *gsi, elem_op_func f,
   int i;
   location_t loc = gimple_location (gsi_stmt (*gsi));
 
+  if (nunits == 0)
+    return NULL_TREE;
+
   if (ret_type
       || types_compatible_p (gimple_expr_type (gsi_stmt (*gsi)), type))
     warning_at (loc, OPT_Wvector_operation_performance,
@@ -439,6 +445,10 @@ add_rshift (gimple_stmt_iterator *gsi, tree type, tree op0, int *shiftcnts)
   unsigned int i, nunits = nunits_for_known_piecewise_op (type);
   bool scalar_shift = true;
 
+  /* ??? */
+  if (nunits == 0)
+    return NULL_TREE;
+
   for (i = 1; i < nunits; i++)
     {
       if (shiftcnts[i] != shiftcnts[0])
@@ -495,6 +505,10 @@ expand_vector_divmod (gimple_stmt_iterator *gsi, tree type, tree op0,
   tree cur_op, mulcst, tem;
   optab op;
 
+  /* ??? */
+  if (nunits == 0)
+    return NULL_TREE;
+
   if (prec > HOST_BITS_PER_WIDE_INT)
     return NULL_TREE;
 
@@ -1000,6 +1014,10 @@ expand_vector_condition (gimple_stmt_iterator *gsi)
 
   /* TODO: try and find a smaller vector type.  */
 
+  int nunits = nunits_for_known_piecewise_op (type);
+  if (nunits == 0)
+    return;
+
   warning_at (loc, OPT_Wvector_operation_performance,
 	      "vector condition will be expanded piecewise");
 
@@ -1017,7 +1035,6 @@ expand_vector_condition (gimple_stmt_iterator *gsi)
       a = gimplify_build1 (gsi, VIEW_CONVERT_EXPR, atype, a);
     }
 
-  int nunits = nunits_for_known_piecewise_op (type);
   vec_alloc (v, nunits);
   for (i = 0; i < nunits; i++)
     {
-- 
2.25.1

